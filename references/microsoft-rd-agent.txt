Directory structure:
└── microsoft-rd-agent/
    ├── README.md
    ├── CHANGELOG.md
    ├── CODE_OF_CONDUCT.md
    ├── LICENSE
    ├── Makefile
    ├── SECURITY.md
    ├── SUPPORT.md
    ├── TODO.md
    ├── pyproject.toml
    ├── requirements.txt
    ├── .bumpversion.cfg
    ├── .commitlintrc.js
    ├── .env.example
    ├── .readthedocs.yaml
    ├── constraints/
    │   ├── 3.10.txt
    │   └── 3.11.txt
    ├── docs/
    │   ├── Makefile
    │   ├── api_reference.rst
    │   ├── changelog.md
    │   ├── conf.py
    │   ├── development.rst
    │   ├── index.rst
    │   ├── installation_and_configuration.rst
    │   ├── introduction.rst
    │   ├── make.bat
    │   ├── policy.rst
    │   ├── project_framework_introduction.rst
    │   ├── requirements.txt
    │   ├── ui.rst
    │   ├── _static/
    │   │   └── RD2bench.json
    │   ├── research/
    │   │   ├── benchmark.rst
    │   │   ├── catalog.rst
    │   │   └── dev.rst
    │   └── scens/
    │       ├── catalog.rst
    │       ├── data_agent_fin.rst
    │       ├── data_copilot_fin.rst
    │       ├── kaggle_agent.rst
    │       ├── model_agent_fin.rst
    │       ├── model_agent_med.rst
    │       └── model_copilot_general.rst
    ├── rdagent/
    │   ├── app/
    │   │   ├── cli.py
    │   │   ├── CI/
    │   │   │   ├── README.md
    │   │   │   ├── ci.ipynb
    │   │   │   ├── prompts.yaml
    │   │   │   └── run.py
    │   │   ├── benchmark/
    │   │   │   ├── factor/
    │   │   │   │   ├── analysis.py
    │   │   │   │   └── eval.py
    │   │   │   └── model/
    │   │   │       ├── README.md
    │   │   │       └── eval.py
    │   │   ├── data_mining/
    │   │   │   ├── conf.py
    │   │   │   └── model.py
    │   │   ├── data_science/
    │   │   │   ├── conf.py
    │   │   │   ├── debug.py
    │   │   │   └── loop.py
    │   │   ├── general_model/
    │   │   │   └── general_model.py
    │   │   ├── kaggle/
    │   │   │   ├── conf.py
    │   │   │   └── loop.py
    │   │   ├── qlib_rd_loop/
    │   │   │   ├── conf.py
    │   │   │   ├── factor.py
    │   │   │   ├── factor_from_report.py
    │   │   │   ├── model.py
    │   │   │   └── prompts.yaml
    │   │   └── utils/
    │   │       ├── ape.py
    │   │       ├── health_check.py
    │   │       ├── info.py
    │   │       └── prompts.yaml
    │   ├── components/
    │   │   ├── benchmark/
    │   │   │   ├── conf.py
    │   │   │   ├── eval_method.py
    │   │   │   └── example.json
    │   │   ├── coder/
    │   │   │   ├── CoSTEER/
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── config.py
    │   │   │   │   ├── evaluators.py
    │   │   │   │   ├── evolvable_subjects.py
    │   │   │   │   ├── evolving_strategy.py
    │   │   │   │   ├── knowledge_management.py
    │   │   │   │   ├── prompts.yaml
    │   │   │   │   ├── scheduler.py
    │   │   │   │   └── task.py
    │   │   │   ├── data_science/
    │   │   │   │   ├── conf.py
    │   │   │   │   ├── ensemble/
    │   │   │   │   │   ├── __init__.py
    │   │   │   │   │   ├── conf.py
    │   │   │   │   │   ├── eval.py
    │   │   │   │   │   ├── exp.py
    │   │   │   │   │   ├── prompts.yaml
    │   │   │   │   │   ├── test.py
    │   │   │   │   │   └── eval_tests/
    │   │   │   │   │       └── ensemble_test.txt
    │   │   │   │   ├── feature/
    │   │   │   │   │   ├── __init__.py
    │   │   │   │   │   ├── eval.py
    │   │   │   │   │   ├── exp.py
    │   │   │   │   │   ├── prompts.yaml
    │   │   │   │   │   ├── test.py
    │   │   │   │   │   └── eval_tests/
    │   │   │   │   │       └── feature_test.txt
    │   │   │   │   ├── model/
    │   │   │   │   │   ├── __init__.py
    │   │   │   │   │   ├── eval.py
    │   │   │   │   │   ├── exp.py
    │   │   │   │   │   ├── prompts.yaml
    │   │   │   │   │   ├── test.py
    │   │   │   │   │   └── eval_tests/
    │   │   │   │   │       └── model_test.txt
    │   │   │   │   ├── raw_data_loader/
    │   │   │   │   │   ├── README.md
    │   │   │   │   │   ├── __init__.py
    │   │   │   │   │   ├── conf.py
    │   │   │   │   │   ├── eval.py
    │   │   │   │   │   ├── exp.py
    │   │   │   │   │   ├── prompts.yaml
    │   │   │   │   │   ├── test.py
    │   │   │   │   │   └── eval_tests/
    │   │   │   │   │       └── data_loader_test.txt
    │   │   │   │   └── workflow/
    │   │   │   │       ├── __init__.py
    │   │   │   │       ├── eval.py
    │   │   │   │       ├── exp.py
    │   │   │   │       ├── prompts.yaml
    │   │   │   │       ├── test.py
    │   │   │   │       └── eval_tests/
    │   │   │   │           └── submission_format_test.txt
    │   │   │   ├── factor_coder/
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── config.py
    │   │   │   │   ├── eva_utils.py
    │   │   │   │   ├── evaluators.py
    │   │   │   │   ├── evolving_strategy.py
    │   │   │   │   ├── factor.py
    │   │   │   │   ├── factor_execution_template.txt
    │   │   │   │   └── prompts.yaml
    │   │   │   └── model_coder/
    │   │   │       ├── __init__.py
    │   │   │       ├── eva_utils.py
    │   │   │       ├── evaluators.py
    │   │   │       ├── evolving_strategy.py
    │   │   │       ├── gt_code.py
    │   │   │       ├── model.py
    │   │   │       ├── model_execute_template_v1.txt
    │   │   │       ├── model_execute_template_v2.txt
    │   │   │       ├── prompts.yaml
    │   │   │       ├── task_loader.py
    │   │   │       ├── benchmark/
    │   │   │       │   ├── eval.py
    │   │   │       │   ├── model_dict.json
    │   │   │       │   └── gt_code/
    │   │   │       │       ├── A-DGN.py
    │   │   │       │       ├── dirgnn.py
    │   │   │       │       ├── gpsconv.py
    │   │   │       │       ├── linkx.py
    │   │   │       │       ├── pmlp.py
    │   │   │       │       └── visnet.py
    │   │   │       └── one_shot/
    │   │   │           ├── __init__.py
    │   │   │           └── prompt.yaml
    │   │   ├── document_reader/
    │   │   │   └── document_reader.py
    │   │   ├── knowledge_management/
    │   │   │   ├── graph.py
    │   │   │   └── vector_base.py
    │   │   ├── loader/
    │   │   │   ├── experiment_loader.py
    │   │   │   └── task_loader.py
    │   │   ├── proposal/
    │   │   │   ├── __init__.py
    │   │   │   └── prompts.yaml
    │   │   ├── runner/
    │   │   │   └── __init__.py
    │   │   └── workflow/
    │   │       ├── conf.py
    │   │       └── rd_loop.py
    │   ├── core/
    │   │   ├── conf.py
    │   │   ├── developer.py
    │   │   ├── evaluation.py
    │   │   ├── evolving_agent.py
    │   │   ├── evolving_framework.py
    │   │   ├── exception.py
    │   │   ├── experiment.py
    │   │   ├── knowledge_base.py
    │   │   ├── prompts.py
    │   │   ├── proposal.py
    │   │   ├── scenario.py
    │   │   └── utils.py
    │   ├── log/
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── logger.py
    │   │   ├── mle_summary.py
    │   │   ├── storage.py
    │   │   ├── utils.py
    │   │   └── ui/
    │   │       ├── __init__.py
    │   │       ├── app.py
    │   │       ├── dsapp.py
    │   │       ├── llm_st.py
    │   │       ├── qlib_report_figure.py
    │   │       ├── st_fixed_container.py
    │   │       └── web.py
    │   ├── oai/
    │   │   ├── llm_conf.py
    │   │   ├── llm_utils.py
    │   │   └── backend/
    │   │       ├── __init__.py
    │   │       ├── base.py
    │   │       ├── deprec.py
    │   │       └── litellm.py
    │   ├── scenarios/
    │   │   ├── data_mining/
    │   │   │   ├── developer/
    │   │   │   │   ├── feedback.py
    │   │   │   │   ├── model_coder.py
    │   │   │   │   └── model_runner.py
    │   │   │   ├── docker/
    │   │   │   │   └── Dockerfile
    │   │   │   ├── experiment/
    │   │   │   │   ├── model_experiment.py
    │   │   │   │   ├── prompts.yaml
    │   │   │   │   ├── workspace.py
    │   │   │   │   └── model_template/
    │   │   │   │       ├── README.md
    │   │   │   │       └── train.py
    │   │   │   └── proposal/
    │   │   │       └── model_proposal.py
    │   │   ├── data_science/
    │   │   │   ├── __init__.py
    │   │   │   ├── share.yaml
    │   │   │   ├── debug/
    │   │   │   │   └── data.py
    │   │   │   ├── dev/
    │   │   │   │   ├── coder.py
    │   │   │   │   ├── feedback.py
    │   │   │   │   ├── prompts.yaml
    │   │   │   │   └── runner/
    │   │   │   │       ├── __init__.py
    │   │   │   │       ├── eval.py
    │   │   │   │       ├── prompts.yaml
    │   │   │   │       └── eval_tests/
    │   │   │   │           └── mle_submission_format_test.txt
    │   │   │   ├── experiment/
    │   │   │   │   ├── __init__.py
    │   │   │   │   └── experiment.py
    │   │   │   ├── proposal/
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── exp_gen.py
    │   │   │   │   └── prompts.yaml
    │   │   │   ├── scen/
    │   │   │   │   ├── __init__.py
    │   │   │   │   ├── prompts.yaml
    │   │   │   │   └── runtime_info.py
    │   │   │   └── sing_docker/
    │   │   │       ├── Dockerfile
    │   │   │       ├── entrypoint.sh
    │   │   │       └── kaggle_environment.yaml
    │   │   ├── general_model/
    │   │   │   ├── prompts.yaml
    │   │   │   └── scenario.py
    │   │   ├── kaggle/
    │   │   │   ├── README.md
    │   │   │   ├── kaggle_crawler.py
    │   │   │   ├── prompts.yaml
    │   │   │   ├── developer/
    │   │   │   │   ├── coder.py
    │   │   │   │   ├── feedback.py
    │   │   │   │   └── runner.py
    │   │   │   ├── docker/
    │   │   │   │   ├── kaggle_docker/
    │   │   │   │   │   └── Dockerfile
    │   │   │   │   └── mle_bench_docker/
    │   │   │   │       └── Dockerfile
    │   │   │   ├── experiment/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── kaggle_experiment.py
    │   │   │   │   ├── prompts.yaml
    │   │   │   │   ├── scenario.py
    │   │   │   │   ├── utils.py
    │   │   │   │   ├── workspace.py
    │   │   │   │   ├── spaceship-titanic_template/
    │   │   │   │   │   ├── fea_share_preprocess.py
    │   │   │   │   │   ├── train.py
    │   │   │   │   │   ├── feature/
    │   │   │   │   │   │   └── feature.py
    │   │   │   │   │   └── model/
    │   │   │   │   │       ├── model_randomforest.py
    │   │   │   │   │       ├── model_xgboost.py
    │   │   │   │   │       ├── select_lightgbm.py
    │   │   │   │   │       ├── select_nn.py
    │   │   │   │   │       ├── select_randomforest.py
    │   │   │   │   │       └── select_xgboost.py
    │   │   │   │   └── templates/
    │   │   │   │       ├── covid19-global-forecasting-week-1/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── digit-recognizer/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_nn.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── feedback-prize-english-language-learning/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── forest-cover-type-prediction/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── train_past.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── meta_tpl_deprecated/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_nn.py
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       └── model_xgboost.py
    │   │   │   │       ├── new-york-city-taxi-fare-prediction/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_linear.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_linear.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── optiver-realized-volatility-prediction/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s3e11/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s3e14/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s3e16/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s3e26/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s4e5/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s4e8/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── playground-series-s4e9/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── sf-crime/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── spaceship-titanic/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── statoil-iceberg-classifier-challenge/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── tabular-playground-series-dec-2021/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       ├── tabular-playground-series-may-2022/
    │   │   │   │       │   ├── fea_share_preprocess.py
    │   │   │   │       │   ├── train.py
    │   │   │   │       │   ├── feature/
    │   │   │   │       │   │   └── feature.py
    │   │   │   │       │   └── model/
    │   │   │   │       │       ├── model_randomforest.py
    │   │   │   │       │       ├── model_xgboost.py
    │   │   │   │       │       ├── select_lightgbm.py
    │   │   │   │       │       ├── select_nn.py
    │   │   │   │       │       ├── select_randomforest.py
    │   │   │   │       │       └── select_xgboost.py
    │   │   │   │       └── ventilator-pressure-prediction/
    │   │   │   │           ├── fea_share_preprocess.py
    │   │   │   │           ├── train.py
    │   │   │   │           ├── feature/
    │   │   │   │           │   └── feature.py
    │   │   │   │           └── model/
    │   │   │   │               ├── model_randomforest.py
    │   │   │   │               ├── model_xgboost.py
    │   │   │   │               ├── select_lightgbm.py
    │   │   │   │               ├── select_nn.py
    │   │   │   │               ├── select_randomforest.py
    │   │   │   │               └── select_xgboost.py
    │   │   │   ├── knowledge_management/
    │   │   │   │   ├── README.md
    │   │   │   │   ├── extract_knowledge.py
    │   │   │   │   ├── graph.py
    │   │   │   │   ├── prompts.yaml
    │   │   │   │   └── vector_base.py
    │   │   │   ├── proposal/
    │   │   │   │   └── proposal.py
    │   │   │   └── tpl_ex/
    │   │   │       ├── aerial-cactus-identification/
    │   │   │       │   ├── README.md
    │   │   │       │   ├── ensemble.py
    │   │   │       │   ├── feature.py
    │   │   │       │   ├── load_data.py
    │   │   │       │   ├── main.py
    │   │   │       │   ├── model01.py
    │   │   │       │   └── spec/
    │   │   │       │       ├── data_loader.md
    │   │   │       │       ├── ensemble.md
    │   │   │       │       ├── feature.md
    │   │   │       │       ├── model.md
    │   │   │       │       └── workflow.md
    │   │   │       └── meta/
    │   │   │           └── spec.md
    │   │   └── qlib/
    │   │       ├── prompts.yaml
    │   │       ├── developer/
    │   │       │   ├── factor_coder.py
    │   │       │   ├── factor_runner.py
    │   │       │   ├── feedback.py
    │   │       │   ├── model_coder.py
    │   │       │   └── model_runner.py
    │   │       ├── docker/
    │   │       │   └── Dockerfile
    │   │       ├── experiment/
    │   │       │   ├── factor_experiment.py
    │   │       │   ├── factor_from_report_experiment.py
    │   │       │   ├── model_experiment.py
    │   │       │   ├── prompts.yaml
    │   │       │   ├── utils.py
    │   │       │   ├── workspace.py
    │   │       │   ├── factor_data_template/
    │   │       │   │   ├── README.md
    │   │       │   │   └── generate.py
    │   │       │   ├── factor_template/
    │   │       │   │   ├── conf.yaml
    │   │       │   │   ├── conf_combined.yaml
    │   │       │   │   └── read_exp_res.py
    │   │       │   └── model_template/
    │   │       │       ├── README.md
    │   │       │       ├── conf.yaml
    │   │       │       └── read_exp_res.py
    │   │       ├── factor_experiment_loader/
    │   │       │   ├── json_loader.py
    │   │       │   ├── pdf_loader.py
    │   │       │   └── prompts.yaml
    │   │       └── proposal/
    │   │           ├── factor_proposal.py
    │   │           └── model_proposal.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── env.py
    │       ├── fmt.py
    │       ├── prompts.yaml
    │       ├── workflow.py
    │       ├── agent/
    │       │   ├── __init__.py
    │       │   ├── ret.py
    │       │   ├── tpl.py
    │       │   ├── tpl.yaml
    │       │   └── workflow.py
    │       └── repo/
    │           ├── README.md
    │           ├── diff.py
    │           └── repo_utils.py
    ├── requirements/
    │   ├── docs.txt
    │   ├── lint.txt
    │   ├── package.txt
    │   └── test.txt
    ├── test/
    │   ├── oai/
    │   │   ├── test_advanced.py
    │   │   ├── test_completion.py
    │   │   └── test_embedding_and_similarity.py
    │   └── utils/
    │       ├── README.md
    │       ├── test_agent_infra.py
    │       ├── test_conf.py
    │       ├── test_env.py
    │       ├── test_import.py
    │       ├── test_kaggle.py
    │       ├── test_misc.py
    │       ├── coder/
    │       │   └── test_CoSTEER.py
    │       └── env_tpl/
    │           ├── README.md
    │           ├── conf.yaml
    │           └── read_exp.py
    ├── .github/
    │   ├── FUNDING.yml
    │   ├── PULL_REQUEST_TEMPLATE.md
    │   ├── dependabot.yml
    │   ├── ISSUE_TEMPLATE/
    │   │   ├── bug-report.md
    │   │   ├── documentation.md
    │   │   ├── feature-request.md
    │   │   └── question.md
    │   └── workflows/
    │       ├── ci.yml
    │       ├── pr.yml
    │       ├── readthedocs-preview.yml
    │       └── release.yml
    └── .streamlit/
        └── config.toml

================================================
File: README.md
================================================
<h4 align="center">
  <img src="docs/_static/logo.png" alt="RA-Agent logo" style="width:70%; ">
  
  <a href="https://rdagent.azurewebsites.net" target="_blank">🖥️ Live Demo</a> | <a href="https://rdagent.azurewebsites.net/factor_loop" target="_blank">🎥 Demo Video</a> <a href="https://www.youtube.com/watch?v=JJ4JYO3HscM&list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR" target="_blank">▶️YouTube</a>   | <a href="https://rdagent.readthedocs.io/en/latest/index.html" target="_blank">📖 Documentation</a> | <a href="#-paperwork-list"> 📃 Papers </a>
</h3>


[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)
[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)
[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)
[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)
[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)
[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)
[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)
[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)
[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)
[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)
[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) <!-- this badge is too long, please place it in the last one to make it pretty --> 

# Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:

https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305

# 📰 News
| 🗞️ News        | 📝 Description                 |
| --            | ------      |
| More General Data Science Agent | 🚀Coming soon! |
| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html)**, try the new features!                  |
| Official WeChat group release  | We created a WeChat group, welcome to join! (🗪[QR Code](docs/WeChat_QR_code.jpg)) |
| Official Discord release  | We launch our first chatting channel in Discord (🗪[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |
| First release | **RDAgent** is released on GitHub |


# 🌟 Introduction
<div align="center">
      <img src="docs/_static/scen.png" alt="Our focused scenario" style="width:80%; ">
</div>

RDAgent aims to automate the most critical and valuable aspects of the industrial R&D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. 
Methodologically, we have identified a framework with two key components: 'R' for proposing new ideas and 'D' for implementing them.
We believe that the automatic evolution of R&D will lead to solutions of significant industrial value.


<!-- Tag Cloud -->
R&D is a very general scenario. The advent of RDAgent can be your
- 💰 **Automatic Quant Factory** ([🎥Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[▶️YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&t=6s))
- 🤖 **Data Mining Agent:** Iteratively proposing data & models ([🎥Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[▶️YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&t=104s)) ([🎥Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[▶️YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.
- 🦾 **Research Copilot:** Auto read research papers ([🎥Demo Video](https://rdagent.azurewebsites.net/report_model)|[▶️YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([🎥Demo Video](https://rdagent.azurewebsites.net/report_factor)|[▶️YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.
- 🤖 **Kaggle Agent:** Auto Model Tuning and Feature Engineering([🎥Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.
- ...

You can click the links above to view the demo. We're continuously adding more methods and scenarios to the project to enhance your R&D processes and boost productivity. 

Additionally, you can take a closer look at the examples in our **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)**.

<div align="center">
    <a href="https://rdagent.azurewebsites.net/" target="_blank">
        <img src="docs/_static/demo.png" alt="Watch the demo" width="80%">
    </a>
</div>


# ⚡ Quick start

You can try above demos by running the following command:

### 🐳 Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the [official 🐳Docker page](https://docs.docker.com/engine/install/) for installation instructions.

### 🐍 Create a Conda Environment
- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
  ```sh
  conda create -n rdagent python=3.10
  ```
- Activate the environment:
  ```sh
  conda activate rdagent
  ```

### 🛠️ Install the RDAgent
- You can directly install the RDAgent package from PyPI:
  ```sh
  pip install rdagent
  ```

### 💊 Health check
- rdagent provides a health check that currently checks two things.
  - whether the docker installation was successful.
  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.
  ```sh
  rdagent health_check
  ```


### ⚙️ Configuration
- The demos requires following ability:
  - ChatCompletion
  - json_mode
  - embedding query

- For example: If you are using the `OpenAI API`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat << EOF  > .env
  OPENAI_API_KEY=<replace_with_your_openai_api_key>
  # EMBEDDING_MODEL=text-embedding-3-small
  CHAT_MODEL=gpt-4-turbo
  EOF
  ```
- However, not every API services support these features by default. For example: `AZURE OpenAI`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat << EOF  > .env
  USE_AZURE=True
  EMBEDDING_OPENAI_API_KEY=<replace_with_your_azure_openai_api_key>
  EMBEDDING_AZURE_API_BASE=<replace_with_your_azure_endpoint>
  EMBEDDING_AZURE_API_VERSION=<replace_with_the_version_of_your_azure_openai_api>
  EMBEDDING_MODEL=text-embedding-3-small
  CHAT_OPENAI_API_KEY=<replace_with_your_azure_openai_api_key>
  CHAT_AZURE_API_BASE=<replace_with_your_azure_endpoint>
  CHAT_AZURE_API_VERSION=<replace_with_the_version_of_your_azure_openai_api>
  CHAT_MODEL=<replace_it_with_the_name_of_your_azure_chat_model>
  EOF
  ```
- For more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html).

### 🚀 Run the Application

The **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):

- Run the **Automated Quantitative Trading & Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application
  ```sh
  rdagent fin_factor
  ```

- Run the **Automated Quantitative Trading & Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application
  ```sh
  rdagent fin_model
  ```

- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application
  >(1) Apply for an account at [PhysioNet](https://physionet.org/). <br /> (2) Request access to FIDDLE preprocessed data: [FIDDLE Dataset](https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/). <br />
  (3) Place your username and password in `.env`.
  ```bash
  cat << EOF  >> .env
  DM_USERNAME=<your_username>
  DM_PASSWORD=<your_password>
  EOF
  ```
  ```sh
  rdagent med_model
  ```

- Run the **Automated Quantitative Trading & Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports
  ```sh
  # 1. Generally, you can run this scenario using the following command:
  rdagent fin_factor_report --report_folder=<Your financial reports folder path>

  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
  unzip all_reports.zip -d git_ignore_folder/reports
  rdagent fin_factor_report --report_folder=git_ignore_folder/reports
  ```

- Run the **Automated Model Research & Development Copilot**: model extraction and implementation application
  ```sh
  # 1. Generally, you can run your own papers/reports with the following command:
  rdagent general_model <Your paper URL>

  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
  rdagent general_model  "https://arxiv.org/pdf/2210.09789"
  ```

- Run the **Automated Kaggle Model Tuning & Feature Engineering**:  self-loop model proposal and feature engineering implementation application <br />
  > Using **sf-crime** *(San Francisco Crime Classification)* as an example. <br />
  > 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. <br />
  > 2. Configuring the Kaggle API. <br />
  > (1) Click on the avatar (usually in the top right corner of the page) -> `Settings` -> `Create New Token`, A file called `kaggle.json` will be downloaded. <br />
  > (2) Move `kaggle.json` to `~/.config/kaggle/` <br />
  > (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` <br />
  > 3. Join the competition: Click `Join the competition` -> `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/sf-crime/data).
  ```bash
  # Generally, you can run the Kaggle competition program with the following command:
  rdagent kaggle --competition <your competition name>
  
  # Specifically, you will need to first prepare some competition description files and configure the competition description file path, which you can follow for this specific example:
  
  # 1. Prepare the competition description files
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/kaggle_data/kaggle_data.zip
  unzip kaggle_data.zip -d git_ignore_folder/kaggle_data

  # 2. Add the competition description file path to the `.env` file.
  dotenv set KG_LOCAL_DATA_PATH "$(pwd)/git_ignore_folder/kaggle_data"

  # 3. run the application
  rdagent kaggle --competition sf-crime
  ```
  > **Description of the above example:** <br />
  > - Kaggle competition data, contains two parts: competition description file (json file) and competition dataset (zip file). We prepare the competition description file for you, the competition dataset will be downloaded automatically when you run the program, as in the example. <br />
  > - If you want to download the competition description file automatically, you need to install chromedriver, The instructions for installing chromedriver can be found in the [documentation](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#example-guide). <br />
  > - The **Competition List Available** can be found [here](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#competition-list-available). <br />

### 🖥️ Monitor the Application Results
- You can run the following command for our demo program to see the run logs.

  ```sh
  rdagent ui --port 19899 --log_dir <your log folder like "log/">
  ```

  **Note:** Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.

  You can check if a port is occupied by running the following command.

  ```sh
  rdagent health_check
  ```

# 🏭 Scenarios

We have applied RD-Agent to multiple valuable data-driven industrial scenarios.


## 🎯 Goal: Agent for Data-driven R&D

In this project, we are aiming to build an Agent to automate Data-Driven R\&D that can
+ 📄 Read real-world material (reports, papers, etc.) and **extract** key formulas, descriptions of interested **features** and **models**, which are the key components of data-driven R&D .
+ 🛠️ **Implement** the extracted formulas (e.g., features, factors, and models) in runnable codes.
   + Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.
+ 💡 Propose **new ideas** based on current knowledge and observations.

<!-- ![Data-Centric R&D Overview](docs/_static/overview.png) -->

## 📈 Scenarios/Demos

In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: 🦾Copilot and 🤖Agent. 
- The 🦾Copilot follows human instructions to automate repetitive tasks. 
- The 🤖Agent, being more autonomous, actively proposes ideas for better results in the future.

The supported scenarios are listed below:

| Scenario/Target | Model Implementation                   | Data Building                                                                      |
| --              | --                                     | --                                                                                 |
| **💹 Finance**      | 🤖 [Iteratively Proposing Ideas & Evolving](https://rdagent.azurewebsites.net/model_loop)[▶️YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&t=104s) |  🤖 [Iteratively Proposing Ideas & Evolving](https://rdagent.azurewebsites.net/factor_loop) [▶️YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&t=6s) <br/>   🦾 [Auto reports reading & implementation](https://rdagent.azurewebsites.net/report_factor)[▶️YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)  |
| **🩺 Medical**      | 🤖 [Iteratively Proposing Ideas & Evolving](https://rdagent.azurewebsites.net/dmm)[▶️YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4) | -                                                                                  |
| **🏭 General**      | 🦾 [Auto paper reading & implementation](https://rdagent.azurewebsites.net/report_model)[▶️YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o) <br/> 🤖 Auto Kaggle Model Tuning   | 🤖Auto Kaggle feature Engineering |

- **[RoadMap](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#roadmap)**: Currently, we are working hard to add new features to the Kaggle scenario.

Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.

Here is a gallery of [successful explorations](https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip) (5 traces showed in **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)**). You can download and view the execution trace using [this command](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) from the documentation.

Please refer to **[📖readthedocs_scen](https://rdagent.readthedocs.io/en/latest/scens/catalog.html)** for more details of the scenarios.

# ⚙️ Framework

<div align="center">
    <img src="docs/_static/Framework-RDAgent.png" alt="Framework-RDAgent" width="85%">
</div>


Automating the R&D process in data science is a highly valuable yet underexplored area in industry. We propose a framework to push the boundaries of this important research field.

The research questions within this framework can be divided into three main categories:
| Research Area | Paper/Work List |
|--------------------|-----------------|
| **Benchmark the R&D abilities** | [Benchmark](#benchmark) |
| **Idea proposal:** Explore new ideas or refine existing ones | [Research](#research) |
| **Ability to realize ideas:** Implement and execute ideas | [Development](#development) |

We believe that the key to delivering high-quality solutions lies in the ability to evolve R&D capabilities. Agents should learn like human experts, continuously improving their R&D skills.

More documents can be found in the **[📖 readthedocs](https://rdagent.readthedocs.io/)**.

# 📃 Paper/Work list

## 📊 Benchmark
- [Towards Data-Centric Automatic R&D](https://arxiv.org/abs/2404.11276)
```BibTeX
@misc{chen2024datacentric,
    title={Towards Data-Centric Automatic R&D},
    author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2404.11276},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841)

## 🔍 Research

In a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.

Based on the principles above, we have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real-world practice. This is the first scientific research automation framework that supports linking with real-world verification.

For more detail, please refer to our **[🖥️ Live Demo page](https://rdagent.azurewebsites.net)**.

## 🛠️ Development

- [Collaborative Evolving Strategy for Automatic Data-Centric Development](https://arxiv.org/abs/2407.18690)
```BibTeX
@misc{yang2024collaborative,
    title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
    author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
    year={2024},
    eprint={2407.18690},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b)


# 🤝 Contributing

## 📝 Guidelines
This project welcomes contributions and suggestions.
Contributing to this project is straightforward and rewarding. Whether it's solving an issue, addressing a bug, enhancing documentation, or even correcting a typo, every contribution is valuable and helps improve RDAgent.

To get started, you can explore the issues list, or search for `TODO:` comments in the codebase by running the command `grep -r "TODO:"`.

<img src="https://img.shields.io/github/contributors-anon/microsoft/RD-Agent"/>

<a href="https://github.com/microsoft/RD-Agent/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=microsoft/RD-Agent&max=100&columns=15" />
</a>

Before we released RD-Agent as an open-source project on GitHub, it was an internal project within our group. Unfortunately, the internal commit history was not preserved when we removed some confidential code. As a result, some contributions from our group members, including Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, and Jinhui Li, were not included in the public commits.

# ⚖️ Legal disclaimer
<p style="line-height: 1; font-style: italic;">The RD-agent is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. The RD-agent is aimed to facilitate research and development process in the financial industry and not ready-to-use for any financial investment or advice. Users shall independently assess and test the risks of the RD-agent in a specific use scenario, ensure the responsible use of AI technology, including but not limited to developing and integrating risk mitigation measures, and comply with all applicable laws and regulations in all applicable jurisdictions. The RD-agent does not provide financial opinions or reflect the opinions of Microsoft, nor is it designed to replace the role of qualified financial professionals in formulating, assessing, and approving finance products. The inputs and outputs of the RD-agent belong to the users and users shall assume all liability under any theory of liability, whether in contract, torts, regulatory, negligence, products liability, or otherwise, associated with use of the RD-agent and any inputs and outputs thereof.</p>



================================================
File: CHANGELOG.md
================================================
# Changelog

## [0.3.0](https://github.com/microsoft/RD-Agent/compare/v0.2.1...v0.3.0) (2024-10-21)


### Features

* add a new template for kaggle ([#289](https://github.com/microsoft/RD-Agent/issues/289)) ([eee3ab5](https://github.com/microsoft/RD-Agent/commit/eee3ab5b25198224826cb7a8a17eab28bd5d1f7d))
* add download submission.csv button for kaggle scenario ([#317](https://github.com/microsoft/RD-Agent/issues/317)) ([dcdcbe4](https://github.com/microsoft/RD-Agent/commit/dcdcbe46b4858bfb133ae3cca056e7f602d5cf63))
* add kaggle command ([#271](https://github.com/microsoft/RD-Agent/issues/271)) ([0938394](https://github.com/microsoft/RD-Agent/commit/0938394b7084ffbf3294d8c23d2d34bf7322ca0b))
* add kaggle tpl: feedback-prize ([#331](https://github.com/microsoft/RD-Agent/issues/331)) ([a288e39](https://github.com/microsoft/RD-Agent/commit/a288e399e6b0beec62729bd7d46b98a55de5ab79))
* add more templates for kaggle ([#291](https://github.com/microsoft/RD-Agent/issues/291)) ([da752ec](https://github.com/microsoft/RD-Agent/commit/da752ec806e6f5f5679bc27ac1c072ed9a319251))
* add normal rag into framework ([#360](https://github.com/microsoft/RD-Agent/issues/360)) ([91b0b1f](https://github.com/microsoft/RD-Agent/commit/91b0b1f66c3c1bf757cb64c4cfbdcaafe59eab74))
* add qlib_factor_strategy ([#307](https://github.com/microsoft/RD-Agent/issues/307)) ([f8f59ff](https://github.com/microsoft/RD-Agent/commit/f8f59ff0a1be4428a68c8c27f220aabad0b6c9f0))
* Add ranking in kaggle scenario ([#401](https://github.com/microsoft/RD-Agent/issues/401)) ([b16b4be](https://github.com/microsoft/RD-Agent/commit/b16b4beb402e0c27dfb39ee9d2a120f1b56d447c))
* Add runtime measurement for each step and loop in RDLoop. ([#281](https://github.com/microsoft/RD-Agent/issues/281)) ([83058c8](https://github.com/microsoft/RD-Agent/commit/83058c864ceeec413dd29bf501030d5a7bd34679))
* add s3e11 kaggle template ([#324](https://github.com/microsoft/RD-Agent/issues/324)) ([8c57524](https://github.com/microsoft/RD-Agent/commit/8c57524bead1c8f655a08763d608eb7a6dd5975e))
* Added RepoAnalyzer to empower auto-summary of a workspace ([#264](https://github.com/microsoft/RD-Agent/issues/264)) ([0bd349a](https://github.com/microsoft/RD-Agent/commit/0bd349af50b9b881ba1774bdeb4d723529ef2aa9))
* Added support for loading and storing RAG in Kaggle scenarios. ([#269](https://github.com/microsoft/RD-Agent/issues/269)) ([c4895de](https://github.com/microsoft/RD-Agent/commit/c4895de83f1ed000e563d42b3468a6bd9e5a4965))
* announce Discord and WeChat ([#367](https://github.com/microsoft/RD-Agent/issues/367)) ([acac507](https://github.com/microsoft/RD-Agent/commit/acac5078a103b71afa6bd6c053b0766a6a7e609d))
* auto submit result after one kaggle RDLoop ([#345](https://github.com/microsoft/RD-Agent/issues/345)) ([ab55d70](https://github.com/microsoft/RD-Agent/commit/ab55d7052b53a928b84dc5d5d0d2999d90ca9056))
* better feedback & evaluation ([#346](https://github.com/microsoft/RD-Agent/issues/346)) ([cc9a8c1](https://github.com/microsoft/RD-Agent/commit/cc9a8c1eab3ca89f8c1e5de4a2bb4e7fcc0cc615))
* Dynamic scenario based on task ([#392](https://github.com/microsoft/RD-Agent/issues/392)) ([665a037](https://github.com/microsoft/RD-Agent/commit/665a037e4fd7326c450e3fa0d0605eea26fd9ef3))
* Factor Implement Search Enhancement ([#294](https://github.com/microsoft/RD-Agent/issues/294)) ([4ecf25f](https://github.com/microsoft/RD-Agent/commit/4ecf25f0acf2389a172b14d3dab20895daf2ab89))
* Feature selection v3 to support all actions  ([#280](https://github.com/microsoft/RD-Agent/issues/280)) ([0047641](https://github.com/microsoft/RD-Agent/commit/00476413fbf00e36e71ab3ccb48d4e766b6ccf4d))
* fix some bugs and add original features' description ([#259](https://github.com/microsoft/RD-Agent/issues/259)) ([1a5f45a](https://github.com/microsoft/RD-Agent/commit/1a5f45a40d821c017bdba14af8c93710707c5ea5))
* get kaggle notebooks & disscussion text for RAG ([#371](https://github.com/microsoft/RD-Agent/issues/371)) ([cead345](https://github.com/microsoft/RD-Agent/commit/cead3450a14bf4b142ac988c27fa098c7656a95c))
* Iceberge competition ([#372](https://github.com/microsoft/RD-Agent/issues/372)) ([c10ea4f](https://github.com/microsoft/RD-Agent/commit/c10ea4f5d4cc56a75b47cf23c7084ee189ba1a25))
* implement isolated model feature selection loop ([#370](https://github.com/microsoft/RD-Agent/issues/370)) ([cf1292d](https://github.com/microsoft/RD-Agent/commit/cf1292de1a0153ca14ea64971e73a1c93f7d89e3))
* Initial version if Graph RAG in KAGGLE scenario ([#301](https://github.com/microsoft/RD-Agent/issues/301)) ([fd3c0fd](https://github.com/microsoft/RD-Agent/commit/fd3c0fd26eff7d3be72fa4f2a234e33b9f796627))
* Integrate RAG into the Kaggle scenarios. ([#262](https://github.com/microsoft/RD-Agent/issues/262)) ([be0e48a](https://github.com/microsoft/RD-Agent/commit/be0e48a7dfbee2b5d2947d09115db5db2e5266f1))
* Kaggle loop update (Feature & Model) ([#241](https://github.com/microsoft/RD-Agent/issues/241)) ([4cf22a6](https://github.com/microsoft/RD-Agent/commit/4cf22a65c964123b4267569ee02c0c7094c54ca4))
* kaggle templates related ([#287](https://github.com/microsoft/RD-Agent/issues/287)) ([785fdc1](https://github.com/microsoft/RD-Agent/commit/785fdc144d16fa8454b7c9d2e53e78fe7f22a29a))
* Model context for tuning and selection ([#284](https://github.com/microsoft/RD-Agent/issues/284)) ([f2831e7](https://github.com/microsoft/RD-Agent/commit/f2831e7442510668b0ca75953b3359894803ef3c))
* Modify FactorRowCountEvaluator and FactorIndexEvaluator to return the ratio ([#328](https://github.com/microsoft/RD-Agent/issues/328)) ([8f43f8e](https://github.com/microsoft/RD-Agent/commit/8f43f8e87a92e05b541e925910608606ec8f6c4b))
* New competition - Optiver ([#356](https://github.com/microsoft/RD-Agent/issues/356)) ([3705efe](https://github.com/microsoft/RD-Agent/commit/3705efe3b923748655a57d76b7a236e54d361831))
* random forest for s3e11 ([#347](https://github.com/microsoft/RD-Agent/issues/347)) ([b57846d](https://github.com/microsoft/RD-Agent/commit/b57846d29314e9a5967945d1b4895f0f48c0f5ce))
* refine the code in model description and fix some bugs in feedback.py ([#288](https://github.com/microsoft/RD-Agent/issues/288)) ([5b124d7](https://github.com/microsoft/RD-Agent/commit/5b124d7372137e4c613eb2749ddcc773922cc7b6))
* refine the template in several Kaggle competitions ([#343](https://github.com/microsoft/RD-Agent/issues/343)) ([034f238](https://github.com/microsoft/RD-Agent/commit/034f238ed5ec351486b21250eabc75114961936c))
* Revise to support better hypothesis proposal  ([#390](https://github.com/microsoft/RD-Agent/issues/390)) ([c55ec0a](https://github.com/microsoft/RD-Agent/commit/c55ec0a0f577bbf7fc6228f7b87d2089ded83b31))
* show workspace in demo ([#348](https://github.com/microsoft/RD-Agent/issues/348)) ([ddf567c](https://github.com/microsoft/RD-Agent/commit/ddf567c551b553788be022e9312c209ef6137d64))
* support Multi output ([#330](https://github.com/microsoft/RD-Agent/issues/330)) ([3d36c45](https://github.com/microsoft/RD-Agent/commit/3d36c452ff0983800e5343834cc69f24a508ea70))
* Supporting COVID-19 competition ([#374](https://github.com/microsoft/RD-Agent/issues/374)) ([a1b63db](https://github.com/microsoft/RD-Agent/commit/a1b63db79600edc9a74ba713c9d0be290214a592))
* supporting Mnist competition ([#375](https://github.com/microsoft/RD-Agent/issues/375)) ([e958a34](https://github.com/microsoft/RD-Agent/commit/e958a34f5632a46ac43bff8e0d07d6ed020fdfc2))
* Supporting Model Specifications ([#319](https://github.com/microsoft/RD-Agent/issues/319)) ([e126471](https://github.com/microsoft/RD-Agent/commit/e1264719e10b76158a91cd0ef331848e7c2de7c7))
* supporting various Kaggle competitions & scenarios for RD-Agent ([#409](https://github.com/microsoft/RD-Agent/issues/409)) ([75eea22](https://github.com/microsoft/RD-Agent/commit/75eea22cc3d4e6f5a94c88cce915e27c507f8c50))
* template for kaggle ([#308](https://github.com/microsoft/RD-Agent/issues/308)) ([ff97cf0](https://github.com/microsoft/RD-Agent/commit/ff97cf0155ab6941e4b5cf7d103575f934b70dc9))
* use auto gen seed when using LLM cache ([#441](https://github.com/microsoft/RD-Agent/issues/441)) ([ca15365](https://github.com/microsoft/RD-Agent/commit/ca15365d23eeb094f42cf3dc8f5269b2f1c42bd3))
* use unified pickle cacher & move llm config into a isolated config ([#424](https://github.com/microsoft/RD-Agent/issues/424)) ([2879ecf](https://github.com/microsoft/RD-Agent/commit/2879ecff816d97688b60909a79c7e568d42608a1))
* xgboost gpu accelerate ([#359](https://github.com/microsoft/RD-Agent/issues/359)) ([56a5b8f](https://github.com/microsoft/RD-Agent/commit/56a5b8f9b2c6726cc64ec5b04b4ce7935d59b572))


### Bug Fixes

* a bug of developer& edit s4e8 template ([#338](https://github.com/microsoft/RD-Agent/issues/338)) ([f12ce72](https://github.com/microsoft/RD-Agent/commit/f12ce726e7de96d478a232a3c27f92439820f8b4))
* actively raised errors aer also considered as negative feedback. ([#268](https://github.com/microsoft/RD-Agent/issues/268)) ([46ec908](https://github.com/microsoft/RD-Agent/commit/46ec908e3594ac5e4cdc4057268e2f8800f5ed1f))
* bug of saving preprocess cache files ([#310](https://github.com/microsoft/RD-Agent/issues/310)) ([5fb0608](https://github.com/microsoft/RD-Agent/commit/5fb0608f39f113cc9807fb1f381284a0bd4da318))
* cache ([#383](https://github.com/microsoft/RD-Agent/issues/383)) ([f2a6e75](https://github.com/microsoft/RD-Agent/commit/f2a6e75b36ca96f7733b9c2a7154ac67bd2d7c6f))
* change css tag of kaggle competition info crawler ([#306](https://github.com/microsoft/RD-Agent/issues/306)) ([1e3d38b](https://github.com/microsoft/RD-Agent/commit/1e3d38bf1ca3654f3a90ff392ecba1dbb4e80224))
* debug dsagent ([#387](https://github.com/microsoft/RD-Agent/issues/387)) ([8fe9511](https://github.com/microsoft/RD-Agent/commit/8fe9511e606ba148c66f384add6ab94857079541))
* eval_method cannot catch run factor error ([#260](https://github.com/microsoft/RD-Agent/issues/260)) ([2aaab31](https://github.com/microsoft/RD-Agent/commit/2aaab317ccb7a0121063bcd85fc36c21c7b8a391))
* fix a bug in competition metric evaluation ([#407](https://github.com/microsoft/RD-Agent/issues/407)) ([94c47d6](https://github.com/microsoft/RD-Agent/commit/94c47d6fd5c3e38fc786a83e6d0d05e8d04498f3))
* fix a bug in mini case ([#389](https://github.com/microsoft/RD-Agent/issues/389)) ([e75bb57](https://github.com/microsoft/RD-Agent/commit/e75bb5746f63933b750406bbd34ee63c5ba76b9f))
* fix a bug in model tuning feedback ([#316](https://github.com/microsoft/RD-Agent/issues/316)) ([8aa088d](https://github.com/microsoft/RD-Agent/commit/8aa088da2dc7525a3970c01d01987246f47d6238))
* fix a bug in scenario.py ([#388](https://github.com/microsoft/RD-Agent/issues/388)) ([999a1eb](https://github.com/microsoft/RD-Agent/commit/999a1eb0eff9088e1b02419db741db4acf8d9ff7))
* fix a bug in the format of the model input ([#327](https://github.com/microsoft/RD-Agent/issues/327)) ([8f0574e](https://github.com/microsoft/RD-Agent/commit/8f0574eaaadb245b8c38e09ad4821306996d926f))
* fix a small bug in cache using module name and function name as unique folder name ([#429](https://github.com/microsoft/RD-Agent/issues/429)) ([4f8134a](https://github.com/microsoft/RD-Agent/commit/4f8134a697d952f7ac824d7ebeec64bbc4545ab3))
* fix a typo ([#362](https://github.com/microsoft/RD-Agent/issues/362)) ([9fafabd](https://github.com/microsoft/RD-Agent/commit/9fafabdf321b818bdd2211a2324d50cd0ebe1c1f))
* fix cache result logic ([#430](https://github.com/microsoft/RD-Agent/issues/430)) ([5e34263](https://github.com/microsoft/RD-Agent/commit/5e342637dcc862679fd0642c6ba9ef048c984845))
* fix command injection ([#421](https://github.com/microsoft/RD-Agent/issues/421)) ([52f30a6](https://github.com/microsoft/RD-Agent/commit/52f30a6184af1295be15e855a80b84bc424fc75d))
* fix json load error ([#386](https://github.com/microsoft/RD-Agent/issues/386)) ([bba55fb](https://github.com/microsoft/RD-Agent/commit/bba55fb48fe105f4847c1b9c476eedc80835f523))
* fix some bugs in feedback.py and refine the prompt ([#292](https://github.com/microsoft/RD-Agent/issues/292)) ([d834052](https://github.com/microsoft/RD-Agent/commit/d8340527f133dcc649d599d90d6402eddd37859e))
* fix some bugs in knowledge base ([#378](https://github.com/microsoft/RD-Agent/issues/378)) ([fa6ff8e](https://github.com/microsoft/RD-Agent/commit/fa6ff8e591cf1847df77d73116649c5623161573))
* fix some bugs in rag ([#399](https://github.com/microsoft/RD-Agent/issues/399)) ([194215c](https://github.com/microsoft/RD-Agent/commit/194215c4559aee5b6ece18d65c95fb30968e2db6))
* fix some bugs in the entire loop ([#274](https://github.com/microsoft/RD-Agent/issues/274)) ([8a564ec](https://github.com/microsoft/RD-Agent/commit/8a564ece1d87b27ee98b76db317935e802468965))
* fix some errors in scenario.py, proposal.py and runner.py and several complex competition scenarios([#365](https://github.com/microsoft/RD-Agent/issues/365)) ([2e383b1](https://github.com/microsoft/RD-Agent/commit/2e383b175d8448a67cb470f4e3ae8977d8ec6b5b))
* improve_execution_time_in_kaggle_loop ([#279](https://github.com/microsoft/RD-Agent/issues/279)) ([4c8f998](https://github.com/microsoft/RD-Agent/commit/4c8f998c76f1e983a5687d2c65d3251750f2a9a0))
* kaggle data mount problem ([#297](https://github.com/microsoft/RD-Agent/issues/297)) ([795df31](https://github.com/microsoft/RD-Agent/commit/795df311e3f93cd2f3fb51ba5698adaf10f6bd62))
* Optiver fixes ([#357](https://github.com/microsoft/RD-Agent/issues/357)) ([b054017](https://github.com/microsoft/RD-Agent/commit/b054017463af0d1784407030f2477d212118f341))
* partial bug in bench ([#368](https://github.com/microsoft/RD-Agent/issues/368)) ([af9808f](https://github.com/microsoft/RD-Agent/commit/af9808f98736a2df07e121c2f6d7bfeb7b7d3581))
* preprocess output format & some mistake in spelling ([#358](https://github.com/microsoft/RD-Agent/issues/358)) ([b8b2cd6](https://github.com/microsoft/RD-Agent/commit/b8b2cd6ccd3b27aa73de847e50899a8a53b71b8f))
* rag save file ([#385](https://github.com/microsoft/RD-Agent/issues/385)) ([1cb01dd](https://github.com/microsoft/RD-Agent/commit/1cb01dd6fe595f2f5fb86487601326611dd1a57a))
* raise error in demo when no Metric in a Loop ([#313](https://github.com/microsoft/RD-Agent/issues/313)) ([e46a78e](https://github.com/microsoft/RD-Agent/commit/e46a78eb69271cb19978aab2f3b976c2870ca082))
* refactor Bench ([#302](https://github.com/microsoft/RD-Agent/issues/302)) ([78a87f6](https://github.com/microsoft/RD-Agent/commit/78a87f624780ff67c0fa995ae4692678a120f99c))
* refine some codes ([#353](https://github.com/microsoft/RD-Agent/issues/353)) ([866c2e6](https://github.com/microsoft/RD-Agent/commit/866c2e63ffa3876a3d16ad37f96da41d0558b714))
* refine the prompt ([#286](https://github.com/microsoft/RD-Agent/issues/286)) ([77966c4](https://github.com/microsoft/RD-Agent/commit/77966c4f5e9f492c437c5b4b78d89c0f875ef0d8))
* refine the ucb algorithm ([#406](https://github.com/microsoft/RD-Agent/issues/406)) ([14f7d97](https://github.com/microsoft/RD-Agent/commit/14f7d976e03c92d6e727524e0cdad8a03b585016))
* revert model and make SOTA model available to COSTEER ([#351](https://github.com/microsoft/RD-Agent/issues/351)) ([3b7437b](https://github.com/microsoft/RD-Agent/commit/3b7437b87e685188259779cd85a78a0b592de9de))
* stop using markup in docker env print ([#336](https://github.com/microsoft/RD-Agent/issues/336)) ([3009889](https://github.com/microsoft/RD-Agent/commit/3009889b5e2605b5427c76f3084e0e58026bb5ae))
* support seed and fix absolute path ([#278](https://github.com/microsoft/RD-Agent/issues/278)) ([26352e1](https://github.com/microsoft/RD-Agent/commit/26352e13121cad5be95c0de78bb9f5dda4330614))
* template for kaggle foreset & s4e9 ([#334](https://github.com/microsoft/RD-Agent/issues/334)) ([2393a41](https://github.com/microsoft/RD-Agent/commit/2393a41e7237615ced2c3fdd5c49308236b9f276))
* test kaggle method ([#296](https://github.com/microsoft/RD-Agent/issues/296)) ([91a6196](https://github.com/microsoft/RD-Agent/commit/91a619618be1d7db660ea2b413a78dfaba9417a1))
* update code to fix a small bug in model cache md5 hash ([#303](https://github.com/microsoft/RD-Agent/issues/303)) ([b00e4dc](https://github.com/microsoft/RD-Agent/commit/b00e4dc2eff5b16029a2a12a6589eadac5cfd148))
* update new feature engineering code format ([#272](https://github.com/microsoft/RD-Agent/issues/272)) ([7850b80](https://github.com/microsoft/RD-Agent/commit/7850b8006a7c89d22629b345b4f361b0f35bc60d))
* Update prompts.yaml to constrain only one model type   ([#341](https://github.com/microsoft/RD-Agent/issues/341)) ([5b5dfee](https://github.com/microsoft/RD-Agent/commit/5b5dfeefbc7eb9dcbd9923544005c5d281262c03))
* Update runner.py to fix a small bug ([#282](https://github.com/microsoft/RD-Agent/issues/282)) ([8aef3ab](https://github.com/microsoft/RD-Agent/commit/8aef3abcecd6002bd4bfeedcbe2c786d8bbfe2be))
* Use fixed file name in model costeer & fixing cache ([#311](https://github.com/microsoft/RD-Agent/issues/311)) ([1f910a5](https://github.com/microsoft/RD-Agent/commit/1f910a5248bc576895ed66c2f7b2c3e046a2bc28))


### Performance Improvements

* some small upgrade to factor costeer to improve the performance ([#420](https://github.com/microsoft/RD-Agent/issues/420)) ([9eb931f](https://github.com/microsoft/RD-Agent/commit/9eb931ffd971f252380dbd33ad1db259a4f229fd))


### Reverts

* Revert feat: Factor Implement Search Enhancement ([#294](https://github.com/microsoft/RD-Agent/issues/294)) ([#305](https://github.com/microsoft/RD-Agent/issues/305)) ([f663cf4](https://github.com/microsoft/RD-Agent/commit/f663cf42a2f75cd52aef1c6b18be7c27f0641fed))

## [0.2.1](https://github.com/microsoft/RD-Agent/compare/v0.2.0...v0.2.1) (2024-09-10)


### Bug Fixes

* default model value in config ([#256](https://github.com/microsoft/RD-Agent/issues/256)) ([c097585](https://github.com/microsoft/RD-Agent/commit/c097585f631f401c2c0966f6ad4c17286924f011))
* fix_dotenv_error ([#257](https://github.com/microsoft/RD-Agent/issues/257)) ([923063c](https://github.com/microsoft/RD-Agent/commit/923063c1fd957c4ed42e97272c72b5e9545451dc))
* readme ([#248](https://github.com/microsoft/RD-Agent/issues/248)) ([8cede22](https://github.com/microsoft/RD-Agent/commit/8cede2209922876490148459e1134da828e1fda0))

## [0.2.0](https://github.com/microsoft/RD-Agent/compare/v0.1.0...v0.2.0) (2024-09-07)


### Features

* add collect info ([#233](https://github.com/microsoft/RD-Agent/issues/233)) ([89f4af9](https://github.com/microsoft/RD-Agent/commit/89f4af90fb4d95a0689bf9efc8ffd9326469c0aa))
* add cross validation for kaggle scenario ([#236](https://github.com/microsoft/RD-Agent/issues/236)) ([e0b03ba](https://github.com/microsoft/RD-Agent/commit/e0b03ba6b5c3d9aa552b99d470e106d4e348e64d))
* add progress status for docker env ([#215](https://github.com/microsoft/RD-Agent/issues/215)) ([538d4ef](https://github.com/microsoft/RD-Agent/commit/538d4ef2e52de795b90d3f75b2e1e877ab85c18d))
* Added loop code for Kaggle scene. ([#211](https://github.com/microsoft/RD-Agent/issues/211)) ([975c327](https://github.com/microsoft/RD-Agent/commit/975c32715e51aec6b49537401f5fc59115e04a01))
* Demo display effect and usage ([#162](https://github.com/microsoft/RD-Agent/issues/162)) ([8cf122a](https://github.com/microsoft/RD-Agent/commit/8cf122a0155f434fa4477ae7a6d616b5caecd3e0))
* piloting of the framework ([#227](https://github.com/microsoft/RD-Agent/issues/227)) ([e9b103e](https://github.com/microsoft/RD-Agent/commit/e9b103e684fdd2b98cd1a89971a3fce2d6e884a1))
* support more models for kaggle scenario ([#223](https://github.com/microsoft/RD-Agent/issues/223)) ([e3a9659](https://github.com/microsoft/RD-Agent/commit/e3a96598c0720fe092ec86d7ca8c195c7d6bcc72))
* update model_experiment.py to support basic EDA ([#220](https://github.com/microsoft/RD-Agent/issues/220)) ([bf2684c](https://github.com/microsoft/RD-Agent/commit/bf2684c4d55ab8e1048ac0291695475ad53b0cd6))


### Bug Fixes

* fix some bugs in llm calling ([#217](https://github.com/microsoft/RD-Agent/issues/217)) ([7b010f8](https://github.com/microsoft/RD-Agent/commit/7b010f8b5940aba65a58f1d78192aa80bcd0e654))
* package dependency. ([#234](https://github.com/microsoft/RD-Agent/issues/234)) ([46be295](https://github.com/microsoft/RD-Agent/commit/46be2952952af534fd8d98a656c704c688d7cbdd))
* remove useless line ([#177](https://github.com/microsoft/RD-Agent/issues/177)) ([64e9a8e](https://github.com/microsoft/RD-Agent/commit/64e9a8e39a2072a962111db18f5b9565df5b0176))

## [0.1.0](https://github.com/microsoft/RD-Agent/compare/v0.0.1...v0.1.0) (2024-08-09)


### Features

* add entry for rdagent. ([#187](https://github.com/microsoft/RD-Agent/issues/187)) ([121b6d9](https://github.com/microsoft/RD-Agent/commit/121b6d98de38cd03be30cbee47b40baf39a2b60b))
* change ui entry ([#197](https://github.com/microsoft/RD-Agent/issues/197)) ([fa5d335](https://github.com/microsoft/RD-Agent/commit/fa5d3354d22240888f4fc4007d9834f7424632aa))
* remove pdfs and enable online pdf readings ([#183](https://github.com/microsoft/RD-Agent/issues/183)) ([18c0501](https://github.com/microsoft/RD-Agent/commit/18c05016a23d694c7b12759cf1322562dcffc56a))


### Bug Fixes

* Fix a fail href in readme ([#189](https://github.com/microsoft/RD-Agent/issues/189)) ([1b89218](https://github.com/microsoft/RD-Agent/commit/1b89218f6bc697494f4a1b8a42ad18963002714f))
* fix quick start problem ([#191](https://github.com/microsoft/RD-Agent/issues/191)) ([44f61bf](https://github.com/microsoft/RD-Agent/commit/44f61bfa1058a8efb59ca48b7f1417765aeea33e))
* update command line in readme.md ([#192](https://github.com/microsoft/RD-Agent/issues/192)) ([9c45d24](https://github.com/microsoft/RD-Agent/commit/9c45d24a192da02f7d9765cb001097da1bc36c61))

## 0.0.1 (2024-08-08)


### Features

* Add description for scenario experiments. ([#174](https://github.com/microsoft/RD-Agent/issues/174)) ([fbd8c6d](https://github.com/microsoft/RD-Agent/commit/fbd8c6d87e1424c08997103b8e8fbf264858c4ed))
* Added QlibFactorFromReportScenario and improved the report-factor loop. ([#161](https://github.com/microsoft/RD-Agent/issues/161)) ([882c79b](https://github.com/microsoft/RD-Agent/commit/882c79bf11583980e646b130f71cfa20201ffc7b))
* filter feature which is high correlation to former implemented features ([#145](https://github.com/microsoft/RD-Agent/issues/145)) ([e818326](https://github.com/microsoft/RD-Agent/commit/e818326422740e04a4863f7c3c18744dde2ad98f))
* Remove redundant 'key steps' section in frontend scene display. ([#169](https://github.com/microsoft/RD-Agent/issues/169)) ([e767005](https://github.com/microsoft/RD-Agent/commit/e76700513bee29232c93b97414419df330d9be8d))
* streamlit webapp demo for different scenarios ([#135](https://github.com/microsoft/RD-Agent/issues/135)) ([d8da7db](https://github.com/microsoft/RD-Agent/commit/d8da7db865e6653fc4740efee9a843b69bd79699))
* Uploaded Documentation, Updated Prompts & Some Code for model demo ([#144](https://github.com/microsoft/RD-Agent/issues/144)) ([529f935](https://github.com/microsoft/RD-Agent/commit/529f935aa98623f0dc1dda29eecee3ef738dd446))


### Bug Fixes

* Add framework handling for task coding failure. ([#176](https://github.com/microsoft/RD-Agent/issues/176)) ([5e14fa5](https://github.com/microsoft/RD-Agent/commit/5e14fa54a9dd30a94aebe2643b8c9a3b85517a11))
* Comprehensive update to factor extraction. ([#143](https://github.com/microsoft/RD-Agent/issues/143)) ([b5ea040](https://github.com/microsoft/RD-Agent/commit/b5ea04019fd5fa15c0f8b9a7e4f18f490f7057d4))
* first round app folder cleaning ([#166](https://github.com/microsoft/RD-Agent/issues/166)) ([6a5a750](https://github.com/microsoft/RD-Agent/commit/6a5a75021912927deb5e8e4c7ad3ec4b51bfc788))
* fix pickle problem ([#140](https://github.com/microsoft/RD-Agent/issues/140)) ([7ee4258](https://github.com/microsoft/RD-Agent/commit/7ee42587b60d94417f34332cee395cf210dc8a0e))
* fix release CI ([#165](https://github.com/microsoft/RD-Agent/issues/165)) ([85d6a5e](https://github.com/microsoft/RD-Agent/commit/85d6a5ed91113fda34ae079b23c89aa24acd2cb2))
* fix release CI error ([#160](https://github.com/microsoft/RD-Agent/issues/160)) ([1c9f8ef](https://github.com/microsoft/RD-Agent/commit/1c9f8ef287961731944acc9008496b4dddeddca7))
* fix several bugs in data mining scenario ([#147](https://github.com/microsoft/RD-Agent/issues/147)) ([b233380](https://github.com/microsoft/RD-Agent/commit/b233380e2c66fb030db39424f0f040c86e37f5c4))
* fix some small bugs in report-factor loop ([#152](https://github.com/microsoft/RD-Agent/issues/152)) ([a79f9f9](https://github.com/microsoft/RD-Agent/commit/a79f9f93406aff6305a76e6a6abd3852642e4c62))
* fix_release_ci_error ([#150](https://github.com/microsoft/RD-Agent/issues/150)) ([4f82e99](https://github.com/microsoft/RD-Agent/commit/4f82e9960a2638af9d831581185ddd3bac5711fc))
* Fixed some bugs introduced during refactoring. ([#167](https://github.com/microsoft/RD-Agent/issues/167)) ([f8f1445](https://github.com/microsoft/RD-Agent/commit/f8f1445283fb89aefeb2918243c35a219a51a56c))
* optimize some prompts in factor loop. ([#158](https://github.com/microsoft/RD-Agent/issues/158)) ([c2c1330](https://github.com/microsoft/RD-Agent/commit/c2c13300b9ad315a663ec2d0eada414e56c6f54f))


### Miscellaneous Chores

* release 0.0.1 ([1feacd3](https://github.com/microsoft/RD-Agent/commit/1feacd39b21193de11e9bbecf880ddf96d7c261c))



================================================
File: CODE_OF_CONDUCT.md
================================================
# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns



================================================
File: LICENSE
================================================
    MIT License

    Copyright (c) Microsoft Corporation.

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE



================================================
File: Makefile
================================================
.PHONY: clean deepclean install init-qlib-env dev constraints black isort mypy ruff toml-sort lint pre-commit test-run test build upload docs-autobuild changelog docs-gen docs-mypy docs-coverage docs
#You can modify it according to your terminal
SHELL := /bin/bash

########################################################################################
# Variables
########################################################################################

# Determine whether to invoke pipenv based on CI environment variable and the availability of pipenv.
PIPRUN := $(shell [ "$$CI" != "true" ] && command -v pipenv > /dev/null 2>&1 && echo "pipenv run")

# Get the Python version in `major.minor` format, using the environment variable or the virtual environment if exists.
PYTHON_VERSION := $(shell echo $${PYTHON_VERSION:-$$(python -V 2>&1 | cut -d ' ' -f 2)} | cut -d '.' -f 1,2)

# Determine the constraints file based on the Python version.
CONSTRAINTS_FILE := constraints/$(PYTHON_VERSION).txt

# Documentation target directory, will be adapted to specific folder for readthedocs.
PUBLIC_DIR := $(shell [ "$$READTHEDOCS" = "True" ] && echo "$$READTHEDOCS_OUTPUT/html" || echo "public")

# URL and Path of changelog source code.
CHANGELOG_URL := $(shell echo $${CI_PAGES_URL:-https://microsoft.github.io/rdagent}/_sources/changelog.md.txt)
CHANGELOG_PATH := docs/changelog.md

########################################################################################
# Development Environment Management
########################################################################################

# Remove common intermediate files.
clean:
	-rm -rf \
		$(PUBLIC_DIR) \
		.coverage \
		.mypy_cache \
		.pytest_cache \
		.ruff_cache \
		Pipfile* \
		coverage.xml \
		dist \
		release-notes.md
	find . -name '*.egg-info' -print0 | xargs -0 rm -rf
	find . -name '*.pyc' -print0 | xargs -0 rm -f
	find . -name '*.swp' -print0 | xargs -0 rm -f
	find . -name '.DS_Store' -print0 | xargs -0 rm -f
	find . -name '__pycache__' -print0 | xargs -0 rm -rf

# Remove pre-commit hook, virtual environment alongside itermediate files.
deepclean: clean
	if command -v pre-commit > /dev/null 2>&1; then pre-commit uninstall --hook-type pre-push; fi
	if command -v pipenv >/dev/null 2>&1 && pipenv --venv >/dev/null 2>&1; then pipenv --rm; fi

# Install the package in editable mode.
install:
	$(PIPRUN) pip install -e . -c $(CONSTRAINTS_FILE)

# Install the package in editable mode with specific optional dependencies.
dev-%:
	$(PIPRUN) pip install -e .[$*] -c $(CONSTRAINTS_FILE)

# Prepare the development environment.
# Build submodules.
# Install the pacakge in editable mode with all optional dependencies and pre-commit hook.
init-qlib-env:
	# note: You may need to install torch manually
	# todo: downgrade ruamel.yaml in pyqlib
	conda create -n qlibRDAgent python=3.8 -y
	@source $$(conda info --base)/etc/profile.d/conda.sh && conda activate qlibRDAgent && which pip && pip install pyqlib && pip install ruamel-yaml==0.17.21 && pip install torch==2.1.1 && pip install catboost==0.24.3 && conda deactivate

dev:
	$(PIPRUN) pip install -e .[docs,lint,package,test] -c $(CONSTRAINTS_FILE)
	$(PIPRUN) pip install -U kaggle
	if [ "$(CI)" != "true" ] && command -v pre-commit > /dev/null 2>&1; then pre-commit install --hook-type pre-push; fi

# Generate constraints for current Python version.
constraints: deepclean
	$(PIPRUN) --python $(PYTHON_VERSION) pip install --upgrade -e .[docs,lint,package,test]
	$(PIPRUN) pip freeze --exclude-editable > $(CONSTRAINTS_FILE)

########################################################################################
# Lint and pre-commit
########################################################################################

# Check lint with black.
black:
	$(PIPRUN) python -m black --check --diff . --extend-exclude test/scripts --extend-exclude git_ignore_folder -l 120

# Check lint with isort.
isort:
	$(PIPRUN) python -m isort --check . -s git_ignore_folder -s test/scripts

# Check lint with mypy.
# First deal with the core folder, and then gradually increase the scope of detection,
# and eventually realize the detection of the complete project.
mypy:
	$(PIPRUN) python -m mypy rdagent/core   #  --exclude rdagent/scripts,git_ignore_folder

# Check lint with ruff.
# First deal with the core folder, and then gradually increase the scope of detection,
# and eventually realize the detection of the complete project.
ruff:
	$(PIPRUN) ruff check rdagent/core --ignore FBT001,FBT002,I001   # --exclude rdagent/scripts,git_ignore_folder

# Check lint with toml-sort.
toml-sort:
	$(PIPRUN) toml-sort --check pyproject.toml

# Check lint with all linters.
# Prioritize fixing isort, then black, otherwise you'll get weird and unfixable black errors.
# lint: mypy ruff
lint: mypy ruff isort black toml-sort

# Run pre-commit with autofix against all files.
pre-commit:
	pre-commit run --all-files

########################################################################################
# Auto Lint
########################################################################################

# Auto lint with black.
auto-black:
	$(PIPRUN) python -m black . --extend-exclude test/scripts --extend-exclude git_ignore_folder -l 120

# Auto lint with isort.
auto-isort:
	$(PIPRUN) python -m isort . -s git_ignore_folder -s test/scripts

# Auto lint with toml-sort.
auto-toml-sort:
	$(PIPRUN) toml-sort pyproject.toml

# Auto lint with all linters.
auto-lint: auto-isort auto-black auto-toml-sort

########################################################################################
# Test
########################################################################################

# Clean and run test with coverage.
test-run:
	$(PIPRUN) python -m coverage erase
	$(PIPRUN) python -m coverage run --concurrency=multiprocessing -m pytest --ignore test/scripts
	$(PIPRUN) python -m coverage combine

test-run-offline:
	# some test that does not require api calling
	$(PIPRUN) python -m coverage erase
	$(PIPRUN) python -m coverage run --concurrency=multiprocessing -m pytest -m "offline" --ignore test/scripts
	$(PIPRUN) python -m coverage combine

# Generate coverage report for terminal and xml.
# TODO: we may have higher coverage rate if we have more test
test: test-run
	$(PIPRUN) python -m coverage report --fail-under 20  # 80
	$(PIPRUN) python -m coverage xml --fail-under 20  # 80

test-offline: test-run-offline
	$(PIPRUN) python -m coverage report --fail-under 20  # 80
	$(PIPRUN) python -m coverage xml --fail-under 20  # 80

########################################################################################
# Package
########################################################################################

# Build the package.
build:
	$(PIPRUN) python -m build

# Upload the package.
upload:
	$(PIPRUN) python -m twine upload dist/*

########################################################################################
# Documentation
########################################################################################

# Generate documentation with auto build when changes happen.
docs-autobuild:
	$(PIPRUN) python -m sphinx_autobuild docs $(PUBLIC_DIR) \
		--watch README.md \
		--watch rdagent

# Generate changelog from git commits.
# The -c and -s arguments should match
# If -c uses Basic (default, inherits from base class), -s optional argument: # If -c uses conventional (inherits from base class), -s optional parameter: add,fix,change,remove,merge,doc
# If -c uses conventional (inherits from base class), -s is optional: build,chore,ci,deps,doc,docs,feat,fix,perf,ref,refactor,revert,style,test,tests
# If -c uses angular (inherits from conventional), -s optional argument: build,chore,ci,deps,doc,docs,feat,fix,perf,ref,refactor,revert,style,test,tests
# NOTE(xuan.hu): Need to be run before document generation to take effect.
# $(PIPRUN) git-changelog -ETrio $(CHANGELOG_PATH) -c conventional -s build,chore,ci,docs,feat,fix,perf,refactor,revert,style,test
changelog:
	@if wget -q --spider $(CHANGELOG_URL); then \
		echo "Existing Changelog found at '$(CHANGELOG_URL)', download for incremental generation."; \
		wget -q -O $(CHANGELOG_PATH) $(CHANGELOG_URL); \
	fi
	$(PIPRUN) LATEST_TAG=$$(git tag --sort=-creatordate | head -n 1); \
	git-changelog --bump $$LATEST_TAG -Tio docs/changelog.md -c conventional -s build,chore,ci,deps,doc,docs,feat,fix,perf,ref,refactor,revert,style,test,tests

# Generate release notes from changelog.
release-notes:
	@$(PIPRUN) git-changelog --input $(CHANGELOG_PATH) --release-notes

# Build documentation only from rdagent.
docs-gen:
	$(PIPRUN) python -m sphinx.cmd.build -W docs $(PUBLIC_DIR)

# Generate mypy reports.
docs-mypy: docs-gen
	$(PIPRUN) python -m mypy rdagent test --exclude git_ignore_folder --exclude rdagent/scripts --html-report $(PUBLIC_DIR)/reports/mypy

# Generate html coverage reports with badge.
docs-coverage: test-run docs-gen
	$(PIPRUN) python -m coverage html -d $(PUBLIC_DIR)/reports/coverage --fail-under 80
	$(PIPRUN) bash scripts/generate-coverage-badge.sh $(PUBLIC_DIR)/_static/badges

# Generate all documentation with reports.
docs: changelog docs-gen docs-mypy docs-coverage


########################################################################################
# End
########################################################################################



================================================
File: SECURITY.md
================================================
<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). 

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->



================================================
File: SUPPORT.md
================================================
# TODO: The maintainer of this repo has not yet edited this file

**REPO OWNER**: Do you want Customer Service & Support (CSS) support for this product/project?

- **No CSS support:** Fill out this template with information about how to file issues and get help.
- **Yes CSS support:** Fill out an intake form at [aka.ms/onboardsupport](https://aka.ms/onboardsupport). CSS will work with/help you to determine next steps.
- **Not sure?** Fill out an intake as though the answer were "Yes". CSS will help you decide.

*Then remove this first heading from this SUPPORT.MD file before publishing your repo.*

# Support

## How to file issues and get help  

This project uses GitHub Issues to track bugs and feature requests. Please search the existing 
issues before filing new issues to avoid duplicates.  For new issues, file your bug or 
feature request as a new Issue.

For help and questions about using this project, please **REPO MAINTAINER: INSERT INSTRUCTIONS HERE 
FOR HOW TO ENGAGE REPO OWNERS OR COMMUNITY FOR HELP. COULD BE A STACK OVERFLOW TAG OR OTHER
CHANNEL. WHERE WILL YOU HELP PEOPLE?**.

## Microsoft Support Policy  

Support for this **PROJECT or PRODUCT** is limited to the resources listed above.



================================================
File: TODO.md
================================================
We encourage to set the TODOs in code. But some TODOs are more global.
So we place it here.


- [ ] Aligning the naming of files in components & scenarios.
  - We would like to have the same logic for naming convention in components(reusable components for all scenarios) and scenarios (componets for specific scenario).
  - But now we have following mismatch
    - `coder` in `components` & `developer` in `components`
- [ ] The name of the folders mismatch with the content in them.
  - Why are scenarios in experiments?



================================================
File: pyproject.toml
================================================
[build-system]
build-backend = "setuptools.build_meta"
requires = [
  "setuptools",
  "setuptools-scm",
]

[project]
authors = [
  {email = "xuyang1@microsoft.com", name = "MSRA-MIIC"},
]
classifiers = [
  "Development Status :: 3 - Alpha",
  "License :: OSI Approved :: MIT License",
  "Operating System :: OS Independent",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3.11",
]
description = "Research & Development Agent"
dynamic = [
  "dependencies",
  "optional-dependencies",
  "version",
]
keywords = [
  "Autonomous Agents",
  "Large Language Models",
  "Research and Development",
]
name = "rdagent"
readme = "README.md"
requires-python = ">=3.10"

[project.scripts]
rdagent = "rdagent.app.cli:app"

[project.urls]
homepage = "https://github.com/microsoft/RD-Agent/"
issue = "https://github.com/microsoft/RD-Agent/issues"

[tool.coverage.report]
fail_under = 80

[tool.coverage.run]
source = [
  "rdagent",
]

[tool.isort]
color_output = true
profile = "black"

[tool.mypy]
check_untyped_defs = true
disallow_any_unimported = true
disallow_untyped_defs = true
enable_error_code = [
  "ignore-without-code",
]
explicit_package_bases = true
warn_return_any = true
warn_unused_ignores = true

[[tool.mypy.overrides]]
ignore_missing_imports = true
module = "llama"

[tool.pytest.ini_options]
addopts = "-l -s --durations=0"
log_cli = true
log_cli_level = "info"
log_date_format = "%Y-%m-%d %H:%M:%S"
log_format = "%(asctime)s %(levelname)s %(message)s"
minversion = "6.0"

[tool.ruff]
fix = true
line-length = 120
src = ["rdagent"]

[tool.ruff.lint]
ignore = [
  # https://docs.astral.sh/ruff/rules/#pydocstyle-d
  "ANN401",
  "D",
  "ERA001",
  "EXE002",
  "FIX",
  "INP001",
  "PGH",
  "PLR0913",
  "S101",
  "S301",
  "T20",
  "TCH003",
  "TD",
]
select = ["ALL"]

[tool.ruff.lint.per-file-ignores]
"docs/conf.py" = ["INP001"]
"test/*" = ["S101"]

[tool.setuptools]
packages = ["rdagent"]

[tool.setuptools.dynamic]
dependencies = {file = ["requirements.txt"]}

[tool.setuptools.dynamic.optional-dependencies]
docs = {file = ["requirements/docs.txt"]}
lint = {file = ["requirements/lint.txt"]}
package = {file = ["requirements/package.txt"]}
test = {file = ["requirements/test.txt"]}

[tool.setuptools_scm]
local_scheme = "no-local-version"
version_scheme = "guess-next-dev"

[tool.tomlsort]
all = true
in_place = true
trailing_comma_inline_array = true



================================================
File: requirements.txt
================================================
# Requirements for runtime.
pydantic-settings

python-Levenshtein
scikit-learn
filelock
loguru
fire
fuzzywuzzy
openai
litellm
azure.identity

numpy # we use numpy as default data format. So we have to install numpy
pandas # we use pandas as default data format. So we have to install pandas
pandarallel # parallelize pandas
matplotlib
langchain
langchain-community
tiktoken
pymupdf  # Extract shotsreens from pdf

# PDF related
pypdf
azure-ai-formrecognizer

# factor implementations
tables

# CI Fix Tool
tree-sitter-python
tree-sitter

python-dotenv

# infrastructure related.
docker

# demo related
streamlit
plotly
st-theme

# kaggle crawler
selenium
kaggle
nbformat

# tool
setuptools-scm
seaborn
azure.ai.inference



================================================
File: .bumpversion.cfg
================================================
[bumpversion]
current_version = 0.0.0
commit = True
tag = True

[bumpversion:file:pyproject.toml]



================================================
File: .commitlintrc.js
================================================
module.exports = {
    extends: ["@commitlint/config-conventional"],
    rules: {
        // Configuration Format: [level, applicability, value]
        // level: Error level, usually expressed as a number:
        //     0 - disable rule
        //     1 - Warning (does not prevent commits)
        //     2 - Error (will block the commit)
        // applicability: the conditions under which the rule applies, commonly used values:
        //     “always” - always apply the rule
        //     “never” - never apply the rule
        // value: the specific value of the rule, e.g. a maximum length of 100.
        // Refs: https://commitlint.js.org/reference/rules-configuration.html
      "header-max-length": [2, "always", 100],
      "type-enum": [
        2,
        "always",
        ["build", "chore", "ci", "docs", "feat", "fix", "perf", "refactor", "revert", "style", "test", "Release-As"]
      ]
    }
  };



================================================
File: .env.example
================================================
"""
This file is a template for the .env file.

Please copy this file to .env and fill in the values.

For more information about configuration options, please refer to the documentation

"""

# Global configs:
USE_AZURE=False
CHAT_USE_AZURE_TOKEN_PROVIDER=False
EMBEDDING_USE_AZURE_TOKEN_PROVIDER=False
MAX_RETRY=10
RETRY_WAIT_SECONDS=20

# LLM API Setting:
OPENAI_API_KEY=<your_api_key>
CHAT_MODEL=gpt-4-turbo
CHAT_MAX_TOKENS=3000
CHAT_TEMPERATURE=0.7
# CHAT_AZURE_API_BASE=<for_Azure_user>
# CHAT_AZURE_API_VERSION=<for_Azure_user>

EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_AZURE_API_BASE=<for_Azure_user>
# EMBEDDING_AZURE_API_VERSION=<for_Azure_user>

# Cache Setting (Optional):

# Senario Configs:


================================================
File: .readthedocs.yaml
================================================
# .readthedocs.yml
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required
version: 2

# Set the version of Python and other tools you might need
build:
  os: ubuntu-22.04
  tools:
    python: "3.10"

# Build documentation in the docs/ directory with Sphinx
sphinx:
  configuration: docs/conf.py

# Build all formats
formats: all

# Optionally set the version of Python and requirements required to build your docs
python:
  install:
    - requirements: requirements/docs.txt
    - method: pip
      path: .



================================================
File: constraints/3.10.txt
================================================
azure-identity==1.17.1
dill==0.3.9
pillow==10.4.0
psutil==6.1.0
rich==13.9.2
scipy==1.14.1
tqdm==4.66.5



================================================
File: constraints/3.11.txt
================================================
azure-identity==1.17.1
dill==0.3.9
pillow==10.4.0
psutil==6.1.0
rich==13.9.2
scipy==1.14.1
tqdm==4.66.5



================================================
File: docs/Makefile
================================================
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = .
BUILDDIR      = build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)



================================================
File: docs/api_reference.rst
================================================
=============
API Reference
=============

Here you can find all ``RDAgent``'s interfaces.


RD Loop
=======

Research
--------

.. automodule:: rdagent.core.proposal
    :members:



================================================
File: docs/changelog.md
================================================
# Changelog

## [Unreleased]
<!-- insertion marker -->



================================================
File: docs/conf.py
================================================
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information

import subprocess

latest_tag = subprocess.check_output(["git", "describe", "--tags", "--abbrev=0"], text=True).strip()

project = "RDAgent"
copyright = "2024, Microsoft"
author = "Microsoft"

# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration

extensions = ["sphinx.ext.autodoc", "sphinxcontrib.autodoc_pydantic"]

autodoc_member_order = "bysource"

# The suffix of source filenames.
source_suffix = {".rst": "restructuredtext"}

# The encoding of source files.
source_encoding = "utf-8"

# The main toctree document.
master_doc = "index"

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = latest_tag
release = latest_tag

# The language for content autogenerated by Sphinx. Refer to documentation for
# a list of supported languages.
language = "en"

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ["build"]

# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = "sphinx"

try:
    import furo

    html_theme = "furo"
    html_theme_options = {
        "navigation_with_keys": True,
    }
except ImportError:
    html_theme = "default"

html_logo = "_static/logo.png"
html_static_path = ["_static"]
html_favicon = "_static/favicon.ico"

html_theme_options = {
    "source_repository": "https://github.com/microsoft/RD-Agent",
    "source_branch": "main",
    "source_directory": "docs/",
}



================================================
File: docs/development.rst
================================================
=========================
For Development
=========================

If you want to try the latest version or contribute to RD-Agent. You can install it from the source and follow the commands in this page.

   .. code-block:: bash

      git clone https://github.com/microsoft/RD-Agent


🔧Prepare for development
=========================

- Set up the development environment.

   .. code-block:: bash

      make dev

- Run linting and checking.

   .. code-block:: bash

      make lint


- Some linting issues can be fixed automatically. We have added a command in the Makefile for easy use.

   .. code-block:: bash

      make auto-lint



Code Structure
=========================

.. code-block:: text

    📂 src
    ➥ 📂 <project name>: avoid namespace conflict
      ➥ 📁 core
      ➥ 📁 components/A
      ➥ 📁 components/B
      ➥ 📁 components/C
      ➥ 📁 scenarios/X
      ➥ 📁 scenarios/Y
      ➥ 📂 app
    ➥ 📁 scripts

.. list-table::
   :header-rows: 1

   * - Folder Name
     - Description
   * - 📁 core
     - The core framework of the system. All classes should be abstract and usually can't be used directly.
   * - 📁 component/A
     - Useful components that can be used by others (e.g., scenarios). Many subclasses of core classes are located here.
   * - 📁 scenarios/X
     - Concrete features for specific scenarios (usually built based on components or core). These modules are often unreusable across scenarios.
   * - 📁 app
     - Applications for specific scenarios (usually built based on components or scenarios). Removing any of them does not affect the system's completeness or other scenarios.
   * - 📁 scripts
     - Quick and dirty things. These are candidates for core, components, scenarios, and apps.



Conventions
===========


File Naming Convention
----------------------

.. list-table::
   :header-rows: 1

   * - Name
     - Description
   * - `conf.py`
     - The configuration for the module, app, and project.

.. <!-- TODO: renaming files -->



================================================
File: docs/index.rst
================================================
.. RDAgent documentation master file, created by
   sphinx-quickstart on Mon Jul 15 04:27:50 2024.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to RDAgent's documentation!
===================================

.. image:: _static/logo.png
   :alt: RD-Agent Logo

.. toctree::
   :maxdepth: 3
   :caption: Doctree:

   introduction
   installation_and_configuration
   scens/catalog
   project_framework_introduction
   ui
   research/catalog
   development
   api_reference
   policy

   GitHub <https://github.com/microsoft/RD-Agent>


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`



================================================
File: docs/installation_and_configuration.rst
================================================
==============================
Installation and Configuration
==============================

Installation
============

**Install RDAgent**: For different scenarios

- for purely users: please use ``pip install rdagent`` to install RDAgent
- for dev users: `See development <development.html>`_

**Install Docker**: RDAgent is designed for research and development, acting like a human researcher and developer. It can write and run code in various environments, primarily using Docker for code execution. This keeps the remaining dependencies simple. Users must ensure Docker is installed before attempting most scenarios. Please refer to the `official 🐳Docker page <https://docs.docker.com/engine/install/>`_ for installation instructions.

Configuration
=============

To run the application, please create a `.env` file in the root directory of the project and add environment variables according to your requirements.

The standard configuration options for the user using the OpenAI API are provided in the `.env.example` file.

Here are some other configuration options that you can use:

OpenAI API
------------

Here is a standard configuration for the user using the OpenAI API.

   .. code-block:: Properties

      OPENAI_API_KEY=<your_api_key>
      EMBEDDING_MODEL=text-embedding-3-small
      CHAT_MODEL=gpt-4-turbo

Azure OpenAI
------------

The following environment variables are standard configuration options for the user using the OpenAI API.

   .. code-block:: Properties

      USE_AZURE=True

      EMBEDDING_OPENAI_API_KEY=<replace_with_your_azure_openai_api_key>
      EMBEDDING_AZURE_API_BASE=  # The endpoint for the Azure OpenAI API.
      EMBEDDING_AZURE_API_VERSION=  # The version of the Azure OpenAI API.
      EMBEDDING_MODEL=text-embedding-3-small

      CHAT_OPENAI_API_KEY=<replace_with_your_azure_openai_api_key>
      CHAT_AZURE_API_BASE=  # The endpoint for the Azure OpenAI API.
      CHAT_AZURE_API_VERSION=  # The version of the Azure OpenAI API.
      CHAT_MODEL=  # The model name of the Azure OpenAI API.

Use Azure Token Provider
------------------------

If you are using the Azure token provider, you need to set the `CHAT_USE_AZURE_TOKEN_PROVIDER` and `EMBEDDING_USE_AZURE_TOKEN_PROVIDER` environment variable to `True`. then 
use the environment variables provided in the `Azure Configuration section <installation_and_configuration.html#azure-openai>`_.


☁️ Azure Configuration
- Install Azure CLI:

   ```sh
   curl -L https://aka.ms/InstallAzureCli | bash
   ```

- Log in to Azure:

   ```sh
   az login --use-device-code
   ```

- `exit` and re-login to your environment (this step may not be necessary).


Configuration List
------------------

.. TODO: use `autodoc-pydantic` .

- OpenAI API Setting

+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| Configuration Option              | Meaning                                                         | Default Value           |
+===================================+=================================================================+=========================+
| OPENAI_API_KEY                    | API key for both chat and embedding models                      | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| EMBEDDING_OPENAI_API_KEY          | Use a different API key for embedding model                     | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| CHAT_OPENAI_API_KEY               | Set to use a different API key for chat model                   | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| EMBEDDING_MODEL                   | Name of the embedding model                                     | text-embedding-3-small  |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| CHAT_MODEL                        | Name of the chat model                                          | gpt-4-turbo             |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| EMBEDDING_AZURE_API_BASE          | Base URL for the Azure OpenAI API                               | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| EMBEDDING_AZURE_API_VERSION       | Version of the Azure OpenAI API                                 | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| CHAT_AZURE_API_BASE               | Base URL for the Azure OpenAI API                               | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| CHAT_AZURE_API_VERSION            | Version of the Azure OpenAI API                                 | None                    |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| USE_AZURE                         | True if you are using Azure OpenAI                              | False                   |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| CHAT_USE_AZURE_TOKEN_PROVIDER     | True if you are using an Azure Token Provider in chat model     | False                   |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+
| EMBEDDING_USE_AZURE_TOKEN_PROVIDER| True if you are using an Azure Token Provider in embedding model| False                   |
+-----------------------------------+-----------------------------------------------------------------+-------------------------+

- Globol Setting

+-----------------------------+--------------------------------------------------+-------------------------+
| Configuration Option        | Meaning                                          | Default Value           |
+=============================+==================================================+=========================+
| max_retry                   | Maximum number of times to retry                 | 10                      |
+-----------------------------+--------------------------------------------------+-------------------------+
| retry_wait_seconds          | Number of seconds to wait before retrying        | 1                       |
+-----------------------------+--------------------------------------------------+-------------------------+
+ log_trace_path              | Path to log trace file                           | None                    |
+-----------------------------+--------------------------------------------------+-------------------------+
+ log_llm_chat_content        | Flag to indicate if chat content is logged       | True                    |
+-----------------------------+--------------------------------------------------+-------------------------+


- Cache Setting

.. TODO: update Meaning for caches

+------------------------------+--------------------------------------------------+-------------------------+
| Configuration Option         | Meaning                                          | Default Value           |
+==============================+==================================================+=========================+
| dump_chat_cache              | Flag to indicate if chat cache is dumped         | False                   |
+------------------------------+--------------------------------------------------+-------------------------+
| dump_embedding_cache         | Flag to indicate if embedding cache is dumped    | False                   |
+------------------------------+--------------------------------------------------+-------------------------+
| use_chat_cache               | Flag to indicate if chat cache is used           | False                   |
+------------------------------+--------------------------------------------------+-------------------------+
| use_embedding_cache          | Flag to indicate if embedding cache is used      | False                   |
+------------------------------+--------------------------------------------------+-------------------------+
| prompt_cache_path            | Path to prompt cache                             | ./prompt_cache.db       |
+------------------------------+--------------------------------------------------+-------------------------+
| max_past_message_include     | Maximum number of past messages to include       | 10                      |
+------------------------------+--------------------------------------------------+-------------------------+




Loading Configuration
---------------------

For users' convenience, we provide a CLI interface called `rdagent`, which automatically runs `load_dotenv()` to load environment variables from the `.env` file.
However, this feature is not enabled by default for other scripts. We recommend users load the environment with the following steps:


- ⚙️ Environment Configuration
    - Place the `.env` file in the same directory as the `.env.example` file.
        - The `.env.example` file contains the environment variables required for users using the OpenAI API (Please note that `.env.example` is an example file. `.env` is the one that will be finally used.)

    - Export each variable in the .env file:

      .. code-block:: sh

          export $(grep -v '^#' .env | xargs)
    
    - If you want to change the default environment variables, you can refer to the above configuration and edith the `.env` file.




================================================
File: docs/introduction.rst
================================================
=========================
Introduction
=========================



In modern industry, research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automate these high-value generic R&D processes through our open source R&D automation tool RDAgent, which let AI drive data-driven AI.

.. image:: _static/scen.png
   :alt: Our focused scenario


Our RDAgent is designed to automate the most critical industrial R&D processes, focusing first on data-driven scenarios, to greatly boost the development productivity of models and data. 

Methodologically, we propose an autonomous agent framework that consists of two key parts: (R)esearch stands for actively exploring by proposing new ideas, and (D)evelopment stands for realizing these ideas. The effectiveness of these two components will ultimately get feedbacks through practice, and both research and development capabilities can continuously learn and grow in the process.


For a quick start, visit `our GitHub home page <https://github.com/microsoft/RD-Agent>`_ ⚡. If you've already checked it out and want more details, please keep reading.



================================================
File: docs/make.bat
================================================
@ECHO OFF

pushd %~dp0

REM Command file for Sphinx documentation

if "%SPHINXBUILD%" == "" (
	set SPHINXBUILD=sphinx-build
)
set SOURCEDIR=.
set BUILDDIR=build

%SPHINXBUILD% >NUL 2>NUL
if errorlevel 9009 (
	echo.
	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
	echo.installed, then set the SPHINXBUILD environment variable to point
	echo.to the full path of the 'sphinx-build' executable. Alternatively you
	echo.may add the Sphinx directory to PATH.
	echo.
	echo.If you don't have Sphinx installed, grab it from
	echo.https://www.sphinx-doc.org/
	exit /b 1
)

if "%1" == "" goto help

%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
goto end

:help
%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%

:end
popd



================================================
File: docs/policy.rst
================================================
======
Policy
======

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

Trademarks
==========

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.



================================================
File: docs/project_framework_introduction.rst
================================================
===============================
Framework Design & Components
===============================

Framework & Components
=========================

.. NOTE: This depends on the correctness of `c-v` of github.

.. image:: _static/Framework-RDAgent.png
    :alt: Components & Feature Level

The image above shows the overall framework of RDAgent.

In a data mining expert's daily research and development process, they propose a hypothesis (e.g., a model structure like RNN can capture patterns in time-series data), design experiments (e.g., finance data contains time-series and we can verify the hypothesis in this scenario), implement the experiment as code (e.g., Pytorch model structure), and then execute the code to get feedback (e.g., metrics, loss curve, etc.). The experts learn from the feedback and improve in the next iteration.

We have established a basic method framework that continuously proposes hypotheses, verifies them, and gets feedback from the real world. This is the first scientific research automation framework that supports linking with real-world verification.


.. image:: https://github.com/user-attachments/assets/60cc2712-c32a-4492-a137-8aec59cdc66e
    :alt: Class Level Figure

The figure above shows the main classes and how they fit into the workflow for those interested in the detailed code.


.. Detailed Design
.. ===============



================================================
File: docs/requirements.txt
================================================
sphinx
sphinx_rtd_theme
furo
importlib.metadata


================================================
File: docs/ui.rst
================================================
==============
User Interface
==============


Introduction
============

RD-Agent will generate some logs during the R&D process. These logs are very useful for debugging and understanding the R&D process. However, just viewing the terminal log is not intuitive enough. RD-Agent provides a web app as UI to visualize the R&D process. You can easily view the R&D process and understand the R&D process better.

A Quick Demo
============

Start Web App
-------------

In `RD-Agent/` folder, run:

.. code-block:: bash

    rdagent ui --port <port> --log_dir <log_dir like "log/"> [--debug]

This will start a web app on `http://localhost:<port>`.

**NOTE**: The log_dir parameter is not required. You can manually enter the log_path in the web app. If you set the log_dir parameter, you can easily select a different log_path in the web app.

--debug is optional, it will show a "Single Step Run" button in sidebar and saved objects info in the web app.

Use Web App
-----------

1. Open the sidebar.

.. TODO: update these

2. Select the scenario you want to show. There are some pre-defined scenarios:
    - Qlib Model
    - Qlib Factor
    - Data Mining
    - Model from Paper
    - Kaggle

3. Click the `Config⚙️` button and input the log path (if you set the log_dir parameter, you can select a log_path in the dropdown list).

4. Click the buttons below Config⚙️ to show the scenario execution process. Buttons are:
    - All Loops: Show complete scenario execution process.
    - Next Loop: Show one success **R&D Loop**.
    - One Evolving: Show one **evolving** step of **development** part.
    - refresh logs: clear shown logs.



================================================
File: docs/_static/RD2bench.json
================================================
{
  "alpha053_15": {
    "description": "Reversal class factor, negative delta of a ratio involving close, low, and high prices over 15 days.",
    "formulation": "-1 times Deltaleft(frac{(text{close} - text{low}) - (text{high} - text{close})}{text{close} - text{low}}, 15right)",
    "variables": {
      "Delta(x, d)": "Change in 'x' over 'd' days.",
      "text{close}": "Closing price of the stock.",
      "text{low}": "Lowest price of the stock for the day.",
      "text{high}": "Highest price of the stock for the day."
    },
    "Category": "Volume&Price",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha053\nnew_df['ratio'] =  (new_df['$close'] - new_df['$low'] - (new_df['$high'] - new_df['$close'])) / (new_df['$close'] - new_df['$low'])\n# the change of ratio in new_df over the 15 days\nnew_df['result']=-new_df['ratio'].diff(15)\n# transfer the result to series\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "liquidity_imbalance": {
    "description": "liquidity_imbalance=std(minute trading liquidity_imbalance)/mean(minute trading liquidity_imbalance).",
    "formulation": "liquidity_imbalance = frac{text{std}(text{minute trading liquidity_imbalance})}{text{mean}(text{minute liquidity_imbalance})}",
    "variables": {
      "std(minute liquidity_imbalance)": "Standard deviation of trading liquidity_imbalance for each minute of the trading day.",
      "mean(minute liquidity_imbalance)": "Mean of trading liquidity_imbalance for each minute of the trading day.",
      "liquidity_imbalance": "(bid_size-ask_size)/(bid_size+ask_size), we use something like bidV for the size"
    },
    "Category": "High-Frequency",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['liquidity_imbalance']=(sample_df['bidV']-sample_df['askV'])/(sample_df['bidV']+sample_df['askV'])\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['liquidity_imbalance']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\n# Calculate Z value for each instrument per day\nstats['liquidity_imbalance'] = stats['std'] / stats['mean']\n# Display the calculated Z values\nresult=stats['liquidity_imbalance']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "liquidity_imbalance_2": {
    "description": "liquidity_imbalance=std(minute trading liquidity_imbalance)/mean(minute trading liquidity_imbalance).",
    "formulation": "liquidity_imbalance = frac{text{std}(text{minute trading liquidity_imbalance})}{text{mean}(text{minute liquidity_imbalance})}",
    "variables": {
      "std(minute liquidity_imbalance)": "Standard deviation of trading liquidity_imbalance for each minute of the trading day.",
      "mean(minute liquidity_imbalance)": "Mean of trading liquidity_imbalance for each minute of the trading day.",
      "liquidity_imbalance": "(bid_size-ask_size)/2*(bid_size+ask_size), we use something like bidV for the size"
    },
    "Category": "High-Frequency",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['liquidity_imbalance']=(sample_df['bidV']-sample_df['askV'])/((sample_df['bidV']+sample_df['askV'])*2)\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['liquidity_imbalance']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\n# Calculate Z value for each instrument per day\nstats['liquidity_imbalance'] = stats['std'] / stats['mean']\n# Display the calculated Z values\nresult=stats['liquidity_imbalance']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "liquidity_imbalance_3": {
    "description": "liquidity_imbalance=std(minute trading liquidity_imbalance)/mean(minute trading liquidity_imbalance).",
    "formulation": "liquidity_imbalance = frac{text{std}(text{minute trading liquidity_imbalance})}{text{mean}(text{minute liquidity_imbalance})}",
    "variables": {
      "std(minute liquidity_imbalance)": "Standard deviation of trading liquidity_imbalance for each minute of the trading day.",
      "mean(minute liquidity_imbalance)": "Mean of trading liquidity_imbalance for each minute of the trading day.",
      "liquidity_imbalance": "(bid_size-ask_size)/3*(bid_size+ask_size), we use something like bidV for the size"
    },
    "Category": "High-Frequency",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['liquidity_imbalance']=(sample_df['bidV']-sample_df['askV'])/((sample_df['bidV']+sample_df['askV'])*3)\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['liquidity_imbalance']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\n# Calculate Z value for each instrument per day\nstats['liquidity_imbalance'] = stats['std'] / stats['mean']\n# Display the calculated Z values\nresult=stats['liquidity_imbalance']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "micro_price": {
    "description": "micro_price=std(minute trading micro_price)/mean(minute trading micro_price).",
    "formulation": "micro_price = frac{text{std}(text{minute trading micro_price})}{text{mean}(text{minute micro_price})}",
    "variables": {
      "std(minute micro_price)": "Standard deviation of trading micro_price for each minute of the trading day.",
      "mean(minute micro_price)": "Mean of trading micro_price for each minute of the trading day.",
      "micro_price": "((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])"
    },
    "Category": "High-Frequency",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['micro_price']=(sample_df['bid']*sample_df['askV']+sample_df['ask']*sample_df['bidV'])/(sample_df['bidV']+sample_df['askV'])\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['micro_price']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\n# Calculate Z value for each instrument per day\nstats['micro_price'] = stats['std'] / stats['mean']\n# Display the calculated Z values\nresult=stats['micro_price']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "micro_price_2": {
    "description": "micro_price_2=std(minute trading micro_price)/mean(minute trading micro_price).",
    "formulation": "micro_price_2 = frac{text{std}(text{minute trading micro_price})}{text{mean}(text{minute micro_price})}",
    "variables": {
      "std(minute micro_price)": "Standard deviation of trading micro_price for each minute of the trading day.",
      "mean(minute micro_price)": "Mean of trading micro_price for each minute of the trading day.",
      "micro_price": "((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / 2*(df['bid_size'] + df['ask_size']), we use something like bidV for the size"
    },
    "Category": "High-Frequency",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['micro_price']=(sample_df['bid']*sample_df['askV']+sample_df['ask']*sample_df['bidV'])/((sample_df['bidV']+sample_df['askV'])*2)\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['micro_price']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\n# Calculate Z value for each instrument per day\nstats['micro_price'] = stats['std'] / stats['mean']\n# Display the calculated Z values\nresult=stats['micro_price']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "micro_price_3": {
    "description": "micro_price_3=std(minute trading micro_price)/mean(minute trading micro_price).",
    "formulation": "micro_price_3 = frac{text{std}(text{minute trading micro_price})}{text{mean}(text{minute micro_price})}",
    "variables": {
      "std(minute micro_price)": "Standard deviation of trading micro_price for each minute of the trading day.",
      "mean(minute micro_price)": "Mean of trading micro_price for each minute of the trading day.",
      "micro_price": "((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / 3*(df['bid_size'] + df['ask_size']), we use something like bidV for the size"
    },
    "Category": "High-Frequency",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['micro_price']=(sample_df['bid']*sample_df['askV']+sample_df['ask']*sample_df['bidV'])/((sample_df['bidV']+sample_df['askV'])*3)\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['micro_price']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\n# Calculate Z value for each instrument per day\nstats['micro_price'] = stats['std'] / stats['mean']\n# Display the calculated Z values\nresult=stats['micro_price']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "mid_price": {
    "description": "mid_price=std(minute trading mid_price)/mean(minute trading mid_price).",
    "formulation": "mid_price = frac{text{std}(text{minute trading mid price})}{text{mean}(text{minute mid price})}",
    "variables": {
      "std(minute mid_price)": "Standard deviation of trading mid_price for each minute of the trading day.",
      "mean(minute mid_price)": "Mean of trading mid_price for each minute of the trading day.",
      "mid_price": "The average of the bid and ask prices."
    },
    "Category": "High-Frequency",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['mid_price']=(sample_df['bid']+sample_df['ask'])/2\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['mid_price']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\nstats['mid_price'] = stats['std'] / stats['mean']\nresult=stats['mid_price']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "mid_price_2": {
    "description": "mid_price=std(minute trading mid_price)/mean(minute trading mid_price).",
    "formulation": "mid_price = frac{text{std}(text{minute trading mid price})}{text{mean}(text{minute mid price})}",
    "variables": {
      "std(minute mid_price)": "Standard deviation of trading mid_price for each minute of the trading day.",
      "mean(minute mid_price)": "Mean of trading mid_price for each minute of the trading day.",
      "mid_price_2": "the average of the bid and ask prices plus the the average of the bid and ask size (bidV and askV)."
    },
    "Category": "High-Frequency",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['mid_price']=(sample_df['bid']+sample_df['ask'])/2+(sample_df['bidV']+sample_df['askV'])/2\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['mid_price']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\nstats['mid_price'] = stats['std'] / stats['mean']\nresult=stats['mid_price']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "mid_price_3": {
    "description": "mid_price=std(minute trading mid_price)/mean(minute trading mid_price).",
    "formulation": "mid_price = frac{text{std}(text{minute trading mid price})}{text{mean}(text{minute mid price})}",
    "variables": {
      "std(minute mid_price)": "Standard deviation of trading mid_price for each minute of the trading day.",
      "mean(minute mid_price)": "Mean of trading mid_price for each minute of the trading day.",
      "mid_price_3": "The coefficient of variation (CV) of the mid-price for each minute of the trading day, calculated as the standard deviation of the mid-price divided by the mean mid-price."
    },
    "Category": "High-Frequency",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_hf = pd.read_hdf('high_freq.h5')\nsample_df= data_hf.reset_index()\n# Convert 'datetime' column to datetime and extract date for grouping\nsample_df['date'] = sample_df['datetime'].dt.date\nsample_df['mid_price']=(sample_df['bid']+sample_df['ask'])/3\n# Group by instrument and date\ngrouped = sample_df.groupby(['date','instrument'])['mid_price']\n# Calculate mean and standard deviation of the volume for each group\nstats = grouped.agg(['mean', 'std'])\nstats['mid_price'] = stats['std'] / stats['mean']\nresult=stats['mid_price']\nresult.index.names = ['datetime','instrument']\n# result = result.swaplevel().sort_index()\nresult.to_hdf('result.h5', key='data')"
  },
  "PB_ROE": {
    "description": "Constructed using the ranking difference between PB and ROE, with regression versions of PB and ROE replacing original PB and ROE to obtain reconstructed factor values.",
    "formulation": "text{rank}(PB_t) - rank(ROE_t)",
    "variables": {
      "text{rank}(PB_t)": "Ranking of regression version PB on cross-section at time t.",
      "text{rank}(ROE_t)": "Ranking of regression version single-quarter ROE on cross-section at time t."
    },
    "Category": "Fundamentals",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\ndata = data_f.reset_index()\n# Calculate the rank of PB and ROE\ndata['PB_rank'] = data.groupby('datetime')['B/P'].rank()\ndata['ROE_rank'] = data.groupby('datetime')['ROE'].rank()\n# Calculate the difference between the ranks\ndata['PB_ROE'] = data['PB_rank'] - data['ROE_rank']\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(data['PB_ROE']).set_index(data_f.index)\n# transfer the result to series\nresult=result['PB_ROE']\nresult.to_hdf('result.h5', key='data')"
  },
  "PB_ROE_2": {
    "description": "Constructed using the ranking difference between PB/2 and ROE, with regression versions of PB and ROE replacing original PB and ROE to obtain reconstructed factor values.",
    "formulation": "text{rank}(PB_t)/2 - rank(ROE_t)",
    "variables": {
      "text{rank}(PB_t)": "Ranking of regression version PB on cross-section at time t.",
      "text{rank}(ROE_t)": "Ranking of regression version single-quarter ROE on cross-section at time t."
    },
    "Category": "Fundamentals",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\ndata = data_f.reset_index()\n# Calculate the rank of PB and ROE\ndata['PB_rank'] = data.groupby('datetime')['B/P'].rank()\ndata['ROE_rank'] = data.groupby('datetime')['ROE'].rank()\n# Calculate the difference between the ranks\ndata['PB_ROE'] = data['PB_rank']/2 - data['ROE_rank']\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(data['PB_ROE']).set_index(data_f.index)\n# transfer the result to series\nresult=result['PB_ROE']\nresult.to_hdf('result.h5', key='data')"
  },
  "PB_ROE_3": {
    "description": "Constructed using the ranking difference between PB/3 and ROE, with regression versions of PB and ROE replacing original PB and ROE to obtain reconstructed factor values.",
    "formulation": "text{rank}(PB_t)/3 - rank(ROE_t)",
    "variables": {
      "text{rank}(PB_t)": "Ranking of regression version PB on cross-section at time t.",
      "text{rank}(ROE_t)": "Ranking of regression version single-quarter ROE on cross-section at time t."
    },
    "Category": "Fundamentals",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\ndata = data_f.reset_index()\n# Calculate the rank of PB and ROE\ndata['PB_rank'] = data.groupby('datetime')['B/P'].rank()\ndata['ROE_rank'] = data.groupby('datetime')['ROE'].rank()\n# Calculate the difference between the ranks\ndata['PB_ROE'] = data['PB_rank']/3 - data['ROE_rank']\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(data['PB_ROE']).set_index(data_f.index)\n# transfer the result to series\nresult=result['PB_ROE']\nresult.to_hdf('result.h5', key='data')"
  },
  "PB_ROE_movement": {
    "description": "PB_ROE_movement=five day PB_ROE movement indicator(-1 and 1 or 0).",
    "formulation": "PB_ROE_movement = 5_day_movement(PB_ROE), PB_ROE = text{rank}(PB_t) - rank(ROE_t)",
    "variables": {
      "PB_ROE": "the ranking difference between PB and ROE.",
      "5_day_PB_ROE_movement": "1 if PB_ROE is higher than the PB_ROE 5 days ago, -1 if PB_ROE is lower than the PB_ROE 5 days ago, 0 if PB_ROE is the same as the PB_ROE 5 days ago.",
      "text{rank}(PB_t)": "Ranking of regression version PB on cross-section at time t.",
      "text{rank}(ROE_t)": "Ranking of regression version single-quarter ROE on cross-section at time t."
    },
    "Category": "Fundamentals",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\nsample_df = data_f.reset_index()\n# Calculate the rank of PB and ROE\nsample_df['PB_rank'] = sample_df.groupby('datetime')['B/P'].rank()\nsample_df['ROE_rank'] = sample_df.groupby('datetime')['ROE'].rank()\nsample_df['PB_ROE'] = sample_df['PB_rank'] - sample_df['ROE_rank']\n# Group by instrument and date\nsample_df['PB_ROE_movement'] = sample_df['PB_ROE'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#calculate the mid_price_movement ratio for each day\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(sample_df['PB_ROE_movement']).set_index(data_f.index)\n# transfer the result to series\nresult=result['PB_ROE_movement']\nresult.to_hdf('result.h5', key='data')"
  },
  "PB_ROE_movement_10": {
    "description": "PB_ROE_movement=10 days PB_ROE movement indicator(-1 and 1 or 0).",
    "formulation": "PB_ROE_movement = 10_day_movement(PB_ROE), PB_ROE = text{rank}(PB_t) - rank(ROE_t)",
    "variables": {
      "PB_ROE": "the ranking difference between PB and ROE.",
      "10_day_PB_ROE_movement": "1 if PB_ROE is higher than the PB_ROE 10 days ago, -1 if PB_ROE is lower than the PB_ROE 10 days ago, 0 if PB_ROE is the same as the PB_ROE 10 days ago.",
      "text{rank}(PB_t)": "Ranking of regression version PB on cross-section at time t.",
      "text{rank}(ROE_t)": "Ranking of regression version single-quarter ROE on cross-section at time t."
    },
    "Category": "Fundamentals",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\nsample_df = data_f.reset_index()\n# Calculate the rank of PB and ROE\nsample_df['PB_rank'] = sample_df.groupby('datetime')['B/P'].rank()\nsample_df['ROE_rank'] = sample_df.groupby('datetime')['ROE'].rank()\nsample_df['PB_ROE'] = sample_df['PB_rank'] - sample_df['ROE_rank']\n# Group by instrument and date\nsample_df['PB_ROE_movement'] = sample_df['PB_ROE'].diff(periods=10).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#calculate the mid_price_movement ratio for each day\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(sample_df['PB_ROE_movement']).set_index(data_f.index)\n# transfer the result to series\nresult=result['PB_ROE_movement']\nresult.to_hdf('result.h5', key='data')"
  },
  "PB_ROE_movement_20": {
    "description": "PB_ROE_movement=20 days PB_ROE movement indicator(-1 and 1 or 0).",
    "formulation": "PB_ROE_movement = 20_day_movement(PB_ROE), PB_ROE = text{rank}(PB_t) - rank(ROE_t)",
    "variables": {
      "PB_ROE": "the ranking difference between PB and ROE.",
      "20_day_PB_ROE_movement": "1 if PB_ROE is higher than the PB_ROE 20 days ago, -1 if PB_ROE is lower than the PB_ROE 20 days ago, 0 if PB_ROE is the same as the PB_ROE 20 days ago.",
      "text{rank}(PB_t)": "Ranking of regression version PB on cross-section at time t.",
      "text{rank}(ROE_t)": "Ranking of regression version single-quarter ROE on cross-section at time t."
    },
    "Category": "Fundamentals",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\nsample_df = data_f.reset_index()\n# Calculate the rank of PB and ROE\nsample_df['PB_rank'] = sample_df.groupby('datetime')['B/P'].rank()\nsample_df['ROE_rank'] = sample_df.groupby('datetime')['ROE'].rank()\nsample_df['PB_ROE'] = sample_df['PB_rank'] - sample_df['ROE_rank']\n# Group by instrument and date\nsample_df['PB_ROE_movement'] = sample_df['PB_ROE'].diff(periods=20).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#calculate the mid_price_movement ratio for each day\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(sample_df['PB_ROE_movement']).set_index(data_f.index)\n# transfer the result to series\nresult=result['PB_ROE_movement']\nresult.to_hdf('result.h5', key='data')"
  },
  "ROE_movement": {
    "description": "ROE_movement=five day ROE movement indicator(-1 and 1 or 0).",
    "formulation": "ROE_movement = 5_day_movement(ROE)",
    "variables": {
      "ROE": "ROE in fundamental statistics.",
      "5_day_ROE_movement": "1 if ROE is higher than the ROE 5 days ago, -1 if ROE is lower than the ROE 5 days ago, 0 if ROE is the same as the ROE 5 days ago."
    },
    "Category": "Fundamentals",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\nsample_df = data_f.reset_index()\n# Group by instrument and date\nsample_df['ROE_movement'] = sample_df['ROE'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#calculate the mid_price_movement ratio for each day\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(sample_df['ROE_movement']).set_index(data_f.index)\n# transfer the result to series\nresult=result['ROE_movement']\nresult.to_hdf('result.h5', key='data')"
  },
  "ROE_movement_10": {
    "description": "ROE_movement_10=ten day ROE movement indicator(-1 and 1 or 0).",
    "formulation": "ROE_movement = 10_day_movement(ROE)",
    "variables": {
      "ROE": "ROE in fundamental statistics.",
      "10_day_ROE_movement": "1 if ROE is higher than the ROE 10 days ago, -1 if ROE is lower than the ROE 10 days ago, 0 if ROE is the same as the ROE 10 days ago."
    },
    "Category": "Fundamentals",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\nsample_df = data_f.reset_index()\n# Group by instrument and date\nsample_df['ROE_movement'] = sample_df['ROE'].diff(periods=10).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#calculate the mid_price_movement ratio for each day\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(sample_df['ROE_movement']).set_index(data_f.index)\n# transfer the result to series\nresult=result['ROE_movement']\nresult.to_hdf('result.h5', key='data')"
  },
  "ROE_movement_20": {
    "description": "ROE_movement_20=20 day ROE movement indicator(-1 and 1 or 0).",
    "formulation": "ROE_movement_20 = 20_day_movement(ROE)",
    "variables": {
      "ROE": "ROE in fundamental statistics.",
      "20_day_ROE_movement": "1 if ROE is higher than the ROE 20 days ago, -1 if ROE is lower than the ROE 20 days ago, 0 if ROE is the same as the ROE 20 days ago."
    },
    "Category": "Fundamentals",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_f = pd.read_hdf('daily_f.h5')\nsample_df = data_f.reset_index()\n# Group by instrument and date\nsample_df['ROE_movement'] = sample_df['ROE'].diff(periods=20).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n#calculate the mid_price_movement ratio for each day\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(sample_df['ROE_movement']).set_index(data_f.index)\n# transfer the result to series\nresult=result['ROE_movement']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha_pv_diff": {
    "description": "alpha_pv_diff is defined as the ratio of the difference between close prices 10 days change and open prices 10 days change to the sum of the highest minus lowest prices plus a small constant.",
    "formulation": "frac{(text{close_diff10} - text{open_diff10})}{(text{high} - text{low} + 0.001)}",
    "variables": {
      "close": "Closing price of the stock",
      "open": "Opening price of the stock",
      "high": "Highest price of the stock during the day",
      "low": "Lowest price of the stock during the day"
    },
    "Category": "Volume&Price",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha101\nnew_df['result'] = (new_df['$close'].diff(10) - new_df['$open'].diff(10)) / (new_df['$high'] - new_df['$low'] + 0.001)\n# keep the index of the original dataframe\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\n# transfer the result to series\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha_pv_diff_15": {
    "description": "alpha_pv_diff is defined as the ratio of the difference between close prices 15 days change and open prices 15 days change to the sum of the highest minus lowest prices plus a small constant.",
    "formulation": "frac{(text{close_diff15} - text{open_diff15})}{(text{high} - text{low} + 0.001)}",
    "variables": {
      "close": "Closing price of the stock",
      "open": "Opening price of the stock",
      "high": "Highest price of the stock during the day",
      "low": "Lowest price of the stock during the day"
    },
    "Category": "Volume&Price",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha101\nnew_df['result'] = (new_df['$close'].diff(15) - new_df['$open'].diff(15)) / (new_df['$high'] - new_df['$low'] + 0.001)\n# keep the index of the original dataframe\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\n# transfer the result to series\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha_pv_diff_20": {
    "description": "alpha_pv_diff is defined as the ratio of the difference between close prices 20 days change and open prices 20 days change to the sum of the highest minus lowest prices plus a small constant.",
    "formulation": "frac{(text{close_diff20} - text{open_diff20})}{(text{high} - text{low} + 0.001)}",
    "variables": {
      "close": "Closing price of the stock",
      "open": "Opening price of the stock",
      "high": "Highest price of the stock during the day",
      "low": "Lowest price of the stock during the day"
    },
    "Category": "Volume&Price",
    "Difficulty": "Medium",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha101\nnew_df['result'] = (new_df['$close'].diff(20) - new_df['$open'].diff(20)) / (new_df['$high'] - new_df['$low'] + 0.001)\n# keep the index of the original dataframe\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\n# transfer the result to series\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha_pv_diff_pct": {
    "description": "alpha_pv is defined as the ratio of the difference between close prices 10 days change and open prices 10 days change to the sum of the highest prices 10 days change ratio minus lowest prices 10 days change ratio plus a small constant.",
    "formulation": "frac{(text{close_diff10} - text{open_diff10})}{(text{high_pct10} - text{low_pct10} + 0.001)}",
    "variables": {
      "close": "Closing price of the stock",
      "open": "Opening price of the stock",
      "high": "Highest price of the stock during the day",
      "low": "Lowest price of the stock during the day"
    },
    "Category": "Volume&Price",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha101\nnew_df['result'] = (new_df['$close'].diff(10) - new_df['$open'].diff(10)) / (new_df['$high'].pct_change(10) - new_df['$low'].pct_change(10) + 0.001)\n# keep the index of the original dataframe\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\n# transfer the result to series\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha_pv_diff_pct_15": {
    "description": "alpha_pv is defined as the ratio of the difference between close prices 15 days change and open prices 15 days change to the sum of the highest prices 10 days change ratio minus lowest prices 10 days change ratio plus a small constant.",
    "formulation": "frac{(text{close_diff15} - text{open_diff15})}{(text{high_pct10} - text{low_pct10} + 0.001)}",
    "variables": {
      "close": "Closing price of the stock",
      "open": "Opening price of the stock",
      "high": "Highest price of the stock during the day",
      "low": "Lowest price of the stock during the day"
    },
    "Category": "Volume&Price",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha101\nnew_df['result'] = (new_df['$close'].diff(15) - new_df['$open'].diff(15)) / (new_df['$high'].pct_change(10) - new_df['$low'].pct_change(10) + 0.001)\n# keep the index of the original dataframe\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\n# transfer the result to series\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha_pv_diff_pct_20": {
    "description": "alpha_pv is defined as the ratio of the difference between close prices 20 days change and open prices 20 days change to the sum of the highest prices 10 days change ratio minus lowest prices 10 days change ratio plus a small constant.",
    "formulation": "frac{(text{close_diff20} - text{open_diff20})}{(text{high_pct10} - text{low_pct10} + 0.001)}",
    "variables": {
      "close": "Closing price of the stock",
      "open": "Opening price of the stock",
      "high": "Highest price of the stock during the day",
      "low": "Lowest price of the stock during the day"
    },
    "Category": "Volume&Price",
    "Difficulty": "Hard",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha101\nnew_df['result'] = (new_df['$close'].diff(20) - new_df['$open'].diff(20)) / (new_df['$high'].pct_change(10) - new_df['$low'].pct_change(10) + 0.001)\n# keep the index of the original dataframe\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\n# transfer the result to series\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha053": {
    "description": "Reversal class factor, negative delta of a ratio involving close, low, and high prices over 9 days.",
    "formulation": "-1 times Deltaleft(frac{(text{close} - text{low}) - (text{high} - text{close})}{text{close} - text{low}}, 9right)",
    "variables": {
      "Delta(x, d)": "Change in 'x' over 'd' days.",
      "text{close}": "Closing price of the stock.",
      "text{low}": "Lowest price of the stock for the day.",
      "text{high}": "Highest price of the stock for the day."
    },
    "Category": "Volume&Price",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha053\nnew_df['ratio'] =  (new_df['$close'] - new_df['$low'] - (new_df['$high'] - new_df['$close'])) / (new_df['$close'] - new_df['$low'])\n# the change of ratio in new_df over the 9 days\nnew_df['result']=-new_df['ratio'].diff(9)\n# transfer the result to series\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  },
  "alpha053_5": {
    "description": "Reversal class factor, negative delta of a ratio involving close, low, and high prices over 5 days.",
    "formulation": "-1 times Deltaleft(frac{(text{close} - text{low}) - (text{high} - text{close})}{text{close} - text{low}}, 5right)",
    "variables": {
      "Delta(x, d)": "Change in 'x' over 'd' days.",
      "text{close}": "Closing price of the stock.",
      "text{low}": "Lowest price of the stock for the day.",
      "text{high}": "Highest price of the stock for the day."
    },
    "Category": "Volume&Price",
    "Difficulty": "Easy",
    "gt_code": "import pandas as pd\ndata_pv = pd.read_hdf('daily_pv.h5')\nnew_df= data_pv.reset_index()\n# Calculate Alpha053\nnew_df['ratio'] =  (new_df['$close'] - new_df['$low'] - (new_df['$high'] - new_df['$close'])) / (new_df['$close'] - new_df['$low'])\n# the change of ratio in new_df over the 5 days\nnew_df['result']=-new_df['ratio'].diff(5)\n# transfer the result to series\nresult=pd.DataFrame(new_df['result']).set_index(data_pv.index)\nresult=result['result']\nresult.to_hdf('result.h5', key='data')"
  }
}



================================================
File: docs/research/benchmark.rst
================================================
==============================
Benchmark
==============================

Introduction
=============

Benchmarking the capabilities of R&D is a crucial research problem in this area. We are continuously exploring methods to benchmark these capabilities. The current benchmarks are listed on this page.

Development Capability Benchmarking
===================================

Benchmarking is used to evaluate the effectiveness of factors with fixed data. It mainly includes the following steps:

1. :ref:`read and prepare the eval_data <data>`

2. :ref:`declare the method to be tested and pass the arguments <config>`

3. :ref:`declare the eval method and pass the arguments <config>`

4. :ref:`run the eval <run>`

5. :ref:`save and show the result <show>`

Configuration
-------------
.. _config:

.. autopydantic_settings:: rdagent.components.benchmark.conf.BenchmarkSettings

Example
+++++++
.. _example:

The default value for ``bench_test_round`` is 10, which takes about 2 hours to run. To modify it from ``10`` to ``2``, adjust the environment variables in the .env file as shown below.

.. code-block:: Properties

      BENCHMARK_BENCH_TEST_ROUND=2

Data Format
-------------
.. _data:

The sample data in ``bench_data_path`` is a dictionary where each key represents a factor name. The value associated with each key is factor data containing the following information:

- **description**: A textual description of the factor.
- **formulation**: A LaTeX formula representing the model's formulation.
- **variables**: A dictionary of variables involved in the factor.
- **Category**: The category or classification of the factor.
- **Difficulty**: The difficulty level of implementing or understanding the factor.
- **gt_code**: A piece of code associated with the factor.

Here is an example of this data format:

.. literalinclude:: ../../rdagent/components/benchmark/example.json
   :language: json

Ensure the data is placed in the ``FACTOR_COSTEER_SETTINGS.data_folder_debug``. The data files should be in ``.h5`` or ``.md`` format and must not be stored in any subfolders. LLM-Agents will review the file content and implement the tasks.

.. TODO: Add a script to automatically generate the data in the `rdagent/app/quant_factor_benchmark/data` folder.

Run Benchmark
-------------
.. _run:

Start the benchmark after completing the :doc:`../installation_and_configuration`.

.. code-block:: Properties

      dotenv run -- python rdagent/app/benchmark/factor/eval.py

Once completed, a pkl file will be generated, and its path will be printed on the last line of the console.

Show Result
-------------
.. _show:

The ``analysis.py`` script reads data from the pkl file and converts it to an image. Modify the Python code in ``rdagent/app/quant_factor_benchmark/analysis.py`` to specify the path to the pkl file and the output path for the png file.

.. code-block:: Properties

      dotenv run -- python rdagent/app/benchmark/factor/analysis.py <log/path to.pkl>

A png file will be saved to the designated path as shown below.

.. image:: ../_static/benchmark.png

Related Paper
-------------

- `Towards Data-Centric Automatic R&D <https://arxiv.org/abs/2404.11276>`_:
  We have developed a comprehensive benchmark called RD2Bench to assess data and model R&D capabilities. This benchmark includes a series of tasks that outline the features or structures of models. These tasks are used to evaluate the ability of LLM-Agents to implement them.

.. code-block:: bibtex

    @misc{chen2024datacentric,
        title={Towards Data-Centric Automatic R&D},
        author={Haotian Chen and Xinjie Shen and Zeqi Ye and Wenjun Feng and Haoxue Wang and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
        year={2024},
        eprint={2404.11276},
        archivePrefix={arXiv},
        primaryClass={cs.AI}
    }

.. image:: https://github.com/user-attachments/assets/494f55d3-de9e-4e73-ba3d-a787e8f9e841

To replicate the benchmark detailed in the paper, please consult the factors listed in the following file: `RD2bench.json <../_static/RD2bench.json>`_.
Please note use ``only_correct_format=False`` when evaluating the results.



================================================
File: docs/research/catalog.rst
================================================
===========
Research
===========

To achieve the good effects and improve R&D capabilities, we face multiple challenges, the most important of which is the continuous evolution capability. Existing large language models (LLMs) find it difficult to continue growing their capabilities after training is completed. Moreover, the training process of LLMs focuses more on general knowledge, and the lack of depth in more specialized knowledge becomes an obstacle to solving professional R&D problems within the industry. This specialized knowledge needs to be learned and acquired from in-depth industry practice.


Our RD-Agent, on the other hand, can continuously acquire in-depth domain knowledge through deep exploration during the R&D phase, allowing its R&D capabilities to keep growing.

To address these key challenges and achieve industrial value, a series of research work needs to be completed.


.. list-table:: Research Areas and Descriptions
   :header-rows: 1

   * - Research Area
     - Description
   * - :doc:`Benchmark <benchmark>`
     - Benchmark the R&D abilities
   * - Research
     - Idea proposal: Explore new ideas or refine existing ones
   * - :doc:`Development <dev>`
     - Ability to realize ideas: Implement and execute ideas




.. toctree::
   :maxdepth: 1
   :caption: Doctree:
   :hidden:

   benchmark
   dev



================================================
File: docs/research/dev.rst
================================================
==============================
Development
==============================


Related Paper
-------------

- `Collaborative Evolving Strategy for Automatic Data-Centric Development <https://arxiv.org/abs/2407.18690>`_
  Co-STEER is a method to tackle data-centric development (AD2) tasks and highlight its main challenges, which need expert-like implementation (i.e., learning domain knowledge from practice) and task scheduling capability (e.g., starting with easier tasks for better overall efficiency), areas that previous work has largely overlooked. Our Co-STEER agent enhances its domain knowledge through our evolving strategy and improves both its scheduling and implementation skills by gathering and using domain-specific practical experience. With a better schedule, implementation becomes faster. At the same time, as implementation feedback becomes more detailed, scheduling accuracy improves. These two capabilities grow together through practical feedback, enabling a collaborative evolution process.

.. code-block:: bibtex

    @misc{yang2024collaborative,
        title={Collaborative Evolving Strategy for Automatic Data-Centric Development},
        author={Xu Yang and Haotian Chen and Wenjun Feng and Haoxue Wang and Zeqi Ye and Xinjie Shen and Xiao Yang and Shizhao Sun and Weiqing Liu and Jiang Bian},
        year={2024},
        eprint={2407.18690},
        archivePrefix={arXiv},
        primaryClass={cs.AI}
    }

.. image:: https://github.com/user-attachments/assets/75d9769b-0edd-4caf-9d45-57d1e577054b
   :alt: Collaborative Evolving Strategy for Automatic Data-Centric Development




================================================
File: docs/scens/catalog.rst
================================================
=========================
Scenarios
=========================

Scenario lists
=========================

In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: 🦾copilot and 🤖agent.

- The 🦾copilot follows human instructions to automate repetitive tasks.
- The 🤖agent, being more autonomous, actively proposes ideas for better results in the future.

The supported scenarios are listed below:



.. list-table:: 
    :header-rows: 1

    * - Scenario/Target
      - Model Implementation
      - Data Building
    * - 💹 Finance
      - :ref:`🤖Iteratively Proposing Ideas & Evolving <model_agent_fin>`
      - :ref:`🦾Auto reports reading & implementation <data_copilot_fin>`

        :ref:`🤖Iteratively Proposing Ideas & Evolving <data_agent_fin>`
    * - 🩺 Medical
      - :ref:`🤖Iteratively Proposing Ideas & Evolving <model_agent_med>`
      - 
    * - 🏭 General
      - :ref:`🦾Auto paper reading & implementation <model_copilot_general>`

        :ref:`🤖Auto Kaggle Model Tuning <kaggle_agent>`
      - :ref:`🤖Auto Kaggle feature Engineering <kaggle_agent>`


.. toctree::
    :maxdepth: 1
    :caption: Doctree:
    :hidden:

    data_agent_fin
    data_copilot_fin
    model_agent_fin
    model_agent_med
    model_copilot_general
    kaggle_agent




================================================
File: docs/scens/data_agent_fin.rst
================================================
.. _data_agent_fin:

=====================
Finance Data Agent
=====================


**🤖 Automated Quantitative Trading & Iterative Factors Evolution**
-------------------------------------------------------------------

📖 Background
~~~~~~~~~~~~~~
In the dynamic world of quantitative trading, **factors** serve as the strategic tools that enable traders to exploit market inefficiencies. 
These factors—ranging from simple metrics like price-to-earnings ratios to complex models like discounted cash flows—are the key to predicting stock prices with a high degree of accuracy.

By leveraging these factors, quantitative traders can develop sophisticated strategies that not only identify market patterns but also significantly enhance trading efficiency and precision. 
The ability to systematically analyze and apply these factors is what separates ordinary trading from truly strategic market outmaneuvering.
And this is where the **Finance Model Agent** comes into play.

🎥 `Demo <https://rdagent.azurewebsites.net/factor_loop>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <div style="display: flex; justify-content: center; align-items: center;">
      <video width="600" controls>
        <source src="https://rdagent.azurewebsites.net/media/65bb598f1372c1857ccbf09b2acf5d55830911625048c03102291098.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>


🌟 Introduction
~~~~~~~~~~~~~~~~
In this scenario, our agent illustrates the iterative process of hypothesis generation, knowledge construction, and decision-making. 

It highlights how financial factors evolve through continuous feedback and refinement. 

Here's an enhanced outline of the steps:

**Step 1 : Hypothesis Generation 🔍**

- Generate and propose initial hypotheses based on previous experiment analysis and domain expertise, with thorough reasoning and financial justification.

**Step 2 : Factor Creation ✨**

- Based on the hypothesis, divide the tasks.
- Each task involves developing, defining, and implementing a new financial factor, including its name, description, formulation, and variables.

**Step 3 : Factor Implementation 👨‍💻**

- Implement the factor code based on the description, evolving it as a developer would.
- Quantitatively validate the newly created factors.

**Step 4 : Backtesting with Qlib 📉**

- Integrate the full dataset into the factor implementation code and prepare the factor library.
- Conduct backtesting using the Alpha158 plus newly developed factors and LGBModel in Qlib to evaluate the new factors' effectiveness and performance.

+----------------+------------+----------------+----------------------------------------------------+
| Dataset        | Model      | Factors        | Data Split                                         |
+================+============+================+====================================================+
| CSI300         | LGBModel   | Alpha158 Plus  | +-----------+--------------------------+           |
|                |            |                | | Train     | 2008-01-01 to 2014-12-31 |           |
|                |            |                | +-----------+--------------------------+           |
|                |            |                | | Valid     | 2015-01-01 to 2016-12-31 |           |
|                |            |                | +-----------+--------------------------+           |
|                |            |                | | Test      | 2017-01-01 to 2020-08-01 |           |
|                |            |                | +-----------+--------------------------+           |
+----------------+------------+----------------+----------------------------------------------------+


**Step 5 : Feedback Analysis 🔍**

- Analyze backtest results to assess performance.
- Incorporate feedback to refine hypotheses and improve the model.

**Step 6 :Hypothesis Refinement ♻️**

- Refine hypotheses based on feedback from backtesting.
- Repeat the process to continuously improve the model.

⚡ Quick Start
~~~~~~~~~~~~~~~~~

Please refer to the installation part in :doc:`../installation_and_configuration` to prepare your system dependency.

You can try our demo by running the following command:

- 🐍 Create a Conda Environment

  - Create a new conda environment with Python (3.10 and 3.11 are well tested in our CI):

    .. code-block:: sh

          conda create -n rdagent python=3.10

  - Activate the environment:

    .. code-block:: sh

        conda activate rdagent

- 📦 Install the RDAgent
  
  - You can install the RDAgent package from PyPI:

    .. code-block:: sh

        pip install rdagent

- 🚀 Run the Application
    
  - You can directly run the application by using the following command:
    
    .. code-block:: sh

        rdagent fin_factor


🛠️ Usage of modules
~~~~~~~~~~~~~~~~~~~~~

.. _Env Config: 

- **Env Config**

The following environment variables can be set in the `.env` file to customize the application's behavior:

.. autopydantic_settings:: rdagent.app.qlib_rd_loop.conf.FactorBasePropSetting
    :settings-show-field-summary: False
    :exclude-members: Config

.. autopydantic_settings:: rdagent.components.coder.factor_coder.config.FactorCoSTEERSettings
    :settings-show-field-summary: False
    :members: coder_use_cache, data_folder, data_folder_debug, file_based_execution_timeout, select_method, select_threshold, max_loop, knowledge_base_path, new_knowledge_base_path
    :exclude-members: Config, fail_task_trial_limit, v1_query_former_trace_limit, v1_query_similar_success_limit, v2_query_component_limit, v2_query_error_limit, v2_query_former_trace_limit, v2_error_summary, v2_knowledge_sampler
    :no-index:



================================================
File: docs/scens/data_copilot_fin.rst
================================================
.. _data_copilot_fin:

=====================
Finance Data Copilot
=====================


**🤖 Automated Quantitative Trading & Factors Extraction from Financial Reports**
---------------------------------------------------------------------------------

📖 Background
~~~~~~~~~~~~~~
**Research reports** are treasure troves of insights, often unveiling potential **factors** that can drive successful quantitative trading strategies. 
Yet, with the sheer volume of reports available, extracting the most valuable insights efficiently becomes a daunting task.

Furthermore, rather than hastily replicating factors from a report, it's essential to delve into the underlying logic of their construction. 
Does the factor capture the essential market dynamics? How unique is it compared to the factors already in your library?

Therefore, there is an urgent need for a systematic approach to design a framework that can effectively manage this process. 
And this is where the **Finance Data Copilot** steps in.


🎥 `Demo <https://rdagent.azurewebsites.net/report_factor>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <div style="display: flex; justify-content: center; align-items: center;">
      <video width="600" controls>
        <source src="https://rdagent.azurewebsites.net/media/7b14b2bd3d8771da9cf7eb799b6d96729cec3d35c8d4f68060f3e2fd.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>


🌟 Introduction
~~~~~~~~~~~~~~~~
In this scenario, RDAgent demonstrates the process of extracting factors from financial research reports, implementing these factors, and analyzing their performance through Qlib backtesting. 
This process continually expands and refines the factor library.

Here's an enhanced outline of the steps:

**Step 1 : Hypothesis Generation 🔍**

- Generate and propose initial hypotheses based on insights from financial reports with thorough reasoning and financial justification.

**Step 2 : Factor Creation ✨**

- Based on the hypothesis and financial reports, divide the tasks. 
- Each task involves developing, defining, and implementing a new financial factor, including its name, description, formulation, and variables.

**Step 3 : Factor Implementation 👨‍💻**

- Implement the factor code based on the description, evolving it as a developer would.
- Quantitatively validate the newly created factors.

**Step 4 : Backtesting with Qlib 📉**

- Integrate the full dataset into the factor implementation code and prepare the factor library.
- Conduct backtesting using the Alpha158 plus newly developed factors and LGBModel in Qlib to evaluate the new factors' effectiveness and performance.

+----------------+------------+----------------+----------------------------------------------------+
| Dataset        | Model      | Factors        | Data Split                                         |
+================+============+================+====================================================+
| CSI300         | LGBModel   | Alpha158 Plus  | +-----------+--------------------------+           |
|                |            |                | | Train     | 2008-01-01 to 2014-12-31 |           |
|                |            |                | +-----------+--------------------------+           |
|                |            |                | | Valid     | 2015-01-01 to 2016-12-31 |           |
|                |            |                | +-----------+--------------------------+           |
|                |            |                | | Test      | 2017-01-01 to 2020-08-01 |           |
|                |            |                | +-----------+--------------------------+           |
+----------------+------------+----------------+----------------------------------------------------+

**Step 5 : Feedback Analysis 🔍**

- Analyze backtest results to assess performance.
- Incorporate feedback to refine hypotheses and improve the model.

**Step 6 :Hypothesis Refinement ♻️**

- Refine hypotheses based on feedback from backtesting.
- Repeat the process to continuously improve the model.

⚡ Quick Start
~~~~~~~~~~~~~~~~~

Please refer to the installation part in :doc:`../installation_and_configuration` to prepare your system dependency.

You can try our demo by running the following command:

- 🐍 Create a Conda Environment
    
  - Create a new conda environment with Python (3.10 and 3.11 are well tested in our CI):
    
    .. code-block:: sh
    
        conda create -n rdagent python=3.10

  - Activate the environment:

    .. code-block:: sh

        conda activate rdagent

- 📦 Install the RDAgent
  
  - You can install the RDAgent package from PyPI:

    .. code-block:: sh

        pip install rdagent

- 🚀 Run the Application
    
  - Download the financial reports you wish to extract factors from and store them in your preferred folder.

  - Specifically, you can follow this example, or use your own method:

    .. code-block:: sh

        wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
        unzip all_reports.zip -d git_ignore_folder/reports

  - Run the application with the following command:

    .. code-block:: sh

        rdagent fin_factor_report --report_folder=git_ignore_folder/reports

  - Alternatively, you can store the paths of the reports in `report_result_json_file_path`. The format should be:

    .. code-block:: json

        [
            "git_ignore_folder/report/fin_report1.pdf",
            "git_ignore_folder/report/fin_report2.pdf",
            "git_ignore_folder/report/fin_report3.pdf"
        ]

  - Then, run the application using the following command:

    .. code-block:: sh

        rdagent fin_factor_report

🛠️ Usage of modules
~~~~~~~~~~~~~~~~~~~~~

.. _Env Config: 

- **Env Config**

The following environment variables can be set in the `.env` file to customize the application's behavior:

.. autopydantic_settings:: rdagent.app.qlib_rd_loop.conf.FactorFromReportPropSetting
    :settings-show-field-summary: False
    :show-inheritance:
    :exclude-members: Config

.. autopydantic_settings:: rdagent.components.coder.factor_coder.config.FactorCoSTEERSettings
    :settings-show-field-summary: False
    :members: coder_use_cache, data_folder, data_folder_debug, file_based_execution_timeout, select_method, select_threshold, max_loop, knowledge_base_path, new_knowledge_base_path
    :exclude-members: Config, python_bin, fail_task_trial_limit, v1_query_former_trace_limit, v1_query_similar_success_limit, v2_query_component_limit, v2_query_error_limit, v2_query_former_trace_limit, v2_error_summary, v2_knowledge_sampler
    :no-index:



================================================
File: docs/scens/kaggle_agent.rst
================================================
.. _kaggle_agent:

=======================
Kaggle Agent
=======================

**🤖 Automated Feature Engineering & Model Tuning Evolution**
------------------------------------------------------------------------------------------

🎨 Design
~~~~~~~~~~~

.. image:: kaggle_design.png
   :alt: Design of Kaggle Agent
   :align: center

📖 Background
~~~~~~~~~~~~~~
In the landscape of data science competitions, Kaggle serves as the ultimate arena where data enthusiasts harness the power of algorithms to tackle real-world challenges.
The Kaggle Agent stands as a pivotal tool, empowering participants to seamlessly integrate cutting-edge models and datasets, transforming raw data into actionable insights.

By utilizing the **Kaggle Agent**, data scientists can craft innovative solutions that not only uncover hidden patterns but also drive significant advancements in predictive accuracy and model robustness.


🌟 Introduction
~~~~~~~~~~~~~~~~

In this scenario, our automated system proposes hypothesis, choose action, implements code, conducts validation, and utilizes feedback in a continuous, iterative process.

The goal is to automatically optimize performance metrics within the validation set or Kaggle Leaderboard, ultimately discovering the most efficient features and models through autonomous research and development.

Here's an enhanced outline of the steps:

**Step 1 : Hypothesis Generation 🔍**

- Generate and propose initial hypotheses based on previous experiment analysis and domain expertise, with thorough reasoning and financial justification.

**Step 2 : Experiment Creation ✨**

- Transform the hypothesis into a task.
- Choose a specific action within feature engineering or model tuning.
- Develop, define, and implement a new feature or model, including its name, description, and formulation.

**Step 3 : Model/Feature Implementation 👨‍💻**

- Implement the model code based on the detailed description.
- Evolve the model iteratively as a developer would, ensuring accuracy and efficiency.

**Step 4 : Validation on Test Set or Kaggle 📉**

- Validate the newly developed model using the test set or Kaggle dataset.
- Assess the model's effectiveness and performance based on the validation results.

**Step 5: Feedback Analysis 🔍**

- Analyze validation results to assess performance.
- Use insights to refine hypotheses and enhance the model.

**Step 6: Hypothesis Refinement ♻️**

- Adjust hypotheses based on validation feedback.
- Iterate the process to continuously improve the model.

🧭 Example Guide
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

- 🔧 **Set up RD-Agent Environment**

  - Before you start, please make sure you have installed RD-Agent and configured the environment for RD-Agent correctly. If you want to know how to install and configure the RD-Agent, please refer to the `documentation <../installation_and_configuration.html>`_.

- 🔨 **Configuring the Kaggle API**
  
  - Register and login on the `Kaggle <https://www.kaggle.com/>`_ website.
  - Click on the avatar (usually in the top right corner of the page) -> ``Settings`` -> ``Create New Token``, A file called ``kaggle.json`` will be downloaded.
  - Move ``kaggle.json`` to ``~/.config/kaggle/``
  - Modify the permissions of the ``kaggle.json`` file.

    .. code-block:: sh

      chmod 600 ~/.config/kaggle/kaggle.json

  - For more information about Kaggle API Settings, refer to the `Kaggle API <https://github.com/Kaggle/kaggle-api>`_.

- 🔩 **Setting the Environment variables at .env file**

  - Determine the path where the data will be stored and add it to the ``.env`` file.

  .. code-block:: sh

    dotenv set KG_LOCAL_DATA_PATH <your local directory>/kaggle_data

- 📥 **Download Competition Data**

  - Kaggle competition data, contains two parts: competition description file (json file) and competition dataset (zip file).

    - **How to get the competition description file**

      - *Manual Download (General User Suggestions):*

        - Download the competition description file prepared in advance, and extract it to the specified directory.

          .. code-block:: sh

            wget https://github.com/SunsetWolf/rdagent_resource/releases/download/kaggle_data/kaggle_data.zip
            unzip kaggle_data.zip -d <your local directory>/kaggle_data

      - *Automatic Download (Developer Suggestions):*
      
        - Alternatively, you can choose to download the competition description file automatically when you run the program, but it requires ``chromedriver`` to be installed, as follows:

          .. code-block:: sh

            # install chrome
            wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
            sudo apt install ./google-chrome-stable_current_amd64.deb
            google-chrome --version

            # install chromedriver
            wget "https://storage.googleapis.com/chrome-for-testing-public/$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+')/linux64/chromedriver-linux64.zip"
            unzip chromedriver-linux64.zip
            cd chromedriver-linux64
            sudo mv chromedriver /usr/local/bin
            sudo chmod +x /usr/local/bin/chromedriver
            chromedriver --version

    - **How to get the competition dataset**

      - The competition dataset is downloaded and extracted automatically when the program is run. If the zip file exists, the download will be skipped, if the unzip folder exists, the unzip will be skipped.

    - **Correct directory structure (Here is an example of competition data with id sf-crime)**

      .. code-block:: text

        kaggle_data
        └── zip_files
        | └── sf-crime.zip
        ├── sf-crime.json
        └── sf-crime
          └── ...
        
      - ``kaggle_data/zip_files/sf-crime.zip:`` Competition dataset zip files downloaded from the Kaggle website.

      - ``kaggle_data/sf-crime.json:`` Competition description file.

      - ``kaggle_data/sf-crime:`` The target folder for unzipping the competition dataset.

- 🗳️ **Join the competition**

  - If your Kaggle API account has not joined a competition, you will need to join the competition before running the program.
    
    - At the bottom of the competition details page, you can find the ``Join the competition`` button, click on it and select ``I Understand and Accept`` to join the competition.
    
    - In the **Competition List Available** below, you can jump to the competition details page.

- 🚀 **Run the Application**

  - You can directly run the application by using the following command:
    
    .. code-block:: sh

        rdagent kaggle --competition <Competition ID>

- 📤 **Submit the Result Automatically or Manually**

  - If Auto: You need to set ``KG_AUTO_SUBMIT`` to ``true`` in the ``.env`` file.

    .. code-block:: sh

      dotenv set KG_AUTO_SUBMIT true
  
  - Else: You can download the prediction results from the UI interface and submit them manually. For more details, refer to the :doc:`UI guide <../ui>`.

📋 Competition List Available
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| **index** | **Competition Name**              | **Task**         | **Modal** | **ID**                                                                                                  |
+===========+===================================+==================+===========+=========================================================================================================+
| 01        | Media Campaign Cost Dataset       | Regression       | Tabular   | `playground-series-s3e11 <https://www.kaggle.com/competitions/playground-series-s3e11/data>`_           |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 02        | Wild Blueberry Yield Dataset      | Regression       | Tabular   | `playground-series-s3e14 <https://www.kaggle.com/competitions/playground-series-s3e14/data>`_           |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 03        | Crab Age Dataset                  | Regression       | Tabular   | `playground-series-s3e16 <https://www.kaggle.com/competitions/playground-series-s3e16/data>`_           |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 04        | Flood Prediction Dataset          | Regression       | Tabular   | `playground-series-s4e5 <https://www.kaggle.com/competitions/playground-series-s4e5/data>`_             |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 05        | Used Car Prices Dataset           | Regression       | Tabular   | `playground-series-s4e9 <https://www.kaggle.com/competitions/playground-series-s4e9/data>`_             |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 06        | Cirrhosis Outcomes Dataset        | Multi-Class      | Tabular   | `playground-series-s3e26 <https://www.kaggle.com/competitions/playground-series-s3e26/data>`_           |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 07        | San Francisco Crime Classification| Multi-Class      | Tabular   | `sf-crime <https://www.kaggle.com/competitions/sf-crime/data>`_                                         |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 08        | Poisonous Mushrooms Dataset       | Classification   | Tabular   | `playground-series-s4e8 <https://www.kaggle.com/competitions/playground-series-s4e8/data>`_             |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 09        | Spaceship Titanic                 | Classification   | Tabular   | `spaceship-titanic <https://www.kaggle.com/competitions/spaceship-titanic/data>`_                       |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 10        | Forest Cover Type Prediction      | Classification   | Tabular   | `forest-cover-type-prediction <https://www.kaggle.com/competitions/forest-cover-type-prediction/data>`_ |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| 11        | Digit Recognizer                  | Classification   | Image     | `digit-recognizer <https://www.kaggle.com/competitions/digit-recognizer>`_                              |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+
| To be continued ...                                                                                                                                                                    |
+-----------+-----------------------------------+------------------+-----------+---------------------------------------------------------------------------------------------------------+



🎨 Customize one template for a new competition
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In order to facilitate RD-Agent to generate competition codes, we have specified a competition code structure:

.. image:: kaggle_template.png
   :alt: Design of Kaggle Code Template
   :align: center

- **feature directory** contains the feature engineering code. Generally no modification is required.
- **model directory** contains the model codes.
  select_xx.py is used to select different features according to different models.
  model_xx.py is the basic code of different models. Generally, only some initial parameters need to be adjusted.
- **fea_share_preprocess.py** is some basic preprocessing code shared by different models. The degree of customization here is high, but the preprocess_script() function needs to be retained, which will be called by train.py
- **train.py** is the main code, which connects all the codes and is also the code called during the final execution.

**We will soon provide a tool for automatic/semi-automatic template generation.**
If you want to try a different competition now, you can refer to our current template structure and content to write a new template.


🎯 Roadmap
~~~~~~~~~~~

**Completed:**

- **Kaggle Project Schema Design** ✅

- **RD-Agent Integration with kaggle schema** ✅

**Ongoing:**

- **Template auto generation**

- **Bench Optimization**

  - **Online Bench**

    - **RealMLBench**

      - Ongoing integration

      - Auto online submission

      - Batch Evaluation

  - **Offline Bench**
  
    - MLE-Bench


🛠️ Usage of modules
~~~~~~~~~~~~~~~~~~~~~

.. _Env Config: 

- **Env Config**

The following environment variables can be set in the `.env` file to customize the application's behavior:

.. autopydantic_settings:: rdagent.app.kaggle.conf.KaggleBasePropSetting
    :settings-show-field-summary: False
    :exclude-members: Config

.. autopydantic_settings:: rdagent.components.coder.factor_coder.config.FactorCoSTEERSettings
    :settings-show-field-summary: False
    :members: coder_use_cache, file_based_execution_timeout, select_method, max_loop
    :exclude-members: Config, fail_task_trial_limit, v1_query_former_trace_limit, v1_query_similar_success_limit, v2_query_component_limit, v2_query_error_limit, v2_query_former_trace_limit, v2_error_summary, v2_knowledge_sampler, v2_add_fail_attempt_to_latest_successful_execution, new_knowledge_base_path, knowledge_base_path, data_folder, data_folder_debug, select_threshold
    :no-index:



================================================
File: docs/scens/model_agent_fin.rst
================================================
.. _model_agent_fin:

=======================
Finance Model Agent
=======================

**🤖 Automated Quantitative Trading & Iterative Model Evolution**
------------------------------------------------------------------------------------------

📖 Background
~~~~~~~~~~~~~~
In the realm of quantitative finance, both factor discovery and model development play crucial roles in driving performance. 
While much attention is often given to the discovery of new financial factors, the **models** that leverage these factors are equally important. 
The effectiveness of a quantitative strategy depends not only on the factors used but also on how well these factors are integrated into robust, predictive models.

However, the process of developing and optimizing these models can be labor-intensive and complex, requiring continuous refinement and adaptation to ever-changing market conditions. 
And this is where the **Finance Model Agent** steps in.


🎥 `Demo <https://rdagent.azurewebsites.net/model_loop>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <div style="display: flex; justify-content: center; align-items: center;">
      <video width="600" controls>
        <source src="https://rdagent.azurewebsites.net/media/d85e8cab1da1cd3501d69ce837452f53a971a24911eae7bfa9237137.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>


🌟 Introduction
~~~~~~~~~~~~~~~~

In this scenario, our automated system proposes hypothesis, constructs model, implements code, conducts back-testing, and utilizes feedback in a continuous, iterative process.

The goal is to automatically optimize performance metrics within the Qlib library, ultimately discovering the most efficient code through autonomous research and development.

Here's an enhanced outline of the steps:

**Step 1 : Hypothesis Generation 🔍**

- Generate and propose initial hypotheses based on previous experiment analysis and domain expertise, with thorough reasoning and financial justification.

**Step 2 : Model Creation ✨**

- Transform the hypothesis into a task.
- Develop, define, and implement a quantitative model, including its name, description, and formulation.

**Step 3 : Model Implementation 👨‍💻**

- Implement the model code based on the detailed description.
- Evolve the model iteratively as a developer would, ensuring accuracy and efficiency.

**Step 4 : Backtesting with Qlib 📉**

- Conduct backtesting using the newly developed model and 20 factors extracted from Alpha158 in Qlib.
- Evaluate the model's effectiveness and performance.

+----------------+------------+------------------------+----------------------------------------------------+
| Dataset        | Model      | Factors                | Data Split                                         |
+================+============+========================+====================================================+
| CSI300         | RDAgent-dev| 20 factors (Alpha158)  | +-----------+--------------------------+           |
|                |            |                        | | Train     | 2008-01-01 to 2014-12-31 |           |
|                |            |                        | +-----------+--------------------------+           |
|                |            |                        | | Valid     | 2015-01-01 to 2016-12-31 |           |
|                |            |                        | +-----------+--------------------------+           |
|                |            |                        | | Test      | 2017-01-01 to 2020-08-01 |           |
|                |            |                        | +-----------+--------------------------+           |
+----------------+------------+------------------------+----------------------------------------------------+

**Step 5 : Feedback Analysis 🔍**

- Analyze backtest results to assess performance.
- Incorporate feedback to refine hypotheses and improve the model.

**Step 6 :Hypothesis Refinement ♻️**

- Refine hypotheses based on feedback from backtesting.
- Repeat the process to continuously improve the model.

⚡ Quick Start
~~~~~~~~~~~~~~~~~

Please refer to the installation part in :doc:`../installation_and_configuration` to prepare your system dependency.

You can try our demo by running the following command:

- 🐍 Create a Conda Environment

  - Create a new conda environment with Python (3.10 and 3.11 are well tested in our CI):

    .. code-block:: sh
    
        conda create -n rdagent python=3.10

  - Activate the environment:

    .. code-block:: sh

        conda activate rdagent

- 📦 Install the RDAgent
    
  - You can install the RDAgent package from PyPI:

    .. code-block:: sh

        pip install rdagent

- 🚀 Run the Application
    
  - You can directly run the application by using the following command:
    
    .. code-block:: sh

        rdagent fin_model

🛠️ Usage of modules
~~~~~~~~~~~~~~~~~~~~~

.. _Env Config: 

- **Env Config**

The following environment variables can be set in the `.env` file to customize the application's behavior:

.. autopydantic_settings:: rdagent.app.qlib_rd_loop.conf.ModelBasePropSetting
    :settings-show-field-summary: False
    :exclude-members: Config

- **Qlib Config**
    - The `config.yaml` file located in the `model_template` folder contains the relevant configurations for running the developed model in Qlib. The default settings include key information such as:
        - **market**: Specifies the market, which is set to `csi300`.
        - **fields_group**: Defines the fields group, with the value `feature`.
        - **col_list**: A list of columns used, including various indicators such as `RESI5`, `WVMA5`, `RSQR5`, and others.
        - **start_time**: The start date for the data, set to `2008-01-01`.
        - **end_time**: The end date for the data, set to `2020-08-01`.
        - **fit_start_time**: The start date for fitting the model, set to `2008-01-01`.
        - **fit_end_time**: The end date for fitting the model, set to `2014-12-31`.

    - The default hyperparameters used in the configuration are as follows:
        - **n_epochs**: The number of epochs, set to `100`.
        - **lr**: The learning rate, set to `1e-3`.
        - **early_stop**: The early stopping criterion, set to `10`.
        - **batch_size**: The batch size, set to `2000`.
        - **metric**: The evaluation metric, set to `loss`.
        - **loss**: The loss function, set to `mse`.
        - **n_jobs**: The number of parallel jobs, set to `20`.



================================================
File: docs/scens/model_agent_med.rst
================================================
.. _model_agent_med:

=======================
Medical Model Agent
=======================

**🤖 Automated Medical Predtion Model Evolution**
------------------------------------------------------------------------------------------

📖 Background
~~~~~~~~~~~~~~
In this scenario, we consider the problem of risk prediction from patients' ICU monitoring data. We use the a public EHR dataset - MIMIC-III and extract a binary classification task for evaluating the framework.
In this task, we aim at predicting the whether the patients will suffer from Acute Respiratory Failure (ARF) based their first 12 hours ICU monitoring data. 

🎥 `Demo <https://rdagent.azurewebsites.net/dmm>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <div style="display: flex; justify-content: center; align-items: center;">
      <video width="600" controls>
        <source src="https://rdagent.azurewebsites.net/media/1653542fc1b9fa14a306c35c1b1fc48288f980793f38abe82b023af9.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>


🌟 Introduction
~~~~~~~~~~~~~~~~

In this scenario, our automated system proposes hypothesis, constructs model, implements code, receives back-testing, and uses feedbacks. 
Hypothesis is iterated in this continuous process. 
The system aims to automatically optimise performance metrics of medical prediction thereby finding the optimised code through autonomous research and development.

Here's an enhanced outline of the steps:

**Step 1 : Hypothesis Generation 🔍**

- Generate and propose initial hypotheses based on previous experiment analysis and domain expertise, with thorough reasoning and justification.

**Step 2 : Model Creation ✨**

- Transform the hypothesis into a model.
- Develop, define, and implement a machine learning model, including its name, description, and formulation.

**Step 3 : Model Implementation 👨‍💻**

- Implement the model code based on the detailed description.
- Evolve the model iteratively as a developer would, ensuring accuracy and efficiency.

**Step 4 : Backtesting with MIMIC-III 📉**

- Conduct backtesting using the newly developed model on the extracted task from MIMIC-III.
- Evaluate the model's effectiveness and performance in terms of AUROC score.

**Step 5 : Feedback Analysis 🔍**

- Analyze backtest results to assess performance.
- Incorporate feedback to refine hypotheses and improve the model.

**Step 6 :Hypothesis Refinement ♻️**

- Refine hypotheses based on feedback from backtesting.
- Repeat the process to continuously improve the model.

⚡ Quick Start
~~~~~~~~~~~~~~~~~

Please refer to the installation part in :doc:`../installation_and_configuration` to prepare your system dependency.

You can try our demo by running the following command:

- 🐍 Create a Conda Environment
  
  - Create a new conda environment with Python (3.10 and 3.11 are well tested in our CI):

    .. code-block:: sh
    
        conda create -n rdagent python=3.10

  - Activate the environment:

    .. code-block:: sh

        conda activate rdagent

- 📦 Install the RDAgent
    
  - You can install the RDAgent package from PyPI:

    .. code-block:: sh

        pip install rdagent

- 📦 Request PhysioNet Account
    
  - Apply for an account at `PhysioNet <https://physionet.org/>`_.
  - Request access to FIDDLE preprocessed data: `FIDDLE Dataset <https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/>`_.
  - Place your username and password in `.env`.

    .. code-block:: bash

        cat << EOF  >> .env
        DM_USERNAME=<your_username>
        DM_PASSWORD=<your_password>
        EOF


- 🚀 Run the Application
    
  - You can directly run the application by using the following command:
    
    .. code-block:: sh

        rdagent med_model

🛠️ Usage of modules
~~~~~~~~~~~~~~~~~~~~~

.. _Env Config: 

- **Env Config**

The following environment variables can be set in the `.env` file to customize the application's behavior:

.. autopydantic_settings:: rdagent.app.data_mining.conf.MedBasePropSetting
    :settings-show-field-summary: False
    :exclude-members: Config



================================================
File: docs/scens/model_copilot_general.rst
================================================
.. _model_copilot_general:

======================
General Model Copilot
======================

**🤖 Automated Model Research & Development Co-Pilot**
--------------------------------------------------------

📖 Background
~~~~~~~~~~~~~~
In the fast-paced field of artificial intelligence, the number of academic papers published each year is skyrocketing. 
These papers introduce new models, techniques, and approaches that can significantly advance the state of the art. 
However, reproducing and implementing these models can be a daunting task, requiring substantial time and expertise. 
Researchers often face challenges in extracting the essential details from these papers and converting them into functional code.
And this is where the **General Model Copilot** steps in.

🎥 `Demo <https://rdagent.azurewebsites.net/report_model>`_
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. raw:: html

    <div style="display: flex; justify-content: center; align-items: center;">
      <video width="600" controls>
        <source src="https://rdagent.azurewebsites.net/media/b35f904765b05099b0fcddbebe041a04f4d7bde239657e5fc24bf0cc.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>

🌟 Introduction
~~~~~~~~~~~~~~~~
In this scenario, our automated system proposes hypotheses, constructs models, implements code, performs back-testing, and uses feedback to iterate continuously. The system aims to automatically optimize performance metrics from the Qlib library, finding the best code through autonomous research and development.

Model R&D CoPilot Scenario
~~~~~~~~~~~~~~~~~~~~~~~~~~
**Overview**

This demo automates the extraction and iterative development of models from academic papers, ensuring functionality and correctness. This scenario automates the development of PyTorch models by reading academic papers or other sources. It supports various data types, including tabular, time-series, and graph data. The primary workflow involves two main components: the Reader and the Coder.

**Workflow Components**

1. **Reader**
   - Parses and extracts relevant model information from academic papers or sources, including architectures, parameters, and implementation details.
   - Uses Large Language Models to convert content into a structured format for the Coder.

2. **Evolving Coder**
   - Translates structured information from the Reader into executable PyTorch code.
   - Utilizes an evolving coding mechanism to ensure correct tensor shapes, verified with sample input tensors.
   - Iteratively refines the code to align with source material specifications.

**Supported Data Types**

- **Tabular Data:** Structured data with rows and columns, such as spreadsheets or databases.
- **Time-Series Data:** Sequential data points indexed in time order, useful for forecasting and temporal pattern recognition.
- **Graph Data:** Data structured as nodes and edges, suitable for network analysis and relational tasks.

⚡ Quick Start
~~~~~~~~~~~~~~~~~

Please refer to the installation part in :doc:`../installation_and_configuration` to prepare your system dependency.

You can try our demo by running the following command:

- 🐍 Create a Conda Environment
  
  - Create a new conda environment with Python (3.10 and 3.11 are well tested in our CI):

    .. code-block:: sh
    
        conda create -n rdagent python=3.10

  - Activate the environment:

    .. code-block:: sh

        conda activate rdagent

- 📦 Install the RDAgent
    
  - You can install the RDAgent package from PyPI:

    .. code-block:: sh

        pip install rdagent


- 🚀 Run the Application
    
  - Prepare relevant files (in pdf format) by uploading papers to the directory below and copy the path as report_file_path.
      
    .. code-block:: sh

        rdagent/scenarios/general_model
    
  - Run the following command in your terminal within the same virtual environment:
  
    .. code-block:: sh

        rdagent general_model --report_file_path=<path_to_pdf_file>



================================================
File: rdagent/app/cli.py
================================================
"""
CLI entrance for all rdagent application.

This will
- make rdagent a nice entry and
- autoamtically load dotenv
"""

from dotenv import load_dotenv

load_dotenv(".env")
# 1) Make sure it is at the beginning of the script so that it will load dotenv before initializing BaseSettings.
# 2) The ".env" argument is necessary to make sure it loads `.env` from the current directory.

import subprocess
from importlib.resources import path as rpath

import fire

from rdagent.app.data_mining.model import main as med_model
from rdagent.app.general_model.general_model import (
    extract_models_and_implement as general_model,
)
from rdagent.app.kaggle.loop import main as kaggle_main
from rdagent.app.qlib_rd_loop.factor import main as fin_factor
from rdagent.app.qlib_rd_loop.factor_from_report import main as fin_factor_report
from rdagent.app.qlib_rd_loop.model import main as fin_model
from rdagent.app.utils.health_check import health_check
from rdagent.app.utils.info import collect_info


def ui(port=19899, log_dir="", debug=False):
    """
    start web app to show the log traces.
    """
    with rpath("rdagent.log.ui", "app.py") as app_path:
        cmds = ["streamlit", "run", app_path, f"--server.port={port}"]
        if log_dir or debug:
            cmds.append("--")
        if log_dir:
            cmds.append(f"--log_dir={log_dir}")
        if debug:
            cmds.append("--debug")
        subprocess.run(cmds)


def app():
    fire.Fire(
        {
            "fin_factor": fin_factor,
            "fin_factor_report": fin_factor_report,
            "fin_model": fin_model,
            "med_model": med_model,
            "general_model": general_model,
            "ui": ui,
            "health_check": health_check,
            "collect_info": collect_info,
            "kaggle": kaggle_main,
        }
    )



================================================
File: rdagent/app/CI/README.md
================================================
# CI 检查

`.github/workflows/ci.yml`配置了提交时自动运行`Makefile`: 91~103行的命令，可以在这调整执行的命令

在`.env`中设置`USE_CHAT_CACHE=True`可以让第二次修复快一些

# Rules

`pyproject.toml`中配置全局屏蔽的规则
- ruff: `[tool.ruff.lint].ignore`
- mypy: `[tool.mypy]`

## ruff rules
ruff rules 比较好修改, 大多可以自动修复

对于一些规则可以在代码中添加注释来局部屏蔽, 例如添加 `# noqa E234,ANN001`
遇到的不好修改的规则:
- 捕获异常时应该处理每一种异常，不应该统一当作`Exception`处理
- `subprogress()` 调用命令应该先判断命令是否安全
- ...

规则列表: [ruff rules](https://docs.astral.sh/ruff/rules/)

## mypy rules

Mypy检查Python中类型标注, 常遇到需要修改结构/同时修改其他文件的情况, 自动修复效果不好

局部屏蔽: `# type: ignore`

规则列表: [mypy rules](https://mypy.readthedocs.io/en/stable/error_code_list.html)

# Optimization (Maybe)

- 添加指定文件夹检查的功能
- 增加一个修改选项: 调用`vim`, 用户直接修改此部分代码
- 显示时把`Original Code`部分去掉, 直接在输出的表示修改的diff部分用`^^^^^^`在代码行下标注出错误位置，这样能更直观地观察错误修复情况
- 当前为线性执行完所有修复后交给用户检查, 可修改成 后台多线程 / 进程处理修复的任务, 终端实时展示处理完的修复让用户检查
- ...



================================================
File: rdagent/app/CI/ci.ipynb
================================================
# Jupyter notebook converted to Python script.



================================================
File: rdagent/app/CI/prompts.yaml
================================================
generate_lint_command_template: |
  Please generate a command to lint or format a {language} repository.
  Here are some information about different linting tools ```{linting_tools}```
linting_system_prompt_template: |
  You are a software engineer. You can write code to a high standard and are adept at solving {language} linting problems.
session_manual_template: |
  There are some problems with the code you provided, please modify the code again according to the instruction and return the errors list you modified.
  
  Instruction:
  {operation}
  
  Your response format should be like this:
  
  ```python
  <modified code>
  ```
  
  ```json
  {{
      "errors": ["<Line Number>:<Error Start Position> <Error Code>", ...]
  }}
  ```
session_normal_template: |
  Please modify this code snippet based on the lint info. Here is the code snippet:
  ```Python
  {code}
  ```

  -----Lint info-----
  {lint_info}
  -------------------

  The lint info contains one or more errors. Different errors are separated by blank lines. Each error follows this format:
  -----Lint info format-----
  <Line Number>:<Error Start Position> <Error Code> <Error Message>
  <Error Position (maybe multiple lines)>
  <Helpful Information (sometimes have)>
  --------------------------
  The error code is an abbreviation set by the checker for ease of describing the error. The error position includes the relevant code around the error, and the helpful information provides useful information or possible fix method.

  Please simply reply the code after you fix all linting errors. You should be aware of the following:
  1. The indentation of the code should be consistent with the original code.
  2. You should just replace the code I provided you, which starts from line {start_line} to line {end_line}.
  3. You'll need to add line numbers to the modified code which starts from {start_lineno}.
  4. You don't need to add comments to explain your changes.
  Please wrap your code with following format:

  ```python
  <your code..>
  ```
session_start_template: |
  Please modify the Python code based on the lint info.
  Due to the length of the code, I will first tell you the entire code, and then each time I ask a question, I will extract a portion of the code and tell you the error information contained in this code segment.
  You need to fix the corresponding error in the code segment and return the code that can replace the corresponding code segment.

  The Python code is from a complete Python project file. Each line of the code is annotated with a line number, separated from the original code by three characters ("<white space>|<white space>"). The vertical bars are aligned.
  Here is the complete code, please be prepared to fix it:
  ```Python
  {code}
  ```
suffix2language_template: |
  Here are the files suffix in one code repo: {suffix}.
  Please tell me the programming language used in this repo and which language has linting-tools.
  Your response should follow this template:
  {{
      "languages": <languages list>,
      "languages_with_linting_tools": <languages with lingting tools list>
  }}
user_get_files_contain_lint_commands_template: |
  You get a file list of a repository. Some files may contain linting rules or linting commands defined by repo authors.
  Here are the file list:
  ```
  {file_list}
  ```
  
  Please find all files that may correspond to linting from it.
  Please respond with the following JSON template:
  {{
      "files": </path/to/file>,
  }}
user_get_makefile_lint_commands_template: |
  You get a Makefile which contains some linting rules. Here are its content:
  ```
  {file_text}
  ```
  Please find executable commands about linting from it.
  Please respond with the following JSON template:
  {{
      "commands": ["python -m xxx --params"...],
  }}
user_template_for_code_snippet: |
  Please modify the Python code based on the lint info.
  -----Python Code-----
  {code}
  ---------------------

  -----Lint info-----
  {lint_info}
  -------------------

  The Python code is a snippet from a complete Python project file. Each line of the code is annotated with a line number, separated from the original code by three characters ("<white space>|<white space>"). The vertical bars are aligned.

  The lint info contains one or more errors. Different errors are separated by blank lines. Each error follows this format:
  -----Lint info format-----
  <Line Number>:<Error Start Position> <Error Code> <Error Message>
  <Error Context (multiple lines)>
  <Helpful Information (last line)>
  --------------------------
  The error code is an abbreviation set by the checker for ease of describing the error. The error context includes the relevant code around the error, and the helpful information suggests possible fixes.

  Please simply reply the code after you fix all linting errors.
  The code you return does not require line numbers, and should just replace the code I provided you, and does not require comments.
  Please wrap your code with following format:

  ```python
  <your code..>
  ```


================================================
File: rdagent/app/CI/run.py
================================================
from __future__ import annotations

import datetime
import json
import re
import shlex
import subprocess
import time
from collections import defaultdict
from dataclasses import dataclass
from difflib import ndiff
from pathlib import Path
from typing import Any, Literal

import tree_sitter_python
from rich import print
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn
from rich.prompt import Prompt
from rich.rule import Rule
from rich.syntax import Syntax
from rich.table import Table
from rich.text import Text
from tree_sitter import Language, Node, Parser

from rdagent.core.evaluation import Evaluator
from rdagent.core.evolving_agent import EvoAgent
from rdagent.core.evolving_framework import (
    EvolvableSubjects,
    EvolvingStrategy,
    EvoStep,
    Feedback,
    Knowledge,
)
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_utils import APIBackend

py_parser = Parser(Language(tree_sitter_python.language()))
CI_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


@dataclass
class CIError:
    raw_str: str
    file_path: Path | str
    line: int
    column: int
    code: str
    msg: str
    hint: str
    checker: Literal["ruff", "mypy"]

    def to_dict(self) -> dict[str, object]:
        return self.__dict__

    def __str__(self) -> str:
        return f"{self.file_path}:{self.line}:{self.column}: {self.code} {self.msg}\n{self.hint}".strip()


@dataclass
class CIFeedback(Feedback):
    errors: dict[str, list[CIError]]

    def statistics(self) -> dict[Literal["ruff", "mypy"], dict[str, int]]:
        error_counts = defaultdict(lambda: defaultdict(int))
        for file_errors in self.errors.values():
            for error in file_errors:
                error_counts[error.checker][error.code] += 1
        return error_counts


@dataclass
class FixRecord:
    skipped_errors: list[CIError]
    directly_fixed_errors: list[CIError]
    manually_fixed_errors: list[CIError]
    manual_instructions: dict[str, list[CIError]]

    def to_dict(self) -> dict[str, Any]:
        return {
            "skipped_errors": [error.to_dict() for error in self.skipped_errors],
            "directly_fixed_errors": [error.to_dict() for error in self.directly_fixed_errors],
            "manually_fixed_errors": [error.to_dict() for error in self.manually_fixed_errors],
            "manual_instructions": {
                key: [error.to_dict() for error in errors] for key, errors in self.manual_instructions.items()
            },
        }


class CodeFile:
    def __init__(self, path: Path | str) -> None:
        self.path = Path(path)
        self.load()

    @classmethod
    def add_line_number(cls: CodeFile, code: list[str] | str, start: int = 1) -> list[str] | str:
        code_lines = code.split("\n") if isinstance(code, str) else code

        lineno_width = len(str(start - 1 + len(code_lines)))
        code_with_lineno = []
        for i, code_line in enumerate(code_lines):
            code_with_lineno.append(f"{i+start: >{lineno_width}} | {code_line}")

        return code_with_lineno if isinstance(code, list) else "\n".join(code_with_lineno)

    @classmethod
    def remove_line_number(cls: CodeFile, code: list[str] | str) -> list[str] | str:
        code_lines = code.split("\n") if isinstance(code, str) else code

        try:
            code_without_lineno = [re.split(r"\| ", code_line, maxsplit=1)[1] for code_line in code_lines]
        except IndexError:
            code_without_lineno = ["something went wrong when remove line numbers", *code_lines]

        return code_without_lineno if isinstance(code, list) else "\n".join(code_without_lineno)

    def load(self) -> None:
        code = self.path.read_text(encoding="utf-8")
        self.code_lines = code.split("\n")

        # line numbers
        self.lineno = len(self.code_lines)
        self.lineno_width = len(str(self.lineno))
        self.code_lines_with_lineno = self.add_line_number(self.code_lines)

    def get(
        self,
        start: int = 1,
        end: int | None = None,
        *,
        add_line_number: bool = False,
        return_list: bool = False,
    ) -> list[str] | str:
        """
        Retrieves a portion of the code lines.
        line number starts from 1, return codes in [start, end].

        Args:
            start (int): The starting line number (inclusive). Defaults to 1.
            end (int | None): The ending line number (inclusive). Defaults to None, which means the last line.
            add_line_number (bool): Whether to include line numbers in the result. Defaults to False.
            return_list (bool): Whether to return the result as a list of lines
                or as a single string. Defaults to False.

        Returns:
            list[str] | str: The code lines as a list of strings or as a
                single string, depending on the value of `return_list`.
        """
        start -= 1
        if start < 0:
            start = 0
        end = self.lineno if end is None else end
        if end <= start:
            res = []
        res = self.code_lines_with_lineno[start:end] if add_line_number else self.code_lines[start:end]

        return res if return_list else "\n".join(res)

    def apply_changes(self, changes: list[tuple[int, int, str]]) -> None:
        """
        Applies the given changes to the code lines.

        Args:
            changes (List[Tuple[int, int, str]]): A list of tuples representing the changes to be applied.
                Each tuple contains the start line number, end line number, and the new code to be inserted.

        Returns:
            None
        """
        offset = 0
        for start, end, code in changes:
            # starts from 1  -->  starts from 0
            adjusted_start = max(start - 1, 0)

            new_code = code.split("\n")
            self.code_lines[adjusted_start + offset : end + offset] = new_code
            offset += len(new_code) - (end - adjusted_start)

        self.path.write_text("\n".join(self.code_lines), encoding="utf-8")
        self.load()

    def get_code_blocks(self, max_lines: int = 30) -> list[tuple[int, int]]:
        tree = py_parser.parse(bytes("\n".join(self.code_lines), "utf8"))

        def get_blocks_in_node(node: Node, max_lines: int) -> list[tuple[int, int]]:
            if node.type == "assignment":
                return [(node.start_point.row, node.end_point.row + 1)]

            blocks: list[tuple[int, int]] = []
            block: tuple[int, int] | None = None  # [start, end), line number starts from 0

            for child in node.children:
                if child.end_point.row + 1 - child.start_point.row > max_lines:
                    if block is not None:
                        blocks.append(block)
                    block = None
                    blocks.extend(get_blocks_in_node(child, max_lines))
                elif block is None:
                    block = (child.start_point.row, child.end_point.row + 1)
                elif child.end_point.row + 1 - block[0] <= max_lines:
                    block = (block[0], child.end_point.row + 1)
                else:
                    blocks.append(block)
                    block = (child.start_point.row, child.end_point.row + 1)

            if block is not None:
                blocks.append(block)

            return blocks

        # change line number to start from 1 and [start, end) to [start, end]
        return [(a + 1, b) for a, b in get_blocks_in_node(tree.root_node, max_lines)]

    def __str__(self) -> str:
        return f"{self.path}"


class Repo(EvolvableSubjects):
    def __init__(self, project_path: Path | str, excludes: list[Path] | None = None, **kwargs: Any) -> None:
        if excludes is None:
            excludes = []
        self.params = kwargs
        self.project_path = Path(project_path)

        excludes = [self.project_path / path for path in excludes]

        git_ignored_output = subprocess.check_output(
            ["/usr/bin/git", "status", "--ignored", "-s"],  # noqa: S603
            cwd=str(self.project_path),
            stderr=subprocess.STDOUT,
            text=True,
        )
        git_ignored_files = [
            (self.project_path / Path(line[3:])).resolve()
            for line in git_ignored_output.split("\n")
            if line.startswith("!!")
        ]

        excludes.extend(git_ignored_files)

        files = [
            file
            for file in self.project_path.glob("**/*")
            if file.is_file()
            and not any(str(file).startswith(str(path)) for path in excludes)
            and ".git/" not in str(file)
            and file.suffix == ".py"
        ]
        self.files = {file: CodeFile(file) for file in files}

        self.fix_records: dict[str, FixRecord] | None = None


@dataclass
class RuffRule:
    """
    Example:
    {
        "name": "missing-trailing-comma",
        "code": "COM812",
        "linter": "flake8-commas",
        "summary": "Trailing comma missing",
        "message_formats": [
            "Trailing comma missing"
        ],
        "fix": "Fix is always available.",
        "explanation": "...",
        "preview": false
    }
    """

    name: str
    code: str
    linter: str
    summary: str
    message_formats: list[str]
    fix: str
    explanation: str
    preview: bool


class RuffEvaluator(Evaluator):
    """
    The error message are generated by command
    """

    def __init__(self, command: str | None = None) -> None:
        if command is None:
            self.command = "ruff check . --output-format full"
        else:
            self.command = command

    @staticmethod
    def explain_rule(error_code: str) -> RuffRule:
        explain_command = f"ruff rule {error_code} --output-format json"
        try:
            out = subprocess.check_output(
                shlex.split(explain_command),  # noqa: S603
                stderr=subprocess.STDOUT,
                text=True,
            )
        except subprocess.CalledProcessError as e:
            out = e.output

        return RuffRule(**json.loads(out))

    def evaluate(self, evo: Repo, **kwargs: dict) -> CIFeedback:
        """Simply run ruff to get the feedbacks."""
        try:
            out = subprocess.check_output(
                shlex.split(self.command),  # noqa: S603
                cwd=evo.project_path,
                stderr=subprocess.STDOUT,
                text=True,
            )
        except subprocess.CalledProcessError as e:
            out = e.output

        """ruff output format:
        rdagent/cli.py:9:5: ANN201 Missing return type annotation for public function `main`
        |
        9 | def main(prompt=None):
        |     ^^^^ ANN201
        10 |     load_dotenv(verbose=True, override=True)
        11 |     wm = WorkflowManager()
        |
        = help: Add return type annotation: `None`
        """

        # extract error info
        pattern = r"(([^\n]*):(\d+):(\d+): (\w+) ([^\n]*)\n(.*?))\n\n"
        matches = re.findall(pattern, out, re.DOTALL)

        errors = defaultdict(list)

        for match in matches:
            raw_str, file_path, line_number, column_number, error_code, error_message, error_hint = match

            # TODO @bowen: filter these files when running the check command
            if evo.project_path / Path(file_path) not in evo.files:
                continue
            error = CIError(
                raw_str=raw_str,
                file_path=file_path,
                line=int(line_number),
                column=int(column_number),
                code=error_code,
                msg=error_message,
                hint=error_hint,
                checker="ruff",
            )

            errors[file_path].append(error)

        return CIFeedback(errors=errors)


class MypyEvaluator(Evaluator):
    def __init__(self, command: str | None = None) -> None:
        if command is None:
            self.command = "mypy . --pretty --no-error-summary --show-column-numbers"
        else:
            self.command = command

    def evaluate(self, evo: Repo, **kwargs: dict) -> CIFeedback:
        try:
            out = subprocess.check_output(
                shlex.split(self.command),  # noqa: S603
                cwd=evo.project_path,
                stderr=subprocess.STDOUT,
                text=True,
            )
        except subprocess.CalledProcessError as e:
            out = e.output

        errors = defaultdict(list)

        out = re.sub(r"([^\n]*?:\d+:\d+): error:", r"\n\1: error:", out)
        out += "\n"
        pattern = r"(([^\n]*?):(\d+):(\d+): error:(.*?)\s\[([\w-]*?)\]\s(.*?))\n\n"
        for match in re.findall(pattern, out, re.DOTALL):
            raw_str, file_path, line_number, column_number, error_message, error_code, error_hint = match
            error_message = error_message.strip().replace("\n", " ")
            if re.match(r".*[^\n]*?:\d+:\d+: note:.*", error_hint, flags=re.DOTALL) is not None:
                error_hint_position = re.split(
                    pattern=r"[^\n]*?:\d+:\d+: note:",
                    string=error_hint,
                    maxsplit=1,
                    flags=re.DOTALL,
                )[0]
                error_hint_help = re.findall(r"^.*?:\d+:\d+: note: (.*)$", error_hint, flags=re.MULTILINE)
                error_hint_help = "\n".join(error_hint_help)
                error_hint = f"{error_hint_position}\nHelp:\n{error_hint_help}"

            if evo.project_path / Path(file_path) not in evo.files:
                continue
            error = CIError(
                raw_str=raw_str,
                file_path=file_path,
                line=int(line_number),
                column=int(column_number),
                code=error_code,
                msg=error_message,
                hint=error_hint,
                checker="mypy",
            )

            errors[file_path].append(error)

        return CIFeedback(errors=errors)


class MultiEvaluator(Evaluator):
    def __init__(self, *evaluators: Evaluator) -> None:
        self.evaluators = evaluators

    def evaluate(self, evo: Repo, **kwargs: dict) -> CIFeedback:
        all_errors = defaultdict(list)
        for evaluator in self.evaluators:
            feedback: CIFeedback = evaluator.evaluate(evo, **kwargs)
            for file_path, errors in feedback.errors.items():
                all_errors[file_path].extend(errors)

        # sort errors by position
        for file_path in all_errors:
            all_errors[file_path].sort(key=lambda x: (x.line, x.column))

        return CIFeedback(errors=all_errors)


class CIEvoStr(EvolvingStrategy):
    def evolve(  # noqa: C901, PLR0912, PLR0915
        self,
        evo: Repo,
        evolving_trace: list[EvoStep] | None = None,
        knowledge_l: list[Knowledge] | None = None,
        **kwargs: dict,
    ) -> Repo:
        @dataclass
        class CodeFixGroup:
            start_line: int
            end_line: int
            errors: list[CIError]
            session_id: str
            responses: list[str]

        api = APIBackend()
        system_prompt = CI_prompts["linting_system_prompt_template"].format(language="Python")

        if len(evolving_trace) > 0:
            last_feedback: CIFeedback = evolving_trace[-1].feedback

            # print statistics
            checker_error_counts = {
                checker: sum(c_statistics.values()) for checker, c_statistics in last_feedback.statistics().items()
            }
            print(
                f"Found [red]{sum(checker_error_counts.values())}[/red] errors, "
                "including: "
                + ", ".join(
                    f"[red]{count}[/red] [magenta]{checker}[/magenta] errors"
                    for checker, count in checker_error_counts.items()
                ),
            )

            fix_records: dict[str, FixRecord] = defaultdict(
                lambda: FixRecord([], [], [], defaultdict(list)),
            )

            # Group errors by code blocks
            fix_groups: dict[str, list[CodeFixGroup]] = defaultdict(list)
            changes: dict[str, list[tuple[int, int, str]]] = defaultdict(list)
            for file_path, errors in last_feedback.errors.items():
                file = evo.files[evo.project_path / Path(file_path)]

                # check if the file needs to add `from __future__ import annotations`
                # need to add rules here for different languages/tools
                # TODO @bowen: current way of handling errors like 'Add import statement' may be not good
                for error in errors:
                    if error.code in ("FA100", "FA102"):
                        changes[file_path].append((1, 1, "from __future__ import annotations\n"))
                        break

                # Group errors by code blocks
                error_p = 0
                for start_line, end_line in file.get_code_blocks(max_lines=30):
                    group_errors: list[CIError] = []

                    # collect errors in the same code block
                    while error_p < len(errors) and start_line <= errors[error_p].line <= end_line:
                        if errors[error_p].code not in ("FA100", "FA102"):
                            group_errors.append(errors[error_p])
                        error_p += 1

                    # process errors in the code block
                    if group_errors:
                        session = api.build_chat_session(session_system_prompt=system_prompt)
                        session_id = session.get_conversation_id()
                        session.build_chat_completion(
                            CI_prompts["session_start_template"].format(code=file.get(add_line_number=True)),
                        )

                        fix_groups[file_path].append(
                            CodeFixGroup(start_line, end_line, group_errors, session_id, []),
                        )

            # Fix errors in each code block
            with Progress(SpinnerColumn(), *Progress.get_default_columns(), TimeElapsedColumn()) as progress:
                group_counts = sum([len(groups) for groups in fix_groups.values()])
                task_id = progress.add_task("Fixing repo...", total=group_counts)

                for file_path in fix_groups:
                    file = evo.files[evo.project_path / Path(file_path)]
                    for code_fix_g in fix_groups[file_path]:
                        start_line = code_fix_g.start_line
                        end_line = code_fix_g.end_line
                        group_errors = code_fix_g.errors
                        code_snippet_with_lineno = file.get(
                            start_line,
                            end_line,
                            add_line_number=True,
                            return_list=False,
                        )
                        errors_str = "\n\n".join(str(e) for e in group_errors)

                        # ask LLM to repair current code snippet
                        user_prompt = CI_prompts["session_normal_template"].format(
                            code=code_snippet_with_lineno,
                            lint_info=errors_str,
                            start_line=start_line,
                            end_line=end_line,
                            start_lineno=start_line,
                        )

                        session = api.build_chat_session(conversation_id=code_fix_g.session_id)
                        res = session.build_chat_completion(user_prompt)
                        code_fix_g.responses.append(res)
                        progress.update(
                            task_id,
                            description=f"[green]Fixing[/green] [cyan]{file_path}[/cyan]...",
                            advance=1,
                        )

            # Manual inspection and repair
            for file_path in last_feedback.errors:
                print(
                    Rule(
                        f"[bright_blue]Checking[/bright_blue] [cyan]{file_path}[/cyan]",
                        style="bright_blue",
                        align="left",
                        characters=".",
                    ),
                )

                file = evo.files[evo.project_path / Path(file_path)]

                # generate changes
                for group_id, code_fix_g in enumerate(fix_groups[file_path], start=1):
                    start_line, end_line, group_errors = code_fix_g.start_line, code_fix_g.end_line, code_fix_g.errors
                    session = api.build_chat_session(conversation_id=code_fix_g.session_id)

                    print(f"[yellow]Checking part {group_id}...[/yellow]")

                    front_context = file.get(start_line - 3, start_line - 1)
                    rear_context = file.get(end_line + 1, end_line + 3)
                    front_context_with_lineno = file.get(start_line - 3, start_line - 1, add_line_number=True)
                    rear_context_with_lineno = file.get(end_line + 1, end_line + 3, add_line_number=True)

                    code_snippet_with_lineno = file.get(start_line, end_line, add_line_number=True, return_list=False)

                    # print errors
                    printed_errors_str = "\n".join(
                        [
                            f"[{error.checker}] {error.line: >{file.lineno_width}}:{error.column: <4}"
                            f" {error.code}  {error.msg}"
                            for error in group_errors
                        ],
                    )
                    print(
                        Panel.fit(
                            Syntax(printed_errors_str, lexer="python", background_color="default"),
                            title=f"{len(group_errors)} Errors",
                        ),
                    )

                    # print original code
                    table = Table(show_header=False, box=None)
                    table.add_column()
                    table.add_row(Syntax(front_context_with_lineno, lexer="python", background_color="default"))
                    table.add_row(Rule(style="dark_orange"))
                    table.add_row(Syntax(code_snippet_with_lineno, lexer="python", background_color="default"))
                    table.add_row(Rule(style="dark_orange"))
                    table.add_row(Syntax(rear_context_with_lineno, lexer="python", background_color="default"))
                    print(Panel.fit(table, title="Original Code"))

                    res = code_fix_g.responses[0]
                    code_snippet_lines = file.get(start_line, end_line, add_line_number=False, return_list=True)

                    while True:
                        try:
                            new_code = re.search(r".*```[Pp]ython\n(.*?)\n```.*", res, re.DOTALL).group(1)
                        except (re.error, AttributeError) as exc:
                            print(f"[red]Error when extract codes[/red]:\n {res}\nException: {exc}")
                        try:
                            fixed_errors_info = re.search(r".*```[Jj]son\n(.*?)\n```.*", res, re.DOTALL).group(1)
                            fixed_errors_info = json.loads(fixed_errors_info)
                        except AttributeError:
                            fixed_errors_info = None
                        except (json.JSONDecodeError, re.error) as exc:
                            fixed_errors_info = None
                            print(f"[red]Error when extracting fixed_errors[/red]: {exc}")

                        new_code = CodeFile.remove_line_number(new_code)

                        # print repair status (code diff)
                        diff = ndiff(code_snippet_lines, new_code.split("\n"))

                        # add 2 spaces to align with diff format
                        front_context = re.sub(r"^", "  ", front_context, flags=re.MULTILINE)
                        rear_context = re.sub(r"^", "  ", rear_context, flags=re.MULTILINE)

                        table = Table(show_header=False, box=None)
                        table.add_column()
                        table.add_column()
                        table.add_column()
                        table.add_row("", "", Syntax(front_context, lexer="python", background_color="default"))
                        table.add_row("", "", Rule(style="dark_orange"))
                        diff_original_lineno = start_line
                        diff_new_lineno = start_line
                        for i in diff:
                            if i.startswith("+"):
                                table.add_row(
                                    "",
                                    Text(str(diff_new_lineno), style="green bold"),
                                    Text(i, style="green"),
                                )
                                diff_new_lineno += 1
                            elif i.startswith("-"):
                                table.add_row(
                                    Text(str(diff_original_lineno), style="red bold"),
                                    "",
                                    Text(i, style="red"),
                                )
                                diff_original_lineno += 1
                            elif i.startswith("?"):
                                table.add_row("", "", Text(i, style="yellow"))
                            else:
                                table.add_row(
                                    str(diff_original_lineno),
                                    str(diff_new_lineno),
                                    Syntax(i, lexer="python", background_color="default"),
                                )
                                diff_original_lineno += 1
                                diff_new_lineno += 1
                        table.add_row("", "", Rule(style="dark_orange"))
                        table.add_row("", "", Syntax(rear_context, lexer="python", background_color="default"))
                        print(Panel.fit(table, title="Repair Status"))

                        operation = Prompt.ask(
                            "Input your operation [ [red]([bold]s[/bold])kip[/red] / "
                            "[green]([bold]a[/bold])pply[/green] / "
                            "[yellow]manual instruction[/yellow] ]",
                        )
                        print()
                        if operation in ("s", "skip"):
                            fix_records[file_path].skipped_errors.extend(group_errors)
                            break
                        if operation in ("a", "apply"):
                            if fixed_errors_info:
                                fixed_errors_str = "\n".join(fixed_errors_info["errors"])
                                for error in group_errors:
                                    if f"{error.line}:{error.column}" in fixed_errors_str:
                                        fix_records[file_path].manually_fixed_errors.append(error)
                                    else:
                                        fix_records[file_path].skipped_errors.append(error)
                            else:
                                fix_records[file_path].directly_fixed_errors.extend(group_errors)

                            changes[file_path].append((start_line, end_line, new_code))
                            break

                        fix_records[file_path].manual_instructions[operation].extend(group_errors)
                        res = session.build_chat_completion(
                            CI_prompts["session_manual_template"].format(operation=operation),
                        )
                        code_fix_g.responses.append(res)

                # apply changes
                file.apply_changes(changes[file_path])

            evo.fix_records = fix_records

        return evo


class CIEvoAgent(EvoAgent):
    def __init__(self, evolving_strategy: CIEvoStr) -> None:
        super().__init__(max_loop=1, evolving_strategy=evolving_strategy)
        self.evolving_trace = []

    def multistep_evolve(self, evo: Repo, eva: Evaluator) -> Repo:
        evo = self.evolving_strategy.evolve(
            evo=evo,
            evolving_trace=self.evolving_trace,
        )

        self.evolving_trace.append(EvoStep(evo, feedback=eva.evaluate(evo)))

        return evo


DIR = None
while DIR is None or not DIR.exists():
    DIR = Prompt.ask("Please input the [cyan]project directory[/cyan]")
    DIR = Path(DIR)

excludes = Prompt.ask(
    "Input the [dark_orange]excluded directories[/dark_orange] (relative to "
    "[cyan]project path[/cyan] and separated by whitespace)",
).split(" ")
excludes = [Path(exclude.strip()) for exclude in excludes if exclude.strip() != ""]

start_time = time.time()
start_timestamp = datetime.datetime.now(datetime.timezone.utc).strftime("%m%d%H%M")

repo = Repo(DIR, excludes=excludes)
# evaluator = MultiEvaluator(MypyEvaluator(), RuffEvaluator())
evaluator = RuffEvaluator()
estr = CIEvoStr()
ea = CIEvoAgent(estr)
ea.multistep_evolve(repo, evaluator)
while True:
    print(Rule(f"Round {len(ea.evolving_trace)} repair", style="blue"))
    repo: Repo = ea.multistep_evolve(repo, evaluator)

    fix_records = repo.fix_records
    filename = f"{DIR.name}_{start_timestamp}_round_{len(ea.evolving_trace)}_fix_records.json"
    with Path(filename).open("w") as file:
        json.dump({k: v.to_dict() for k, v in fix_records.items()}, file, indent=4)

    # Count the number of skipped errors
    skipped_errors_count = 0
    directly_fixed_errors_count = 0
    manually_fixed_errors_count = 0
    skipped_errors_code_count = defaultdict(int)
    directly_fixed_errors_code_count = defaultdict(int)
    manually_fixed_errors_code_count = defaultdict(int)
    code_message = defaultdict(str)
    for record in fix_records.values():
        skipped_errors_count += len(record.skipped_errors)
        directly_fixed_errors_count += len(record.directly_fixed_errors)
        manually_fixed_errors_count += len(record.manually_fixed_errors)
        for error in record.skipped_errors:
            skipped_errors_code_count[error.code] += 1
            code_message[error.code] = error.msg
        for error in record.directly_fixed_errors:
            directly_fixed_errors_code_count[error.code] += 1
            code_message[error.code] = error.msg
        for error in record.manually_fixed_errors:
            manually_fixed_errors_code_count[error.code] += 1
            code_message[error.code] = error.msg

    skipped_errors_statistics = ""
    directly_fixed_errors_statistics = ""
    manually_fixed_errors_statistics = ""
    for code, count in sorted(skipped_errors_code_count.items(), key=lambda x: x[1], reverse=True):
        skipped_errors_statistics += f"{count: >5} {code: >10} {code_message[code]}\n"
    for code, count in sorted(directly_fixed_errors_code_count.items(), key=lambda x: x[1], reverse=True):
        directly_fixed_errors_statistics += f"{count: >5} {code: >10} {code_message[code]}\n"
    for code, count in sorted(manually_fixed_errors_code_count.items(), key=lambda x: x[1], reverse=True):
        manually_fixed_errors_statistics += f"{count: >5} {code: >10} {code_message[code]}\n"

    # Create a table to display the counts and ratios
    table = Table(title="Error Fix Statistics")
    table.add_column("Type")
    table.add_column("Statistics")
    table.add_column("Count")
    table.add_column("Ratio")

    total_errors_count = skipped_errors_count + directly_fixed_errors_count + manually_fixed_errors_count
    table.add_row("Total Errors", "", Text(str(total_errors_count), style="cyan"), "")
    table.add_row(
        Text("Skipped Errors", style="red"),
        skipped_errors_statistics,
        Text(str(skipped_errors_count), style="red"),
        Text(f"{skipped_errors_count / total_errors_count:.2%}"),
        style="red",
    )
    table.add_row(
        Text("Directly Fixed Errors", style="green"),
        directly_fixed_errors_statistics,
        Text(str(directly_fixed_errors_count), style="green"),
        Text(f"{directly_fixed_errors_count / total_errors_count:.2%}"),
        style="green",
    )
    table.add_row(
        Text("Manually Fixed Errors", style="yellow"),
        manually_fixed_errors_statistics,
        Text(str(manually_fixed_errors_count), style="yellow"),
        Text(f"{manually_fixed_errors_count / total_errors_count:.2%}"),
        style="yellow",
    )

    print(table)
    operation = Prompt.ask("Start next round? (y/n)", choices=["y", "n"])
    if operation == "n":
        break


end_time = time.time()
execution_time = end_time - start_time
print(f"Execution time: {execution_time} seconds")

""" Please commit it by hand... and then run the next round
git add -u
git commit --no-verify  -v
"""



================================================
File: rdagent/app/benchmark/factor/analysis.py
================================================
import json
import pickle
from pathlib import Path

import fire
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

from rdagent.components.benchmark.conf import BenchmarkSettings
from rdagent.components.benchmark.eval_method import FactorImplementEval


class BenchmarkAnalyzer:
    def __init__(self, settings, only_correct_format=False):
        self.settings = settings
        self.index_map = self.load_index_map()
        self.only_correct_format = only_correct_format

    def load_index_map(self):
        index_map = {}
        with open(self.settings.bench_data_path, "r") as file:
            factor_dict = json.load(file)
        for factor_name, data in factor_dict.items():
            index_map[factor_name] = (factor_name, data["Category"], data["Difficulty"])
        return index_map

    def load_data(self, file_path):
        file_path = Path(file_path)
        if not (file_path.is_file() and file_path.suffix == ".pkl"):
            raise ValueError("Invalid file path")

        with file_path.open("rb") as f:
            res = pickle.load(f)

        return res

    def process_results(self, results):
        final_res = {}
        for experiment, path in results.items():
            data = self.load_data(path)
            summarized_data = FactorImplementEval.summarize_res(data)
            processed_data = self.analyze_data(summarized_data)
            final_res[experiment] = processed_data.iloc[-1, :]
        return final_res

    def reformat_index(self, display_df):
        """
        reform the results from

        .. code-block:: python

                              success rate
            High_Beta_Factor           0.2

        to

        .. code-block:: python

                                                    success rate
            Category Difficulty Factor
            量价       Hard       High_Beta_Factor           0.2

        """
        new_idx = []
        display_df = display_df[display_df.index.isin(self.index_map.keys())]
        for idx in display_df.index:
            new_idx.append(self.index_map[idx])

        display_df.index = pd.MultiIndex.from_tuples(
            new_idx,
            names=["Factor", "Category", "Difficulty"],
        )
        display_df = display_df.swaplevel(0, 2).swaplevel(0, 1).sort_index(axis=0)

        return display_df.sort_index(
            key=lambda x: [{"Easy": 0, "Medium": 1, "Hard": 2, "New Discovery": 3}.get(i, i) for i in x]
        )

    def result_all_key_order(self, x):
        order_v = []
        for i in x:
            order_v.append(
                {
                    "Avg Run SR": 0,
                    "Avg Format SR": 1,
                    "Avg Correlation": 2,
                    "Max Correlation": 3,
                    "Max Accuracy": 4,
                    "Avg Accuracy": 5,
                }.get(i, i),
            )
        return order_v

    def analyze_data(self, sum_df):
        index = [
            "FactorSingleColumnEvaluator",
            "FactorRowCountEvaluator",
            "FactorIndexEvaluator",
            "FactorEqualValueRatioEvaluator",
            "FactorCorrelationEvaluator",
            "run factor error",
        ]
        sum_df = sum_df.reindex(index, axis=0)
        sum_df_clean = sum_df.T.groupby(level=0).apply(lambda x: x.reset_index(drop=True))

        run_error = sum_df_clean["run factor error"].unstack().T.fillna(False).astype(bool)
        succ_rate = ~run_error
        succ_rate = succ_rate.mean(axis=0).to_frame("success rate")

        succ_rate_f = self.reformat_index(succ_rate)

        # if it rasis Error when running the evaluator, we will get NaN
        # Running failures are reguarded to zero score.
        format_issue = sum_df_clean[["FactorRowCountEvaluator", "FactorIndexEvaluator"]].apply(
            lambda x: np.mean(x.fillna(0.0)), axis=1
        )
        format_succ_rate = format_issue.unstack().T.mean(axis=0).to_frame("success rate")
        format_succ_rate_f = self.reformat_index(format_succ_rate)

        corr = sum_df_clean["FactorCorrelationEvaluator"].fillna(0.0)
        if self.only_correct_format:
            corr = corr.loc[format_issue == 1.0]

        corr_res = corr.unstack().T.mean(axis=0).to_frame("corr(only success)")
        corr_res = self.reformat_index(corr_res)

        corr_max = corr.unstack().T.max(axis=0).to_frame("corr(only success)")
        corr_max_res = self.reformat_index(corr_max)

        value_max = sum_df_clean["FactorEqualValueRatioEvaluator"]
        value_max = value_max.unstack().T.max(axis=0).to_frame("max_value")
        value_max_res = self.reformat_index(value_max)

        value_avg = (
            (sum_df_clean["FactorEqualValueRatioEvaluator"] * format_issue)
            .unstack()
            .T.mean(axis=0)
            .to_frame("avg_value")
        )
        value_avg_res = self.reformat_index(value_avg)

        result_all = pd.concat(
            {
                "Avg Correlation": corr_res.iloc[:, 0],
                "Avg Format SR": format_succ_rate_f.iloc[:, 0],
                "Avg Run SR": succ_rate_f.iloc[:, 0],
                "Max Correlation": corr_max_res.iloc[:, 0],
                "Max Accuracy": value_max_res.iloc[:, 0],
                "Avg Accuracy": value_avg_res.iloc[:, 0],
            },
            axis=1,
        )

        df = result_all.sort_index(axis=1, key=self.result_all_key_order).sort_index(axis=0)
        print(df)

        print()
        print(df.groupby("Category").mean())

        print()
        print(df.mean())

        # Calculate the mean of each column
        mean_values = df.fillna(0.0).mean()
        mean_df = pd.DataFrame(mean_values).T

        # Assign the MultiIndex to the DataFrame
        mean_df.index = pd.MultiIndex.from_tuples([("-", "-", "Average")], names=["Factor", "Category", "Difficulty"])

        # Append the mean values to the end of the dataframe
        df_w_mean = pd.concat([df, mean_df]).astype("float")

        return df_w_mean


class Plotter:
    @staticmethod
    def change_fs(font_size):
        plt.rc("font", size=font_size)
        plt.rc("axes", titlesize=font_size)
        plt.rc("axes", labelsize=font_size)
        plt.rc("xtick", labelsize=font_size)
        plt.rc("ytick", labelsize=font_size)
        plt.rc("legend", fontsize=font_size)
        plt.rc("figure", titlesize=font_size)

    @staticmethod
    def plot_data(data, file_name, title):
        plt.figure(figsize=(10, 10))
        plt.ylabel("Value")
        colors = ["#3274A1", "#E1812C", "#3A923A", "#C03D3E"]
        plt.bar(data["a"], data["b"], color=colors, capsize=5)
        for idx, row in data.iterrows():
            plt.text(idx, row["b"] + 0.01, f"{row['b']:.2f}", ha="center", va="bottom")
        plt.suptitle(title, y=0.98)
        plt.xticks(rotation=45)
        plt.ylim(0, 1)
        plt.tight_layout()
        plt.savefig(file_name)


def main(
    path="git_ignore_folder/eval_results/res_promptV220240724-060037.pkl",
    round=1,
    title="Comparison of Different Methods",
    only_correct_format=False,
):
    settings = BenchmarkSettings()
    benchmark = BenchmarkAnalyzer(settings, only_correct_format=only_correct_format)
    results = {
        f"{round} round experiment": path,
    }
    final_results = benchmark.process_results(results)
    final_results_df = pd.DataFrame(final_results)

    Plotter.change_fs(20)
    plot_data = final_results_df.drop(["Max Accuracy", "Avg Accuracy"], axis=0).T
    plot_data = plot_data.reset_index().melt("index", var_name="a", value_name="b")
    Plotter.plot_data(plot_data, "./comparison_plot.png", title)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/benchmark/factor/eval.py
================================================
from rdagent.app.qlib_rd_loop.conf import FACTOR_PROP_SETTING
from rdagent.components.benchmark.conf import BenchmarkSettings
from rdagent.components.benchmark.eval_method import FactorImplementEval
from rdagent.core.scenario import Scenario
from rdagent.core.utils import import_class
from rdagent.log import rdagent_logger as logger
from rdagent.scenarios.qlib.factor_experiment_loader.json_loader import (
    FactorTestCaseLoaderFromJsonFile,
)

if __name__ == "__main__":
    # 1.read the settings
    bs = BenchmarkSettings()

    # 2.read and prepare the eval_data
    test_cases = FactorTestCaseLoaderFromJsonFile().load(bs.bench_data_path)

    # 3.declare the method to be tested and pass the arguments.

    scen: Scenario = import_class(FACTOR_PROP_SETTING.scen)()
    generate_method = import_class(bs.bench_method_cls)(scen=scen, **bs.bench_method_extra_kwargs)
    # 4.declare the eval method and pass the arguments.
    eval_method = FactorImplementEval(
        method=generate_method,
        test_cases=test_cases,
        scen=scen,
        catch_eval_except=True,
        test_round=bs.bench_test_round,
    )

    # 5.run the eval
    res = eval_method.eval(eval_method.develop())

    # 6.save the result
    logger.log_object(res)



================================================
File: rdagent/app/benchmark/model/README.md
================================================
# Tasks

## Task Extraction
From paper to task.
```bash
# python rdagent/app/model_implementation/task_extraction.py
# It may based on rdagent/document_reader/document_reader.py
python rdagent/components/task_implementation/model_implementation/task_extraction.py ./PaperImpBench/raw_paper/
```

## Complete workflow
From paper to implementation
``` bash
# Similar to
# rdagent/app/factor_extraction_and_implementation/factor_extract_and_implement.py
```

## Paper benchmark
```bash
# TODO: it does not work well now.
python rdagent/app/model_implementation/eval.py
```

TODO:
- Create reasonable benchmark
  - with uniform input
  - manually create task
- Create reasonable evaluation metrics

## Evolving



================================================
File: rdagent/app/benchmark/model/eval.py
================================================
from pathlib import Path

from rdagent.components.coder.model_coder import ModelCoSTEER
from rdagent.components.loader.task_loader import ModelTaskLoaderJson, ModelWsLoader
from rdagent.scenarios.qlib.experiment.model_experiment import (
    QlibModelExperiment,
    QlibModelScenario,
)

if __name__ == "__main__":
    DIRNAME = Path(__file__).absolute().resolve().parent

    from rdagent.components.coder.model_coder.benchmark.eval import ModelImpValEval
    from rdagent.components.coder.model_coder.one_shot import ModelCodeWriter

    bench_folder = DIRNAME.parent.parent.parent / "components" / "coder" / "model_coder" / "benchmark"
    mtl = ModelTaskLoaderJson(str(bench_folder / "model_dict.json"))

    task_l = mtl.load()

    task_l = [t for t in task_l if t.name == "A-DGN"]  # FIXME: other models does not work well

    model_experiment = QlibModelExperiment(sub_tasks=task_l)
    # mtg = ModelCodeWriter(scen=QlibModelScenario())
    mtg = ModelCoSTEER(scen=QlibModelScenario())

    model_experiment = mtg.develop(model_experiment)

    # TODO: Align it with the benchmark framework after @wenjun's refine the evaluation part.
    # Currently, we just handcraft a workflow for fast evaluation.

    mil = ModelWsLoader(bench_folder / "gt_code")

    mie = ModelImpValEval()
    # Evaluation:
    eval_l = []
    for impl in model_experiment.sub_workspace_list:
        print(impl.target_task)
        gt_impl = mil.load(impl.target_task)
        eval_l.append(mie.evaluate(gt_impl, impl))

    print(eval_l)



================================================
File: rdagent/app/data_mining/conf.py
================================================
from pathlib import Path

from pydantic_settings import SettingsConfigDict

from rdagent.components.workflow.conf import BasePropSetting


class MedBasePropSetting(BasePropSetting):
    model_config = SettingsConfigDict(env_prefix="DM_", protected_namespaces=())

    # 1) overriding the default
    scen: str = "rdagent.scenarios.data_mining.experiment.model_experiment.DMModelScenario"
    """Scenario class for data mining model"""

    hypothesis_gen: str = "rdagent.scenarios.data_mining.proposal.model_proposal.DMModelHypothesisGen"
    """Hypothesis generation class"""

    hypothesis2experiment: str = "rdagent.scenarios.data_mining.proposal.model_proposal.DMModelHypothesis2Experiment"
    """Hypothesis to experiment class"""

    coder: str = "rdagent.scenarios.data_mining.developer.model_coder.DMModelCoSTEER"
    """Coder class"""

    runner: str = "rdagent.scenarios.data_mining.developer.model_runner.DMModelRunner"
    """Runner class"""

    summarizer: str = "rdagent.scenarios.data_mining.developer.feedback.DMModelExperiment2Feedback"
    """Summarizer class"""

    evolving_n: int = 10
    """Number of evolutions"""

    evolving_n: int = 10

    # 2) Extra config for the scenario
    # physionet account
    # NOTE: You should apply the account in https://physionet.org/
    username: str = ""
    """Physionet account username"""

    password: str = ""
    """Physionet account password"""


MED_PROP_SETTING = MedBasePropSetting()



================================================
File: rdagent/app/data_mining/model.py
================================================
import fire

from rdagent.app.data_mining.conf import MED_PROP_SETTING
from rdagent.components.workflow.rd_loop import RDLoop
from rdagent.core.exception import ModelEmptyError


class ModelRDLoop(RDLoop):
    skip_loop_error = (ModelEmptyError,)


def main(path=None, step_n=None):
    """
    Auto R&D Evolving loop for models in a medical scenario.

    You can continue running session by

    .. code-block:: python

        dotenv run -- python rdagent/app/data_mining/model.py $LOG_PATH/__session__/1/0_propose  --step_n 1   # `step_n` is a optional paramter

    """
    if path is None:
        model_loop = ModelRDLoop(MED_PROP_SETTING)
    else:
        model_loop = ModelRDLoop.load(path)
    model_loop.run(step_n=step_n)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/data_science/conf.py
================================================
from pydantic_settings import SettingsConfigDict

from rdagent.app.kaggle.conf import KaggleBasePropSetting


class DataScienceBasePropSetting(KaggleBasePropSetting):
    model_config = SettingsConfigDict(env_prefix="DS_", protected_namespaces=())

    # Main components
    ## Scen
    scen: str = "rdagent.scenarios.data_science.scen.KaggleScen"
    """Scenario class for data mining model"""

    ## Workflow Related
    consecutive_errors: int = 5

    debug_timeout: int = 600
    """The timeout limit for running on debugging data"""
    full_timeout: int = 3600
    """The timeout limit for running on full data"""


DS_RD_SETTING = DataScienceBasePropSetting()



================================================
File: rdagent/app/data_science/debug.py
================================================
import fire

from rdagent.scenarios.data_science.debug.data import create_debug_data

if __name__ == "__main__":
    fire.Fire(create_debug_data)



================================================
File: rdagent/app/data_science/loop.py
================================================
from pathlib import Path
from typing import Any

import fire

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.data_science.ensemble import EnsembleCoSTEER
from rdagent.components.coder.data_science.ensemble.exp import EnsembleTask
from rdagent.components.coder.data_science.feature import FeatureCoSTEER
from rdagent.components.coder.data_science.feature.exp import FeatureTask
from rdagent.components.coder.data_science.model import ModelCoSTEER
from rdagent.components.coder.data_science.model.exp import ModelTask
from rdagent.components.coder.data_science.raw_data_loader import DataLoaderCoSTEER
from rdagent.components.coder.data_science.raw_data_loader.exp import DataLoaderTask
from rdagent.components.coder.data_science.workflow import WorkflowCoSTEER
from rdagent.components.coder.data_science.workflow.exp import WorkflowTask
from rdagent.components.workflow.conf import BasePropSetting
from rdagent.components.workflow.rd_loop import RDLoop
from rdagent.core.exception import CoderError, RunnerError
from rdagent.core.proposal import ExperimentFeedback
from rdagent.core.scenario import Scenario
from rdagent.core.utils import import_class
from rdagent.log import rdagent_logger as logger
from rdagent.scenarios.data_science.dev.feedback import DSExperiment2Feedback
from rdagent.scenarios.data_science.dev.runner import DSCoSTEERRunner
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.proposal.exp_gen import DSExpGen, DSTrace
from rdagent.scenarios.kaggle.kaggle_crawler import download_data


class DataScienceRDLoop(RDLoop):
    skip_loop_error = (CoderError, RunnerError)

    def __init__(self, PROP_SETTING: BasePropSetting):
        logger.log_object(PROP_SETTING.competition, tag="competition")
        scen: Scenario = import_class(PROP_SETTING.scen)(PROP_SETTING.competition)

        ### shared components in the workflow  # TODO: check if
        knowledge_base = (
            import_class(PROP_SETTING.knowledge_base)(PROP_SETTING.knowledge_base_path, scen)
            if PROP_SETTING.knowledge_base != ""
            else None
        )

        # 1) task generation from scratch
        # self.scratch_gen: tuple[HypothesisGen, Hypothesis2Experiment] = DummyHypothesisGen(scen),

        # 2) task generation from a complete solution
        # self.exp_gen: ExpGen = import_class(PROP_SETTING.exp_gen)(scen)
        self.exp_gen = DSExpGen(scen)
        self.data_loader_coder = DataLoaderCoSTEER(scen)
        self.feature_coder = FeatureCoSTEER(scen)
        self.model_coder = ModelCoSTEER(scen)
        self.ensemble_coder = EnsembleCoSTEER(scen)
        self.workflow_coder = WorkflowCoSTEER(scen)

        self.runner = DSCoSTEERRunner(scen)
        # self.summarizer: Experiment2Feedback = import_class(PROP_SETTING.summarizer)(scen)
        # logger.log_object(self.summarizer, tag="summarizer")

        # self.trace = KGTrace(scen=scen, knowledge_base=knowledge_base)
        self.trace = DSTrace(scen=scen)
        self.summarizer = DSExperiment2Feedback(scen)
        super(RDLoop, self).__init__()

    def direct_exp_gen(self, prev_out: dict[str, Any]):
        exp = self.exp_gen.gen(self.trace)
        logger.log_object(exp)

        # FIXME: this is for LLM debug webapp, remove this when the debugging is done.
        logger.log_object(exp, tag="debug_exp_gen")
        return exp

    def coding(self, prev_out: dict[str, Any]):
        exp = prev_out["direct_exp_gen"]
        for tasks in exp.pending_tasks_list:
            exp.sub_tasks = tasks
            if isinstance(exp.sub_tasks[0], DataLoaderTask):
                exp = self.data_loader_coder.develop(exp)
            elif isinstance(exp.sub_tasks[0], FeatureTask):
                exp = self.feature_coder.develop(exp)
            elif isinstance(exp.sub_tasks[0], ModelTask):
                exp = self.model_coder.develop(exp)
            elif isinstance(exp.sub_tasks[0], EnsembleTask):
                exp = self.ensemble_coder.develop(exp)
            elif isinstance(exp.sub_tasks[0], WorkflowTask):
                exp = self.workflow_coder.develop(exp)
            else:
                raise NotImplementedError(f"Unsupported component in DataScienceRDLoop: {exp.hypothesis.component}")
            exp.sub_tasks = []
        logger.log_object(exp)
        return exp

    def running(self, prev_out: dict[str, Any]):
        exp: DSExperiment = prev_out["coding"]
        if exp.is_ready_to_run():
            new_exp = self.runner.develop(exp)
            logger.log_object(new_exp)
            return new_exp
        return exp

    def feedback(self, prev_out: dict[str, Any]) -> ExperimentFeedback:
        """
        Assumption:
        - If we come to feedback phase, the previous development steps are successful.
        """
        exp: DSExperiment = prev_out["running"]
        if self.trace.next_incomplete_component() is None:
            # we have alreadly completed components in previous trace. So current loop is focusing on a new proposed idea.
            # So we need feedback for the proposal.
            feedback = self.summarizer.generate_feedback(exp, self.trace)
        else:
            # Otherwise, it is on drafting stage, don't need complicated feedbacks.
            feedback = ExperimentFeedback(
                reason=f"{exp.hypothesis.component} is completed.",
                decision=True,
            )
        logger.log_object(feedback)
        return feedback

    def record(self, prev_out: dict[str, Any]):
        e = prev_out.get(self.EXCEPTION_KEY, None)
        if e is None:
            self.trace.hist.append((prev_out["running"], prev_out["feedback"]))
        else:
            self.trace.hist.append(
                (
                    prev_out["direct_exp_gen"] if isinstance(e, CoderError) else prev_out["coding"],
                    ExperimentFeedback.from_exception(e),
                )
            )
            if self.trace.sota_experiment() is None and len(self.trace.hist) >= DS_RD_SETTING.consecutive_errors:
                # if {in inital/drafting stage} and {tried enough times}
                for _, fb in self.trace.hist[-DS_RD_SETTING.consecutive_errors :]:
                    if fb:
                        break  # any success will stop restarting.
                else:  # otherwise restart it
                    logger.error("Consecutive errors reached the limit. Dumping trace.")
                    logger.log_object(self.trace, tag="trace before restart")
                    self.trace = DSTrace(scen=self.trace.scen, knowledge_base=self.trace.knowledge_base)
        logger.log_object(self.trace, tag="trace")
        logger.log_object(self.trace.sota_experiment(), tag="SOTA experiment")


def main(
    path=None, output_path=None, step_n=None, loop_n=None, competition="bms-molecular-translation", do_truncate=True
):
    """

    Parameters
    ----------
    path :
        path like `$LOG_PATH/__session__/1/0_propose`. It indicates that we restore the state that after finish the step 0 in loop 1
    output_path :
        path like `$LOG_PATH`. It indicates that where we want to save our session and log information.
    step_n :
        How many steps to run; if None, it will run forever until error or KeyboardInterrupt
    loop_n :
        How many loops to run; if None, it will run forever until error or KeyboardInterrupt
        - if current loop is incomplete, it will be counted as the first loop for completion.
        - if both step_n and loop_n are provided, the process will stop as soon as either condition is met.
    competition :
    do_truncate :
        If set to True, the logger will truncate the future log messages by calling `logger.storage.truncate`.


    Auto R&D Evolving loop for models in a Kaggle scenario.
    You can continue running session by
    .. code-block:: bash
        dotenv run -- python rdagent/app/data_science/loop.py [--competition titanic] $LOG_PATH/__session__/1/0_propose  --step_n 1   # `step_n` is a optional parameter
        rdagent kaggle --competition playground-series-s4e8  # You are encouraged to use this one.
    """
    if competition is not None:
        DS_RD_SETTING.competition = competition

    if DS_RD_SETTING.competition:
        if DS_RD_SETTING.scen.endswith("KaggleScen"):
            download_data(competition=DS_RD_SETTING.competition, settings=DS_RD_SETTING)
        else:
            if not Path(f"{DS_RD_SETTING.local_data_path}/{competition}").exists():
                logger.error(f"Please prepare data for competition {competition} first.")
                return
    else:
        logger.error("Please specify competition name.")
    if path is None:
        kaggle_loop = DataScienceRDLoop(DS_RD_SETTING)
    else:
        kaggle_loop = DataScienceRDLoop.load(path, output_path, do_truncate)
    kaggle_loop.run(step_n=step_n, loop_n=loop_n)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/general_model/general_model.py
================================================
import fire

from rdagent.components.coder.model_coder.task_loader import (
    ModelExperimentLoaderFromPDFfiles,
)
from rdagent.components.document_reader.document_reader import (
    extract_first_page_screenshot_from_pdf,
)
from rdagent.log import rdagent_logger as logger
from rdagent.scenarios.general_model.scenario import GeneralModelScenario
from rdagent.scenarios.qlib.developer.model_coder import QlibModelCoSTEER


def extract_models_and_implement(report_file_path: str) -> None:
    """
    This is a research copilot to automatically implement models from a report file or paper.

    It extracts models from a given PDF report file and implements the necessary operations.

    Parameters:
    report_file_path (str): The path to the report file. The file must be a PDF file.

    Example URLs of PDF reports:
    - https://arxiv.org/pdf/2210.09789
    - https://arxiv.org/pdf/2305.10498
    - https://arxiv.org/pdf/2110.14446
    - https://arxiv.org/pdf/2205.12454
    - https://arxiv.org/pdf/2210.16518

    Returns:
    None
    """
    with logger.tag("init"):
        scenario = GeneralModelScenario()
        logger.log_object(scenario, tag="scenario")
    with logger.tag("r"):
        # Save Relevant Images
        img = extract_first_page_screenshot_from_pdf(report_file_path)
        logger.log_object(img, tag="pdf_image")
        exp = ModelExperimentLoaderFromPDFfiles().load(report_file_path)
        logger.log_object(exp, tag="load_experiment")
    with logger.tag("d"):
        exp = QlibModelCoSTEER(scenario).develop(exp)
        logger.log_object(exp, tag="developed_experiment")


if __name__ == "__main__":
    fire.Fire(extract_models_and_implement)



================================================
File: rdagent/app/kaggle/conf.py
================================================
from pydantic_settings import SettingsConfigDict

from rdagent.core.conf import ExtendedBaseSettings


class KaggleBasePropSetting(ExtendedBaseSettings):
    model_config = SettingsConfigDict(env_prefix="KG_", protected_namespaces=())

    # 1) overriding the default
    scen: str = "rdagent.scenarios.kaggle.experiment.scenario.KGScenario"
    """Scenario class for data mining model"""

    hypothesis_gen: str = "rdagent.scenarios.kaggle.proposal.proposal.KGHypothesisGen"
    """Hypothesis generation class"""

    hypothesis2experiment: str = "rdagent.scenarios.kaggle.proposal.proposal.KGHypothesis2Experiment"
    """Hypothesis to experiment class"""

    feature_coder: str = "rdagent.scenarios.kaggle.developer.coder.KGFactorCoSTEER"
    """Feature Coder class"""

    model_feature_selection_coder: str = "rdagent.scenarios.kaggle.developer.coder.KGModelFeatureSelectionCoder"
    """Model Feature Selection Coder class"""

    model_coder: str = "rdagent.scenarios.kaggle.developer.coder.KGModelCoSTEER"
    """Model Coder class"""

    feature_runner: str = "rdagent.scenarios.kaggle.developer.runner.KGFactorRunner"
    """Feature Runner class"""

    model_runner: str = "rdagent.scenarios.kaggle.developer.runner.KGModelRunner"
    """Model Runner class"""

    summarizer: str = "rdagent.scenarios.kaggle.developer.feedback.KGExperiment2Feedback"
    """Summarizer class"""

    evolving_n: int = 10
    """Number of evolutions"""

    competition: str = ""
    """Kaggle competition name, e.g., 'sf-crime'"""

    template_path: str = "rdagent/scenarios/kaggle/experiment/templates"
    """Kaggle competition base templates path"""

    local_data_path: str = ""
    """Folder storing Kaggle competition data"""

    if_using_mle_data: bool = False
    auto_submit: bool = False
    """Automatically upload and submit each experiment result to Kaggle platform"""
    # Conditionally set the knowledge_base based on the use of graph RAG
    knowledge_base: str = ""
    """Knowledge base class, uses 'KGKnowledgeGraph' when advanced graph-based RAG is enabled, otherwise empty."""
    if_action_choosing_based_on_UCB: bool = False
    """Enable decision mechanism based on UCB algorithm"""

    domain_knowledge_path: str = "/data/userdata/share/kaggle/domain_knowledge"
    """Folder storing domain knowledge files in .case format"""

    knowledge_base_path: str = "kg_graph.pkl"
    """Advanced version of graph-based RAG"""

    rag_path: str = "git_ignore_folder/kaggle_vector_base.pkl"
    """Base version of vector-based RAG"""

    if_using_vector_rag: bool = False
    """Enable basic vector-based RAG"""

    if_using_graph_rag: bool = False
    """Enable advanced graph-based RAG"""

    mini_case: bool = False
    """Enable mini-case study for experiments"""


KAGGLE_IMPLEMENT_SETTING = KaggleBasePropSetting()



================================================
File: rdagent/app/kaggle/loop.py
================================================
import subprocess
from typing import Any

import fire

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.components.workflow.conf import BasePropSetting
from rdagent.components.workflow.rd_loop import RDLoop
from rdagent.core.developer import Developer
from rdagent.core.exception import CoderError, FactorEmptyError, ModelEmptyError
from rdagent.core.proposal import (
    Experiment2Feedback,
    Hypothesis2Experiment,
    HypothesisGen,
)
from rdagent.core.scenario import Scenario
from rdagent.core.utils import import_class
from rdagent.log import rdagent_logger as logger
from rdagent.scenarios.kaggle.experiment.scenario import (
    KG_ACTION_FEATURE_ENGINEERING,
    KG_ACTION_FEATURE_PROCESSING,
    KG_ACTION_MODEL_FEATURE_SELECTION,
)
from rdagent.scenarios.kaggle.experiment.utils import python_files_to_notebook
from rdagent.scenarios.kaggle.kaggle_crawler import download_data
from rdagent.scenarios.kaggle.proposal.proposal import KGTrace


class KaggleRDLoop(RDLoop):
    def __init__(self, PROP_SETTING: BasePropSetting):
        with logger.tag("init"):
            scen: Scenario = import_class(PROP_SETTING.scen)(PROP_SETTING.competition)
            logger.log_object(scen, tag="scenario")
            knowledge_base = (
                import_class(PROP_SETTING.knowledge_base)(PROP_SETTING.knowledge_base_path, scen)
                if PROP_SETTING.knowledge_base != ""
                else None
            )
            logger.log_object(knowledge_base, tag="knowledge_base")
            self.hypothesis_gen: HypothesisGen = import_class(PROP_SETTING.hypothesis_gen)(scen)
            logger.log_object(self.hypothesis_gen, tag="hypothesis generator")
            self.hypothesis2experiment: Hypothesis2Experiment = import_class(PROP_SETTING.hypothesis2experiment)()
            logger.log_object(self.hypothesis2experiment, tag="hypothesis2experiment")
            self.feature_coder: Developer = import_class(PROP_SETTING.feature_coder)(scen)
            logger.log_object(self.feature_coder, tag="feature coder")
            self.model_feature_selection_coder: Developer = import_class(PROP_SETTING.model_feature_selection_coder)(
                scen
            )
            logger.log_object(self.model_feature_selection_coder, tag="model feature selection coder")
            self.model_coder: Developer = import_class(PROP_SETTING.model_coder)(scen)
            logger.log_object(self.model_coder, tag="model coder")
            self.feature_runner: Developer = import_class(PROP_SETTING.feature_runner)(scen)
            logger.log_object(self.feature_runner, tag="feature runner")
            self.model_runner: Developer = import_class(PROP_SETTING.model_runner)(scen)
            logger.log_object(self.model_runner, tag="model runner")
            self.summarizer: Experiment2Feedback = import_class(PROP_SETTING.summarizer)(scen)
            logger.log_object(self.summarizer, tag="summarizer")
            self.trace = KGTrace(scen=scen, knowledge_base=knowledge_base)
            super(RDLoop, self).__init__()

    def coding(self, prev_out: dict[str, Any]):
        with logger.tag("d"):  # develop
            if prev_out["direct_exp_gen"]["propose"].action in [
                KG_ACTION_FEATURE_ENGINEERING,
                KG_ACTION_FEATURE_PROCESSING,
            ]:
                exp = self.feature_coder.develop(prev_out["direct_exp_gen"]["exp_gen"])
            elif prev_out["direct_exp_gen"]["propose"].action == KG_ACTION_MODEL_FEATURE_SELECTION:
                exp = self.model_feature_selection_coder.develop(prev_out["direct_exp_gen"]["exp_gen"])
            else:
                exp = self.model_coder.develop(prev_out["direct_exp_gen"]["exp_gen"])
            logger.log_object(exp.sub_workspace_list, tag="coder result")
        return exp

    def running(self, prev_out: dict[str, Any]):
        with logger.tag("ef"):  # evaluate and feedback
            if prev_out["direct_exp_gen"]["propose"].action in [
                KG_ACTION_FEATURE_ENGINEERING,
                KG_ACTION_FEATURE_PROCESSING,
            ]:
                exp = self.feature_runner.develop(prev_out["coding"])
            else:
                exp = self.model_runner.develop(prev_out["coding"])
            logger.log_object(exp, tag="runner result")
            if KAGGLE_IMPLEMENT_SETTING.competition in [
                "optiver-realized-volatility-prediction",
                "covid19-global-forecasting-week-1",
            ]:
                try:
                    python_files_to_notebook(
                        KAGGLE_IMPLEMENT_SETTING.competition, exp.experiment_workspace.workspace_path
                    )
                except Exception as e:
                    logger.error(f"Merge python files to one file failed: {e}")
            if KAGGLE_IMPLEMENT_SETTING.auto_submit:
                csv_path = exp.experiment_workspace.workspace_path / "submission.csv"
                try:
                    subprocess.run(
                        [
                            "kaggle",
                            "competitions",
                            "submit",
                            "-f",
                            str(csv_path.absolute()),
                            "-m",
                            str(csv_path.parent.absolute()),
                            KAGGLE_IMPLEMENT_SETTING.competition,
                        ],
                        check=True,
                    )
                except subprocess.CalledProcessError as e:
                    logger.error(f"Auto submission failed: \n{e}")
                except Exception as e:
                    logger.error(f"Other exception when use kaggle api:\n{e}")

        return exp

    skip_loop_error = (ModelEmptyError, FactorEmptyError, CoderError)


def main(path=None, step_n=None, competition=None):
    """
    Auto R&D Evolving loop for models in a kaggle{} scenario.
    You can continue running session by
    .. code-block:: bash
        dotenv run -- python rdagent/app/kaggle/loop.py [--competition titanic] $LOG_PATH/__session__/1/0_propose  --step_n 1   # `step_n` is a optional parameter
        rdagent kaggle --competition playground-series-s4e8  # You are encouraged to use this one.
    """
    if competition:
        KAGGLE_IMPLEMENT_SETTING.competition = competition
        download_data(competition=competition, settings=KAGGLE_IMPLEMENT_SETTING)
        if KAGGLE_IMPLEMENT_SETTING.if_using_graph_rag:
            KAGGLE_IMPLEMENT_SETTING.knowledge_base = (
                "rdagent.scenarios.kaggle.knowledge_management.graph.KGKnowledgeGraph"
            )
    else:
        logger.error("Please specify competition name.")
    if path is None:
        kaggle_loop = KaggleRDLoop(KAGGLE_IMPLEMENT_SETTING)
    else:
        kaggle_loop = KaggleRDLoop.load(path)
    kaggle_loop.run(step_n=step_n)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/qlib_rd_loop/conf.py
================================================
from pydantic_settings import SettingsConfigDict

from rdagent.components.workflow.conf import BasePropSetting


class ModelBasePropSetting(BasePropSetting):
    model_config = SettingsConfigDict(env_prefix="QLIB_MODEL_", protected_namespaces=())

    # 1) override base settings
    scen: str = "rdagent.scenarios.qlib.experiment.model_experiment.QlibModelScenario"
    """Scenario class for Qlib Model"""

    hypothesis_gen: str = "rdagent.scenarios.qlib.proposal.model_proposal.QlibModelHypothesisGen"
    """Hypothesis generation class"""

    hypothesis2experiment: str = "rdagent.scenarios.qlib.proposal.model_proposal.QlibModelHypothesis2Experiment"
    """Hypothesis to experiment class"""

    coder: str = "rdagent.scenarios.qlib.developer.model_coder.QlibModelCoSTEER"
    """Coder class"""

    runner: str = "rdagent.scenarios.qlib.developer.model_runner.QlibModelRunner"
    """Runner class"""

    summarizer: str = "rdagent.scenarios.qlib.developer.feedback.QlibModelExperiment2Feedback"
    """Summarizer class"""

    evolving_n: int = 10
    """Number of evolutions"""


class FactorBasePropSetting(BasePropSetting):
    model_config = SettingsConfigDict(env_prefix="QLIB_FACTOR_", protected_namespaces=())

    # 1) override base settings
    scen: str = "rdagent.scenarios.qlib.experiment.factor_experiment.QlibFactorScenario"
    """Scenario class for Qlib Factor"""

    hypothesis_gen: str = "rdagent.scenarios.qlib.proposal.factor_proposal.QlibFactorHypothesisGen"
    """Hypothesis generation class"""

    hypothesis2experiment: str = "rdagent.scenarios.qlib.proposal.factor_proposal.QlibFactorHypothesis2Experiment"
    """Hypothesis to experiment class"""

    coder: str = "rdagent.scenarios.qlib.developer.factor_coder.QlibFactorCoSTEER"
    """Coder class"""

    runner: str = "rdagent.scenarios.qlib.developer.factor_runner.QlibFactorRunner"
    """Runner class"""

    summarizer: str = "rdagent.scenarios.qlib.developer.feedback.QlibFactorExperiment2Feedback"
    """Summarizer class"""

    evolving_n: int = 10
    """Number of evolutions"""


class FactorFromReportPropSetting(FactorBasePropSetting):
    # 1) override the scen attribute
    scen: str = "rdagent.scenarios.qlib.experiment.factor_from_report_experiment.QlibFactorFromReportScenario"
    """Scenario class for Qlib Factor from Report"""

    # 2) sub task specific:
    report_result_json_file_path: str = "git_ignore_folder/report_list.json"
    """Path to the JSON file listing research reports for factor extraction"""

    max_factors_per_exp: int = 10000
    """Maximum number of factors implemented per experiment"""

    is_report_limit_enabled: bool = False
    """Limits report processing count if True; processes all if False"""


FACTOR_PROP_SETTING = FactorBasePropSetting()
FACTOR_FROM_REPORT_PROP_SETTING = FactorFromReportPropSetting()
MODEL_PROP_SETTING = ModelBasePropSetting()



================================================
File: rdagent/app/qlib_rd_loop/factor.py
================================================
"""
Factor workflow with session control
"""

from typing import Any

import fire

from rdagent.app.qlib_rd_loop.conf import FACTOR_PROP_SETTING
from rdagent.components.workflow.rd_loop import RDLoop
from rdagent.core.exception import FactorEmptyError
from rdagent.log import rdagent_logger as logger


class FactorRDLoop(RDLoop):
    skip_loop_error = (FactorEmptyError,)

    def running(self, prev_out: dict[str, Any]):
        with logger.tag("ef"):  # evaluate and feedback
            exp = self.runner.develop(prev_out["coding"])
            if exp is None:
                logger.error(f"Factor extraction failed.")
                raise FactorEmptyError("Factor extraction failed.")
            logger.log_object(exp, tag="runner result")
        return exp


def main(path=None, step_n=None):
    """
    Auto R&D Evolving loop for fintech factors.

    You can continue running session by

    .. code-block:: python

        dotenv run -- python rdagent/app/qlib_rd_loop/factor.py $LOG_PATH/__session__/1/0_propose  --step_n 1   # `step_n` is a optional paramter

    """
    if path is None:
        model_loop = FactorRDLoop(FACTOR_PROP_SETTING)
    else:
        model_loop = FactorRDLoop.load(path)
    model_loop.run(step_n=step_n)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/qlib_rd_loop/factor_from_report.py
================================================
import json
from pathlib import Path
from typing import Any, Dict, Tuple

import fire
from jinja2 import Environment, StrictUndefined

from rdagent.app.qlib_rd_loop.conf import FACTOR_FROM_REPORT_PROP_SETTING
from rdagent.app.qlib_rd_loop.factor import FactorRDLoop
from rdagent.components.document_reader.document_reader import (
    extract_first_page_screenshot_from_pdf,
    load_and_process_pdfs_by_langchain,
)
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import Hypothesis
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorExperiment
from rdagent.scenarios.qlib.factor_experiment_loader.pdf_loader import (
    FactorExperimentLoaderFromPDFfiles,
)
from rdagent.utils.workflow import LoopMeta

prompts_path = Path(__file__).parent / "prompts.yaml"
prompts = Prompts(file_path=prompts_path)


def generate_hypothesis(factor_result: dict, report_content: str) -> str:
    """
    Generate a hypothesis based on factor results and report content.

    Args:
        factor_result (dict): The results of the factor analysis.
        report_content (str): The content of the report.

    Returns:
        str: The generated hypothesis.
    """
    system_prompt = (
        Environment(undefined=StrictUndefined).from_string(prompts["hypothesis_generation"]["system"]).render()
    )
    user_prompt = (
        Environment(undefined=StrictUndefined)
        .from_string(prompts["hypothesis_generation"]["user"])
        .render(factor_descriptions=json.dumps(factor_result), report_content=report_content)
    )

    response = APIBackend().build_messages_and_create_chat_completion(
        user_prompt=user_prompt,
        system_prompt=system_prompt,
        json_mode=True,
        json_target_type=Dict[str, str],
    )

    response_json = json.loads(response)

    return Hypothesis(
        hypothesis=response_json.get("hypothesis", "No hypothesis provided"),
        reason=response_json.get("reason", "No reason provided"),
        concise_reason=response_json.get("concise_reason", "No concise reason provided"),
        concise_observation=response_json.get("concise_observation", "No concise observation provided"),
        concise_justification=response_json.get("concise_justification", "No concise justification provided"),
        concise_knowledge=response_json.get("concise_knowledge", "No concise knowledge provided"),
    )


def extract_hypothesis_and_exp_from_reports(report_file_path: str) -> Tuple[QlibFactorExperiment, Hypothesis]:
    """
    Extract hypothesis and experiment details from report files.

    Args:
        report_file_path (str): Path to the report file.

    Returns:
        Tuple[QlibFactorExperiment, Hypothesis]: The extracted experiment and generated hypothesis.
    """
    with logger.tag("extract_factors_and_implement"):
        with logger.tag("load_factor_tasks"):
            exp = FactorExperimentLoaderFromPDFfiles().load(report_file_path)
            if exp is None or exp.sub_tasks == []:
                return None, None

        with logger.tag("load_pdf_screenshot"):
            pdf_screenshot = extract_first_page_screenshot_from_pdf(report_file_path)
            logger.log_object(pdf_screenshot)

    docs_dict = load_and_process_pdfs_by_langchain(report_file_path)

    factor_result = {
        task.factor_name: {
            "description": task.factor_description,
            "formulation": task.factor_formulation,
            "variables": task.variables,
            "resources": task.factor_resources,
        }
        for task in exp.sub_tasks
    }

    report_content = "\n".join(docs_dict.values())
    hypothesis = generate_hypothesis(factor_result, report_content)
    exp.hypothesis = hypothesis
    return exp, hypothesis


class FactorReportLoop(FactorRDLoop, metaclass=LoopMeta):
    def __init__(self, report_folder: str = None):
        super().__init__(PROP_SETTING=FACTOR_FROM_REPORT_PROP_SETTING)
        if report_folder is None:
            self.judge_pdf_data_items = json.load(
                open(FACTOR_FROM_REPORT_PROP_SETTING.report_result_json_file_path, "r")
            )
        else:
            self.judge_pdf_data_items = [i for i in Path(report_folder).rglob("*.pdf")]

        self.pdf_file_index = 0
        self.valid_pdf_file_count = 0
        self.current_loop_hypothesis = None
        self.current_loop_exp = None
        self.steps = ["propose_hypo_exp", "propose", "exp_gen", "coding", "running", "feedback"]

    def propose_hypo_exp(self, prev_out: dict[str, Any]):
        with logger.tag("r"):
            while True:
                if FACTOR_FROM_REPORT_PROP_SETTING.is_report_limit_enabled and self.valid_pdf_file_count > 15:
                    break
                report_file_path = self.judge_pdf_data_items[self.pdf_file_index]
                logger.info(f"Processing number {self.pdf_file_index} report: {report_file_path}")
                self.pdf_file_index += 1
                exp, hypothesis = extract_hypothesis_and_exp_from_reports(str(report_file_path))
                if exp is None:
                    continue
                self.valid_pdf_file_count += 1
                exp.based_experiments = [QlibFactorExperiment(sub_tasks=[], hypothesis=hypothesis)] + [
                    t[0] for t in self.trace.hist if t[1]
                ]
                exp.sub_workspace_list = exp.sub_workspace_list[: FACTOR_FROM_REPORT_PROP_SETTING.max_factors_per_exp]
                exp.sub_tasks = exp.sub_tasks[: FACTOR_FROM_REPORT_PROP_SETTING.max_factors_per_exp]
                logger.log_object(hypothesis, tag="hypothesis generation")
                logger.log_object(exp.sub_tasks, tag="experiment generation")
                self.current_loop_hypothesis = hypothesis
                self.current_loop_exp = exp
                return None

    def propose(self, prev_out: dict[str, Any]):
        return self.current_loop_hypothesis

    def exp_gen(self, prev_out: dict[str, Any]):
        return self.current_loop_exp

    def coding(self, prev_out: dict[str, Any]):
        with logger.tag("d"):  # develop
            exp = self.coder.develop(prev_out["exp_gen"])
            logger.log_object(exp.sub_workspace_list, tag="coder result")
        return exp


def main(report_folder=None, path=None, step_n=None):
    """
    Auto R&D Evolving loop for fintech factors (the factors are extracted from finance reports).

    Args:
        report_folder (str, optional): The folder contains the report PDF files. Reports will be loaded from this folder.
        path (str, optional): The path for loading a session. If provided, the session will be loaded.
        step_n (int, optional): Step number to continue running a session.
    """
    if path is None and report_folder is None:
        model_loop = FactorReportLoop()
    elif path is not None:
        model_loop = FactorReportLoop.load(path)
    else:
        model_loop = FactorReportLoop(report_folder=report_folder)

    model_loop.run(step_n=step_n)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/qlib_rd_loop/model.py
================================================
"""
Model workflow with session control
"""

import fire

from rdagent.app.qlib_rd_loop.conf import MODEL_PROP_SETTING
from rdagent.components.workflow.rd_loop import RDLoop
from rdagent.core.exception import ModelEmptyError


class ModelRDLoop(RDLoop):
    skip_loop_error = (ModelEmptyError,)


def main(path=None, step_n=None):
    """
    Auto R&D Evolving loop for fintech models

    You can continue running session by

    .. code-block:: python

        dotenv run -- python rdagent/app/qlib_rd_loop/model.py $LOG_PATH/__session__/1/0_propose  --step_n 1   # `step_n` is a optional paramter

    """
    if path is None:
        model_loop = ModelRDLoop(MODEL_PROP_SETTING)
    else:
        model_loop = ModelRDLoop.load(path)
    model_loop.run(step_n=step_n)


if __name__ == "__main__":
    fire.Fire(main)



================================================
File: rdagent/app/qlib_rd_loop/prompts.yaml
================================================
hypothesis_generation:
  system: |-
    You are an expert in financial analysis. Your task is to generate a well-reasoned hypothesis based on the provided financial factors and report content.
    Please ensure your response is in JSON format as shown below:
    {
      "hypothesis": "A clear and concise hypothesis based on the provided information.",
      "reason": "A detailed explanation supporting the generated hypothesis.",
      "concise_reason": "One line summary that focuses on the justification for the change that leads to the hypothesis (like a part of a knowledge that we are building)",
      "concise_observation": "One line summary. It focuses on the observation of the given scenario, data characteristics, or previous experiences (failures & succeses).",
      "concise_justification": "One line summary. It focuses on the justification for the change in new hypothesis and the route of exploration supporting the growth of the hypothesis, based on the observation. ",
      "concise_knowledge": "One line summary. It focuses on a transferable knowledege that comes with the new hypothesis. Use conditional grammar. eg. "If...., ..; When..., .; and etc"
    }

  user: |-
    The following are the financial factors and their descriptions:
    {{ factor_descriptions }}

    The report content is as follows:
    {{ report_content }}


================================================
File: rdagent/app/utils/ape.py
================================================
"""
This is the preliminary version of the APE (Automated Prompt Engineering)
"""

import pickle
from pathlib import Path

from rdagent.core.conf import RD_AGENT_SETTINGS


def get_llm_qa(file_path):
    data_flt = []
    with open(file_path, "rb") as f:
        data = pickle.load(f)
        print(len(data))
        for item in data:
            if "debug_llm" in item["tag"]:
                data_flt.append(item)
    return data_flt


# Example usage
# use
file_path = Path(RD_AGENT_SETTINGS.log_trace_path) / "debug_llm.pkl"
llm_qa = get_llm_qa(file_path)
print(len(llm_qa))

print(llm_qa[0])

# Initialize APE backend
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T

api = APIBackend()

# Analyze test data and generate improved prompts
for qa in llm_qa:
    # Generate system prompt for APE
    system_prompt = T(".prompts:ape.system").r()

    # Generate user prompt with context from LLM QA
    user_prompt = T(".prompts:ape.user").r(
        system=qa["obj"].get("system", ""), user=qa["obj"]["user"], answer=qa["obj"]["resp"]
    )
    analysis_result = api.build_messages_and_create_chat_completion(
        system_prompt=system_prompt, user_prompt=user_prompt
    )
    print(f"█" * 60)
    yes = input("Do you want to continue? (y/n)")



================================================
File: rdagent/app/utils/health_check.py
================================================
import socket

import docker

from rdagent.log import rdagent_logger as logger


def check_docker() -> None:
    try:
        client = docker.from_env()
        client.images.pull("hello-world")
        container = client.containers.run("hello-world", detach=True)
        logs = container.logs().decode("utf-8")
        print(logs)
        container.remove()
        logger.info(f"The docker status is normal")
    except docker.errors.DockerException as e:
        logger.error(f"An error occurred: {e}")
        logger.warning(
            f"Docker status is exception, please check the docker configuration or reinstall it. Refs: https://docs.docker.com/engine/install/ubuntu/."
        )


def is_port_in_use(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(("127.0.0.1", port)) == 0


def check_and_list_free_ports(start_port=19899, max_ports=10) -> None:
    is_occupied = is_port_in_use(port=start_port)
    if is_occupied:
        free_ports = []
        for port in range(start_port, start_port + max_ports):
            if not is_port_in_use(port):
                free_ports.append(port)
        logger.warning(
            f"Port 19899 is occupied, please replace it with an available port when running the `rdagent ui` command. Available ports: {free_ports}"
        )
    else:
        logger.info(f"Port 19899 is not occupied, you can run the `rdagent ui` command")


def health_check():
    """
    Check that docker is installed correctly,
    and that the ports used in the sample README are not occupied.
    """
    check_docker()
    check_and_list_free_ports()



================================================
File: rdagent/app/utils/info.py
================================================
import importlib.metadata
import platform
import sys
from pathlib import Path

import docker
import requests
from setuptools_scm import get_version

from rdagent.log import rdagent_logger as logger


def sys_info():
    """collect system related info"""
    method_list = [
        ["Name of current operating system: ", "system"],
        ["Processor architecture: ", "machine"],
        ["System, version, and hardware information: ", "platform"],
        ["Version number of the system: ", "version"],
    ]
    for method in method_list:
        logger.info(f"{method[0]}{getattr(platform, method[1])()}")
    return None


def python_info():
    """collect Python related info"""
    python_version = sys.version.replace("\n", " ")
    logger.info(f"Python version: {python_version}")
    return None


def docker_info():
    client = docker.from_env()
    containers = client.containers.list(all=True)
    if containers:
        containers.sort(key=lambda c: c.attrs["Created"])
        last_container = containers[-1]
        logger.info(f"Container ID: {last_container.id}")
        logger.info(f"Container Name: {last_container.name}")
        logger.info(f"Container Status: {last_container.status}")
        logger.info(f"Image ID used by the container: {last_container.image.id}")
        logger.info(f"Image tag used by the container: {last_container.image.tags}")
        logger.info(f"Container port mapping: {last_container.ports}")
        logger.info(f"Container Label: {last_container.labels}")
        logger.info(f"Startup Commands: {' '.join(client.containers.get(last_container.id).attrs['Config']['Cmd'])}")
    else:
        logger.info(f"No run containers.")


def rdagent_info():
    """collect rdagent related info"""
    current_version = importlib.metadata.version("rdagent")
    logger.info(f"RD-Agent version: {current_version}")
    api_url = f"https://api.github.com/repos/microsoft/RD-Agent/contents/requirements.txt?ref=main"
    response = requests.get(api_url)
    if response.status_code == 200:
        files = response.json()
        file_url = files["download_url"]
        file_response = requests.get(file_url)
        if file_response.status_code == 200:
            all_file_contents = file_response.text.split("\n")
        else:
            logger.warning(f"Failed to retrieve {files['name']}, status code: {file_response.status_code}")
    else:
        logger.warning(f"Failed to retrieve files in folder, status code: {response.status_code}")
    package_list = [
        item.split("#")[0].strip() for item in all_file_contents if item.strip() and not item.startswith("#")
    ]
    package_version_list = []
    for package in package_list:
        if package == "typer[all]":
            package = "typer"
        version = importlib.metadata.version(package)
        package_version_list.append(f"{package}=={version}")
    logger.info(f"Package version: {package_version_list}")
    return None


def collect_info():
    """Prints information about the system and the installed packages."""
    sys_info()
    python_info()
    docker_info()
    rdagent_info()
    return None



================================================
File: rdagent/app/utils/prompts.yaml
================================================
ape:
  system: |-
    We'll provide you with a pair of Chat QA about data science.
    We are creating solutions for a Kaggle Competition based on the answers.
    Good questions are crucial for getting good answers.
    Please suggest how to improve the question.
    You can analyze based on these aspects:
    - Is the question complete (is all the information needed to answer the question provided?)

    The conversation will be provided in the following format:

    <question>
      <part1>
      ...text to describe the question...
      </part1>
      <part2>
      ...text to describe the question...
      </part2>
    </question>

    <answer>
      ...text to describe the answer.
    </answer>

    You response should be very concorete and concise(less than 20 words) and focuse on the mentioned aspects, like
    ```
    Info Missing: the question ask for changing code, but it does not provide the description of current code.
    ```
    Please be very conversatiive when you propose improvements. Only propose improvements when it becomes impossible to give the answer.

    Don't propose conerete modifications

  user: |-
    <question>
      <part1>
      {{system}}
      </part1>
      <part2>
      {{user}}
      </part2>
    </question>

    <answer>
      {{answer}}
    </answer>

  optional: |-
    If you want to suggest modification on  the question. Please follow the *SEARCH/REPLACE block* Rules!!!! It is optional.
    Please make it concise and less than 20 lines!!!

    # *SEARCH/REPLACE block* Rules:

    Every *SEARCH/REPLACE block* must use this format:
    1. The *FULL* file path alone on a line, verbatim. No bold asterisks, no quotes around it, no escaping of characters, etc.
    2. The opening fence and code language, eg: ```python
    3. The start of search block: <<<<<<< SEARCH
    4. A contiguous chunk of lines to search for in the existing source code
    5. The dividing line: =======
    6. The lines to replace into the source code
    7. The end of the replace block: >>>>>>> REPLACE
    8. The closing fence: ```

    Use the *FULL* file path, as shown to you by the user.

    Every *SEARCH* section must *EXACTLY MATCH* the existing file content, character for character, including all comments, docstrings, etc.
    If the file contains code or other data wrapped/escaped in json/xml/quotes or other containers, you need to propose edits to the literal contents of the file, including the container markup.

    *SEARCH/REPLACE* blocks will *only* replace the first match occurrence.
    Including multiple unique *SEARCH/REPLACE* blocks if needed.
    Include enough lines in each SEARCH section to uniquely match each set of lines that need to change.

    Keep *SEARCH/REPLACE* blocks concise.
    Break large *SEARCH/REPLACE* blocks into a series of smaller blocks that each change a small portion of the file.
    Include just the changing lines, and a few surrounding lines if needed for uniqueness.
    Do not include long runs of unchanging lines in *SEARCH/REPLACE* blocks.

    Only create *SEARCH/REPLACE* blocks for files that the user has added to the chat!

    To move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.

    Pay attention to which filenames the user wants you to edit, especially if they are asking you to create a new file.

    If you want to put code in a new file, use a *SEARCH/REPLACE block* with:
    - A new file path, including dir name if needed
    - An empty `SEARCH` section
    - The new file's contents in the `REPLACE` section

    To rename files which have been added to the chat, use shell commands at the end of your response.

    If the user just says something like "ok" or "go ahead" or "do that" they probably want you to make SEARCH/REPLACE blocks for the code changes you just proposed.
    The user will say when they've applied your edits. If they haven't explicitly confirmed the edits have been applied, they probably want proper SEARCH/REPLACE blocks.

    You are diligent and tireless!
    You NEVER leave comments describing code without implementing it!
    You always COMPLETELY IMPLEMENT the needed code!


    ONLY EVER RETURN CODE IN A *SEARCH/REPLACE BLOCK*!
    Examples of when to suggest shell commands:

    - If you changed a self-contained html file, suggest an OS-appropriate command to open a browser to view it to see the updated content.
    - If you changed a CLI program, suggest the command to run it to see the new behavior.
    - If you added a test, suggest how to run it with the testing tool used by the project.
    - Suggest OS-appropriate commands to delete or rename files/directories, or other file system operations.
    - If your code changes add new dependencies, suggest the command to install them.
    - Etc.

    Here is a example of SEARCH/REPLACE BLOCK to change a function implementation to import.

    <<<<<<< SEARCH
    def hello():
        "print a greeting"

        print("hello")
    =======
    from hello import hello

    >>>>>>> REPLACE
# - Is there any ambiguity in the question?



================================================
File: rdagent/components/benchmark/conf.py
================================================
from dataclasses import field
from pathlib import Path
from typing import Optional

from rdagent.core.conf import ExtendedBaseSettings

DIRNAME = Path("./")


class BenchmarkSettings(ExtendedBaseSettings):
    class Config:
        env_prefix = "BENCHMARK_"
        """Use `BENCHMARK_` as prefix for environment variables"""

    bench_data_path: Path = DIRNAME / "example.json"
    """data for benchmark"""

    bench_test_round: int = 10
    """how many rounds to run, each round may cost 10 minutes"""

    bench_test_case_n: Optional[int] = None
    """how many test cases to run; If not given, all test cases will be run"""

    bench_method_cls: str = "rdagent.components.coder.factor_coder.FactorCoSTEER"
    """method to be used for test cases"""

    bench_method_extra_kwargs: dict = field(
        default_factory=dict,
    )
    """extra kwargs for the method to be tested except the task list"""

    bench_result_path: Path = DIRNAME / "result"
    """result save path"""



================================================
File: rdagent/components/benchmark/eval_method.py
================================================
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple, Union

import pandas as pd
from tqdm import tqdm

from rdagent.components.coder.factor_coder.config import FACTOR_COSTEER_SETTINGS
from rdagent.components.coder.factor_coder.eva_utils import (
    FactorCorrelationEvaluator,
    FactorEqualValueRatioEvaluator,
    FactorEvaluator,
    FactorIndexEvaluator,
    FactorRowCountEvaluator,
    FactorSingleColumnEvaluator,
)
from rdagent.components.coder.factor_coder.factor import FactorFBWorkspace
from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.developer import Developer
from rdagent.core.exception import CoderError
from rdagent.core.experiment import Experiment, Task, Workspace
from rdagent.core.scenario import Scenario
from rdagent.core.utils import multiprocessing_wrapper

EVAL_RES = Dict[
    str,
    List[Tuple[FactorEvaluator, Union[object, CoderError]]],
]


class TestCase:
    def __init__(
        self,
        target_task: Task,
        ground_truth: Workspace,
    ):
        self.target_task = target_task
        self.ground_truth = ground_truth


class TestCases:
    def __init__(self, test_case_l: list[TestCase] = []):
        # self.test_case_l = [TestCase(task, gt) for task, gt in zip(target_task, ground_truth)]
        self.test_case_l = test_case_l

    def __getitem__(self, item):
        return self.test_case_l[item]

    def __len__(self):
        return len(self.test_case_l)

    def get_exp(self):
        return Experiment([case.target_task for case in self.test_case_l])

    @property
    def target_task(self):
        return [case.target_task for case in self.test_case_l]

    @property
    def ground_truth(self):
        return [case.ground_truth for case in self.test_case_l]


class BaseEval:
    """
    The benchmark benchmark evaluation.
    """

    def __init__(
        self,
        evaluator_l: List[FactorEvaluator],
        test_cases: TestCases,
        generate_method: Developer,
        catch_eval_except: bool = True,
    ):
        """Parameters
        ----------
        test_cases : TestCases
            cases to be evaluated, ground truth are included in the test cases.
        evaluator_l : List[FactorEvaluator]
            A list of evaluators to evaluate the generated code.
        catch_eval_except : bool
            If we want to debug the evaluators, we recommend to set the this parameter to True.
        """
        self.evaluator_l = evaluator_l
        self.test_cases = test_cases
        self.generate_method = generate_method
        self.catch_eval_except = catch_eval_except

    def load_cases_to_eval(
        self,
        path: Union[Path, str],
        **kwargs,
    ) -> List[Workspace]:
        path = Path(path)
        fi_l = []
        for tc in self.test_cases:
            try:
                fi = FactorFBWorkspace.from_folder(tc.task, path, **kwargs)
                fi_l.append(fi)
            except FileNotFoundError:
                print("Fail to load test case for factor: ", tc.task.factor_name)
        return fi_l

    def eval_case(
        self,
        case_gt: Workspace,
        case_gen: Workspace,
    ) -> List[Union[Tuple[FactorEvaluator, object], Exception]]:
        """Parameters
        ----------
        case_gt : FactorImplementation

        case_gen : FactorImplementation


        Returns
        -------
        List[Union[Tuple[FactorEvaluator, object],Exception]]
            for each item
                If the evaluation run successfully, return the evaluate results.  Otherwise, return the exception.
        """
        eval_res = []
        for ev in self.evaluator_l:
            try:
                case_gen.raise_exception = True
                eval_res.append((ev, ev.evaluate(implementation=case_gen, gt_implementation=case_gt)))
                # if the corr ev is successfully evaluated and achieve the best performance, then break
            except CoderError as e:
                return e
            except Exception as e:
                # exception when evaluation
                if self.catch_eval_except:
                    eval_res.append((ev, e))
                else:
                    raise e
        return eval_res


class FactorImplementEval(BaseEval):
    def __init__(
        self,
        test_cases: TestCases,
        method: Developer,
        *args,
        scen: Scenario,
        test_round: int = 10,
        **kwargs,
    ):
        online_evaluator_l = [
            FactorSingleColumnEvaluator(scen),
            FactorRowCountEvaluator(scen),
            FactorIndexEvaluator(scen),
            FactorEqualValueRatioEvaluator(scen),
            FactorCorrelationEvaluator(hard_check=False, scen=scen),
        ]
        super().__init__(online_evaluator_l, test_cases, method, *args, **kwargs)
        self.test_round = test_round

    def develop(self):
        gen_factor_l_all_rounds = []
        for _ in tqdm(range(self.test_round), desc="Rounds of Eval"):
            print("\n========================================================")
            print(f"Eval {_}-th times...")
            print("========================================================\n")
            try:
                gen_factor_l = self.generate_method.develop(self.test_cases.get_exp())
            except KeyboardInterrupt:
                # TODO: Why still need to save result after KeyboardInterrupt?
                print("Manually interrupted the evaluation. Saving existing results")
                break

            if len(gen_factor_l.sub_workspace_list) != len(self.test_cases.ground_truth):
                raise ValueError(
                    "The number of cases to eval should be equal to the number of test cases.",
                )
            gen_factor_l_all_rounds.extend(gen_factor_l.sub_workspace_list)

        return gen_factor_l_all_rounds

    def eval(self, gen_factor_l_all_rounds):
        test_cases_all_rounds = []
        res = defaultdict(list)
        for _ in range(self.test_round):
            test_cases_all_rounds.extend(self.test_cases.ground_truth)
        eval_res_list = multiprocessing_wrapper(
            [
                (self.eval_case, (gt_case, gen_factor))
                for gt_case, gen_factor in zip(test_cases_all_rounds, gen_factor_l_all_rounds)
            ],
            n=RD_AGENT_SETTINGS.multi_proc_n,
        )

        for gt_case, eval_res, gen_factor in tqdm(zip(test_cases_all_rounds, eval_res_list, gen_factor_l_all_rounds)):
            res[gt_case.target_task.factor_name].append((gen_factor, eval_res))

        return res

    @staticmethod
    def summarize_res(res: EVAL_RES) -> pd.DataFrame:
        # None: indicate that it raises exception and get no results
        sum_res = {}
        for factor_name, runs in res.items():
            for fi, err_or_res_l in runs:
                # NOTE:  str(fi) may not be unique!!  Because the workspace can be skipped when hitting the cache.
                uniq_key = f"{str(fi)},{id(fi)}"

                key = (factor_name, uniq_key)
                val = {}
                if isinstance(err_or_res_l, Exception):
                    val["run factor error"] = str(err_or_res_l.__class__)
                else:
                    val["run factor error"] = None
                    for ev_obj, err_or_res in err_or_res_l:
                        if isinstance(err_or_res, Exception):
                            val[str(ev_obj)] = None
                        else:
                            feedback, metric = err_or_res
                            val[str(ev_obj)] = metric
                sum_res[key] = val

        return pd.DataFrame(sum_res)



================================================
File: rdagent/components/benchmark/example.json
================================================
{
    "Turnover_Rate_Factor": {
        "description": "A traditional factor based on 20-day average turnover rate, adjusted for market capitalization, which is further improved by applying the information distribution theory.",
        "formulation": "\\text{Adjusted Turnover Rate} = \\frac{\\text{mean}(20\\text{-day turnover rate})}{\\text{Market Capitalization}}",
        "variables": {
            "20-day turnover rate": "Average turnover rate over the past 20 days.",
            "Market Capitalization": "Total market value of a company's outstanding shares."
        },
        "Category": "Fundamentals",
        "Difficulty": "Easy",
        "gt_code": "import pandas as pd\n\ndata_f = pd.read_hdf('daily_f.h5')\n\ndata = data_f.reset_index()\nwindow_size = 20\n\nnominator=data.groupby('instrument')[['TurnoverRate_30D']].rolling(window=window_size).mean().reset_index(0, drop=True)\n# transfer to series\nnew=nominator['TurnoverRate_30D']\ndata['Turnover_Rate_Factor']=new/data['TradableACapital']\n\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(data['Turnover_Rate_Factor']).set_index(data_f.index)\n\n# transfer the result to series\nresult=result['Turnover_Rate_Factor']\nresult.to_hdf(\"result.h5\", key=\"data\")" 
    },
    "PctTurn20": {
        "description": "A factor representing the percentage change in turnover rate over the past 20 trading days, market-value neutralized.",
        "formulation": "\\text{PctTurn20} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{\\text{Turnover}_{i, t} - \\text{Turnover}_{i, t-20}}{\\text{Turnover}_{i, t-20}} \\right)",
        "variables": {
            "N": "Number of stocks in the market.",
            "Turnover_{i, t}": "Turnover of stock i at day t.",
            "Turnover_{i, t-20}": "Turnover of stock i at day t-20."
        },
        "Category": "Volume&Price",
        "Difficulty": "Medium",
        "gt_code": "import pandas as pd\nfrom statsmodels import api as sm\n\ndef fill_mean(s: pd.Series) -> pd.Series:\n    return s.fillna(s.mean()).fillna(0.0)\n\ndef market_value_neutralize(s: pd.Series, mv: pd.Series) -> pd.Series:\n    s = s.groupby(\"datetime\", group_keys=False).apply(fill_mean)\n    mv = mv.groupby(\"datetime\", group_keys=False).apply(fill_mean)\n\n    df_f = mv.to_frame(\"MarketValue\")\n    df_f[\"const\"] = 1\n    X = df_f[[\"MarketValue\", \"const\"]]\n\n    # Perform the Ordinary Least Squares (OLS) regression\n    model = sm.OLS(s, X)\n    results = model.fit()\n\n    # Calculate the residuals\n    df_f[\"residual\"] = results.resid\n    df_f[\"norm_resi\"] = df_f.groupby(level=\"datetime\", group_keys=False)[\"residual\"].apply(\n        lambda x: (x - x.mean()) / x.std(),\n    )\n    return df_f[\"norm_resi\"]\n\n\n# get_turnover\ndf_pv = pd.read_hdf(\"daily_pv.h5\", key=\"data\")\ndf_f = pd.read_hdf(\"daily_f.h5\", key=\"data\")\nturnover = df_pv[\"$money\"] / df_f[\"TradableMarketValue\"]\n\nf = turnover.groupby(\"instrument\").pct_change(periods=20)\n\nf_neutralized = market_value_neutralize(f, df_f[\"TradableMarketValue\"])\n\nf_neutralized.to_hdf(\"result.h5\", key=\"data\")"
    },
    "PB_ROE": {
        "description": "Constructed using the ranking difference between PB and ROE, with PB and ROE replacing original PB and ROE to obtain reconstructed factor values.",
        "formulation": "\\text{rank}(PB\\_t) - rank(ROE_t)",
        "variables": {
            "\\text{rank}(PB_t)": "Ranking PB on cross-section at time t.",
            "\\text{rank}(ROE_t)": "Ranking single-quarter ROE on cross-section at time t."
        },
        "Category": "High-Frequency",
        "Difficulty": "Hard",
        "gt_code": "#!/usr/bin/env python\n\nimport pandas as pd\n\ndata_f = pd.read_hdf('daily_f.h5')\n\ndata = data_f.reset_index()\n\n# Calculate the rank of PB and ROE\ndata['PB_rank'] = data.groupby('datetime')['B/P'].rank()\ndata['ROE_rank'] = data.groupby('datetime')['ROE'].rank()\n\n# Calculate the difference between the ranks\ndata['PB_ROE'] = data['PB_rank'] - data['ROE_rank']\n\n# set the datetime and instrument as index and drop the original index\nresult=pd.DataFrame(data['PB_ROE']).set_index(data_f.index)\n\n# transfer the result to series\nresult=result['PB_ROE']\nresult.to_hdf(\"result.h5\", key=\"data\")"
    }
}


================================================
File: rdagent/components/coder/CoSTEER/__init__.py
================================================
import pickle
from datetime import datetime
from pathlib import Path

from rdagent.components.coder.CoSTEER.config import CoSTEERSettings
from rdagent.components.coder.CoSTEER.evaluators import CoSTEERMultiFeedback
from rdagent.components.coder.CoSTEER.evolvable_subjects import EvolvingItem
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERKnowledgeBaseV1,
    CoSTEERKnowledgeBaseV2,
    CoSTEERRAGStrategyV1,
    CoSTEERRAGStrategyV2,
)
from rdagent.core.developer import Developer
from rdagent.core.evaluation import Evaluator, Feedback
from rdagent.core.evolving_agent import EvolvingStrategy, RAGEvoAgent
from rdagent.core.exception import CoderError
from rdagent.core.experiment import Experiment
from rdagent.log import rdagent_logger as logger


class CoSTEER(Developer[Experiment]):
    def __init__(
        self,
        settings: CoSTEERSettings,
        eva: Evaluator,
        es: EvolvingStrategy,
        evolving_version: int,
        *args,
        with_knowledge: bool = True,
        with_feedback: bool = True,
        knowledge_self_gen: bool = True,
        filter_final_evo: bool = True,
        max_loop: int | None = None,
        **kwargs,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.max_loop = settings.max_loop if max_loop is None else max_loop
        self.max_seconds = settings.max_seconds
        self.knowledge_base_path = (
            Path(settings.knowledge_base_path) if settings.knowledge_base_path is not None else None
        )
        self.new_knowledge_base_path = (
            Path(settings.new_knowledge_base_path) if settings.new_knowledge_base_path is not None else None
        )

        self.with_knowledge = with_knowledge
        self.with_feedback = with_feedback
        self.knowledge_self_gen = knowledge_self_gen
        self.filter_final_evo = filter_final_evo
        self.evolving_strategy = es
        self.evaluator = eva
        self.evolving_version = evolving_version

        # init knowledge base
        self.knowledge_base = self.load_or_init_knowledge_base(
            former_knowledge_base_path=self.knowledge_base_path,
            component_init_list=[],
        )
        # init rag method
        self.rag = (
            CoSTEERRAGStrategyV2(self.knowledge_base, settings=settings)
            if self.evolving_version == 2
            else CoSTEERRAGStrategyV1(self.knowledge_base, settings=settings)
        )

    def load_or_init_knowledge_base(self, former_knowledge_base_path: Path = None, component_init_list: list = []):
        if former_knowledge_base_path is not None and former_knowledge_base_path.exists():
            knowledge_base = pickle.load(open(former_knowledge_base_path, "rb"))
            if self.evolving_version == 1 and not isinstance(knowledge_base, CoSTEERKnowledgeBaseV1):
                raise ValueError("The former knowledge base is not compatible with the current version")
            elif self.evolving_version == 2 and not isinstance(
                knowledge_base,
                CoSTEERKnowledgeBaseV2,
            ):
                raise ValueError("The former knowledge base is not compatible with the current version")
        else:
            knowledge_base = (
                CoSTEERKnowledgeBaseV2(
                    init_component_list=component_init_list,
                )
                if self.evolving_version == 2
                else CoSTEERKnowledgeBaseV1()
            )
        return knowledge_base

    def develop(self, exp: Experiment) -> Experiment:

        # init intermediate items
        evo_exp = EvolvingItem.from_experiment(exp)

        self.evolve_agent = RAGEvoAgent(
            max_loop=self.max_loop,
            evolving_strategy=self.evolving_strategy,
            rag=self.rag,
            with_knowledge=self.with_knowledge,
            with_feedback=self.with_feedback,
            knowledge_self_gen=self.knowledge_self_gen,
        )

        start_datetime = datetime.now()
        for evo_exp in self.evolve_agent.multistep_evolve(evo_exp, self.evaluator):
            assert isinstance(evo_exp, Experiment)  # multiple inheritance
            logger.log_object(evo_exp.sub_workspace_list, tag="evolving code")
            for sw in evo_exp.sub_workspace_list:
                logger.info(f"evolving code workspace: {sw}")
            if (datetime.now() - start_datetime).seconds > self.max_seconds:
                break

        if self.with_feedback and self.filter_final_evo:
            evo_exp = self._exp_postprocess_by_feedback(evo_exp, self.evolve_agent.evolving_trace[-1].feedback)

        # save new knowledge base
        if self.new_knowledge_base_path is not None:
            with self.new_knowledge_base_path.open("wb") as f:
                pickle.dump(self.knowledge_base, f)
            logger.info(f"New knowledge base saved to {self.new_knowledge_base_path}")
        exp.sub_workspace_list = evo_exp.sub_workspace_list
        exp.experiment_workspace = evo_exp.experiment_workspace
        return exp

    def _exp_postprocess_by_feedback(self, evo: Experiment, feedback: CoSTEERMultiFeedback) -> Experiment:
        """
        Responsibility:
        - Raise Error if it failed to handle the develop task
        -
        """
        assert isinstance(evo, Experiment)
        assert isinstance(feedback, CoSTEERMultiFeedback)
        assert len(evo.sub_workspace_list) == len(feedback)

        # FIXME: when whould the feedback be None?
        failed_feedbacks = [
            f"- feedback{index + 1:02d}:\n  - execution: {f.execution}\n  - return_checking: {f.return_checking}\n  - code: {f.code}"
            for index, f in enumerate(feedback)
            if f is not None and not f.final_decision
        ]

        if len(failed_feedbacks) == len(feedback):
            feedback_summary = "\n".join(failed_feedbacks)
            raise CoderError(f"All tasks are failed:\n{feedback_summary}")

        return evo



================================================
File: rdagent/components/coder/CoSTEER/config.py
================================================
from typing import Union

from rdagent.core.conf import ExtendedBaseSettings


class CoSTEERSettings(ExtendedBaseSettings):
    """CoSTEER settings, this setting is supposed not to be used directly!!!"""

    class Config:
        env_prefix = "CoSTEER_"

    coder_use_cache: bool = False
    """Indicates whether to use cache for the coder"""

    max_loop: int = 10
    """Maximum number of task implementation loops"""

    fail_task_trial_limit: int = 20

    v1_query_former_trace_limit: int = 5
    v1_query_similar_success_limit: int = 5

    v2_query_component_limit: int = 1
    v2_query_error_limit: int = 1
    v2_query_former_trace_limit: int = 1
    v2_add_fail_attempt_to_latest_successful_execution: bool = False
    v2_error_summary: bool = False
    v2_knowledge_sampler: float = 1.0

    knowledge_base_path: Union[str, None] = None
    """Path to the knowledge base"""

    new_knowledge_base_path: Union[str, None] = None
    """Path to the new knowledge base"""

    select_threshold: int = 10

    max_seconds: int = 10**6


CoSTEER_SETTINGS = CoSTEERSettings()



================================================
File: rdagent/components/coder/CoSTEER/evaluators.py
================================================
from abc import abstractmethod
from dataclasses import dataclass
from typing import TYPE_CHECKING, List

from rdagent.components.coder.CoSTEER.evolvable_subjects import EvolvingItem
from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.evaluation import Evaluator, Feedback
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import Task, Workspace
from rdagent.core.utils import multiprocessing_wrapper
from rdagent.log import rdagent_logger as logger

if TYPE_CHECKING:
    from rdagent.core.scenario import Scenario

# TODO:
# 1. It seems logically sound, but we currently lack a scenario to apply it.
# 2. If it proves to be useful, relocate it to a more general location.
#
# class FBWorkspaceExeFeedback(Feedback):
#     """
#     It pairs with FBWorkspace in the abstract level.
#     """
#     # ws: FBWorkspace   # potential
#     stdout: str


@dataclass
class CoSTEERSingleFeedback(Feedback):
    # TODO: (xiao)
    # it should be more general class for FBWorkspaceExeFeedback
    # A better name of it may be NormalFeedback
    # TODO: It should be a general feeddback for CoSTEERR
    """
    The feedback for the data loader evaluation.
    It is design align the phases of the implemented code
    - Execution -> Return Value -> Code -> Final Decision
    """
    execution: str
    # execution_feedback
    return_checking: str | None  # including every check in the testing (constraints about the generated value)
    # value_feedback, shape_feedback, value_generated_flag
    code: str
    final_decision: bool

    @staticmethod
    def val_and_update_init_dict(data: dict) -> dict:
        # TODO: (bowen) use a more general method to validate and update the data dictionary before init, like pydantic
        """
        Validates and converts the 'final_decision' field in the given data dictionary.

        Args:
            data (dict): The data dictionary containing the 'final_decision' field.

        Returns:
            dict: The updated data dictionary with 'final_decision' as a boolean.

        Raises:
            ValueError: If 'final_decision' is not present or not a boolean.
        """
        if "final_decision" not in data:
            raise ValueError("'final_decision' is required")

        if isinstance(data["final_decision"], str):
            if data["final_decision"] == "false" or data["final_decision"] == "False":
                data["final_decision"] = False
            elif data["final_decision"] == "true" or data["final_decision"] == "True":
                data["final_decision"] = True

        if not isinstance(data["final_decision"], bool):
            raise ValueError(f"'final_decision' must be a boolean, not {type(data['final_decision'])}")
        return data

    def __str__(self) -> str:
        return f"""------------------Execution------------------
{self.execution}
------------------Return Checking------------------
{self.return_checking if self.return_checking is not None else 'No return checking'}
------------------Code------------------
{self.code}
------------------Final Decision------------------
This implementation is {'SUCCESS' if self.final_decision else 'FAIL'}.
"""

    def __bool__(self):
        return self.final_decision


class CoSTEERSingleFeedbackDeprecated(CoSTEERSingleFeedback):
    """This class is a base class for all code generator feedback to single implementation"""

    def __init__(
        self,
        execution_feedback: str = None,
        shape_feedback: str = None,
        code_feedback: str = None,
        value_feedback: str = None,
        final_decision: bool = None,
        final_feedback: str = None,
        value_generated_flag: bool = None,
        final_decision_based_on_gt: bool = None,
    ) -> None:
        self.execution_feedback = execution_feedback
        self.code_feedback = code_feedback
        self.value_feedback = value_feedback
        self.final_decision = final_decision
        self.final_feedback = final_feedback
        self.value_generated_flag = value_generated_flag
        self.final_decision_based_on_gt = final_decision_based_on_gt

        # TODO:
        # Not general enough. So we should not put them in the general costeer feedback
        # Instead, we should create subclass for it.
        self.shape_feedback = shape_feedback  # Not general enough. So

    # TODO: @property
    @property
    def execution(self):
        return self.execution_feedback

    @property
    def return_checking(self):
        if self.value_generated_flag:
            return f"value feedback: {self.value_feedback}\n\nshape feedback: {self.shape_feedback}"
        return None

    @property
    def code(self):
        return self.code_feedback

    def __str__(self) -> str:
        return f"""------------------Execution Feedback------------------
{self.execution_feedback if self.execution_feedback is not None else 'No execution feedback'}
------------------Shape Feedback------------------
{self.shape_feedback if self.shape_feedback is not None else 'No shape feedback'}
------------------Code Feedback------------------
{self.code_feedback if self.code_feedback is not None else 'No code feedback'}
------------------Value Feedback------------------
{self.value_feedback if self.value_feedback is not None else 'No value feedback'}
------------------Final Feedback------------------
{self.final_feedback if self.final_feedback is not None else 'No final feedback'}
------------------Final Decision------------------
This implementation is {'SUCCESS' if self.final_decision else 'FAIL'}.
"""


class CoSTEERMultiFeedback(Feedback):
    """Feedback contains a list, each element is the corresponding feedback for each factor implementation."""

    def __init__(self, feedback_list: List[CoSTEERSingleFeedback]) -> None:
        self.feedback_list = feedback_list

    def __getitem__(self, index: int) -> CoSTEERSingleFeedback:
        return self.feedback_list[index]

    def __len__(self) -> int:
        return len(self.feedback_list)

    def append(self, feedback: CoSTEERSingleFeedback) -> None:
        self.feedback_list.append(feedback)

    def __iter__(self):
        return iter(self.feedback_list)

    def __bool__(self):
        return all(feedback.final_decision for feedback in self.feedback_list)


class CoSTEEREvaluator(Evaluator):
    def __init__(
        self,
        scen: "Scenario",
    ) -> None:
        self.scen = scen

    # TODO:
    # I think we should have unified interface for all evaluates, for examples.
    # So we should adjust the interface of other factors
    @abstractmethod
    def evaluate(
        self,
        target_task: Task,
        implementation: Workspace,
        gt_implementation: Workspace,
        **kwargs,
    ) -> CoSTEERSingleFeedback:
        raise NotImplementedError("Please implement the `evaluator` method")


class CoSTEERMultiEvaluator(CoSTEEREvaluator):
    """This is for evaluation of experiment. Due to we have multiple tasks, so we will return a list of evaluation feebacks"""

    def __init__(self, single_evaluator: CoSTEEREvaluator, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.single_evaluator = single_evaluator

    def evaluate(
        self,
        evo: EvolvingItem,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> CoSTEERMultiFeedback:
        multi_implementation_feedback = multiprocessing_wrapper(
            [
                (
                    self.single_evaluator.evaluate,
                    (
                        evo.sub_tasks[index],
                        evo.sub_workspace_list[index],
                        evo.sub_gt_implementations[index] if evo.sub_gt_implementations is not None else None,
                        queried_knowledge,
                    ),
                )
                for index in range(len(evo.sub_tasks))
            ],
            n=RD_AGENT_SETTINGS.multi_proc_n,
        )

        final_decision = [
            None if single_feedback is None else single_feedback.final_decision
            for single_feedback in multi_implementation_feedback
        ]
        logger.info(f"Final decisions: {final_decision} True count: {final_decision.count(True)}")

        for index in range(len(evo.sub_tasks)):
            if final_decision[index]:
                evo.sub_tasks[index].factor_implementation = True

        return CoSTEERMultiFeedback(multi_implementation_feedback)



================================================
File: rdagent/components/coder/CoSTEER/evolvable_subjects.py
================================================
from rdagent.core.evolving_framework import EvolvableSubjects
from rdagent.core.experiment import Experiment, FBWorkspace, Task
from rdagent.log import rdagent_logger as logger


class EvolvingItem(Experiment, EvolvableSubjects):
    """
    Intermediate item of factor implementation.
    """

    def __init__(
        self,
        sub_tasks: list[Task],
        sub_gt_implementations: list[FBWorkspace] = None,
    ):
        Experiment.__init__(self, sub_tasks=sub_tasks)
        self.corresponding_selection: list = None
        if sub_gt_implementations is not None and len(
            sub_gt_implementations,
        ) != len(self.sub_tasks):
            self.sub_gt_implementations = None
            logger.warning(
                "The length of sub_gt_implementations is not equal to the length of sub_tasks, set sub_gt_implementations to None",
            )
        else:
            self.sub_gt_implementations = sub_gt_implementations

    @classmethod
    def from_experiment(cls, exp: Experiment) -> Experiment:
        ei = cls(sub_tasks=exp.sub_tasks)
        ei.based_experiments = exp.based_experiments
        ei.experiment_workspace = exp.experiment_workspace
        return ei



================================================
File: rdagent/components/coder/CoSTEER/evolving_strategy.py
================================================
from __future__ import annotations

from abc import abstractmethod
from pathlib import Path

from rdagent.components.coder.CoSTEER.config import CoSTEERSettings
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiFeedback,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolvable_subjects import EvolvingItem
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.components.coder.CoSTEER.scheduler import random_select
from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.evolving_framework import EvolvingStrategy, EvoStep, QueriedKnowledge
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario
from rdagent.core.utils import multiprocessing_wrapper

implement_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class MultiProcessEvolvingStrategy(EvolvingStrategy):
    def __init__(self, scen: Scenario, settings: CoSTEERSettings):
        super().__init__(scen)
        self.settings = settings

    @abstractmethod
    def implement_one_task(
        self,
        target_task: Task,
        queried_knowledge: QueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:  # FIXME: fix interface of previous implement
        """
        This method will input the task & current workspace,
        and output the modification to applied to the workspace.
        (i.e. replace the content <filename> with <content>)

        Parameters
        ----------
        target_task : Task

        queried_knowledge : QueriedKnowledge | None

        workspace : FBWorkspace | None

        prev_task_feedback : CoSTEERSingleFeedback | None
            task feedback for previous evolving step
            None indicate it is the first loop.

        Return
        ------
        The new files {<filename>: <content>} to update the workspace.
        """
        raise NotImplementedError

    def select_one_round_tasks(
        self,
        to_be_finished_task_index: list,
        evo: EvolvingItem,
        selected_num: int,
        queried_knowledge: CoSTEERQueriedKnowledge,
        scen: Scenario,
    ) -> list:
        """Since scheduler is not essential, we implement a simple random selection here."""
        return random_select(to_be_finished_task_index, evo, selected_num, queried_knowledge, scen)

    @abstractmethod
    def assign_code_list_to_evo(self, code_list: list[dict], evo: EvolvingItem) -> None:
        """
        Assign the code list to the evolving item.

        Due to the implement_one_task take `workspace` as input and output the `modification`.
        We should apply implmentation to evo

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        raise NotImplementedError

    def evolve(
        self,
        *,
        evo: EvolvingItem,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        evolving_trace: list[EvoStep] = [],
        **kwargs,
    ) -> EvolvingItem:
        # 1.找出需要evolve的task
        to_be_finished_task_index = []
        for index, target_task in enumerate(evo.sub_tasks):
            target_task_desc = target_task.get_task_information()
            if target_task_desc in queried_knowledge.success_task_to_knowledge_dict:
                evo.sub_workspace_list[index] = queried_knowledge.success_task_to_knowledge_dict[
                    target_task_desc
                ].implementation
            elif (
                target_task_desc not in queried_knowledge.success_task_to_knowledge_dict
                and target_task_desc not in queried_knowledge.failed_task_info_set
            ):
                to_be_finished_task_index.append(index)

        # 2. 选择selection方法
        # if the number of factors to be implemented is larger than the limit, we need to select some of them

        if self.settings.select_threshold < len(to_be_finished_task_index):
            # Select a fixed number of factors if the total exceeds the threshold
            to_be_finished_task_index = self.select_one_round_tasks(
                to_be_finished_task_index, evo, self.settings.select_threshold, queried_knowledge, self.scen
            )

        last_feedback = None
        if len(evolving_trace) > 0:
            last_feedback = evolving_trace[-1].feedback
            assert isinstance(last_feedback, CoSTEERMultiFeedback)
        result = multiprocessing_wrapper(
            [
                (
                    self.implement_one_task,
                    (
                        evo.sub_tasks[target_index],
                        queried_knowledge,
                        evo.experiment_workspace,
                        None if last_feedback is None else last_feedback[target_index],
                    ),
                )
                for target_index in to_be_finished_task_index
            ],
            n=RD_AGENT_SETTINGS.multi_proc_n,
        )
        code_list = [None for _ in range(len(evo.sub_tasks))]
        for index, target_index in enumerate(to_be_finished_task_index):
            code_list[target_index] = result[index]

        evo = self.assign_code_list_to_evo(code_list, evo)
        evo.corresponding_selection = to_be_finished_task_index

        return evo



================================================
File: rdagent/components/coder/CoSTEER/knowledge_management.py
================================================
from __future__ import annotations

import copy
import json
import random
import re
from itertools import combinations
from pathlib import Path
from typing import List, Union

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.CoSTEER.config import CoSTEERSettings
from rdagent.components.coder.CoSTEER.evaluators import CoSTEERSingleFeedback
from rdagent.components.knowledge_management.graph import (
    UndirectedGraph,
    UndirectedNode,
)
from rdagent.core.evolving_agent import Feedback
from rdagent.core.evolving_framework import (
    EvolvableSubjects,
    EvolvingKnowledgeBase,
    EvoStep,
    Knowledge,
    QueriedKnowledge,
    RAGStrategy,
)
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.core.prompts import Prompts
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import (
    APIBackend,
    calculate_embedding_distance_between_str_list,
)


class CoSTEERKnowledge(Knowledge):
    def __init__(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        feedback: Feedback,
    ) -> None:
        self.target_task = target_task
        self.implementation = implementation.copy()
        self.feedback = feedback

    def get_implementation_and_feedback_str(self) -> str:
        return f"""------------------implementation code:------------------
{self.implementation.all_codes}
------------------implementation feedback:------------------
{self.feedback!s}
"""


class CoSTEERQueriedKnowledge(QueriedKnowledge):
    def __init__(self, success_task_to_knowledge_dict: dict = {}, failed_task_info_set: set = set()) -> None:
        self.success_task_to_knowledge_dict = success_task_to_knowledge_dict
        self.failed_task_info_set = failed_task_info_set


class CoSTEERKnowledgeBaseV1(EvolvingKnowledgeBase):
    def __init__(self, path: str | Path = None) -> None:
        self.implementation_trace: dict[str, CoSTEERKnowledge] = dict()
        self.success_task_info_set: set[str] = set()

        self.task_to_embedding = dict()
        super().__init__(path)

    def query(self) -> CoSTEERQueriedKnowledge | None:
        """
        Query the knowledge base to get the queried knowledge. So far is handled in RAG strategy.
        """
        raise NotImplementedError


class CoSTEERQueriedKnowledgeV1(CoSTEERQueriedKnowledge):
    def __init__(
        self,
        *args,
        task_to_former_failed_traces: dict = {},
        task_to_similar_task_successful_knowledge: dict = {},
        **kwargs,
    ) -> None:
        self.task_to_former_failed_traces = task_to_former_failed_traces
        self.task_to_similar_task_successful_knowledge = task_to_similar_task_successful_knowledge
        super().__init__(*args, **kwargs)


class CoSTEERRAGStrategyV1(RAGStrategy):
    def __init__(self, knowledgebase: CoSTEERKnowledgeBaseV1, settings: CoSTEERSettings) -> None:
        super().__init__(knowledgebase)
        self.current_generated_trace_count = 0
        self.settings = settings

    def generate_knowledge(
        self,
        evolving_trace: list[EvoStep],
        *,
        return_knowledge: bool = False,
    ) -> Knowledge | None:
        raise NotImplementedError(
            "This method should be considered as an un-implemented method because we encourage everyone to use v2."
        )
        if len(evolving_trace) == self.current_generated_trace_count:
            return
        else:
            for trace_index in range(
                self.current_generated_trace_count,
                len(evolving_trace),
            ):
                evo_step = evolving_trace[trace_index]
                implementations = evo_step.evolvable_subjects
                feedback = evo_step.feedback
                for task_index in range(len(implementations.sub_tasks)):
                    target_task = implementations.sub_tasks[task_index]
                    target_task_information = target_task.get_task_information()
                    implementation = implementations.sub_workspace_list[task_index]
                    single_feedback = feedback[task_index]
                    if single_feedback is None:
                        continue
                    single_knowledge = CoSTEERKnowledge(
                        target_task=target_task,
                        implementation=implementation,
                        feedback=single_feedback,
                    )
                    if target_task_information not in self.knowledgebase.success_task_info_set:
                        self.knowledgebase.implementation_trace.setdefault(
                            target_task_information,
                            [],
                        ).append(single_knowledge)

                        if single_feedback.final_decision == True:
                            self.knowledgebase.success_task_info_set.add(
                                target_task_information,
                            )
            self.current_generated_trace_count = len(evolving_trace)

    def query(
        self,
        evo: EvolvableSubjects,
        evolving_trace: list[EvoStep],
    ) -> CoSTEERQueriedKnowledge | None:
        raise NotImplementedError(
            "This method should be considered as an un-implemented method because we encourage everyone to use v2."
        )
        v1_query_former_trace_limit = self.settings.v1_query_former_trace_limit
        v1_query_similar_success_limit = self.settings.v1_query_similar_success_limit
        fail_task_trial_limit = self.settings.fail_task_trial_limit

        queried_knowledge = CoSTEERQueriedKnowledgeV1()
        for target_task in evo.sub_tasks:
            target_task_information = target_task.get_task_information()
            if target_task_information in self.knowledgebase.success_task_info_set:
                queried_knowledge.success_task_to_knowledge_dict[target_task_information] = (
                    self.knowledgebase.implementation_trace[target_task_information][-1]
                )
            elif (
                len(
                    self.knowledgebase.implementation_trace.setdefault(
                        target_task_information,
                        [],
                    ),
                )
                >= fail_task_trial_limit
            ):
                queried_knowledge.failed_task_info_set.add(target_task_information)
            else:
                queried_knowledge.task_to_former_failed_traces[target_task_information] = (
                    self.knowledgebase.implementation_trace.setdefault(
                        target_task_information,
                        [],
                    )[-v1_query_former_trace_limit:]
                )

                knowledge_base_success_task_list = list(
                    self.knowledgebase.success_task_info_set,
                )
                similarity = calculate_embedding_distance_between_str_list(
                    [target_task_information],
                    knowledge_base_success_task_list,
                )[0]
                similar_indexes = sorted(
                    range(len(similarity)),
                    key=lambda i: similarity[i],
                    reverse=True,
                )[:v1_query_similar_success_limit]
                similar_successful_knowledge = [
                    self.knowledgebase.implementation_trace.setdefault(
                        knowledge_base_success_task_list[index],
                        [],
                    )[-1]
                    for index in similar_indexes
                ]
                queried_knowledge.task_to_similar_task_successful_knowledge[target_task_information] = (
                    similar_successful_knowledge
                )
        return queried_knowledge


class CoSTEERQueriedKnowledgeV2(CoSTEERQueriedKnowledgeV1):
    # Aggregation of knowledge
    def __init__(
        self,
        task_to_former_failed_traces: dict = {},
        task_to_similar_task_successful_knowledge: dict = {},
        task_to_similar_error_successful_knowledge: dict = {},
        **kwargs,
    ) -> None:
        self.task_to_similar_error_successful_knowledge = task_to_similar_error_successful_knowledge
        super().__init__(
            task_to_former_failed_traces=task_to_former_failed_traces,
            task_to_similar_task_successful_knowledge=task_to_similar_task_successful_knowledge,
            **kwargs,
        )


class CoSTEERRAGStrategyV2(RAGStrategy):
    prompt = Prompts(file_path=Path(__file__).parent / "prompts.yaml")

    def __init__(self, knowledgebase: CoSTEERKnowledgeBaseV2, settings: CoSTEERSettings) -> None:
        super().__init__(knowledgebase)
        self.current_generated_trace_count = 0
        self.settings = settings

    def generate_knowledge(
        self,
        evolving_trace: list[EvoStep],
        *,
        return_knowledge: bool = False,
    ) -> Knowledge | None:
        if len(evolving_trace) == self.current_generated_trace_count:
            return None

        else:
            for trace_index in range(self.current_generated_trace_count, len(evolving_trace)):
                evo_step = evolving_trace[trace_index]
                implementations = evo_step.evolvable_subjects
                feedback = evo_step.feedback
                for task_index in range(len(implementations.sub_tasks)):
                    target_task = implementations.sub_tasks[task_index]
                    target_task_information = target_task.get_task_information()
                    implementation = implementations.sub_workspace_list[task_index]
                    single_feedback: CoSTEERSingleFeedback = feedback[task_index]
                    if implementation is None or single_feedback is None:
                        continue
                    single_knowledge = CoSTEERKnowledge(
                        target_task=target_task,
                        implementation=implementation,
                        feedback=single_feedback,
                    )
                    if (
                        target_task_information not in self.knowledgebase.success_task_to_knowledge_dict
                        and implementation is not None
                    ):
                        self.knowledgebase.working_trace_knowledge.setdefault(target_task_information, []).append(
                            single_knowledge,
                        )  # save to working trace
                        if single_feedback.final_decision == True:
                            self.knowledgebase.success_task_to_knowledge_dict.setdefault(
                                target_task_information,
                                single_knowledge,
                            )
                            # Do summary for the last step and update the knowledge graph
                            self.knowledgebase.update_success_task(
                                target_task_information,
                            )
                        else:
                            # generate error node and store into knowledge base
                            error_analysis_result = []
                            if single_feedback.return_checking:
                                error_analysis_result = self.analyze_error(
                                    single_feedback.return_checking,
                                    feedback_type="value",
                                )
                            else:
                                error_analysis_result = self.analyze_error(
                                    single_feedback.execution,
                                    feedback_type="execution",
                                )
                            self.knowledgebase.working_trace_error_analysis.setdefault(
                                target_task_information,
                                [],
                            ).append(
                                error_analysis_result,
                            )  # save to working trace error record, for graph update

            self.current_generated_trace_count = len(evolving_trace)
            return None

    def query(self, evo: EvolvableSubjects, evolving_trace: list[EvoStep]) -> CoSTEERQueriedKnowledge | None:
        conf_knowledge_sampler = self.settings.v2_knowledge_sampler
        queried_knowledge_v2 = CoSTEERQueriedKnowledgeV2(
            success_task_to_knowledge_dict=self.knowledgebase.success_task_to_knowledge_dict,
        )

        queried_knowledge_v2 = self.former_trace_query(
            evo,
            queried_knowledge_v2,
            self.settings.v2_query_former_trace_limit,
            self.settings.v2_add_fail_attempt_to_latest_successful_execution,
        )
        queried_knowledge_v2 = self.component_query(
            evo,
            queried_knowledge_v2,
            self.settings.v2_query_component_limit,
            knowledge_sampler=conf_knowledge_sampler,
        )
        queried_knowledge_v2 = self.error_query(
            evo,
            queried_knowledge_v2,
            self.settings.v2_query_error_limit,
            knowledge_sampler=conf_knowledge_sampler,
        )
        return queried_knowledge_v2

    def analyze_component(
        self,
        target_task_information,
    ) -> list[UndirectedNode]:  # Hardcode: certain component nodes
        all_component_nodes = self.knowledgebase.graph.get_all_nodes_by_label_list(["component"])
        if not len(all_component_nodes):
            return []
        all_component_content = ""
        for _, component_node in enumerate(all_component_nodes):
            all_component_content += f"{component_node.content}, \n"
        analyze_component_system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(self.prompt["analyze_component_prompt_v1_system"])
            .render(
                all_component_content=all_component_content,
            )
        )

        analyze_component_user_prompt = target_task_information
        try:
            component_no_list = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    system_prompt=analyze_component_system_prompt,
                    user_prompt=analyze_component_user_prompt,
                    json_mode=True,
                    json_target_type=List[int],
                ),
            )["component_no_list"]
            return [all_component_nodes[index - 1] for index in sorted(list(set(component_no_list)))]
        except:
            logger.warning("Error when analyzing components.")
            analyze_component_user_prompt = "Your response is not a valid component index list."

        return []

    def analyze_error(
        self,
        single_feedback,
        feedback_type="execution",
    ) -> list[
        UndirectedNode | str
    ]:  # Hardcode: Raised errors, existed error nodes + not existed error nodes(here, they are strs)
        if feedback_type == "execution":
            match = re.search(
                r'File "(?P<file>.+)", line (?P<line>\d+), in (?P<function>.+)\n\s+(?P<error_line>.+)\n(?P<error_type>\w+): (?P<error_message>.+)',
                single_feedback,
            )
            if match:
                error_details = match.groupdict()
                # last_traceback = f'File "{error_details["file"]}", line {error_details["line"]}, in {error_details["function"]}\n    {error_details["error_line"]}'
                error_type = error_details["error_type"]
                error_line = error_details["error_line"]
                error_contents = [f"ErrorType: {error_type}" + "\n" + f"Error line: {error_line}"]
            else:
                error_contents = ["Undefined Error"]
        elif feedback_type == "value":  # value check error
            value_check_types = r"The source dataframe and the ground truth dataframe have different rows count.|The source dataframe and the ground truth dataframe have different index.|Some values differ by more than the tolerance of 1e-6.|No sufficient correlation found when shifting up|Something wrong happens when naming the multi indices of the dataframe."
            error_contents = re.findall(value_check_types, single_feedback)
        else:
            error_contents = ["Undefined Error"]

        all_error_nodes = self.knowledgebase.graph.get_all_nodes_by_label_list(["error"])
        if not len(all_error_nodes):
            return error_contents
        else:
            error_list = []
            for error_content in error_contents:
                for error_node in all_error_nodes:
                    if error_content == error_node.content:
                        error_list.append(error_node)
                    else:
                        error_list.append(error_content)
                    if error_list[-1] in error_list[:-1]:
                        error_list.pop()

            return error_list

    def former_trace_query(
        self,
        evo: EvolvableSubjects,
        queried_knowledge_v2: CoSTEERQueriedKnowledgeV2,
        v2_query_former_trace_limit: int = 5,
        v2_add_fail_attempt_to_latest_successful_execution: bool = False,
    ) -> Union[CoSTEERQueriedKnowledge, set]:
        """
        Query the former trace knowledge of the working trace, and find all the failed task information which tried more than fail_task_trial_limit times
        """
        fail_task_trial_limit = self.settings.fail_task_trial_limit

        for target_task in evo.sub_tasks:
            target_task_information = target_task.get_task_information()
            if (
                target_task_information not in self.knowledgebase.success_task_to_knowledge_dict
                and target_task_information in self.knowledgebase.working_trace_knowledge
                and len(self.knowledgebase.working_trace_knowledge[target_task_information]) >= fail_task_trial_limit
            ):
                queried_knowledge_v2.failed_task_info_set.add(target_task_information)

            if (
                target_task_information not in self.knowledgebase.success_task_to_knowledge_dict
                and target_task_information not in queried_knowledge_v2.failed_task_info_set
                and target_task_information in self.knowledgebase.working_trace_knowledge
            ):
                former_trace_knowledge = copy.copy(
                    self.knowledgebase.working_trace_knowledge[target_task_information],
                )
                # in former trace query we will delete the right trace in the following order:[..., value_generated_flag is True, value_generated_flag is False, ...]
                # because we think this order means a deterioration of the trial (like a wrong gradient descent)
                current_index = 1
                while current_index < len(former_trace_knowledge):
                    if (
                        not former_trace_knowledge[current_index].feedback.return_checking
                        and former_trace_knowledge[current_index - 1].feedback.return_checking
                    ):
                        former_trace_knowledge.pop(current_index)
                    else:
                        current_index += 1

                latest_attempt = None
                if v2_add_fail_attempt_to_latest_successful_execution:
                    # When the last successful execution is not the last one in the working trace, it means we have tried to correct it. We should tell the agent this fail trial to avoid endless loop in the future.
                    if (
                        len(former_trace_knowledge) > 0
                        and len(self.knowledgebase.working_trace_knowledge[target_task_information]) > 1
                        and self.knowledgebase.working_trace_knowledge[target_task_information].index(
                            former_trace_knowledge[-1]
                        )
                        < len(self.knowledgebase.working_trace_knowledge[target_task_information]) - 1
                    ):
                        latest_attempt = self.knowledgebase.working_trace_knowledge[target_task_information][-1]

                queried_knowledge_v2.task_to_former_failed_traces[target_task_information] = (
                    former_trace_knowledge[-v2_query_former_trace_limit:],
                    latest_attempt,
                )
            else:
                queried_knowledge_v2.task_to_former_failed_traces[target_task_information] = ([], None)

        return queried_knowledge_v2

    def component_query(
        self,
        evo: EvolvableSubjects,
        queried_knowledge_v2: CoSTEERQueriedKnowledgeV2,
        v2_query_component_limit: int = 5,
        knowledge_sampler: float = 1.0,
    ) -> CoSTEERQueriedKnowledge | None:
        for target_task in evo.sub_tasks:
            target_task_information = target_task.get_task_information()
            if (
                target_task_information in self.knowledgebase.success_task_to_knowledge_dict
                or target_task_information in queried_knowledge_v2.failed_task_info_set
            ):
                queried_knowledge_v2.task_to_similar_task_successful_knowledge[target_task_information] = []
            else:
                if target_task_information not in self.knowledgebase.task_to_component_nodes:
                    self.knowledgebase.task_to_component_nodes[target_task_information] = self.analyze_component(
                        target_task_information,
                    )

                component_analysis_result = self.knowledgebase.task_to_component_nodes[target_task_information]

                if len(component_analysis_result) > 1:
                    task_des_node_list = self.knowledgebase.graph_query_by_intersection(
                        component_analysis_result,
                        constraint_labels=["task_description"],
                    )
                    single_component_constraint = (v2_query_component_limit // len(component_analysis_result)) + 1
                else:
                    task_des_node_list = []
                    single_component_constraint = v2_query_component_limit
                queried_knowledge_v2.task_to_similar_task_successful_knowledge[target_task_information] = []
                for component_node in component_analysis_result:
                    # Reverse iterate, a trade-off with intersection search
                    count = 0
                    for task_des_node in self.knowledgebase.graph_query_by_node(
                        node=component_node,
                        step=1,
                        constraint_labels=["task_description"],
                        block=True,
                    )[::-1]:
                        if task_des_node not in task_des_node_list:
                            task_des_node_list.append(task_des_node)
                            count += 1
                        if count >= single_component_constraint:
                            break

                for node in task_des_node_list:
                    for searched_node in self.knowledgebase.graph_query_by_node(
                        node=node,
                        step=50,
                        constraint_labels=[
                            "task_success_implement",
                        ],
                        block=True,
                    ):
                        if searched_node.label == "task_success_implement":
                            target_knowledge = self.knowledgebase.node_to_implementation_knowledge_dict[
                                searched_node.id
                            ]
                        if (
                            target_knowledge
                            not in queried_knowledge_v2.task_to_similar_task_successful_knowledge[
                                target_task_information
                            ]
                        ):
                            queried_knowledge_v2.task_to_similar_task_successful_knowledge[
                                target_task_information
                            ].append(target_knowledge)

                # finally add embedding related knowledge
                knowledge_base_success_task_list = list(self.knowledgebase.success_task_to_knowledge_dict)

                similarity = calculate_embedding_distance_between_str_list(
                    [target_task_information],
                    knowledge_base_success_task_list,
                )[0]
                similar_indexes = sorted(
                    range(len(similarity)),
                    key=lambda i: similarity[i],
                    reverse=True,
                )
                embedding_similar_successful_knowledge = [
                    self.knowledgebase.success_task_to_knowledge_dict[knowledge_base_success_task_list[index]]
                    for index in similar_indexes
                ]
                for knowledge in embedding_similar_successful_knowledge:
                    if (
                        knowledge
                        not in queried_knowledge_v2.task_to_similar_task_successful_knowledge[target_task_information]
                    ):
                        queried_knowledge_v2.task_to_similar_task_successful_knowledge[target_task_information].append(
                            knowledge
                        )

                if knowledge_sampler > 0:
                    queried_knowledge_v2.task_to_similar_task_successful_knowledge[target_task_information] = [
                        knowledge
                        for knowledge in queried_knowledge_v2.task_to_similar_task_successful_knowledge[
                            target_task_information
                        ]
                        if random.uniform(0, 1) <= knowledge_sampler
                    ]

                # Make sure no less than half of the knowledge are from GT
                queried_knowledge_list = queried_knowledge_v2.task_to_similar_task_successful_knowledge[
                    target_task_information
                ]
                queried_from_gt_knowledge_list = [
                    knowledge
                    for knowledge in queried_knowledge_list
                    if knowledge.feedback is not None and knowledge.feedback.final_decision_based_on_gt == True
                ]
                queried_without_gt_knowledge_list = [
                    knowledge
                    for knowledge in queried_knowledge_list
                    if knowledge.feedback is not None and knowledge.feedback.final_decision_based_on_gt == False
                ]
                queried_from_gt_knowledge_count = max(
                    min((v2_query_component_limit // 2 + 1), len(queried_from_gt_knowledge_list)),
                    v2_query_component_limit - len(queried_without_gt_knowledge_list),
                )
                queried_knowledge_v2.task_to_similar_task_successful_knowledge[target_task_information] = (
                    queried_from_gt_knowledge_list[:queried_from_gt_knowledge_count]
                    + queried_without_gt_knowledge_list[: v2_query_component_limit - queried_from_gt_knowledge_count]
                )

        return queried_knowledge_v2

    def error_query(
        self,
        evo: EvolvableSubjects,
        queried_knowledge_v2: CoSTEERQueriedKnowledgeV2,
        v2_query_error_limit: int = 5,
        knowledge_sampler: float = 1.0,
    ) -> CoSTEERQueriedKnowledge | None:
        for task_index, target_task in enumerate(evo.sub_tasks):
            target_task_information = target_task.get_task_information()
            queried_knowledge_v2.task_to_similar_error_successful_knowledge[target_task_information] = []
            if (
                target_task_information in self.knowledgebase.success_task_to_knowledge_dict
                or target_task_information in queried_knowledge_v2.failed_task_info_set
            ):
                queried_knowledge_v2.task_to_similar_error_successful_knowledge[target_task_information] = []
            else:
                queried_knowledge_v2.task_to_similar_error_successful_knowledge[target_task_information] = []
                if (
                    target_task_information in self.knowledgebase.working_trace_error_analysis
                    and len(self.knowledgebase.working_trace_error_analysis[target_task_information]) > 0
                    and len(queried_knowledge_v2.task_to_former_failed_traces[target_task_information]) > 0
                ):
                    queried_last_trace = queried_knowledge_v2.task_to_former_failed_traces[target_task_information][0][
                        -1
                    ]
                    target_index = self.knowledgebase.working_trace_knowledge[target_task_information].index(
                        queried_last_trace,
                    )
                    last_knowledge_error_analysis_result = self.knowledgebase.working_trace_error_analysis[
                        target_task_information
                    ][target_index]
                else:
                    last_knowledge_error_analysis_result = []

                error_nodes = []
                for error_node in last_knowledge_error_analysis_result:
                    if not isinstance(error_node, UndirectedNode):
                        error_node = self.knowledgebase.graph_get_node_by_content(content=error_node)
                        if error_node is None:
                            continue
                    error_nodes.append(error_node)

                if len(error_nodes) > 1:
                    task_trace_node_list = self.knowledgebase.graph_query_by_intersection(
                        error_nodes,
                        constraint_labels=["task_trace"],
                        output_intersection_origin=True,
                    )
                    single_error_constraint = (v2_query_error_limit // len(error_nodes)) + 1
                else:
                    task_trace_node_list = []
                    single_error_constraint = v2_query_error_limit
                for error_node in error_nodes:
                    # Reverse iterate, a trade-off with intersection search
                    count = 0
                    for task_trace_node in self.knowledgebase.graph_query_by_node(
                        node=error_node,
                        step=1,
                        constraint_labels=["task_trace"],
                        block=True,
                    )[::-1]:
                        if task_trace_node not in task_trace_node_list:
                            task_trace_node_list.append([[error_node], task_trace_node])
                            count += 1
                        if count >= single_error_constraint:
                            break

                # for error_node in last_knowledge_error_analysis_result:
                #     if not isinstance(error_node, UndirectedNode):
                #         error_node = self.knowledgebase.graph_get_node_by_content(content=error_node)
                #         if error_node is None:
                #             continue
                #     for searched_node in self.knowledgebase.graph_query_by_node(
                #         node=error_node,
                #         step=1,
                #         constraint_labels=["task_trace"],
                #         block=True,
                #     ):
                #         if searched_node not in [node[0] for node in task_trace_node_list]:
                #             task_trace_node_list.append((searched_node, error_node.content))

                same_error_success_knowledge_pair_list = []
                same_error_success_node_set = set()
                for error_node_list, trace_node in task_trace_node_list:
                    for searched_trace_success_node in self.knowledgebase.graph_query_by_node(
                        node=trace_node,
                        step=50,
                        constraint_labels=[
                            "task_trace",
                            "task_success_implement",
                            "task_description",
                        ],
                        block=True,
                    ):
                        if (
                            searched_trace_success_node not in same_error_success_node_set
                            and searched_trace_success_node.label == "task_success_implement"
                        ):
                            same_error_success_node_set.add(searched_trace_success_node)

                            trace_knowledge = self.knowledgebase.node_to_implementation_knowledge_dict[trace_node.id]
                            success_knowledge = self.knowledgebase.node_to_implementation_knowledge_dict[
                                searched_trace_success_node.id
                            ]
                            error_content = ""
                            for index, error_node in enumerate(error_node_list):
                                error_content += f"{index+1}. {error_node.content}; "
                            same_error_success_knowledge_pair_list.append(
                                (
                                    error_content,
                                    (trace_knowledge, success_knowledge),
                                ),
                            )

                if knowledge_sampler > 0:
                    same_error_success_knowledge_pair_list = [
                        knowledge
                        for knowledge in same_error_success_knowledge_pair_list
                        if random.uniform(0, 1) <= knowledge_sampler
                    ]

                same_error_success_knowledge_pair_list = same_error_success_knowledge_pair_list[:v2_query_error_limit]
                queried_knowledge_v2.task_to_similar_error_successful_knowledge[target_task_information] = (
                    same_error_success_knowledge_pair_list
                )

        return queried_knowledge_v2


class CoSTEERKnowledgeBaseV2(EvolvingKnowledgeBase):
    def __init__(self, init_component_list=None, path: str | Path = None) -> None:
        """
        Load knowledge, offer brief information of knowledge and common handle interfaces
        """
        self.graph: UndirectedGraph = UndirectedGraph(Path.cwd() / "graph.pkl")
        logger.info(f"CoSTEER Knowledge Graph loaded, size={self.graph.size()}")

        if init_component_list:
            for component in init_component_list:
                exist_node = self.graph.get_node_by_content(content=component)
                node = exist_node if exist_node else UndirectedNode(content=component, label="component")
                self.graph.add_nodes(node=node, neighbors=[])

        # A dict containing all working trace until they fail or succeed
        self.working_trace_knowledge = {}

        # A dict containing error analysis each step aligned with working trace
        self.working_trace_error_analysis = {}

        # Add already success task
        self.success_task_to_knowledge_dict = {}

        # key:node_id(for task trace and success implement), value:knowledge instance(aka 'CoSTEERKnowledge')
        self.node_to_implementation_knowledge_dict = {}

        # store the task description to component nodes
        self.task_to_component_nodes = {}

    def get_all_nodes_by_label(self, label: str) -> list[UndirectedNode]:
        return self.graph.get_all_nodes_by_label(label)

    def update_success_task(
        self,
        success_task_info: str,
    ):  # Transfer the success tasks' working trace to knowledge storage & graph
        success_task_trace = self.working_trace_knowledge[success_task_info]
        success_task_error_analysis_record = (
            self.working_trace_error_analysis[success_task_info]
            if success_task_info in self.working_trace_error_analysis
            else []
        )
        task_des_node = UndirectedNode(content=success_task_info, label="task_description")
        self.graph.add_nodes(
            node=task_des_node,
            neighbors=self.task_to_component_nodes[success_task_info],
        )  # 1st version, we assume that all component nodes are given
        for index, trace_unit in enumerate(success_task_trace):  # every unit: single_knowledge
            neighbor_nodes = [task_des_node]
            if index != len(success_task_trace) - 1:
                trace_node = UndirectedNode(
                    content=trace_unit.get_implementation_and_feedback_str(),
                    label="task_trace",
                )
                self.node_to_implementation_knowledge_dict[trace_node.id] = trace_unit
                for node_index, error_node in enumerate(success_task_error_analysis_record[index]):
                    if type(error_node).__name__ == "str":
                        queried_node = self.graph.get_node_by_content(content=error_node)
                        if queried_node is None:
                            new_error_node = UndirectedNode(content=error_node, label="error")
                            self.graph.add_node(node=new_error_node)
                            success_task_error_analysis_record[index][node_index] = new_error_node
                        else:
                            success_task_error_analysis_record[index][node_index] = queried_node
                neighbor_nodes.extend(success_task_error_analysis_record[index])
                self.graph.add_nodes(node=trace_node, neighbors=neighbor_nodes)
            else:
                success_node = UndirectedNode(
                    content=trace_unit.get_implementation_and_feedback_str(),
                    label="task_success_implement",
                )
                self.graph.add_nodes(node=success_node, neighbors=neighbor_nodes)
                self.node_to_implementation_knowledge_dict[success_node.id] = trace_unit

    def query(self):
        pass

    def graph_get_node_by_content(self, content: str) -> UndirectedNode:
        return self.graph.get_node_by_content(content=content)

    def graph_query_by_content(
        self,
        content: Union[str, list[str]],
        topk_k: int = 5,
        step: int = 1,
        constraint_labels: list[str] = None,
        constraint_node: UndirectedNode = None,
        similarity_threshold: float = 0.0,
        constraint_distance: float = 0,
        block: bool = False,
    ) -> list[UndirectedNode]:
        """
        search graph by content similarity and connection relationship, return empty list if nodes' chain without node
        near to constraint_node

        Parameters
        ----------
        constraint_distance
        content
        topk_k: the upper number of output for each query, if the number of fit nodes is less than topk_k, return all fit nodes's content
        step
        constraint_labels
        constraint_node
        similarity_threshold
        block: despite the start node, the search can only flow through the constraint_label type nodes

        Returns
        -------

        """

        return self.graph.query_by_content(
            content=content,
            topk_k=topk_k,
            step=step,
            constraint_labels=constraint_labels,
            constraint_node=constraint_node,
            similarity_threshold=similarity_threshold,
            constraint_distance=constraint_distance,
            block=block,
        )

    def graph_query_by_node(
        self,
        node: UndirectedNode,
        step: int = 1,
        constraint_labels: list[str] = None,
        constraint_node: UndirectedNode = None,
        constraint_distance: float = 0,
        block: bool = False,
    ) -> list[UndirectedNode]:
        """
        search graph by connection, return empty list if nodes' chain without node near to constraint_node
        Parameters
        ----------
        node : start node
        step : the max steps will be searched
        constraint_labels : the labels of output nodes
        constraint_node : the node that the output nodes must connect to
        constraint_distance : the max distance between output nodes and constraint_node
        block: despite the start node, the search can only flow through the constraint_label type nodes

        Returns
        -------
        A list of nodes

        """
        nodes = self.graph.query_by_node(
            node=node,
            step=step,
            constraint_labels=constraint_labels,
            constraint_node=constraint_node,
            constraint_distance=constraint_distance,
            block=block,
        )
        return nodes

    def graph_query_by_intersection(
        self,
        nodes: list[UndirectedNode],
        steps: int = 1,
        constraint_labels: list[str] = None,
        output_intersection_origin: bool = False,
    ) -> list[UndirectedNode] | list[list[list[UndirectedNode], UndirectedNode]]:
        """
        search graph by node intersection, node intersected by a higher frequency has a prior order in the list
        Parameters
        ----------
        nodes : node list
        step : the max steps will be searched
        constraint_labels : the labels of output nodes
        output_intersection_origin: output the list that contains the node which form this intersection node

        Returns
        -------
        A list of nodes

        """
        node_count = len(nodes)
        assert node_count >= 2, "nodes length must >=2"
        intersection_node_list = []
        if output_intersection_origin:
            origin_list = []
        for k in range(node_count, 1, -1):
            possible_combinations = combinations(nodes, k)
            for possible_combination in possible_combinations:
                node_list = list(possible_combination)
                intersection_node_list.extend(
                    self.graph.get_nodes_intersection(node_list, steps=steps, constraint_labels=constraint_labels),
                )
                if output_intersection_origin:
                    for _ in range(len(intersection_node_list)):
                        origin_list.append(node_list)
        intersection_node_list_sort_by_freq = []
        for index, node in enumerate(intersection_node_list):
            if node not in intersection_node_list_sort_by_freq:
                if output_intersection_origin:
                    intersection_node_list_sort_by_freq.append([origin_list[index], node])
                else:
                    intersection_node_list_sort_by_freq.append(node)

        return intersection_node_list_sort_by_freq



================================================
File: rdagent/components/coder/CoSTEER/prompts.yaml
================================================

analyze_component_prompt_v1_system: |-
  User is getting a new task that might consist of the components below (given in component_index: component_description):
  {{all_component_content}}

  You should find out what components does the new task have, and put their indices in a list.
  Please response the critic in the json format. Here is an example structure for the JSON output, please strictly follow the format:
  {
      "component_no_list": the list containing indices of components.
  }


================================================
File: rdagent/components/coder/CoSTEER/scheduler.py
================================================
import random

from rdagent.components.coder.CoSTEER.evolvable_subjects import EvolvingItem
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.core.scenario import Scenario
from rdagent.log import rdagent_logger as logger


def random_select(
    to_be_finished_task_index: list,
    evo: EvolvingItem,
    selected_num: int,
    queried_knowledge: CoSTEERQueriedKnowledge,
    scen: Scenario,
):

    to_be_finished_task_index = random.sample(
        to_be_finished_task_index,
        selected_num,
    )

    logger.info(f"The random selection is: {to_be_finished_task_index}")
    return to_be_finished_task_index



================================================
File: rdagent/components/coder/CoSTEER/task.py
================================================
from rdagent.core.experiment import Task


class CoSTEERTask(Task):
    def __init__(self, base_code: str = None, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        # TODO: we may upgrade the base_code into a workspace-like thing to know previous.
        # NOTE: (xiao) think we don't need the base_code anymore. The information should be retrieved from the workspace.
        self.base_code = base_code



================================================
File: rdagent/components/coder/data_science/conf.py
================================================
from typing import Literal

from rdagent.components.coder.CoSTEER.config import CoSTEERSettings
from rdagent.utils.env import (
    CondaConf,
    DockerEnv,
    DSDockerConf,
    Env,
    LocalEnv,
    MLEBDockerConf,
    MLECondaConf,
)


class DSCoderCoSTEERSettings(CoSTEERSettings):
    """Data Science CoSTEER settings"""

    class Config:
        env_prefix = "DS_Coder_CoSTEER_"

    max_seconds: int = 2400
    env_type: str = "docker"
    # TODO: extract a function for env and conf.


def get_ds_env(conf_type: Literal["kaggle", "mlebench"] = "kaggle") -> Env:
    """
    Retrieve the appropriate environment configuration based on the env_type setting.

    Returns:
        Env: An instance of the environment configured either as DockerEnv or LocalEnv.

    Raises:
        ValueError: If the env_type is not recognized.
    """
    conf = DSCoderCoSTEERSettings()
    assert conf_type in ["kaggle", "mlebench"], f"Unknown conf_type: {conf_type}"

    if conf.env_type == "docker":
        env_conf = DSDockerConf() if conf_type == "kaggle" else MLEBDockerConf()
        env = DockerEnv(conf=env_conf)
    elif conf.env_type == "conda":
        env = LocalEnv(
            conf=(
                CondaConf(conda_env_name=conf_type) if conf_type == "kaggle" else MLECondaConf(conda_env_name=conf_type)
            )
        )
    else:
        raise ValueError(f"Unknown env type: {conf.env_type}")
    return env



================================================
File: rdagent/components/coder/data_science/ensemble/__init__.py
================================================
"""
File structure
- ___init__.py: the entrance/agent of coder
- evaluator.py
- conf.py
- exp.py: everything under the experiment, e.g.
    - Task
    - Experiment
    - Workspace
- test.py
    - Each coder could be tested.
"""

import json
from typing import Dict

from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiEvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.components.coder.data_science.conf import DSCoderCoSTEERSettings
from rdagent.components.coder.data_science.ensemble.eval import EnsembleCoSTEEREvaluator
from rdagent.components.coder.data_science.ensemble.exp import EnsembleTask
from rdagent.core.exception import CoderError
from rdagent.core.experiment import FBWorkspace
from rdagent.core.scenario import Scenario
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T


class EnsembleMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: EnsembleTask,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:
        # Get task information for knowledge querying
        ensemble_information_str = target_task.get_task_information()

        # Query knowledge
        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[ensemble_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[ensemble_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            [
                knowledge
                for knowledge in queried_former_failed_knowledge[0]
                if knowledge.implementation.file_dict.get("ensemble.py") != workspace.file_dict.get("ensemble.py")
            ],
            queried_former_failed_knowledge[1],
        )

        # Generate code with knowledge integration
        competition_info = self.scen.get_scenario_all_desc()
        system_prompt = T(".prompts:ensemble_coder.system").r(
            task_desc=ensemble_information_str,
            competition_info=competition_info,
            queried_similar_successful_knowledge=queried_similar_successful_knowledge,
            queried_former_failed_knowledge=(
                queried_former_failed_knowledge[0] if queried_former_failed_knowledge else None
            ),
            all_code=workspace.all_codes,
        )
        user_prompt = T(".prompts:ensemble_coder.user").r(
            ensemble_spec=workspace.file_dict["spec/ensemble.md"],
            latest_code=workspace.file_dict.get("ensemble.py"),
            latest_code_feedback=prev_task_feedback,
        )

        for _ in range(5):
            ensemble_code = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                    json_mode=True,
                    json_target_type=Dict[str, str],
                )
            )["code"]
            if ensemble_code != workspace.file_dict.get("ensemble.py"):
                break
            else:
                user_prompt = user_prompt + "\nPlease avoid generating same code to former code!"
        else:
            raise CoderError("Failed to generate a new ensemble code.")

        return {
            "ensemble.py": ensemble_code,
        }

    def assign_code_list_to_evo(self, code_list: list[dict[str, str]], evo):
        """
        Assign the code list to the evolving item.

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                # evo.sub_workspace_list[index] = FBWorkspace(target_task=evo.sub_tasks[index])
                evo.sub_workspace_list[index] = evo.experiment_workspace
            evo.sub_workspace_list[index].inject_files(**code_list[index])
        return evo


class EnsembleCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        settings = DSCoderCoSTEERSettings()
        eva = CoSTEERMultiEvaluator(EnsembleCoSTEEREvaluator(scen=scen), scen=scen)
        es = EnsembleMultiProcessEvolvingStrategy(scen=scen, settings=settings)

        super().__init__(*args, settings=settings, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)



================================================
File: rdagent/components/coder/data_science/ensemble/conf.py
================================================
# Configuration file for ensemble component
# Currently empty as no specific configuration is needed



================================================
File: rdagent/components/coder/data_science/ensemble/eval.py
================================================
import json
import re
from pathlib import Path

from jinja2 import Environment, StrictUndefined

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.utils.agent.tpl import T
from rdagent.utils.agent.workflow import build_cls_from_json_with_retry

DIRNAME = Path(__file__).absolute().resolve().parent

EnsembleEvalFeedback = CoSTEERSingleFeedback


class EnsembleCoSTEEREvaluator(CoSTEEREvaluator):
    def evaluate(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        gt_implementation: FBWorkspace,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> EnsembleEvalFeedback:

        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return EnsembleEvalFeedback(
                execution="This task has failed too many times, skip implementation.",
                code="This task has failed too many times, skip implementation.",
                return_checking="This task has failed too many times, skip implementation.",
                final_decision=False,
            )

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/sample/{self.scen.competition}": "/kaggle/input"}

        fname = "test/ensemble_test.txt"
        test_code = (DIRNAME / "eval_tests" / "ensemble_test.txt").read_text()
        test_code = (
            Environment(undefined=StrictUndefined)
            .from_string(test_code)
            .render(
                model_names=[
                    fn[:-3] for fn in implementation.file_dict.keys() if fn.startswith("model_") and "test" not in fn
                ]
            )
        )

        implementation.inject_files(**{fname: test_code})
        stdout, ret_code = implementation.execute_ret_code(env=env, entry=f"python {fname}")

        stdout += f"\nNOTE: the above scripts run with return code {ret_code}"

        if "main.py" in implementation.file_dict:
            workflow_stdout = implementation.execute(env=env, entry="python main.py")
            workflow_stdout = re.sub(r"=== Start of EDA part ===(.*)=== End of EDA part ===", "", workflow_stdout)
        else:
            workflow_stdout = None

        system_prompt = T(".prompts:ensemble_eval.system").r(
            task_desc=target_task_information,
            test_code=test_code,
            code=implementation.file_dict["ensemble.py"],
            workflow_stdout=workflow_stdout,
            workflow_code=implementation.all_codes,
        )
        user_prompt = T(".prompts:ensemble_eval.user").r(
            stdout=stdout,
            workflow_stdout=workflow_stdout,
        )
        efb = build_cls_from_json_with_retry(
            EnsembleEvalFeedback,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            init_kwargs_update_func=EnsembleEvalFeedback.val_and_update_init_dict,
        )
        efb.final_decision = efb.final_decision and ret_code == 0
        return efb



================================================
File: rdagent/components/coder/data_science/ensemble/exp.py
================================================
import pickle
import site
import traceback
from pathlib import Path
from typing import Dict, Optional

from rdagent.components.coder.CoSTEER.task import CoSTEERTask
from rdagent.core.utils import cache_with_pickle


# Because we use isinstance to distinguish between different types of tasks, we need to use sub classes to represent different types of tasks
class EnsembleTask(CoSTEERTask):
    pass



================================================
File: rdagent/components/coder/data_science/ensemble/prompts.yaml
================================================
ensemble_coder:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    ## Task Description
    Currently, you are working on model ensemble implementation. Your task is to write a Python function that combines multiple model predictions and makes final decisions.

    Your specific task as follows:
    {{ task_desc }}

    ## Competition Information for This Task
    {{ competition_info }}

    {% if queried_similar_successful_knowledge|length != 0 or queried_former_failed_knowledge|length != 0 %}
    ## Relevant Information for This Task
    {% endif %}

    {% if queried_similar_successful_knowledge|length != 0 %}
    --------- Successful Implementations for Similar Models ---------
    ====={% for similar_successful_knowledge in queried_similar_successful_knowledge %} Model {{ loop.index }}:=====
    {{ similar_successful_knowledge.target_task.get_task_information() }}
    =====Code:=====
    {{ similar_successful_knowledge.implementation.file_dict["ensemble.py"] }}
    {% endfor %} 
    {% endif %}

    {% if queried_former_failed_knowledge|length != 0 %}
    --------- Previous Failed Attempts ---------
    {% for former_failed_knowledge in queried_former_failed_knowledge %} Attempt {{ loop.index }}:
    =====Code:=====
    {{ former_failed_knowledge.implementation.file_dict["ensemble.py"] }}
    =====Feedback:=====
    {{ former_failed_knowledge.feedback }}
    {% endfor %}
    {% endif %}

    ## Guidelines
    1. The function's code is associated with several other functions including a data loader, feature engineering, and model training. all codes are as follows:
    {{ all_code }}
    2. You should avoid using logging module to output information in your generated code, and instead use the print() function.

    ## Output Format
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }

  user: |-
    --------- Ensemble Specification ---------
    {{ ensemble_spec }}

    {% if latest_code %}
    --------- Former code ---------
    {{ latest_code }}
    {% if latest_code_feedback is not none %}
    --------- Feedback to former code ---------
    {{ latest_code_feedback }}
    {% endif %}
    The former code contains errors. You should correct the code based on the provided information, ensuring you do not repeat the same mistakes.
    {% endif %}


ensemble_eval:
  system: |-
    You are a data scientist responsible for evaluating ensemble implementation code generation.

    ## Task Description
    {{ task_desc }}

    ## Ensemble Code
    ```python
    {{ code }}
    ```

    ## Testing Process
    The ensemble code is tested using the following script:
    ```python
    {{ test_code }}
    ```
    You will analyze the execution results based on the test output provided.
    
    {% if workflow_stdout is not none %}
    ### Whole Workflow Consideration
    The ensemble code is part of the whole workflow. The user has executed the entire pipeline and provided additional stdout.

    **Workflow Code:**
    ```python
    {{ workflow_code }}
    ```

    You should evaluate both the ensemble test results and the overall workflow results. **Approve the code only if both tests pass.**
    {% endif %}

    ## Evaluation Criteria
    - You will be given the standard output (`stdout`) from the ensemble test and, if applicable, the workflow test.
    - Code should have no try-except blocks because they can hide errors.
    - The stdout includes the local variable values from the ensemble code execution. Check whether the validation score is calculated correctly.
    
    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe how well the ensemble executed, including any errors or issues encountered. Append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Detail the checks performed on the ensemble results, including shape and value validation.",
        "code": "Assess code quality, readability, and adherence to specifications.",
        "final_decision": <true/false>
    }
    ```
  user: |-    
    --------- Ensemble test stdout ---------
    {{ stdout }}   
    {% if workflow_stdout is not none %}
    --------- Whole workflow test stdout ---------
    {{ workflow_stdout }}
    {% endif %}



================================================
File: rdagent/components/coder/data_science/ensemble/test.py
================================================
"""
Helper functions for testing the ensemble coder(CoSTEER-based) component.
"""

import sys
from pathlib import Path

from rdagent.components.coder.data_science.ensemble import EnsembleCoSTEER
from rdagent.components.coder.data_science.ensemble.exp import EnsembleTask
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.scen import KaggleScen

# Add the competition folder to path
COMPETITION_PATH = (
    Path(__file__).parent.parent.parent.parent.parent
    / "scenarios"
    / "kaggle"
    / "tpl_ex"
    / "aerial-cactus-identification"
)
sys.path.append(str(COMPETITION_PATH))

EnsembleExperiment = DSExperiment


def load_ensemble_spec():
    spec_path = COMPETITION_PATH / "spec" / "ensemble.md"
    with open(spec_path, "r") as f:
        return f.read()


def develop_one_competition(competition: str):
    # Initialize scenario and coder
    scen = KaggleScen(competition=competition)
    ensemble_coder = EnsembleCoSTEER(scen)
    # Load ensemble specification
    ensemble_spec = load_ensemble_spec()

    # Create the ensemble task with actual data context and specification
    task = EnsembleTask(
        name="EnsembleTask",
        description="""
        Implement ensemble and decision making for model predictions.
        """,
    )

    exp = EnsembleExperiment(sub_tasks=[task])

    # Injecting the corresponding specification
    exp.experiment_workspace.inject_files(**{"spec/ensemble.md": ensemble_spec})

    # Develop the experiment
    exp = ensemble_coder.develop(exp)
    return exp


if __name__ == "__main__":
    develop_one_competition("aerial-cactus-identification")



================================================
File: rdagent/components/coder/data_science/ensemble/eval_tests/ensemble_test.txt
================================================
"""
A qualified ensemble implementation should:
- Successfully run
- Return predictions
- Have correct shapes for inputs and outputs
- Use validation data appropriately
"""

import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
import torch
import tensorflow as tf
from load_data import load_data
from feature import feat_eng
from ensemble import ensemble_workflow

def print_preds_info(model_name, data_type, preds):
    if preds is None:
        print(f"Model {model_name} {data_type} predictions: None")
    else:
        print(f"Model {model_name} {data_type} predictions shape: {preds.shape}")

        if isinstance(preds, (pd.DataFrame, pd.Series)):
            print(preds.head())
        elif isinstance(preds, (np.ndarray, torch.Tensor, tf.Tensor)):
            print(pd.DataFrame(preds).head())
        elif isinstance(preds, list):
            print(pd.DataFrame(preds[:5]))
        else:
            print(f"Unknown prediction type: {type(preds)}")

X, y, test_X, test_ids = load_data()
X, y, test_X = feat_eng(X, y, test_X)
train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the types of train_y and val_y
print(f"train_y type: {type(train_y)}, val_y type: {type(val_y)}")

test_preds_dict = {}
val_preds_dict = {}
{% for mn in model_names %}
from {{mn}} import model_workflow as {{mn}}_workflow
val_preds_dict["{{mn}}"], test_preds_dict["{{mn}}"], _ = {{mn}}_workflow(
    X=train_X,
    y=train_y,
    val_X=val_X,
    val_y=val_y,
    test_X=test_X
)

print_preds_info("{{mn}}", "test", test_preds_dict["{{mn}}"])
{% endfor %}

for key in val_preds_dict.keys():
    if val_preds_dict[key] is None: 
        print(f"Model {key} validation predictions (val_preds_dict[key]) is None.")
    elif isinstance(val_preds_dict[key], list):
        print(f"Model {key} validation predictions (val_preds_dict[key]) (list type) length: {len(val_preds_dict[key])}")
    else:
        print(f"Model {key} validation predictions (val_preds_dict[key]) shape: {val_preds_dict[key].shape}")

    if test_preds_dict[key] is None: 
        print(f"Model {key} test predictions (test_preds_dict[key]) is None.")
    elif isinstance(test_preds_dict[key], list):
        print(f"Model {key} test predictions (test_preds_dict[key]) (list type) length: {len(test_preds_dict[key])}")
    else:
        print(f"Model {key} test predictions (test_preds_dict[key]) shape: {test_preds_dict[key].shape}")

print(f"val_y.shape: {val_y.shape}" if not isinstance(val_y, list) else f"val_y(list)'s length: {len(val_y)}")

import sys
import reprlib
def debug_info_print(func):
    aRepr = reprlib.Repr()
    aRepr.maxother=300
    def wrapper(*args, **kwargs):
        def local_trace(frame, event, arg):
            if event == "return" and frame.f_code == func.__code__:
                print("\n" + "="*20 + "Running ensemble code, local variable values:" + "="*20)
                for k, v in frame.f_locals.items():
                    printed = aRepr.repr(v)
                    print(f"{k}:\n {printed}")
                print("="*20 + "Local variable values end" + "="*20)
            return local_trace
        
        sys.settrace(local_trace)
        try:
            return func(*args, **kwargs)
        finally:
            sys.settrace(None)
    return wrapper


# Run ensemble
final_pred = debug_info_print(ensemble_workflow)(test_preds_dict, val_preds_dict, val_y)

print_preds_info("ensemble", "test", final_pred)

# Check type
pred_type = type(next(iter(test_preds_dict.values())))
assert isinstance(final_pred, pred_type), (
    f"Type mismatch: 'final_pred' is of type {type(final_pred)}, but expected {pred_type} "
)

# Check shape
if isinstance(final_pred, (list, np.ndarray, pd.DataFrame, torch.Tensor, tf.Tensor)):
    assert len(final_pred) == len(test_X), (
        f"Wrong output sample size: len(final_pred)={len(final_pred)} "
        f"vs. len(test_X)={len(test_X)}"
    )

# check scores.csv
assert Path("scores.csv").exists(), "scores.csv is not generated"
score_df = pd.read_csv("scores.csv", index_col=0)
model_set_in_scores = set(score_df.index)

assert model_set_in_scores == set({{model_names}}).union({"ensemble"}), (
    f"The scores dataframe does not contain the correct model names as index.\ncorrect model names are: {{model_names}} + ['ensemble']\nscore_df is:\n{score_df}"
)
assert score_df.index.is_unique, "The scores dataframe has duplicate model names."
assert len(score_df.columns) == 1, f"The scores dataframe should have exactly one column for the scores of the evaluation indicator, but has these columns: {score_df.columns.tolist()}"

print("Ensemble test end.")



================================================
File: rdagent/components/coder/data_science/feature/__init__.py
================================================
import json
from typing import Dict

from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiEvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.components.coder.data_science.conf import DSCoderCoSTEERSettings
from rdagent.components.coder.data_science.feature.eval import FeatureCoSTEEREvaluator
from rdagent.components.coder.data_science.feature.exp import FeatureTask
from rdagent.core.exception import CoderError
from rdagent.core.experiment import FBWorkspace
from rdagent.core.scenario import Scenario
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T


class FeatureMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: FeatureTask,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:
        # return a workspace with "load_data.py", "spec/load_data.md" inside
        # assign the implemented code to the new workspace.
        feature_information_str = target_task.get_task_information()

        # 1. query
        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[feature_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[feature_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            [
                knowledge
                for knowledge in queried_former_failed_knowledge[0]
                if knowledge.implementation.file_dict.get("feature.py") != workspace.file_dict.get("feature.py")
            ],
            queried_former_failed_knowledge[1],
        )

        # 2. code
        system_prompt = T(".prompts:feature_coder.system").r(
            competition_info=self.scen.get_scenario_all_desc(),
            task_desc=feature_information_str,
            data_loader_code=workspace.file_dict.get("load_data.py"),
            queried_similar_successful_knowledge=queried_similar_successful_knowledge,
            queried_former_failed_knowledge=queried_former_failed_knowledge[0],
        )
        user_prompt = T(".prompts:feature_coder.user").r(
            feature_spec=workspace.file_dict["spec/feature.md"],
            latest_code=workspace.file_dict.get("feature.py"),
            latest_code_feedback=prev_task_feedback,
        )

        for _ in range(5):
            feature_code = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                    json_mode=True,
                    json_target_type=Dict[str, str],
                )
            )["code"]
            if feature_code != workspace.file_dict.get("feature.py"):
                break
            else:
                user_prompt = user_prompt + "\nPlease avoid generating same code to former code!"
        else:
            raise CoderError("Failed to generate a new feature code.")

        return {
            "feature.py": feature_code,
        }

    def assign_code_list_to_evo(self, code_list: list[dict[str, str]], evo):
        """
        Assign the code list to the evolving item.

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                # evo.sub_workspace_list[index] = FBWorkspace(target_task=evo.sub_tasks[index])
                evo.sub_workspace_list[index] = evo.experiment_workspace
            evo.sub_workspace_list[index].inject_files(**code_list[index])
        return evo


class FeatureCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        settings = DSCoderCoSTEERSettings()
        eva = CoSTEERMultiEvaluator(
            FeatureCoSTEEREvaluator(scen=scen), scen=scen
        )  # Please specify whether you agree running your eva in parallel or not
        es = FeatureMultiProcessEvolvingStrategy(scen=scen, settings=settings)

        super().__init__(*args, settings=settings, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)



================================================
File: rdagent/components/coder/data_science/feature/eval.py
================================================
import json
import re
from pathlib import Path

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.utils.agent.tpl import T
from rdagent.utils.agent.workflow import build_cls_from_json_with_retry
from rdagent.utils.fmt import shrink_text

DIRNAME = Path(__file__).absolute().resolve().parent

FeatureEvalFeedback = CoSTEERSingleFeedback


class FeatureCoSTEEREvaluator(CoSTEEREvaluator):

    def evaluate(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        gt_implementation: FBWorkspace,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> FeatureEvalFeedback:

        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return FeatureEvalFeedback(
                execution="This task has failed too many times, skip implementation.",
                return_checking="This task has failed too many times, skip implementation.",
                code="This task has failed too many times, skip implementation.",
                final_decision=False,
            )

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/sample/{self.scen.competition}": "/kaggle/input"}

        # TODO: do we need to clean the generated temporary content?
        fname = "test/feature_test.py"
        test_code = (DIRNAME / "eval_tests" / "feature_test.txt").read_text()
        implementation.inject_files(**{fname: test_code})

        stdout = implementation.execute(env=env, entry=f"python {fname}")

        if "main.py" in implementation.file_dict:
            workflow_stdout = implementation.execute(env=env, entry="python main.py")
            workflow_stdout = re.sub(r"=== Start of EDA part ===(.*)=== End of EDA part ===", "", workflow_stdout)
        else:
            workflow_stdout = None

        system_prompt = T(".prompts:feature_eval.system").r(
            task_desc=target_task.get_task_information(),
            test_code=test_code,
            code=implementation.file_dict["feature.py"],
            workflow_stdout=workflow_stdout,
            workflow_code=implementation.all_codes,
        )
        user_prompt = T(".prompts:feature_eval.user").r(
            stdout=shrink_text(stdout),
            workflow_stdout=workflow_stdout,
        )

        return build_cls_from_json_with_retry(
            FeatureEvalFeedback,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            init_kwargs_update_func=FeatureEvalFeedback.val_and_update_init_dict,
        )



================================================
File: rdagent/components/coder/data_science/feature/exp.py
================================================
import pickle
import site
import traceback
from pathlib import Path
from typing import Dict, Optional

from rdagent.components.coder.CoSTEER.task import CoSTEERTask
from rdagent.core.utils import cache_with_pickle


# Because we use isinstance to distinguish between different types of tasks, we need to use sub classes to represent different types of tasks
class FeatureTask(CoSTEERTask):
    pass



================================================
File: rdagent/components/coder/data_science/feature/prompts.yaml
================================================
feature_coder:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    ## Task Description
    {{ task_desc }}
    
    ## Competition Information for This Task
    {{ competition_info }}

    {% if queried_similar_successful_knowledge|length != 0 or queried_former_failed_knowledge|length != 0 %}
    ## Relevant Information for This Task
    {% endif %}
    
    {% if queried_similar_successful_knowledge|length != 0 %}
    --------- Successful Implementations for Similar Models ---------
    ====={% for similar_successful_knowledge in queried_similar_successful_knowledge %} Model {{ loop.index }}:=====
    {{ similar_successful_knowledge.target_task.get_task_information() }}
    =====Code:=====
    {{ similar_successful_knowledge.implementation.file_dict["feature.py"] }}
    {% endfor %} 
    {% endif %}

    {% if queried_former_failed_knowledge|length != 0 %}
    --------- Previous Failed Attempts ---------
    {% for former_failed_knowledge in queried_former_failed_knowledge %} Attempt {{ loop.index }}:
    =====Code:=====
    {{ former_failed_knowledge.implementation.file_dict["feature.py"] }}
    =====Feedback:=====
    {{ former_failed_knowledge.feedback }}
    {% endfor %}
    {% endif %}

    ## Guidelines
    1. If feature engineering is unnecessary or should be combined with model training, you may skip this step.
    2. Be cautious of any column drop in the code. Dropping a column easily without any more attempts, it may not be a good practice.
    3. The function input is the output of the following data loader:
    ```python
    {{ data_loader_code }}
    ```
    3. **Additional Guidance:**
      - If a previous attempt exists, improve upon it without repeating mistakes.
      - If errors indicate a missing file, find a way to download it or implement an alternative solution.
      - You should avoid using logging module to output information in your generated code, and instead use the print() function.

    ## Output Format
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }
  
  user: |-
    --------- Feature Processing Specification ---------
    {{ feature_spec }}

    {% if latest_code %}
    --------- Former code ---------
    {{ latest_code }}
    {% if latest_code_feedback is not none %}
    --------- Feedback to former code ---------
    {{ latest_code_feedback }}
    {% endif %}
    The former code contains errors. You should correct the code based on the provided information, ensuring you do not repeat the same mistakes.
    {% endif %}


feature_eval:
  system: |-
    You are a data scientist responsible for evaluating feature engineering code generation.

    ## Task Description
    {{ task_desc }}

    ## Feature Engineering Code
    ```python
    {{ code }}
    ```

    ## Testing Process
    The feature engineering code is tested using the following script:
    ```python
    {{ test_code }}
    ```
    You will analyze the execution results based on the test output provided.

    {% if workflow_stdout is not none %}
    ### Whole Workflow Consideration
    The feature engineering code is part of the whole workflow. The user has executed the entire pipeline and provided additional stdout.

    **Workflow Code:**
    ```python
    {{ workflow_code }}
    ```

    You should evaluate both the feature engineering test results and the overall workflow results. **Approve the code only if both tests pass.**
    {% endif %}
    
    ## Evaluation Criteria
    You will be given the standard output (`stdout`) from the feature engineering test and, if applicable, the workflow test.
    
    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe how well the feature engineering executed, including any errors or issues encountered. Append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Evaluate the correctness and integrity of processed data, checking for missing values, incorrect transformations, and data consistency.",
        "code": "Assess code quality, readability, and adherence to specifications. Consider efficiency, including whether the code utilizes multi-threading or GPU acceleration for optimization.",
        "final_decision": <true/false>
    }
    ```
  
  user: |-
    --------- Feature engineering test stdout ---------
    {{ stdout }}   
    {% if workflow_stdout is not none %}
    --------- Whole workflow test stdout ---------
    {{ workflow_stdout }}
    {% endif %}



================================================
File: rdagent/components/coder/data_science/feature/test.py
================================================
"""
Helper functions for testing the feature coder(CoSTEER-based) component.
- Does the developer loop work correctly

It is NOT:
- it is not interface unittest(i.e. workspace evaluator in the CoSTEER Loop)
"""

from rdagent.components.coder.data_science.feature import FeatureCoSTEER
from rdagent.components.coder.data_science.feature.exp import FeatureTask
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.scen import KaggleScen


def develop_one_competition(competition: str):  # -> experiment
    scen = KaggleScen(competition=competition)
    feature_coder = FeatureCoSTEER(scen)

    with open("./rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/spec/feature.md", "r") as file:
        feat_spec = file.read()

    # Create the experiment
    ft = FeatureTask(name="FeatureTask", description=scen.get_competition_full_desc())
    exp = DSExperiment(
        sub_tasks=[ft],
    )

    with open("./rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/load_data.py", "r") as file:
        load_data_code = file.read()
    exp.experiment_workspace.inject_files(**{"load_data.py": load_data_code, "spec/feature.md": feat_spec})

    # Develop the experiment
    exp = feature_coder.develop(exp)


if __name__ == "__main__":
    develop_one_competition("aerial-cactus-identification")



================================================
File: rdagent/components/coder/data_science/feature/eval_tests/feature_test.txt
================================================
"""
A qualified data loader should support following features
- successfully run
- len(test) == len(test_ids) == submission length
- len(train) == len(y)

Please make sure the stdout is rich enough to support informative feedback
"""

import pickle
from copy import deepcopy

import numpy as np
import pandas as pd
from feature import feat_eng
from load_data import load_data

X, y, X_test, test_ids = load_data()
print(f"X.shape: {X.shape}")
print(f"y.shape: {y.shape}" if not isinstance(y, list) else f"y(list)'s length: {len(y)}")
print(f"X_test.shape: {X_test.shape}")
print(f"test_ids length: {len(test_ids)}")
X_loaded = deepcopy(X)
y_loaded = deepcopy(y)
X_test_loaded = deepcopy(X_test)

import sys
import reprlib
def debug_info_print(func):
    aRepr = reprlib.Repr()
    aRepr.maxother=300
    def wrapper(*args, **kwargs):
        def local_trace(frame, event, arg):
            if event == "return" and frame.f_code == func.__code__:
                print("\n" + "="*20 + "Running feat_eng code, local variable values:" + "="*20)
                for k, v in frame.f_locals.items():
                    printed = aRepr.repr(v)
                    print(f"{k}:\n {printed}")
                print("="*20 + "Local variable values end" + "="*20)
            return local_trace
        
        sys.settrace(local_trace)
        try:
            return func(*args, **kwargs)
        finally:
            sys.settrace(None)
    return wrapper
X, y, X_test = debug_info_print(feat_eng)(X, y, X_test)


def get_length(data):
    return len(data) if isinstance(data, list) else data.shape[0]


def get_width(data):
    return 1 if isinstance(data, list) else data.shape[1:]


def get_column_list(data):
    return data.columns.tolist() if isinstance(data, pd.DataFrame) else None


assert X is not None, "The feature engineering function returned None for X."
assert y is not None, "The feature engineering function returned None for y."
assert X_test is not None, "The feature engineering function returned None for X_test."

assert get_length(X_test) == get_length(
    test_ids
), f"Mismatch in length of test images and test IDs: X_test ({get_length(X_test)}) and test_ids ({get_length(test_ids)})"
assert get_length(X) == get_length(
    y
), f"Mismatch in length of training images and labels: X ({get_length(X)}) and y ({get_length(y)})"

assert get_length(X) != 0, f"Training data is empty."
assert get_length(y) != 0, f"Training labels are empty."
assert get_length(X_test) != 0, f"Test data is empty."

assert get_width(X) == get_width(
    X_test
), "Mismatch in width of training and test data. Width means the number of features."

if isinstance(X, pd.DataFrame) and isinstance(X_test, pd.DataFrame):
    assert get_column_list(X) == get_column_list(X_test), "Mismatch in column names of training and test data."

if isinstance(X, pd.DataFrame):
    def normalize_dtype(dtype):
        return "numeric" if np.issubdtype(dtype, np.number) else str(dtype)

    X_dtypes_unique_sorted = sorted(set(normalize_dtype(dt) for dt in X.dtypes.unique()))
    X_loaded_dtypes_unique_sorted = sorted(set(normalize_dtype(dt) for dt in X_loaded.dtypes.unique()))

    X_dtypes_unique_sorted_new = [
        dt for dt in X_dtypes_unique_sorted if dt not in X_loaded_dtypes_unique_sorted and dt != "object"
    ]
    assert (
        np.dtypes.ObjectDType in X_loaded_dtypes_unique_sorted or len(X_dtypes_unique_sorted_new) == 0
    ), f"feature engineering has produced new data types which is not allowed, data loader data types are {X_loaded_dtypes_unique_sorted} and feature engineering data types are {X_dtypes_unique_sorted}"


print(
    "Feature Engineering test passed successfully. All checks including length, width, and data types have been validated."
)



================================================
File: rdagent/components/coder/data_science/model/__init__.py
================================================
from typing import Dict

from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiEvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.components.coder.data_science.conf import DSCoderCoSTEERSettings
from rdagent.components.coder.data_science.model.eval import (
    ModelGeneralCaseSpecEvaluator,
)
from rdagent.components.coder.data_science.model.exp import ModelTask
from rdagent.core.exception import CoderError
from rdagent.core.experiment import FBWorkspace
from rdagent.core.scenario import Scenario
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.ret import BatchEditOut
from rdagent.utils.agent.tpl import T


class ModelMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: ModelTask,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:
        model_information_str = target_task.get_task_information()

        # 1. query
        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[model_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[model_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            [
                knowledge
                for knowledge in queried_former_failed_knowledge[0]
                if knowledge.implementation.file_dict.get(f"{target_task.name}.py")
                != workspace.file_dict.get(f"{target_task.name}.py")
            ],
            queried_former_failed_knowledge[1],
        )

        # 2. code
        system_prompt = T(".prompts:model_coder.system").r(
            task_desc=model_information_str,
            competition_info=self.scen.get_scenario_all_desc(),
            data_loader_code=workspace.file_dict.get("load_data.py"),
            feature_code=workspace.file_dict["feature.py"],
            queried_similar_successful_knowledge=queried_similar_successful_knowledge,
            queried_former_failed_knowledge=queried_former_failed_knowledge[0],
            out_spec=BatchEditOut.get_spec(),
        )
        # user_prompt = T(".prompts:model_coder.user").r(
        #     model_spec=workspace.file_dict["spec/model.md"],
        #     feature_code=workspace.file_dict["feature.py"],
        #     latest_code=workspace.file_dict.get(f"{target_task.name}.py", None),
        # )
        # We want to use a simpler way to
        user_prompt = T(".prompts:model_coder.user_general").r(
            model_spec=workspace.file_dict["spec/model.md"],
            latest_model_code=workspace.get_codes(
                r"^model_(?!test)\w+\.py$"
            ),  # TODO: If we have high failure rate here, we should clean this step with less information.
            latest_code_feedback=prev_task_feedback,
        )

        for _ in range(5):
            batch_edit = BatchEditOut.extract_output(
                APIBackend().build_messages_and_create_chat_completion(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                    json_mode=BatchEditOut.json_mode,
                    json_target_type=Dict[str, str],
                )
            )

            # 3. post process to align file name to the task name
            batch_edit = {
                (f"{target_task.name}.py" if value != "__DEL__" and key != f"{target_task.name}.py" else key): value
                for key, value in batch_edit.items()
            }

            user_prompt = user_prompt + "\nPlease avoid generating same code to former code!"
            if batch_edit and max(len(i.encode("utf-8")) for i in batch_edit.keys()) > 255:
                continue

            if batch_edit[f"{target_task.name}.py"] != "__DEL__" and batch_edit[
                f"{target_task.name}.py"
            ] != workspace.file_dict.get(f"{target_task.name}.py"):
                break
        else:
            raise CoderError("Failed to generate a new model code.")

        return batch_edit

    def assign_code_list_to_evo(self, code_list: list[dict[str, str]], evo):
        """
        Assign the code list to the evolving item.

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                # evo.sub_workspace_list[index] = FBWorkspace(target_task=evo.sub_tasks[index])
                evo.sub_workspace_list[index] = evo.experiment_workspace
            evo.sub_workspace_list[index].inject_files(**code_list[index])
        return evo


class ModelCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        settings = DSCoderCoSTEERSettings()
        eva = CoSTEERMultiEvaluator(
            ModelGeneralCaseSpecEvaluator(scen=scen), scen=scen
        )  # Please specify whether you agree running your eva in parallel or not
        # eva = ModelGeneralCaseSpecEvaluator(scen=scen)
        es = ModelMultiProcessEvolvingStrategy(scen=scen, settings=settings)

        super().__init__(*args, settings=settings, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)



================================================
File: rdagent/components/coder/data_science/model/eval.py
================================================
"""
Beyond previous tests
-
"""

import json
import re
from pathlib import Path

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.exception import CoderError
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T
from rdagent.utils.agent.workflow import build_cls_from_json_with_retry

DIRNAME = Path(__file__).absolute().resolve().parent
ModelSingleFeedback = CoSTEERSingleFeedback


# Below are unit tests for testing the specification of the implemented model ------------------
class ModelGeneralCaseSpecEvaluator(CoSTEEREvaluator):
    """
    Motivation case:
    - Simplest case, we already split the data into train_data, valid_data, and test_data. We require the model to learn (optionally validate on valid data), and infer on test data.

    Test workflow:
    - Build train, valid, and test data to run it, and test the output (e.g., shape, etc.)
    """

    def evaluate(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        gt_implementation: FBWorkspace,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> ModelSingleFeedback:
        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return ModelSingleFeedback(
                execution="This task has failed too many times, skip implementation.",
                return_checking="This task has failed too many times, skip implementation.",
                code="This task has failed too many times, skip implementation.",
                final_decision=False,
            )

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/sample/{self.scen.competition}": "/kaggle/input"}

        fname = "test/model_test.py"
        test_code = (
            (DIRNAME / "eval_tests" / "model_test.txt").read_text().replace("model01", target_task.name)
        )  # only check the model changed this time
        implementation.inject_files(**{fname: test_code})
        stdout = implementation.execute(env=env, entry=f"python {fname}")

        if stdout is None:
            raise CoderError(
                "The execution output contains too many progress bars and results in the LLM's token size exceeding the limit."
            )

        if "main.py" in implementation.file_dict:
            workflow_stdout = implementation.execute(env=env, entry="python main.py")
            workflow_stdout = re.sub(r"=== Start of EDA part ===(.*)=== End of EDA part ===", "", workflow_stdout)
        else:
            workflow_stdout = None

        system_prompt = T(".prompts:model_eval.system").r(
            task_desc=target_task.get_task_information(),
            test_code=test_code,
            code=implementation.file_dict[f"{target_task.name}.py"],
            scenario=self.scen.get_scenario_all_desc(),
            spec=implementation.file_dict["spec/model.md"],
            workflow_stdout=workflow_stdout,
            workflow_code=implementation.all_codes,
        )
        user_prompt = T(".prompts:model_eval.user").r(
            stdout=stdout,
            workflow_stdout=workflow_stdout,
        )
        return build_cls_from_json_with_retry(
            ModelSingleFeedback,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            init_kwargs_update_func=ModelSingleFeedback.val_and_update_init_dict,
        )



================================================
File: rdagent/components/coder/data_science/model/exp.py
================================================
from typing import Dict, Optional

from rdagent.components.coder.CoSTEER.task import CoSTEERTask


# Because we use isinstance to distinguish between different types of tasks, we need to use sub classes to represent different types of tasks
class ModelTask(CoSTEERTask):
    def __init__(
        self,
        name: str,
        description: str,
        architecture: str = "",
        *args,
        hyperparameters: Dict[str, str] = {},
        model_type: Optional[str] = None,
        **kwargs,
    ) -> None:
        self.architecture: str = architecture
        self.hyperparameters: str = hyperparameters
        self.model_type: str | None = (
            model_type  # Tabular for tabular model, TimesSeries for time series model, Graph for graph model, XGBoost for XGBoost model
            # TODO: More Models Supported
        )
        super().__init__(name=name, description=description, *args, **kwargs)

    def get_task_information(self):
        task_desc = f"""name: {self.name}
description: {self.description}
"""
        if self.architecture:
            task_desc += f"architecture: {self.architecture}\n"
        if self.hyperparameters:
            task_desc += f"hyperparameters: {self.hyperparameters}\n"
        if self.model_type:
            task_desc += f"model_type: {self.model_type}\n"
        return task_desc



================================================
File: rdagent/components/coder/data_science/model/prompts.yaml
================================================
model_coder:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    ## Task Description
    {{ task_desc }}
    
    ## Competition Information for This Task
    {{ competition_info }}

    {% if queried_similar_successful_knowledge|length != 0 or queried_former_failed_knowledge|length != 0 %}
    ## Relevant Information for This Task
    {% endif %}
    
    {% if queried_similar_successful_knowledge|length != 0 %}
    --------- Successful Implementations for Similar Models ---------
    ====={% for similar_successful_knowledge in queried_similar_successful_knowledge %} Model {{ loop.index }}:=====
    {{ similar_successful_knowledge.target_task.get_task_information() }}
    =====Code:=====
    {{ similar_successful_knowledge.implementation.file_dict[similar_successful_knowledge.target_task.name ~ '.py'] }}
    {% endfor %} 
    {% endif %}

    {% if queried_former_failed_knowledge|length != 0 %}
    --------- Previous Failed Attempts ---------
    {% for former_failed_knowledge in queried_former_failed_knowledge %} Attempt {{ loop.index }}:
    =====Code:=====
    {{ former_failed_knowledge.implementation.file_dict[former_failed_knowledge.target_task.name ~ '.py'] }}
    =====Feedback:=====
    {{ former_failed_knowledge.feedback }}
    {% endfor %}
    {% endif %}

    ## Guidelines
    1. The function's input is from the output of a feature engineering function whose input is the output of a data loading function. The data loader function and feature engineering function code is as follows:
    --------- Data Loader Code ---------
    {{ data_loader_code }}
    --------- Feature Engineering Code ---------
    {{ feature_code }}
    2. You should avoid using logging module to output information in your generated code, and instead use the print() function.
    3. If the model can both be implemented by PyTorch and Tensorflow, please use pytorch for broader compatibility.

    ## Output Format
    {% if out_spec %}
    {{ out_spec }}
    The file name should be the model name described in the model task in the format "{task_name}.py". You should always follow this name format.
    {% else %}
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }
    {% endif %}

  user_general: |-
    --------- Model Building Specification ---------
    {{ model_spec }}

    --------- Former model code ---------
    {% if latest_model_code|length == 0 %}
    So far the workspace is empty. No model code has been implemented yet.
    {% else %}
    {{ latest_model_code }}
    {% if latest_code_feedback is not none %}
    --------- Feedback to former code ---------
    {{ latest_code_feedback }}
    {% endif %}
    {% endif %}

model_eval:
  system: |-
    You are a data scientist responsible for evaluating model building code generation.

    ## Task Description
    {{ task_desc }}

    ## Model Building Code
    ```python
    {{ code }}
    ```

    ## Testing Process
    The model building code is tested using the following script:
    ```python
    {{ test_code }}
    ```

    ### Execution Phases
    The model is tested in two phases:

    1. Initial Training Phase:
       - The model receives **train and valid inputs** with **empty hyperparameters**.
       - The focus is on verifying whether the model successfully trains and produces **valid outputs and hyperparameter outputs**.

    2. Retraining Phase:
       - The model receives **train and test inputs** (without valid inputs).
       - The hyperparameters generated from the first phase are passed back for **retraining**.


    ### Key Requirements for Approval
    A model can only be approved if it meets all of the following conditions:
    1. Hyperparameter Handling
      - If hyperparameters are returned, they must include an early stop round.
      - The hyperparameters must be correctly utilized in the model for retraining.
      - If the early stop round is provided, it must be used in the model implementation.
    2. The model output shape must strictly match the specifications in `spec.md`.

    {% if workflow_stdout is not none %}
    ### Whole Workflow Consideration
    The model building code is part of the whole workflow. The user has executed the entire pipeline and provided additional stdout.

    **Workflow Code:**
    ```python
    {{ workflow_code }}
    ```

    You should evaluate both the model building test results and the overall workflow results. **Approve the code only if both tests pass.**
    {% endif %}
    
    ## Evaluation Criteria
    You will be given the standard output (`stdout`) from the model building test and, if applicable, the workflow test.
    [Note] If no stdout for model buidling test is provided, the model failed due to a timeout or out-of-memory error. You should analyze potential optimizations.

    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe how well the model building executed, including any errors or issues encountered. Append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Check the generated value, including whether the value is generated and comparing the shape of the model output with the requirement in spec.md. You also need to check whether the hyperparameters used for retraining are correctly returned during the test execution of the model.",
        "code": "Assess code quality, readability, and adherence to specifications. Consider efficiency, including whether the code utilizes multi-threading or GPU acceleration for optimization.",
        "final_decision": <true/false>
    }
    ```

  user: |-
    --------- Model building test stdout ---------
    {{ stdout }}   
    {% if workflow_stdout is not none %}
    --------- Whole workflow test stdout ---------
    {{ workflow_stdout }}
    {% endif %}



================================================
File: rdagent/components/coder/data_science/model/test.py
================================================
"""
Generate dataset to test the model workflow output
"""

from pathlib import Path

from rdagent.components.coder.CoSTEER.config import CoSTEER_SETTINGS
from rdagent.components.coder.data_science.model import ModelCoSTEER
from rdagent.components.coder.data_science.model.eval import (
    ModelGeneralCaseSpecEvaluator,
)
from rdagent.components.coder.data_science.model.exp import ModelTask
from rdagent.core.experiment import FBWorkspace
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.scen import KaggleScen


# Take tasks, spec.md and feat as input, generate a feedback as output
def develop_one_competition(competition: str):
    scen = KaggleScen(competition=competition)
    model_coder = ModelCoSTEER(scen)

    # Create the task
    mt = ModelTask(
        name="ModelTask",
        description="A CNN Model",
        model_type="CNN",
        architecture="\hat{y}_u = CNN(X_u)",
        # variables="variables: {'\\hat{y}_u': 'The predicted output for node u', 'X_u': 'The input features for node u'}",
        hyperparameters="...",
        base_code="",
    )

    tpl_ex_path = Path(__file__).resolve() / Path("rdagent/scenarios/kaggle/tpl_ex").resolve() / competition
    injected_file_names = ["spec/model.md", "load_data.py", "feature.py", "model01.py"]

    modelexp = FBWorkspace()
    for file_name in injected_file_names:
        file_path = tpl_ex_path / file_name
        modelexp.inject_files(**{file_name: file_path.read_text()})

    mt.base_code += modelexp.file_dict["model01.py"]
    exp = DSExperiment(
        sub_tasks=[mt],
    )

    # Test the evaluator:
    """eva = ModelGeneralCaseSpecEvaluator(scen=scen)
    exp.feedback = eva.evaluate(target_task=mt, queried_knowledge=None, implementation=modelexp, gt_implementation=None)
    print(exp.feedback)"""

    # Test the evolving strategy:
    """es = ModelMultiProcessEvolvingStrategy(scen=scen, settings=CoSTEER_SETTINGS)
    new_code = es.implement_one_task(target_task=mt, queried_knowledge=None, workspace=modelexp)
    print(new_code)"""

    # Run the experiment
    for file_name in injected_file_names:
        file_path = tpl_ex_path / file_name
        exp.experiment_workspace.inject_files(**{file_name: file_path.read_text()})

    exp = model_coder.develop(exp)


if __name__ == "__main__":
    develop_one_competition("aerial-cactus-identification")
    # dotenv run -- python rdagent/components/coder/data_science/model/test.py



================================================
File: rdagent/components/coder/data_science/model/eval_tests/model_test.txt
================================================
import time

from feature import feat_eng
from load_data import load_data
from model01 import model_workflow
from sklearn.model_selection import train_test_split


def log_execution_results(start_time, val_pred, test_pred, hypers, execution_label):
    """Log the results of a single model execution."""
    feedback_str = f"{execution_label} end.\n"
    feedback_str += f"Validation predictions shape: {val_pred.shape if val_pred is not None else 'None'}\n"
    feedback_str += f"Test predictions shape: {test_pred.shape if test_pred is not None else 'None'}\n"
    feedback_str += f"Hyperparameters: {hypers if hypers is not None else 'None'}\n"
    feedback_str += f"Execution time: {time.time() - start_time:.2f} seconds.\n"
    print(feedback_str)


# Load and preprocess data
X, y, test_X, test_ids = load_data()
X, y, test_X = feat_eng(X, y, test_X)
train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.8, random_state=42)
print(f"train_X.shape: {train_X.shape}")
print(f"train_y.shape: {train_y.shape}" if not isinstance(train_y, list) else f"train_y(list)'s length: {len(train_y)}")
print(f"val_X.shape: {val_X.shape}")
print(f"val_y.shape: {val_y.shape}" if not isinstance(val_y, list) else f"val_y(list)'s length: {len(val_y)}")

import sys
import reprlib
def debug_info_print(func):
    aRepr = reprlib.Repr()
    aRepr.maxother=300
    def wrapper(*args, **kwargs):
        def local_trace(frame, event, arg):
            if event == "return" and frame.f_code == func.__code__:
                print("\n" + "="*20 + "Running model training code, local variable values:" + "="*20)
                for k, v in frame.f_locals.items():
                    printed = aRepr.repr(v)
                    print(f"{k}:\n {printed}")
                print("="*20 + "Local variable values end" + "="*20)
            return local_trace
        
        sys.settrace(local_trace)
        try:
            return func(*args, **kwargs)
        finally:
            sys.settrace(None)
    return wrapper

# First execution
print("The first execution begins.\n")
start_time = time.time()
val_pred, test_pred, hypers = debug_info_print(model_workflow)(
    X=train_X,
    y=train_y,
    val_X=val_X,
    val_y=val_y,
    test_X=None,
)
log_execution_results(start_time, val_pred, test_pred, hypers, "The first execution")

# Second execution
print("The second execution begins.\n")
start_time = time.time()
val_pred, test_pred, final_hypers = debug_info_print(model_workflow)(
    X=train_X,
    y=train_y,
    val_X=None,
    val_y=None,
    test_X=test_X,
    hyper_params=hypers,
)
log_execution_results(start_time, val_pred, test_pred, final_hypers, "The second execution")

print("Model code test end.")



================================================
File: rdagent/components/coder/data_science/raw_data_loader/README.md
================================================
# CoSTEER

- subworkspace使用主experiment_workspace `RD-Agent/rdagent/scenarios/data_science/experiment/experiment.py`

## evolving_strategy ( implement_one_task() )

1. xxxTask (in exp.py)
    - spec
    - description
2. 

## evaluator

1. queried_knowledge部分 共用
2. eval_test脚本


================================================
File: rdagent/components/coder/data_science/raw_data_loader/__init__.py
================================================
"""

Loop should not large change exclude
- Action Choice[current data loader & spec]
- other should share
    - Propose[choice] => Task[Choice] => CoSTEER =>
        -

Extra feature:
- cache


File structure
- ___init__.py: the entrance/agent of coder
- evaluator.py
- conf.py
- exp.py: everything under the experiment, e.g.
    - Task
    - Experiment
    - Workspace
- test.py
    - Each coder could be tested.
"""

import json
import re
from typing import Dict

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiEvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.components.coder.data_science.conf import (
    DSCoderCoSTEERSettings,
    get_ds_env,
)
from rdagent.components.coder.data_science.raw_data_loader.eval import (
    DataLoaderCoSTEEREvaluator,
)
from rdagent.components.coder.data_science.raw_data_loader.exp import DataLoaderTask
from rdagent.core.exception import CoderError
from rdagent.core.experiment import FBWorkspace
from rdagent.core.scenario import Scenario
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T


class DataLoaderMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: DataLoaderTask,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:
        # return a workspace with "load_data.py", "spec/load_data.md" inside
        # assign the implemented code to the new workspace.
        competition_info = self.scen.get_scenario_all_desc()
        runtime_environment = self.scen.get_runtime_environment()
        data_folder_info = self.scen.processed_data_folder_description
        data_loader_task_info = target_task.get_task_information()

        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[data_loader_task_info]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[data_loader_task_info]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            [
                knowledge
                for knowledge in queried_former_failed_knowledge[0]
                if knowledge.implementation.file_dict.get("load_data.py") != workspace.file_dict.get("load_data.py")
            ],
            queried_former_failed_knowledge[1],
        )

        # 1. specifications
        # TODO: We may move spec into a separated COSTEER task
        if "spec/data_loader.md" not in workspace.file_dict:  # Only generate the spec once
            system_prompt = T(".prompts:spec.system").r(
                runtime_environment=runtime_environment,
                task_desc=data_loader_task_info,
                competition_info=competition_info,
                folder_spec=data_folder_info,
            )
            data_loader_prompt = T(".prompts:spec.user.data_loader").r(
                latest_spec=workspace.file_dict.get("spec/data_loader.md")
            )
            feature_prompt = T(".prompts:spec.user.feature").r(latest_spec=workspace.file_dict.get("spec/feature.md"))
            model_prompt = T(".prompts:spec.user.model").r(latest_spec=workspace.file_dict.get("spec/model.md"))
            ensemble_prompt = T(".prompts:spec.user.ensemble").r(
                latest_spec=workspace.file_dict.get("spec/ensemble.md")
            )
            workflow_prompt = T(".prompts:spec.user.workflow").r(
                latest_spec=workspace.file_dict.get("spec/workflow.md")
            )

            spec_session = APIBackend().build_chat_session(session_system_prompt=system_prompt)

            data_loader_spec = json.loads(
                spec_session.build_chat_completion(
                    user_prompt=data_loader_prompt, json_mode=True, json_target_type=Dict[str, str]
                )
            )["spec"]
            feature_spec = json.loads(
                spec_session.build_chat_completion(
                    user_prompt=feature_prompt, json_mode=True, json_target_type=Dict[str, str]
                )
            )["spec"]
            model_spec = json.loads(
                spec_session.build_chat_completion(
                    user_prompt=model_prompt, json_mode=True, json_target_type=Dict[str, str]
                )
            )["spec"]
            ensemble_spec = json.loads(
                spec_session.build_chat_completion(
                    user_prompt=ensemble_prompt, json_mode=True, json_target_type=Dict[str, str]
                )
            )["spec"]
            workflow_spec = json.loads(
                spec_session.build_chat_completion(
                    user_prompt=workflow_prompt, json_mode=True, json_target_type=Dict[str, str]
                )
            )["spec"]
        else:
            data_loader_spec = workspace.file_dict["spec/data_loader.md"]
            feature_spec = workspace.file_dict["spec/feature.md"]
            model_spec = workspace.file_dict["spec/model.md"]
            ensemble_spec = workspace.file_dict["spec/ensemble.md"]
            workflow_spec = workspace.file_dict["spec/workflow.md"]

        # 2. code
        system_prompt = T(".prompts:data_loader_coder.system").r(
            task_desc=data_loader_task_info,
            queried_similar_successful_knowledge=queried_similar_successful_knowledge,
            queried_former_failed_knowledge=queried_former_failed_knowledge[0],
        )
        user_prompt = T(".prompts:data_loader_coder.user").r(
            competition_info=competition_info,
            data_loader_spec=data_loader_spec,
            folder_spec=data_folder_info,
            latest_code=workspace.file_dict.get("load_data.py"),
            latest_code_feedback=prev_task_feedback,
        )

        for _ in range(5):
            data_loader_code = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                    json_mode=True,
                    json_target_type=Dict[str, str],
                )
            )["code"]
            if data_loader_code != workspace.file_dict.get("load_data.py"):
                break
            else:
                user_prompt = user_prompt + "\nPlease avoid generating same code to former code!"
        else:
            raise CoderError("Failed to generate a new data loader code.")

        return {
            "spec/data_loader.md": data_loader_spec,
            "spec/feature.md": feature_spec,
            "spec/model.md": model_spec,
            "spec/ensemble.md": ensemble_spec,
            "spec/workflow.md": workflow_spec,
            "load_data.py": data_loader_code,
        }

    def assign_code_list_to_evo(self, code_list: list[dict[str, str]], evo):
        """
        Assign the code list to the evolving item.

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                # evo.sub_workspace_list[index] = FBWorkspace(target_task=evo.sub_tasks[index])
                evo.sub_workspace_list[index] = evo.experiment_workspace
            evo.sub_workspace_list[index].inject_files(**code_list[index])
        return evo


class DataLoaderCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        settings = DSCoderCoSTEERSettings()
        eva = CoSTEERMultiEvaluator(
            DataLoaderCoSTEEREvaluator(scen=scen), scen=scen
        )  # Please specify whether you agree running your eva in parallel or not
        es = DataLoaderMultiProcessEvolvingStrategy(scen=scen, settings=settings)

        super().__init__(*args, settings=settings, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)

    def develop(self, exp):
        new_exp = super().develop(exp)

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/{self.scen.competition}": "/kaggle/input"}

        stdout = new_exp.experiment_workspace.execute(env=env, entry=f"python test/data_loader_test.py")
        match = re.search(r"(.*?)=== Start of EDA part ===(.*)=== End of EDA part ===", stdout, re.DOTALL)
        eda_output = match.groups()[1] if match else None
        self.scen.eda_output = eda_output
        return new_exp



================================================
File: rdagent/components/coder/data_science/raw_data_loader/conf.py
================================================



================================================
File: rdagent/components/coder/data_science/raw_data_loader/eval.py
================================================
# tess successfully running.
# (GPT) if it aligns with the spec & rationality of the spec.
import json
import re
from pathlib import Path

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledgeV2,
)
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.utils.agent.tpl import T
from rdagent.utils.agent.workflow import build_cls_from_json_with_retry

DIRNAME = Path(__file__).absolute().resolve().parent

DataLoaderEvalFeedback = CoSTEERSingleFeedback


class DataLoaderCoSTEEREvaluator(CoSTEEREvaluator):

    def evaluate(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        gt_implementation: FBWorkspace,
        queried_knowledge: CoSTEERQueriedKnowledgeV2 = None,
        **kwargs,
    ) -> DataLoaderEvalFeedback:

        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return DataLoaderEvalFeedback(
                execution="This task has failed too many times, skip implementation.",
                return_checking="This task has failed too many times, skip implementation.",
                code="This task has failed too many times, skip implementation.",
                final_decision=False,
            )

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/sample/{self.scen.competition}": "/kaggle/input"}

        # TODO: do we need to clean the generated temporary content?
        fname = "test/data_loader_test.py"
        test_code = (DIRNAME / "eval_tests" / "data_loader_test.txt").read_text()
        implementation.inject_files(**{fname: test_code})
        stdout = implementation.execute(env=env, entry=f"python {fname}")
        match = re.search(r"(.*?)=== Start of EDA part ===(.*)=== End of EDA part ===(.*)", stdout, re.DOTALL)
        stdout_part_1, eda_output, stdout_part_2 = match.groups() if match else (stdout, None, "")
        stdout = stdout_part_1 + stdout_part_2
        if eda_output is not None and len(eda_output.split(" ")) > 10000:
            eda_output += "Length of EDA output is too long, truncated. Please reject this implementation and motivate it to reduce the length of EDA output."

        if "main.py" in implementation.file_dict:
            workflow_stdout = implementation.execute(env=env, entry="python main.py")
            workflow_stdout = re.sub(r"=== Start of EDA part ===(.*)=== End of EDA part ===", "", workflow_stdout)
        else:
            workflow_stdout = None

        system_prompt = T(".prompts:data_loader_eval.system").r(
            task_desc=target_task.get_task_information(),
            test_code=test_code,
            code=implementation.file_dict["load_data.py"],
            workflow_stdout=workflow_stdout,
            workflow_code=implementation.all_codes,
        )
        user_prompt = T(".prompts:data_loader_eval.user").r(
            stdout=stdout,
            eda_output=eda_output,
            workflow_stdout=workflow_stdout,
        )

        return build_cls_from_json_with_retry(
            DataLoaderEvalFeedback,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            init_kwargs_update_func=DataLoaderEvalFeedback.val_and_update_init_dict,
        )



================================================
File: rdagent/components/coder/data_science/raw_data_loader/exp.py
================================================
from rdagent.components.coder.CoSTEER.task import CoSTEERTask


# Because we use isinstance to distinguish between different types of tasks, we need to use sub classes to represent different types of tasks
class DataLoaderTask(CoSTEERTask):
    pass



================================================
File: rdagent/components/coder/data_science/raw_data_loader/prompts.yaml
================================================

spec:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    Currently, you are working on a Kaggle competition project. 
    This project involves analyzing data and building models to beat other competitors, with the code being generated by large language models.

    The runtime environment you are working in includes the following libraries and their respective versions:
    {{ runtime_environment }}

    Your overall task is provided below:
    {{ task_desc }}
    
    Your task is to write five specification texts (in markdown format) for the following tasks, based on the competition information provided
    - Data loading (and preprocessing)
    - Feature Engineering
    - Model Building
    - Ensemble
    - The overall workflow

    The specifications for each step should be tailored to the competition information provided. 
    
    Your specification should consists two parts:
    1. The function definition in code format, including type annotations and a clear, complete docstring that describes the function's purpose, input parameters, return value, and any relevant exceptions.
    2. Additional information or notes that the coder should consider while implementing the function.
    
    Your specifications should include only the function definition and docstring, without any code implementation or inline comments.

    ## Competition Information for This Task
    {{ competition_info }}

    ----------- Folder Description (All path are relative to the data folder) ---------
    - Ensure that all columns in sample_submission can be generated.
    {{ folder_spec }}

  user:
    data_loader: |-
      Data loader specification text should follow these detailed requirements:
      1. Function Interface:
        - Function Name: `load_data`
        - Input: No input arguments.
        - Output:
          - `X` (DT, define based on competition information): Feature matrix for training data.
          - `y` (DT): Target vector for training data.
          - `X_test` (DT): Feature matrix for test data.
          - `test_ids` (DT): Identifiers for the test data.
        - Docstring Requirements:
          - Describe the purpose of the function.
          - Specify the data source location (`/kaggle/input/`).
          - Clearly define the structure and type of the output.
          - Inferred data shape to each input and output data variables. To uncertain dimension, use -1.

      2. Precautions for Data Loading and Preprocessing:
        - File Handling:
          - Ensure proper file encoding and delimiters.
          - Combine or process multiple files if necessary.
        - Data Preprocessing:
          - Convert data types correctly (e.g., numeric, categorical, date parsing).
          - Handle missing values appropriately.
          - Optimize memory usage for large datasets using techniques like downcasting or reading data in chunks if necessary.
        - Domain-Specific Handling: 
          - Apply competition-specific preprocessing steps as needed (e.g., text tokenization, image resizing).

      3. Code Standards:
        - Avoid using progress bars (e.g., `tqdm`) in the implementation.

      4. Notes:
        - Update `DT` (data type) based on the specific competition dataset. This can include `pd.DataFrame`, `np.array`, `torch.Tensor`, etc.
        - Never use sample submission as the test index, as it may not be the same as the test data. Use the test index file or test data source to get the test index.
        - Only set the DT of variables without inferring the shape of these variables since you don't know the shape of the data.
      
      5. Exploratory Data Analysis (EDA) part(Required):
        - Before returning the data, you should always add an EDA part describing the data to help the following steps understand the data better.
        - The EDA part should be drafted in plain text with certain format schema with no more than ten thousand characters.
        - An evaluation agent will help to check whether the EDA part is added correctly.

      {% if latest_spec %}
      6. Former Specification:
        {{ latest_spec }}
        You should follow the provided specifications to improve this task.
      {% endif %}

      Please respond with a JSON structure as follows:
      {
          "spec": "The function definition in code format, tailored to the Competition Information, with detailed explanations provided in the docstring."
      }

    feature: |-
      Feature engineering specification text should adhere to the following requirements:
      1. Function Interface:
        - Function Name: `feat_eng`
        - Parameters:
          - `X` (DT): Train data to be transformed.
          - `y` (DT): Train label data.
          - `X_test` (DT): Test data.
        - Output:
          - `X_transformed` (DT): Transformed train data.
          - `y_transformed` (DT): Transformed train label data.
          - `X_test_transformed` (DT): Transformed test data.
        - Docstring Requirements:
          - Describe the purpose of the function.
          - Clarify the input parameters and their data types.
          - Define the structure and format of the output.
          - Inferred data shape to each input and output data variables. To uncertain dimension, use -1.

      2. Precautions for Feature Engineering:
        - Well handle the shape of the data
          - The sample size of the train data and the test data should be the same in all scenarios.
          - To most of the scenario, the input shape and the output shape should be exactly the same.
          - To some tabular data, you may add or remove some columns so your inferred column number may be unsure.
        - Integration with Model Pipeline
          - If feature engineering is strictly part of the model pipeline, state explicitly that it will be handled at the model stage.
          - If integrated here, ensure this function applies all required transformations while avoiding data leakage.
        - General Considerations:
          - Ensure scalability for large datasets.
          - Handle missing values and outliers appropriately (e.g., impute, remove, or replace).
          - Ensure consistency between feature data types and transformations.
          - Avoid data leakage: Only use features derived from training data, excluding information from test or validation sets.
        - Domain-Specific Features:
          - Apply logic for competition-specific features (e.g., text vectorization, image augmentations, categorical encoding).

      3. Code Standards:
        - Avoid using progress bars (e.g., `tqdm`) in the implementation.          

      4. Notes:
        - Align `DT` (data type) definitions with those in the Data Loader specification.
        - Extend or adjust domain-specific transformations based on competition requirements.
        - The device has GPU support, so you can use it for feature engineering if necessary to accelerate the process.
        - Multi processing or parallel processing can be used to speed up the feature engineering process.
        - Only set the DT of variables without inferring the shape of these variables since you don't know the shape of the data.
      
      {% if latest_spec %}
      5. Former Specification:
        {{ latest_spec }}
        You should follow the provided specifications to improve this task.
      {% endif %}

      Please respond with a JSON structure as follows:
      {
          "spec": "The function definition in code format, tailored to the Competition Information, with detailed explanations provided in the docstring."
      }

    model: |-
      Model building specification text should adhere to the following requirements:

      1. Function Interface:
        - Function Name: `model_workflow`
        - Parameters:
          - `X` (DT): Training feature data.
          - `y` (DT): Training label data.
          - `val_X` (Optional[DT]): Validation feature data.
          - `val_y` (Optional[DT]): Validation label data.
          - `test_X` (Optional[DT]): Test feature data.
          - `hyper_params` (dict): Dictionary of hyperparameters for model configuration.
        - Output:
          - `pred_val` (Optional[DT]): Predictions on validation data.
          - `pred_test` (Optional[DT]): Predictions on test data.
          - `hyper_params` (dict): Updated dictionary of hyperparameters after training.
        - Docstring Requirements:
          - Describe the purpose of the function.
          - Clarify the input parameters and their data types.
          - Define the structure and format of the output.
          - Inferred data shape to each input and output data variables. To uncertain dimension, use -1.

      2. Code Standards:
        - Do not use progress bars (e.g., `tqdm`) in the implementation.

      3. Precautions:
        - Ensure input arrays (`X`, `y`, `val_X`, `val_y`, `test_X`) have consistent dimensions and shapes.
        - Use default values for hyperparameters if `hyper_params` is not provided.
        - Train the model on `X` and `y`.
        - Evaluate the model using `val_X` and `val_y` if validation data is available.
        - If `test_X` is provided, generate predictions for it.

      4. Notes:
        - Align `DT` (data type) with the definitions used in Feature Engineering specifications.
        - The device has GPU support, so you can use it for training if necessary to accelerate the process.

      {% if latest_spec %}
      5. Former Specification:
        {{ latest_spec }}
        You should follow the provided specifications to improve this task.
      {% endif %}

      Please respond in the following JSON format:
      {
          "spec": "The function definition in code format, tailored to the Competition Information, with detailed explanations provided in the docstring."
      }

    ensemble: |-
      Ensemble specification text adhere to the following requirements:
      1. Function Interface:
        - Function Name: `ensemble_workflow`
        - Parameters:
          - `test_preds_dict` (Dict[str, DT]): A dictionary of test predictions from different models. The key is the model file name.
          - `val_preds_dict` (Dict[str, DT]): A dictionary of validation predictions from different models. The key is the model file name.
          - `val_label` (DT): Validation label.
        - Output:
          - `final_pred` (DT): Ensemble prediction for the test data.
        - Docstring Requirements:
          - Describe the purpose of the function.
          - Clarify the input parameters and their data types.
          - Define the structure and format of the output.
          - Inferred data shape to each input and output data variables. To uncertain dimension, use -1.

      2. Precautions:
        - Validation of Inputs:
          - Ensure all predictions in `test_preds_dict` and `val_preds_dict` have consistent shapes and dimensions.
          - Verify that `val_label` is provided and matches the length of `val_preds_dict` predictions.
          - Handle empty or invalid inputs gracefully with appropriate error messages.
        - Metric Calculation and Storage:
          - Calculate the metric (mentioned in the evaluation section of the competition information) for each model and ensemble strategy on valid, and save the results in `scores.csv`, e.g.:
            ```python
            scores = {}
            for model_name, val_pred in val_preds_dict.items():
                scores[model_name] = calculate_metric(val_label, val_pred)
            
            ...
            some code about ensemble strategy
            ...
            ensemble_val_pred = ...

            ensemble_score = calculate_metric(val_label, ensemble_val_pred)
            scores["ensemble"] = ensemble_score  # Ensure "ensemble" is explicitly stored
            
            scores_df = pd.DataFrame(scores.items(), columns=["Model", <metric_name>])
            scores_df.to_csv("scores.csv", index=False)
            ```
          - If there is only one model, still compute the ensemble score and store it under "ensemble".
        
      3. Code Standards:
        - Do not use progress bars (e.g., tqdm) in the code.

      4. Notes:
        - Align `DT` (data type) definitions with those used in model specifications.
        - Ensure flexibility to handle multiple ensemble strategies based on competition requirements.
        - Only set the DT of variables without inferring the shape of these variables since you don't know the shape of the data.

      {% if latest_spec %}
      5. Former Specification:
        {{ latest_spec }}
        You should follow the provided specifications to improve this task.
      {% endif %}

      Please respond in the following JSON format:
      {
          "spec": "The function definition in code format, tailored to the Competition Information, with detailed explanations provided in the docstring."
      }

    workflow: |-
      Your task is to implement the main workflow script (`main.py`) for a Kaggle-style machine learning competition project. 
      Follow the provided project structure and specifications to ensure consistency and maintainability:
        1. Workflow Integration:
          - Integrate the following components into the workflow:
            - Data loading (`load_data.py`).
            - Feature engineering (`feature.py`).
            - Model workflow for training and testing (`model_*.py`). 
            - Ensemble workflow that combines results from the model workflow to obtain the final prediction (`ensemble.py`).
          - Treat each component as a modular and callable Python function.
          - The workflow script should be flexible enough to handle either a single model or multiple models, with filenames (model_*.py) that are not determined at the outset.
            For multiple model selection, utilize Python code to identify eligible models based on filenames, for example:
            ```python
            available_models = [f for f in os.listdir('.') if f.startswith('model_') and 'test' not in f]
            ```
        2. Feature Engineering
          - The feature engineering should be called only once. For example:
            `X_transformed, y_transformed, X_test_transformed = feat_eng(X, y, X_test)`
          - It should be called before dataset splitting.

        3. Dataset Splitting
          - The dataset returned by `load_data` is not pre-split. After calling `feat_eng`, split the data into training and test sets.
          - If feasible, apply cross-validation on the training set (`X_transformed`, `y_transformed`) to ensure a reliable assessment of model performance.
          - Keep the test set (`X_test_transformed`) unchanged, as it is only used for generating the final predictions.

        4. Submission File:
          - Save the final predictions as `submission.csv`, ensuring the format matches the competition requirements (refer to `sample_submission` in the Folder Description for the correct structure).
          - Present the required submission format explicitly and ensure the output adheres to it.

        5. Code Standards:
          - Do not use progress bars (e.g., tqdm) in the code.

        6. Ensemble Strategy:
          Consolidate all model outputs into a dictionary, where each key is the model's filename (excluding the .py extension) and its corresponding value is the model's output.
          Sample code:
          {% raw %}
          {% for model_name in model_names %}
          model_module = __import__(model_name.replace('.py', ''))
          val_pred, test_pred, _ = model_module.model_workflow(
            X=train_X,
            y=train_y,
            val_X=val_X,
            val_y=val_y,
            test_X=X_test_transformed
          )
          val_preds_dict[model_module.__name__] = val_pred
          test_preds_dict[model_module.__name__] = test_pred
          {% endfor %}
          final_pred = ensemble_workflow(test_preds_dict, val_preds_dict, val_y)
          {% endraw %}

        {% if latest_spec %}
        7. Former Specification:
          {{ latest_spec }}
          You should follow the provided specifications to improve this task.
        {% endif %}

        Please response the specification in the following json format. Here is an example structure for the JSON output:
        {
            "spec": "The corresponding specification string as described above. You should create the rules based on the competition information instead of copying the requirements."
        }

data_loader_coder:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    ## Task Description
    {{ task_desc }}

    {% if queried_similar_successful_knowledge|length != 0 or queried_former_failed_knowledge|length != 0 %}
    ## Relevant Information for This Task
    {% endif %}
    
    {% if queried_similar_successful_knowledge|length != 0 %}
    --------- Successful Implementations for Similar Models ---------
    ====={% for similar_successful_knowledge in queried_similar_successful_knowledge %} Model {{ loop.index }}:=====
    {{ similar_successful_knowledge.target_task.get_task_information() }}
    =====Code:=====
    {{ similar_successful_knowledge.implementation.all_codes }}
    {% endfor %} 
    {% endif %}

    {% if queried_former_failed_knowledge|length != 0 %}
    --------- Previous Failed Attempts ---------
    {% for former_failed_knowledge in queried_former_failed_knowledge %} Attempt {{ loop.index }}:
    =====Code:=====
    {{ former_failed_knowledge.implementation.all_codes }}
    =====Feedback:=====
    {{ former_failed_knowledge.feedback }}
    {% endfor %}
    {% endif %}

    ## Guidelines
    1. Ensure that the dataset is loaded strictly from `/kaggle/input/`, following the exact folder structure described in the **Data Folder Description**, and do not attempt to load data from the current directory (`./`).
    2. You should avoid using logging module to output information in your generated code, and instead use the print() function.
    
    ## Exploratory Data Analysis (EDA) part(Required):
    - Before returning the data, you should always add an EDA part describing the data to help the following steps understand the data better.
    - The EDA part should include but not limited in the following information in plain text:
      - The shape of the data.
      - The first 5 rows of the data.
      - The data types of each column.
      - The number of missing values in each column.
      - The number of unique values in each column.
      - The distribution of the target variable.
      - Any other information that you think is important for the following steps.
    - The EDA part should be drafted in plain text sending to standard output with command print or other similar functions with no more than ten thousand characters in the following schema: 
      === Start of EDA part ===
      { You EDA output content }
      === End of EDA part ===
      User will use the following code to match: re.search(r"(.*?)=== Start of EDA part ===(.*)=== End of EDA part ===", stdout, re.DOTALL).groups()[1]
    - An evaluation agent will help to check whether the EDA part is added correctly.
    - During the EDA part, you should try to avoid any irrelevant information sending to the standard output.

    ## Output Format
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }

  user: |-
    --------- Competition Information ---------
    {{ competition_info }}

    --------- Data Loader Specification ---------
    {{ data_loader_spec }}

    --------- Data Folder Description (All path are relative to the data folder) ---------
    {{ folder_spec }}
    
    {% if latest_code %}
    --------- Former code ---------
    {{ latest_code }}
    {% if latest_code_feedback is not none %}
    --------- Feedback to former code ---------
    {{ latest_code_feedback }}
    {% endif %}
    The former code contains errors. You should correct the code based on the provided information, ensuring you do not repeat the same mistakes.
    {% endif %} 

    You should strictly follow the function interface specifications provided by the specification to implement the function.


data_loader_eval:
  system: |-
    You are a data scientist responsible for evaluating data loader code for a Kaggle-style machine learning competition project.
    
    ## Task Description
    {{ task_desc }}

    ## Data Loader Code
    The data loader code is located in `load_data.py`:
    ```python
    {{ code }}
    ```

    ## Testing Process
    The data loader is tested using the following script:
    ```python
    {{ test_code }}
    ```

    {% if workflow_stdout is not none %}
    ### Whole Workflow Consideration
    The data loader is part of the whole workflow. The user has executed the entire pipeline and provided additional stdout.

    **Workflow Code:**
    ```python
    {{ workflow_code }}
    ```

    You should evaluate both the data loader test results and the overall workflow execution. **Approve the code only if both tests pass.**
    {% endif %}
    
    ## Evaluation Criteria
    You will be given the standard output (`stdout`) from the data loader test and, if applicable, the workflow test.

    ## Exploratory Data Analysis (EDA) Part evaluation
    - The code has also generated some EDA output to help understand the data better. 
    - The EDA part should be drafted in plain text sending to standard output with command print or other similar functions with no more than ten thousand characters in the following schema: 
      === Start of EDA part ===
      { You EDA output content }
      === End of EDA part ===
      User will use the following code to match: re.search(r"(.*?)=== Start of EDA part ===(.*)=== End of EDA part ===", stdout, re.DOTALL).groups()[1]
    - The EDA part should include but not limited in the following information in plain text:
      - The shape of the data.
      - The first 5 rows of the data.
      - The data types of each column.
      - The number of missing values in each column.
      - The number of unique values in each column.
      - The distribution of the target variable.
      - Any other information that you think is important for the following steps.
     You will be given the EDA output, your job is to check whether the output contains the required and sufficient information. If no EDA output is provided, you should consider it as a failure. Put this evaluation result in the return_checking part.
    
    Your response must follow this structured JSON format:
    ```json
    {
        "execution": "Describe how well the data loader executed, including any errors or issues encountered. Append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Evaluate the correctness and integrity of the loaded data. Check for issues like missing values, incorrect data types, outliers, or formatting inconsistencies.",
        "code": "Assess code quality, readability, and adherence to best practices. Consider efficiency, including whether the code utilizes multi-threading or GPU acceleration for faster data loading.",
        "final_decision": <true/false>
    }
    ```

  user: |-
    --------- Data loader test stdout ---------
    {{ stdout }}   
    --------- Data loader EDA stdout ---------
    {% if eda_output is not none %}
    {{ eda_output }}
    {% else %}
    No EDA output is provided.
    {% endif %}
    {% if workflow_stdout is not none %}
    --------- Whole workflow test stdout ---------
    {{ workflow_stdout }}
    {% endif %}



================================================
File: rdagent/components/coder/data_science/raw_data_loader/test.py
================================================
"""
Helper functions for testing the raw_data_loader coder(CoSTEER-based) component.
- Does the developer loop work correctly

It is NOT:
- it is not interface unittest(i.e. workspace evaluator in the CoSTEER Loop)
"""

from rdagent.components.coder.data_science.raw_data_loader import DataLoaderCoSTEER
from rdagent.components.coder.data_science.raw_data_loader.exp import DataLoaderTask
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.scen import KaggleScen


def develop_one_competition(competition: str):  # -> experiment
    scen = KaggleScen(competition=competition)
    data_loader_coder = DataLoaderCoSTEER(scen)

    # Create the experiment
    dlt = DataLoaderTask(name="DataLoaderTask", description="")
    exp = DSExperiment(
        sub_tasks=[dlt],
    )

    # Develop the experiment
    exp = data_loader_coder.develop(exp)


if __name__ == "__main__":
    develop_one_competition("aerial-cactus-identification")



================================================
File: rdagent/components/coder/data_science/raw_data_loader/eval_tests/data_loader_test.txt
================================================
"""
A qualified data loader should support following features
- successfully run
- len(test) == len(test_ids) == submission length
- len(train) == len(y)

Please make sure the stdout is rich enough to support informative feedback
"""

import pickle

import pandas as pd
from load_data import load_data

import sys
import reprlib
def debug_info_print(func):
    aRepr = reprlib.Repr()
    aRepr.maxother=300
    def wrapper(*args, **kwargs):
        def local_trace(frame, event, arg):
            if event == "return" and frame.f_code == func.__code__:
                print("\n" + "="*20 + "Running data_load code, local variable values:" + "="*20)
                for k, v in frame.f_locals.items():
                    printed = aRepr.repr(v)
                    print(f"{k}:\n {printed}")
                print("="*20 + "Local variable values end" + "="*20)
            return local_trace
        
        sys.settrace(local_trace)
        try:
            return func(*args, **kwargs)
        finally:
            sys.settrace(None)
    return wrapper

X, y, X_test, test_ids = debug_info_print(load_data)()


def get_length(data):
    return len(data) if isinstance(data, list) else data.shape[0]


def get_width(data):
    return 1 if isinstance(data, list) else data.shape[1:]


def get_column_list(data):
    return data.columns.tolist() if isinstance(data, pd.DataFrame) else None

assert X is not None, "Training data (X) is None."
assert y is not None, "Training labels (y) are None."
assert X_test is not None, "Test data (X_test) is None."
assert test_ids is not None, "Test IDs (test_ids) are None."

assert get_length(X_test) == get_length(
    test_ids
), f"Mismatch in length of test images and test IDs: X_test ({get_length(X_test)}) and test_ids ({get_length(test_ids)})"
assert get_length(X) == get_length(
    y
), f"Mismatch in length of training images and labels: X ({get_length(X)}) and y ({get_length(y)})"

assert get_length(X) != 0, f"Training data is empty."
assert get_length(y) != 0, f"Training labels are empty."
assert get_length(X_test) != 0, f"Test data is empty."

assert get_width(X) == get_width(
    X_test
), "Mismatch in width of training and test data. Width means the number of features."

if isinstance(X, pd.DataFrame) and isinstance(X_test, pd.DataFrame):
    assert get_column_list(X) == get_column_list(X_test), "Mismatch in column names of training and test data."

assert get_width(X) == get_width(
    X_test
), "Mismatch in width of training and test data. Width means the number of features."

print("Data loader test passed successfully. Length of test images matches length of test IDs.")



================================================
File: rdagent/components/coder/data_science/workflow/__init__.py
================================================
import json
from typing import Dict

from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiEvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
)
from rdagent.components.coder.data_science.conf import DSCoderCoSTEERSettings
from rdagent.components.coder.data_science.workflow.eval import (
    WorkflowGeneralCaseSpecEvaluator,
)
from rdagent.components.coder.data_science.workflow.exp import WorkflowTask
from rdagent.core.exception import CoderError
from rdagent.core.experiment import FBWorkspace
from rdagent.core.scenario import Scenario
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T


class WorkflowMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: WorkflowTask,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:
        workflow_information_str = target_task.get_task_information()

        # 1. query
        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[workflow_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[workflow_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            [
                knowledge
                for knowledge in queried_former_failed_knowledge[0]
                if knowledge.implementation.file_dict.get("main.py") != workspace.file_dict.get("main.py")
            ],
            queried_former_failed_knowledge[1],
        )

        # 2. code
        system_prompt = T(".prompts:workflow_coder.system").r(
            task_desc=workflow_information_str,
            competition_info=self.scen.get_scenario_all_desc(),
            queried_similar_successful_knowledge=queried_similar_successful_knowledge,
            queried_former_failed_knowledge=queried_former_failed_knowledge[0],
        )
        user_prompt = T(".prompts:workflow_coder.user").r(
            load_data_code=workspace.file_dict["load_data.py"],
            feature_code=workspace.file_dict["feature.py"],
            model_codes=workspace.get_codes(r"^model_(?!test)\w+\.py$"),
            ensemble_code=workspace.file_dict["ensemble.py"],
            latest_code=workspace.file_dict.get("main.py"),
            workflow_spec=workspace.file_dict["spec/workflow.md"],
            latest_code_feedback=prev_task_feedback,
        )

        for _ in range(5):
            workflow_code = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                    json_mode=True,
                    json_target_type=Dict[str, str],
                )
            )["code"]
            if workflow_code != workspace.file_dict.get("main.py"):
                break
            else:
                user_prompt = user_prompt + "\nPlease avoid generating same code to former code!"
        else:
            raise CoderError("Failed to generate a new workflow code.")

        return {"main.py": workflow_code}

    def assign_code_list_to_evo(self, code_list: list[dict[str, str]], evo):
        """
        Assign the code list to the evolving item.

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                # evo.sub_workspace_list[index] = FBWorkspace(target_task=evo.sub_tasks[index])
                evo.sub_workspace_list[index] = evo.experiment_workspace
            evo.sub_workspace_list[index].inject_files(**code_list[index])
        return evo


class WorkflowCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        settings = DSCoderCoSTEERSettings()
        eva = CoSTEERMultiEvaluator(
            WorkflowGeneralCaseSpecEvaluator(scen=scen), scen=scen
        )  # Please specify whether you agree running your eva in parallel or not
        es = WorkflowMultiProcessEvolvingStrategy(scen=scen, settings=settings)
        super().__init__(*args, settings=settings, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)



================================================
File: rdagent/components/coder/data_science/workflow/eval.py
================================================
import json
import re
from pathlib import Path

import pandas as pd

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERMultiFeedback,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.utils.agent.tpl import T
from rdagent.utils.agent.workflow import build_cls_from_json_with_retry

DIRNAME = Path(__file__).absolute().resolve().parent

WorkflowSingleFeedback = CoSTEERSingleFeedback
WorkflowMultiFeedback = CoSTEERMultiFeedback


class WorkflowGeneralCaseSpecEvaluator(CoSTEEREvaluator):
    """
    Motivation case:
    - Simplest case, we already split the data into train_data, valid_data, and test_data. We require the model to learn (optionally validate on valid data), and infer on test data.

    Test workflow:
    - Build train, valid, and test data to run it, and test the output (e.g., shape, etc.)
    """

    def evaluate(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        gt_implementation: FBWorkspace,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> CoSTEERSingleFeedback:
        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return WorkflowSingleFeedback(
                execution="This task has failed too many times, skip implementation.",
                return_checking="This task has failed too many times, skip implementation.",
                code="This task has failed too many times, skip implementation.",
                final_decision=False,
            )

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/sample/{self.scen.competition}": "/kaggle/input"}

        # # DockerEnv for MLEBench submission validation
        # mle_de_conf = MLEBDockerConf()
        # mle_de_conf.extra_volumes = {
        #     f"{DS_RD_SETTING.local_data_path}/zip_files": "/mle/data",
        # }
        # mde = DockerEnv(conf=mle_de_conf)
        # mde.prepare()

        # Clean the scores.csv & submission.csv.
        implementation.execute(env=env, entry=f"rm submission.csv scores.csv")

        stdout = implementation.execute(env=env, entry=f"python main.py")
        stdout = re.sub(r"=== Start of EDA part ===(.*)=== End of EDA part ===", "", stdout)

        # Check score file
        score_fp = implementation.workspace_path / "scores.csv"
        score_ret_code = 0
        score_check_text = ""
        if not score_fp.exists():
            score_check_text = "[Error] Metrics file (scores.csv) is not generated!"
            score_ret_code = 1
        else:
            try:
                score_df = pd.read_csv(score_fp, index_col=0)
                model_set_in_scores = set(score_df.index)
                # We assume that model names in `score_df` are stored without the '.py' file extension.
                model_set_in_folder = set(
                    f[:-3] for f in implementation.file_dict.keys() if re.match(r"^model_(?!test)\w+\.py$", f)
                )
                if model_set_in_scores != model_set_in_folder.union({"ensemble"}):
                    score_check_text += f"\n[Error] The scores dataframe does not contain the correct model names as index.\ncorrect model names are: {model_set_in_folder.union({'ensemble'})}\nscore_df is:\n{score_df}"
                    score_ret_code = 1
            except Exception as e:
                score_check_text += f"\n[Error] in checking the scores.csv file: {e}\nscores.csv's content:\n-----\n{score_fp.read_text()}\n-----"
                score_ret_code = 1

        # Check submission file
        base_check_code = (DIRNAME / "eval_tests" / "submission_format_test.txt").read_text()
        implementation.inject_files(**{"test/submission_format_test.py": base_check_code})
        # stdout += "----Submission Check 1-----\n"
        submission_check_out, submission_ret_code = implementation.execute_ret_code(
            env=env, entry="python test/submission_format_test.py"
        )
        stdout += "\n" + submission_check_out

        # MLEBench Check
        # !!! Since we are running on a sampled dataset, mlebench check is not required.
        # mle_check_code = (
        #     (DIRNAME / "eval_tests" / "mle_submission_format_test.txt")
        #     .read_text()
        #     .replace("<competition_id>", self.scen.competition)
        # )
        # implementation.inject_files(**{"test/mle_submission_format_test.py": mle_check_code})
        # stdout += "----Submission Check 2-----\n"
        # stdout += implementation.execute(env=mde, entry=f"python test/mle_submission_format_test.py")

        system_prompt = T(".prompts:workflow_eval.system").r(
            scenario=self.scen.get_scenario_all_desc(),
            task_desc=target_task.get_task_information(),
            spec=implementation.file_dict["spec/workflow.md"],
        )
        user_prompt = T(".prompts:workflow_eval.user").r(
            stdout=stdout.strip(),
            code=implementation.file_dict["main.py"],
        )
        wfb = build_cls_from_json_with_retry(
            WorkflowSingleFeedback,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            init_kwargs_update_func=WorkflowSingleFeedback.val_and_update_init_dict,
        )
        if score_ret_code != 0:
            wfb.final_decision = False
            wfb.return_checking += "\n" + score_check_text
        if submission_ret_code != 0:
            wfb.final_decision = False
            wfb.return_checking += "\nSubmission file check failed."
        return wfb



================================================
File: rdagent/components/coder/data_science/workflow/exp.py
================================================
import pickle
import site
import traceback
from pathlib import Path
from typing import Dict, Optional

from rdagent.components.coder.CoSTEER.task import CoSTEERTask
from rdagent.core.utils import cache_with_pickle


# Because we use isinstance to distinguish between different types of tasks, we need to use sub classes to represent different types of tasks
class WorkflowTask(CoSTEERTask):
    pass



================================================
File: rdagent/components/coder/data_science/workflow/prompts.yaml
================================================
workflow_coder:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.

    ## Task Description
    {{ task_desc }}

    Here is the competition information for this task:
    {{ competition_info }}

    {% if queried_similar_successful_knowledge|length != 0 or queried_former_failed_knowledge|length != 0 %}
    ## Relevant Information for This Task
    {% endif %}

    {% if queried_similar_successful_knowledge|length != 0 %}
    --------- Successful Implementations for Similar Models ---------
    ====={% for similar_successful_knowledge in queried_similar_successful_knowledge %} Model {{ loop.index }}:=====
    {{ similar_successful_knowledge.target_task.get_task_information() }}
    =====Code:=====
    {{ similar_successful_knowledge.implementation.file_dict["main.py"] }}
    {% endfor %} 
    {% endif %}

    {% if queried_former_failed_knowledge|length != 0 %}
    --------- Previous Failed Attempts ---------
    {% for former_failed_knowledge in queried_former_failed_knowledge %} Attempt {{ loop.index }}:
    =====Code:=====
    {{ former_failed_knowledge.implementation.file_dict["main.py"] }}
    =====Feedback:=====
    {{ former_failed_knowledge.feedback }}
    {% endfor %}
    {% endif %}

    ## Guidelines
    1. Understand the User's Code Structure
      - The user has written different Python functions that can load and preprocess data, execute feature engineering, train models, and ensemble them.
      - Each functionality is in a separate Python file.
    2. Your task is only to integrate the existing processes of load_data, feature, model, and ensemble into a complete workflow. Do not edit or modify the existing Python files. The final step should output the predictions in the required format.
    3. The user may provide specific code organization rules and instructions. Ensure that the integration follows the given framework and structure.
    4. After predicting the output, print the shape and other information of the output to stdout to help the evaluator assess the code.
    5. You should avoid using logging module to output information in your generated code, and instead use the print() function.

    ## Output Format
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }
  
  user: |-
    --------- Workflow Specification ---------
    {{ workflow_spec }}

    --------- load data code ---------
    file: load_data.py
    {{ load_data_code }}

    --------- feature engineering code ---------
    file: feature.py
    {{ feature_code }}

    --------- model training code ---------
    Attention: The input and output of the model function is flexible. Training dataset is necessary, but validation and test dateset might be optional. The hyperparameters can either be passed as arguments or be set as default values in the function. You need to use the function correctly.
    All model files share the same function name. Please import the model files with their name like: from {file_name} import {function_name}
    {{ model_codes }}

    --------- ensemble code ---------
    Note, we will check the index of the score.csv, so please use the model name as the index to feed into ensemble function.
    file: ensemble.py
    {{ ensemble_code }}

    {% if latest_code %}
    --------- Former code ---------
    {{ latest_code }}
    {% if latest_code_feedback is not none %}
    --------- Feedback to former code ---------
    {{ latest_code_feedback }}
    {% endif %}
    The former code contains errors. You should correct the code based on the provided information, ensuring you do not repeat the same mistakes.
    {% endif %}

workflow_eval:
  system: |-
    You are a data scientist responsible for evaluating workflow code generation.

    ## Task Description
    The user is trying to build a workflow in the following scenario:
    {{ scenario }}

    The main code generation task is as follows:
    {{ task_desc }}

    The user provides workflow information and its components.
    The details on how to structure the workflow are given in the specification file:
    ```python
    {{ spec }}
    ```
    This workflow integrates multiple stages, including:
    - Data loading
    - Feature engineering
    - Model training
    - Ensembling

    ## Evaluation Scope
    Your focus is to check whether the workflow code:
    1. Executes successfully, correctly organizing components and generating a final submission.
    2. Generates predictions in the correct format, ensuring they align with the **sample submission** structure!

    [Note] 
    1. The individual components (data loading, feature engineering, model tuning, etc.) have already been evaluated by the user. You should only evaluate and improve the workflow code, unless there are critical issues in the components.
    2. Model performance is NOT a concern in this evaluation—only correct execution and formatting matter.

    ## Evaluation Criteria
    You will be given the workflow execution output (`stdout`) to determine correctness.  
    
    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe whether the main workflow executed successfully, correctly integrating all components and generating the final submission. Include any errors or issues encountered, and append all error messages and full traceback details without summarizing or omitting any information.",
        "return_checking": "Verify the generated files, particularly the submission file. Ensure that its format matches the sample submission, checking the index, column names, and CSV content.",
        "code": "Provide feedback on code quality, readability, and adherence to the given specifications.",
        "final_decision": <true/false>
    }
    ```
  
  user: |-
    --------- Workflow test stdout ---------
    {{ stdout }}
    --------- Workflow code generated by user ---------
    {{ code }}


================================================
File: rdagent/components/coder/data_science/workflow/test.py
================================================
"""
Generate dataset to test the workflow output
"""

from pathlib import Path

from rdagent.components.coder.CoSTEER.config import CoSTEER_SETTINGS
from rdagent.components.coder.data_science.workflow import WorkflowCoSTEER
from rdagent.components.coder.data_science.workflow.eval import (
    WorkflowGeneralCaseSpecEvaluator,
)
from rdagent.components.coder.data_science.workflow.exp import WorkflowTask
from rdagent.core.experiment import FBWorkspace
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.scen import KaggleScen


def develop_one_competition(competition: str):
    scen = KaggleScen(competition=competition)
    workflow_coder = WorkflowCoSTEER(scen)

    wt = WorkflowTask(
        name="WorkflowTask",
        description="Integrate the existing processes of load_data, feature, model, and ensemble into a complete workflow.",
        base_code="",
    )

    tpl_ex_path = Path(__file__).resolve() / Path("rdagent/scenarios/kaggle/tpl_ex").resolve() / competition
    injected_file_names = ["spec/workflow.md", "load_data.py", "feature.py", "model01.py", "ensemble.py", "main.py"]

    workflowexp = FBWorkspace()
    for file_name in injected_file_names:
        file_path = tpl_ex_path / file_name
        workflowexp.inject_files(**{file_name: file_path.read_text()})

    wt.base_code += workflowexp.file_dict["main.py"]
    exp = DSExperiment(
        sub_tasks=[wt],
    )

    """es = WorkflowMultiProcessEvolvingStrategy(scen=scen, settings=CoSTEER_SETTINGS)
    new_code = es.implement_one_task(target_task=wt, queried_knowledge=None, workspace = workflowexp)
    print(new_code)"""

    """eva = WorkflowGeneralCaseSpecEvaluator(scen=scen)
    exp.feedback = eva.evaluate(target_task=wt, queried_knowledge=None, implementation=workflowexp, gt_implementation=None)
    print(exp.feedback)"""

    # Run the experiment
    for file_name in injected_file_names:
        file_path = tpl_ex_path / file_name
        exp.experiment_workspace.inject_files(**{file_name: file_path.read_text()})

    exp = workflow_coder.develop(exp)


if __name__ == "__main__":
    develop_one_competition("aerial-cactus-identification")
    # dotenv run -- python rdagent/components/coder/data_science/workflow/test.py



================================================
File: rdagent/components/coder/data_science/workflow/eval_tests/submission_format_test.txt
================================================
from pathlib import Path
import pandas as pd
import hashlib

def calculate_md5(file_path):
    with open(file_path, "rb") as f:
        file_hash = hashlib.md5(f.read()).hexdigest()
    return file_hash

file_md5 = calculate_md5("scores.csv")

"""
find . | grep -i sample | grep -i submission | grep -v sample_submission.csv | grep -v zip_files  | grep -v 'sample/'
./denoising-dirty-documents/sampleSubmission.csv
./the-icml-2013-whale-challenge-right-whale-redux/sampleSubmission.csv
./text-normalization-challenge-russian-language/ru_sample_submission_2.csv.zip
./text-normalization-challenge-russian-language/ru_sample_submission_2.csv
./random-acts-of-pizza/sampleSubmission.csv
./text-normalization-challenge-english-language/en_sample_submission_2.csv.zip
./text-normalization-challenge-english-language/en_sample_submission_2.csv
./detecting-insults-in-social-commentary/sample_submission_null.csv
"""

# Find sample submission file dynamically
input_dir = Path("/kaggle/input")
# Look for common variations of sample submission filenames
sample_submission_files = list(input_dir.glob("*sample_submission*.csv")) + \
                         list(input_dir.glob("*sampleSubmission*.csv"))

assert sample_submission_files, "Error: No sample submission file found in /kaggle/input/"

# Use first matching file
sample_submission_name = sample_submission_files[0].name
SAMPLE_SUBMISSION_PATH = str(sample_submission_files[0])
print(f"Using sample submission file: {sample_submission_name}")

# Check if the sample submission file exists
assert Path(SAMPLE_SUBMISSION_PATH).exists(), f"Error: {sample_submission_name} not found at {SAMPLE_SUBMISSION_PATH}"

# Check if our submission file exists
assert Path('submission.csv').exists(), "Error: submission.csv not found"

sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)
our_submission = pd.read_csv('submission.csv')

success = True
# Print the columns of the sample submission file
print(f"Columns in {sample_submission_name}:", sample_submission.columns)
print("Columns in our_submission.csv:", our_submission.columns)

for col in sample_submission.columns:
    if col not in our_submission.columns:
        success = False
        print(f'Column {col} not found in submission.csv')

if success:
    print(f'submission.csv\'s columns aligns with {sample_submission_name} .')


# Print the first 5 rows of the two submission files, with columns separated by commas.
def print_first_rows(file_path, file_name, num_rows=5):
    print(f"\nFirst {num_rows} rows of {file_name}:")
    try:
        with open(file_path, 'r') as file:
            for i, line in enumerate(file):
                if i < num_rows:
                    print(line.strip())
                else:
                    break
    except FileNotFoundError:
        print(f"Error: {file_name} not found.")

print_first_rows(SAMPLE_SUBMISSION_PATH, sample_submission_name)
print_first_rows('submission.csv', 'submission.csv')

assert calculate_md5("scores.csv") == file_md5, "scores.csv should not be rewritten"
print(f"\nPlease Checked the content of the submission file(submission.csv should align with {sample_submission_name}). ")



================================================
File: rdagent/components/coder/factor_coder/__init__.py
================================================
from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.evaluators import CoSTEERMultiEvaluator
from rdagent.components.coder.factor_coder.config import FACTOR_COSTEER_SETTINGS
from rdagent.components.coder.factor_coder.evaluators import FactorEvaluatorForCoder
from rdagent.components.coder.factor_coder.evolving_strategy import (
    FactorMultiProcessEvolvingStrategy,
)
from rdagent.core.experiment import Experiment
from rdagent.core.scenario import Scenario


class FactorCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        setting = FACTOR_COSTEER_SETTINGS
        eva = CoSTEERMultiEvaluator(FactorEvaluatorForCoder(scen=scen), scen=scen)
        es = FactorMultiProcessEvolvingStrategy(scen=scen, settings=FACTOR_COSTEER_SETTINGS)

        super().__init__(*args, settings=setting, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)

    def develop(self, exp: Experiment) -> Experiment:
        try:
            exp = super().develop(exp)
        finally:
            es = self.evolve_agent.evolving_trace[-1]
            exp.prop_dev_feedback = es.feedback
        return exp



================================================
File: rdagent/components/coder/factor_coder/config.py
================================================
from pydantic_settings import SettingsConfigDict

from rdagent.components.coder.CoSTEER.config import CoSTEERSettings


class FactorCoSTEERSettings(CoSTEERSettings):
    model_config = SettingsConfigDict(env_prefix="FACTOR_CoSTEER_")

    data_folder: str = "git_ignore_folder/factor_implementation_source_data"
    """Path to the folder containing financial data (default is fundamental data in Qlib)"""

    data_folder_debug: str = "git_ignore_folder/factor_implementation_source_data_debug"
    """Path to the folder containing partial financial data (for debugging)"""

    simple_background: bool = False
    """Whether to use simple background information for code feedback"""

    file_based_execution_timeout: int = 120
    """Timeout in seconds for each factor implementation execution"""

    select_method: str = "random"
    """Method for the selection of factors implementation"""

    python_bin: str = "python"
    """Path to the Python binary"""


FACTOR_COSTEER_SETTINGS = FactorCoSTEERSettings()



================================================
File: rdagent/components/coder/factor_coder/eva_utils.py
================================================
import io
import json
from abc import abstractmethod
from pathlib import Path
from typing import Dict, Tuple

import pandas as pd
from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.factor_coder.config import FACTOR_COSTEER_SETTINGS
from rdagent.components.coder.factor_coder.factor import FactorTask
from rdagent.core.experiment import Task, Workspace
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.oai.llm_utils import APIBackend

evaluate_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class FactorEvaluator:
    """Although the init method is same to Evaluator, but we want to emphasize they are different"""

    def __init__(self, scen=None) -> None:
        self.scen = scen

    @abstractmethod
    def evaluate(
        self,
        target_task: Task,
        implementation: Workspace,
        gt_implementation: Workspace,
        **kwargs,
    ) -> Tuple[str, object]:
        """You can get the dataframe by

        .. code-block:: python

            _, gen_df = implementation.execute()
            _, gt_df = gt_implementation.execute()

        Returns
        -------
        Tuple[str, object]
            - str: the text-based description of the evaluation result
            - object: a comparable metric (bool, integer, float ...) None for evaluator with only text-based result

        """
        raise NotImplementedError("Please implement the `evaluator` method")

    def _get_df(self, gt_implementation: Workspace, implementation: Workspace):
        if gt_implementation is not None:
            _, gt_df = gt_implementation.execute()
            if isinstance(gt_df, pd.Series):
                gt_df = gt_df.to_frame("gt_factor")
            if isinstance(gt_df, pd.DataFrame):
                gt_df = gt_df.sort_index()
        else:
            gt_df = None

        _, gen_df = implementation.execute()
        if isinstance(gen_df, pd.Series):
            gen_df = gen_df.to_frame("source_factor")
        if isinstance(gen_df, pd.DataFrame):
            gen_df = gen_df.sort_index()
        return gt_df, gen_df

    def __str__(self) -> str:
        return self.__class__.__name__


class FactorCodeEvaluator(FactorEvaluator):
    def evaluate(
        self,
        target_task: FactorTask,
        implementation: Workspace,
        execution_feedback: str,
        value_feedback: str = "",
        gt_implementation: Workspace = None,
        **kwargs,
    ):
        factor_information = target_task.get_task_information()
        code = implementation.all_codes

        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(evaluate_prompts["evaluator_code_feedback_v1_system"])
            .render(
                scenario=(
                    self.scen.get_scenario_all_desc(
                        target_task,
                        filtered_tag="feature",
                        simple_background=FACTOR_COSTEER_SETTINGS.simple_background,
                    )
                    if self.scen is not None
                    else "No scenario description."
                )
            )
        )

        execution_feedback_to_render = execution_feedback
        for _ in range(10):  # 10 times to split the content is enough
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(
                    evaluate_prompts["evaluator_code_feedback_v1_user"],
                )
                .render(
                    factor_information=factor_information,
                    code=code,
                    execution_feedback=execution_feedback_to_render,
                    value_feedback=value_feedback,
                    gt_code=gt_implementation.code if gt_implementation else None,
                )
            )
            if (
                APIBackend().build_messages_and_calculate_token(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                )
                > LLM_SETTINGS.chat_token_limit
            ):
                execution_feedback_to_render = execution_feedback_to_render[len(execution_feedback_to_render) // 2 :]
            else:
                break
        critic_response = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            json_mode=False,
        )

        return critic_response, None


class FactorInfEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        _, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                False,
            )
        INF_count = gen_df.isin([float("inf"), -float("inf")]).sum().sum()
        if INF_count == 0:
            return "The source dataframe does not have any infinite values.", True
        else:
            return (
                f"The source dataframe has {INF_count} infinite values. Please check the implementation.",
                False,
            )


class FactorSingleColumnEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        _, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                False,
            )
        if len(gen_df.columns) == 1:
            return "The source dataframe has only one column which is correct.", True
        else:
            return (
                "The source dataframe has more than one column. Please check the implementation. We only evaluate the first column.",
                False,
            )


class FactorOutputFormatEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        gt_df, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Skip the evaluation of the output format.",
                False,
            )
        buffer = io.StringIO()
        gen_df.info(buf=buffer)
        gen_df_info_str = f"The user is currently working on a feature related task.\nThe output dataframe info is:\n{buffer.getvalue()}"
        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(
                evaluate_prompts["evaluator_output_format_system"],
            )
            .render(
                scenario=(
                    self.scen.get_scenario_all_desc(implementation.target_task, filtered_tag="feature")
                    if self.scen is not None
                    else "No scenario description."
                )
            )
        )

        # TODO: with retry_context(retry_n=3, except_list=[KeyError]):
        max_attempts = 3
        attempts = 0
        final_evaluation_dict = None

        while attempts < max_attempts:
            try:
                api = APIBackend() if attempts == 0 else APIBackend(use_chat_cache=False)
                resp = api.build_messages_and_create_chat_completion(
                    user_prompt=gen_df_info_str,
                    system_prompt=system_prompt,
                    json_mode=True,
                    json_target_type=Dict[str, str | bool | int],
                )
                resp_dict = json.loads(resp)
                resp_dict["output_format_decision"] = str(resp_dict["output_format_decision"]).lower() in ["true", "1"]

                return (
                    str(resp_dict["output_format_feedback"]),
                    resp_dict["output_format_decision"],
                )
            except (KeyError, json.JSONDecodeError) as e:
                attempts += 1
                if attempts >= max_attempts:
                    raise KeyError(
                        "Wrong JSON Response or missing 'output_format_decision' or 'output_format_feedback' key after multiple attempts."
                    ) from e

        return "Failed to evaluate output format after multiple attempts.", False


class FactorDatetimeDailyEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str | object]:
        _, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return "The source dataframe is None. Skip the evaluation of the datetime format.", False

        if "datetime" not in gen_df.index.names:
            return "The source dataframe does not have a datetime index. Please check the implementation.", False

        try:
            pd.to_datetime(gen_df.index.get_level_values("datetime"))
        except Exception:
            return (
                f"The source dataframe has a datetime index but it is not in the correct format (maybe a regular string or other objects). Please check the implementation.\n The head of the output dataframe is: \n{gen_df.head()}",
                False,
            )

        time_diff = pd.to_datetime(gen_df.index.get_level_values("datetime")).to_series().diff().dropna().unique()
        if pd.Timedelta(minutes=1) in time_diff:
            return (
                "The generated dataframe is not daily. The implementation is definitely wrong. Please check the implementation.",
                False,
            )
        return "The generated dataframe is daily.", True


class FactorRowCountEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        gt_df, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                False,
            )
        ratio = min(len(gen_df), len(gt_df)) / max(len(gen_df), len(gt_df))
        return (
            (
                f"The ratio of rows count in the source dataframe to the ground truth dataframe is {ratio:.2f}. "
                + "Please verify the implementation. "
                if ratio <= 0.99
                else ""
            ),
            ratio,
        )


class FactorIndexEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        gt_df, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                False,
            )
        gen_index_set, gt_index_set = set(gen_df.index), set(gt_df.index)
        similarity = len(gen_index_set.intersection(gt_index_set)) / len(gen_index_set.union(gt_index_set))
        return (
            (
                f"The source dataframe and the ground truth dataframe have different index with a similarity of {similarity:.2%}. The similarity is calculated by the number of shared indices divided by the union indices. "
                + "Please check the implementation."
                if similarity <= 0.99
                else ""
            ),
            similarity,
        )


class FactorMissingValuesEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        gt_df, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                False,
            )
        if gen_df.isna().sum().sum() == gt_df.isna().sum().sum():
            return "Both dataframes have the same missing values.", True
        else:
            return (
                f"The dataframes do not have the same missing values. The source dataframe has {gen_df.isna().sum().sum()} missing values, while the ground truth dataframe has {gt_df.isna().sum().sum()} missing values. Please check the implementation.",
                False,
            )


class FactorEqualValueRatioEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        gt_df, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                -1,
            )
        try:
            close_values = gen_df.sub(gt_df).abs().lt(1e-6)
            result_int = close_values.astype(int)
            pos_num = result_int.sum().sum()
            acc_rate = pos_num / close_values.size
        except:
            close_values = gen_df
        if close_values.all().iloc[0]:
            return (
                "All values in the dataframes are equal within the tolerance of 1e-6.",
                acc_rate,
            )
        else:
            return (
                "Some values differ by more than the tolerance of 1e-6. Check for rounding errors or differences in the calculation methods.",
                acc_rate,
            )


class FactorCorrelationEvaluator(FactorEvaluator):
    def __init__(self, hard_check: bool, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.hard_check = hard_check

    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
    ) -> Tuple[str, object]:
        gt_df, gen_df = self._get_df(gt_implementation, implementation)
        if gen_df is None:
            return (
                "The source dataframe is None. Please check the implementation.",
                False,
            )
        concat_df = pd.concat([gen_df, gt_df], axis=1)
        concat_df.columns = ["source", "gt"]
        ic = concat_df.groupby("datetime").apply(lambda df: df["source"].corr(df["gt"])).dropna().mean()
        ric = (
            concat_df.groupby("datetime")
            .apply(lambda df: df["source"].corr(df["gt"], method="spearman"))
            .dropna()
            .mean()
        )

        if self.hard_check:
            if ic > 0.99 and ric > 0.99:
                return (
                    f"The dataframes are highly correlated. The ic is {ic:.6f} and the rankic is {ric:.6f}.",
                    True,
                )
            else:
                return (
                    f"The dataframes are not sufficiently high correlated. The ic is {ic:.6f} and the rankic is {ric:.6f}. Investigate the factors that might be causing the discrepancies and ensure that the logic of the factor calculation is consistent.",
                    False,
                )
        else:
            return f"The ic is ({ic:.6f}) and the rankic is ({ric:.6f}).", ic


class FactorValueEvaluator(FactorEvaluator):
    def evaluate(
        self,
        implementation: Workspace,
        gt_implementation: Workspace,
        version: int = 1,  # 1 for qlib factors and 2 for kaggle factors
        **kwargs,
    ) -> Tuple:
        conclusions = []

        # Initialize result variables
        row_result = 0
        index_result = 0
        output_format_result = None
        equal_value_ratio_result = 0
        high_correlation_result = False
        row_result = None

        # Check if both dataframe has only one columns Mute this since factor task might generate more than one columns now
        if version == 1:
            feedback_str, _ = FactorSingleColumnEvaluator(self.scen).evaluate(implementation, gt_implementation)
            conclusions.append(feedback_str)
        elif version == 2:
            input_shape = self.scen.input_shape
            _, gen_df = self._get_df(gt_implementation, implementation)
            if gen_df.shape[-1] > input_shape[-1]:
                conclusions.append(
                    "Output dataframe has more columns than input feature which is not acceptable in feature processing tasks. Please check the implementation to avoid generating too many columns. Consider this implementation as a failure."
                )

        feedback_str, inf_evaluate_res = FactorInfEvaluator(self.scen).evaluate(implementation, gt_implementation)
        conclusions.append(feedback_str)

        # Check if the index of the dataframe is ("datetime", "instrument")
        feedback_str, _ = FactorOutputFormatEvaluator(self.scen).evaluate(implementation, gt_implementation)
        conclusions.append(feedback_str)
        if version == 1:
            feedback_str, daily_check_result = FactorDatetimeDailyEvaluator(self.scen).evaluate(
                implementation, gt_implementation
            )
            conclusions.append(feedback_str)
        else:
            daily_check_result = None

        # Check dataframe format
        if gt_implementation is not None:
            feedback_str, row_result = FactorRowCountEvaluator(self.scen).evaluate(implementation, gt_implementation)
            conclusions.append(feedback_str)

            feedback_str, index_result = FactorIndexEvaluator(self.scen).evaluate(implementation, gt_implementation)
            conclusions.append(feedback_str)

            feedback_str, output_format_result = FactorMissingValuesEvaluator(self.scen).evaluate(
                implementation, gt_implementation
            )
            conclusions.append(feedback_str)

            feedback_str, equal_value_ratio_result = FactorEqualValueRatioEvaluator(self.scen).evaluate(
                implementation, gt_implementation
            )
            conclusions.append(feedback_str)

            if index_result > 0.99:
                feedback_str, high_correlation_result = FactorCorrelationEvaluator(
                    hard_check=True, scen=self.scen
                ).evaluate(implementation, gt_implementation)
            else:
                high_correlation_result = False
                feedback_str = "The source dataframe and the ground truth dataframe have different index. Give up comparing the values and correlation because it's useless"
            conclusions.append(feedback_str)

        # Combine all conclusions into a single string
        conclusion_str = "\n".join(conclusions)

        if gt_implementation is not None and (equal_value_ratio_result > 0.99) or high_correlation_result:
            decision_from_value_check = True
        elif (
            row_result is not None
            and row_result <= 0.99
            or output_format_result is False
            or daily_check_result is False
            or inf_evaluate_res is False
        ):
            decision_from_value_check = False
        else:
            decision_from_value_check = None
        return conclusion_str, decision_from_value_check


class FactorFinalDecisionEvaluator(FactorEvaluator):
    def evaluate(
        self,
        target_task: FactorTask,
        execution_feedback: str,
        value_feedback: str,
        code_feedback: str,
        **kwargs,
    ) -> Tuple:
        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(evaluate_prompts["evaluator_final_decision_v1_system"])
            .render(
                scenario=(
                    self.scen.get_scenario_all_desc(target_task, filtered_tag="feature")
                    if self.scen is not None
                    else "No scenario description."
                )
            )
        )
        execution_feedback_to_render = execution_feedback

        for _ in range(10):  # 10 times to split the content is enough
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(
                    evaluate_prompts["evaluator_final_decision_v1_user"],
                )
                .render(
                    factor_information=target_task.get_task_information(),
                    execution_feedback=execution_feedback_to_render,
                    code_feedback=code_feedback,
                    value_feedback=(
                        value_feedback
                        if value_feedback is not None
                        else "No Ground Truth Value provided, so no evaluation on value is performed."
                    ),
                )
            )
            if (
                APIBackend().build_messages_and_calculate_token(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                )
                > LLM_SETTINGS.chat_token_limit
            ):
                execution_feedback_to_render = execution_feedback_to_render[len(execution_feedback_to_render) // 2 :]
            else:
                break

        # TODO:  with retry_context(retry_n=3, except_list=[KeyError]):
        final_evaluation_dict = None
        attempts = 0
        max_attempts = 3

        while attempts < max_attempts:
            try:
                api = APIBackend() if attempts == 0 else APIBackend(use_chat_cache=False)
                final_evaluation_dict = json.loads(
                    api.build_messages_and_create_chat_completion(
                        user_prompt=user_prompt,
                        system_prompt=system_prompt,
                        json_mode=True,
                        seed=attempts,  # in case of useless retrying when cache enabled.
                        json_target_type=Dict[str, str | bool | int],
                    ),
                )
                final_decision = final_evaluation_dict["final_decision"]
                final_feedback = final_evaluation_dict["final_feedback"]

                final_decision = str(final_decision).lower() in ["true", "1"]
                return final_decision, final_feedback

            except json.JSONDecodeError as e:
                raise ValueError("Failed to decode JSON response from API.") from e
            except KeyError as e:
                attempts += 1
                if attempts >= max_attempts:
                    raise KeyError(
                        "Response from API is missing 'final_decision' or 'final_feedback' key after multiple attempts."
                    ) from e

        return None, None



================================================
File: rdagent/components/coder/factor_coder/evaluators.py
================================================
import re

from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERMultiFeedback,
    CoSTEERSingleFeedbackDeprecated,
)
from rdagent.components.coder.factor_coder.eva_utils import (
    FactorCodeEvaluator,
    FactorFinalDecisionEvaluator,
    FactorValueEvaluator,
)
from rdagent.components.coder.factor_coder.factor import FactorTask
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import Workspace

FactorSingleFeedback = CoSTEERSingleFeedbackDeprecated
FactorMultiFeedback = CoSTEERMultiFeedback


class FactorEvaluatorForCoder(CoSTEEREvaluator):
    """This class is the v1 version of evaluator for a single factor implementation.
    It calls several evaluators in share modules to evaluate the factor implementation.
    """

    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.value_evaluator = FactorValueEvaluator(self.scen)
        self.code_evaluator = FactorCodeEvaluator(self.scen)
        self.final_decision_evaluator = FactorFinalDecisionEvaluator(self.scen)

    def evaluate(
        self,
        target_task: FactorTask,
        implementation: Workspace,
        gt_implementation: Workspace = None,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> FactorSingleFeedback:
        if implementation is None:
            return None

        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return FactorSingleFeedback(
                execution_feedback="This task has failed too many times, skip implementation.",
                value_generated_flag=False,
                code_feedback="This task has failed too many times, skip code evaluation.",
                value_feedback="This task has failed too many times, skip value evaluation.",
                final_decision=False,
                final_feedback="This task has failed too many times, skip final decision evaluation.",
                final_decision_based_on_gt=False,
            )
        else:
            factor_feedback = FactorSingleFeedback()

            # 1. Get factor execution feedback to generated implementation and remove the long list of numbers in execution feedback
            (
                execution_feedback,
                gen_df,
            ) = implementation.execute()

            execution_feedback = re.sub(r"(?<=\D)(,\s+-?\d+\.\d+){50,}(?=\D)", ", ", execution_feedback)
            factor_feedback.execution_feedback = "\n".join(
                [line for line in execution_feedback.split("\n") if "warning" not in line.lower()]
            )

            # 2. Get factor value feedback
            if gen_df is None:
                factor_feedback.value_feedback = "No factor value generated, skip value evaluation."
                factor_feedback.value_generated_flag = False
                decision_from_value_check = None
            else:
                factor_feedback.value_generated_flag = True
                (
                    factor_feedback.value_feedback,
                    decision_from_value_check,
                ) = self.value_evaluator.evaluate(
                    implementation=implementation, gt_implementation=gt_implementation, version=target_task.version
                )

            factor_feedback.final_decision_based_on_gt = gt_implementation is not None

            if decision_from_value_check is not None and decision_from_value_check is True:
                # To avoid confusion, when same_value_or_high_correlation is True, we do not need code feedback
                factor_feedback.code_feedback = "Final decision is True and there are no code critics."
                factor_feedback.final_decision = decision_from_value_check
                factor_feedback.final_feedback = "Value evaluation passed, skip final decision evaluation."
            elif decision_from_value_check is not None and decision_from_value_check is False:
                factor_feedback.code_feedback, _ = self.code_evaluator.evaluate(
                    target_task=target_task,
                    implementation=implementation,
                    execution_feedback=factor_feedback.execution_feedback,
                    value_feedback=factor_feedback.value_feedback,
                    gt_implementation=gt_implementation,
                )
                factor_feedback.final_decision = decision_from_value_check
                factor_feedback.final_feedback = "Value evaluation failed, skip final decision evaluation."
            else:
                factor_feedback.code_feedback, _ = self.code_evaluator.evaluate(
                    target_task=target_task,
                    implementation=implementation,
                    execution_feedback=factor_feedback.execution_feedback,
                    value_feedback=factor_feedback.value_feedback,
                    gt_implementation=gt_implementation,
                )
                (
                    factor_feedback.final_decision,
                    factor_feedback.final_feedback,
                ) = self.final_decision_evaluator.evaluate(
                    target_task=target_task,
                    execution_feedback=factor_feedback.execution_feedback,
                    value_feedback=factor_feedback.value_feedback,
                    code_feedback=factor_feedback.code_feedback,
                )
            return factor_feedback


# TODO:
def shorten_prompt(tpl: str, render_kwargs: dict, shorten_key: str, max_trail: int = 10) -> str:
    """When the prompt is too long. We have to shorten it.
    But we should not truncate the prompt directly, so we should find the key we want to shorten and then shorten it.
    """
    # TODO: this should replace most of code in
    # - FactorFinalDecisionEvaluator.evaluate
    # - FactorCodeEvaluator.evaluate



================================================
File: rdagent/components/coder/factor_coder/evolving_strategy.py
================================================
from __future__ import annotations

import json
from pathlib import Path
from typing import Dict

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.CoSTEER.evaluators import CoSTEERSingleFeedback
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
    CoSTEERQueriedKnowledgeV2,
)
from rdagent.components.coder.factor_coder.config import FACTOR_COSTEER_SETTINGS
from rdagent.components.coder.factor_coder.factor import FactorFBWorkspace, FactorTask
from rdagent.core.experiment import FBWorkspace
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.oai.llm_utils import APIBackend

implement_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class FactorMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.num_loop = 0
        self.haveSelected = False

    def error_summary(
        self,
        target_task: FactorTask,
        queried_former_failed_knowledge_to_render: list,
        queried_similar_error_knowledge_to_render: list,
    ) -> str:
        error_summary_system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(implement_prompts["evolving_strategy_error_summary_v2_system"])
            .render(
                scenario=self.scen.get_scenario_all_desc(target_task),
                factor_information_str=target_task.get_task_information(),
                code_and_feedback=queried_former_failed_knowledge_to_render[-1].get_implementation_and_feedback_str(),
            )
            .strip("\n")
        )
        for _ in range(10):  # max attempt to reduce the length of error_summary_user_prompt
            error_summary_user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(implement_prompts["evolving_strategy_error_summary_v2_user"])
                .render(
                    queried_similar_error_knowledge=queried_similar_error_knowledge_to_render,
                )
                .strip("\n")
            )
            if (
                APIBackend().build_messages_and_calculate_token(
                    user_prompt=error_summary_user_prompt, system_prompt=error_summary_system_prompt
                )
                < LLM_SETTINGS.chat_token_limit
            ):
                break
            elif len(queried_similar_error_knowledge_to_render) > 0:
                queried_similar_error_knowledge_to_render = queried_similar_error_knowledge_to_render[:-1]
        error_summary_critics = APIBackend(
            use_chat_cache=FACTOR_COSTEER_SETTINGS.coder_use_cache
        ).build_messages_and_create_chat_completion(
            user_prompt=error_summary_user_prompt, system_prompt=error_summary_system_prompt, json_mode=False
        )
        return error_summary_critics

    def implement_one_task(
        self,
        target_task: FactorTask,
        queried_knowledge: CoSTEERQueriedKnowledge,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> str:
        target_factor_task_information = target_task.get_task_information()

        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[target_factor_task_information]
            if queried_knowledge is not None
            else []
        )  # A list, [success task implement knowledge]

        if isinstance(queried_knowledge, CoSTEERQueriedKnowledgeV2):
            queried_similar_error_knowledge = (
                queried_knowledge.task_to_similar_error_successful_knowledge[target_factor_task_information]
                if queried_knowledge is not None
                else {}
            )  # A dict, {{error_type:[[error_imp_knowledge, success_imp_knowledge],...]},...}
        else:
            queried_similar_error_knowledge = {}

        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[target_factor_task_information][0]
            if queried_knowledge is not None
            else []
        )

        queried_former_failed_knowledge_to_render = queried_former_failed_knowledge

        latest_attempt_to_latest_successful_execution = queried_knowledge.task_to_former_failed_traces[
            target_factor_task_information
        ][1]

        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(
                implement_prompts["evolving_strategy_factor_implementation_v1_system"],
            )
            .render(
                scenario=self.scen.get_scenario_all_desc(target_task, filtered_tag="feature"),
                queried_former_failed_knowledge=queried_former_failed_knowledge_to_render,
            )
        )
        queried_similar_successful_knowledge_to_render = queried_similar_successful_knowledge
        queried_similar_error_knowledge_to_render = queried_similar_error_knowledge
        # 动态地防止prompt超长
        for _ in range(10):  # max attempt to reduce the length of user_prompt
            # 总结error（可选）
            if (
                isinstance(queried_knowledge, CoSTEERQueriedKnowledgeV2)
                and FACTOR_COSTEER_SETTINGS.v2_error_summary
                and len(queried_similar_error_knowledge_to_render) != 0
                and len(queried_former_failed_knowledge_to_render) != 0
            ):
                error_summary_critics = self.error_summary(
                    target_task,
                    queried_former_failed_knowledge_to_render,
                    queried_similar_error_knowledge_to_render,
                )
            else:
                error_summary_critics = None
            # 构建user_prompt。开始写代码
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(
                    implement_prompts["evolving_strategy_factor_implementation_v2_user"],
                )
                .render(
                    factor_information_str=target_factor_task_information,
                    queried_similar_successful_knowledge=queried_similar_successful_knowledge_to_render,
                    queried_similar_error_knowledge=queried_similar_error_knowledge_to_render,
                    error_summary_critics=error_summary_critics,
                    latest_attempt_to_latest_successful_execution=latest_attempt_to_latest_successful_execution,
                )
                .strip("\n")
            )
            if (
                APIBackend().build_messages_and_calculate_token(user_prompt=user_prompt, system_prompt=system_prompt)
                < LLM_SETTINGS.chat_token_limit
            ):
                break
            elif len(queried_former_failed_knowledge_to_render) > 1:
                queried_former_failed_knowledge_to_render = queried_former_failed_knowledge_to_render[1:]
            elif len(queried_similar_successful_knowledge_to_render) > len(
                queried_similar_error_knowledge_to_render,
            ):
                queried_similar_successful_knowledge_to_render = queried_similar_successful_knowledge_to_render[:-1]
            elif len(queried_similar_error_knowledge_to_render) > 0:
                queried_similar_error_knowledge_to_render = queried_similar_error_knowledge_to_render[:-1]
        for _ in range(10):
            try:
                code = json.loads(
                    APIBackend(
                        use_chat_cache=FACTOR_COSTEER_SETTINGS.coder_use_cache
                    ).build_messages_and_create_chat_completion(
                        user_prompt=user_prompt,
                        system_prompt=system_prompt,
                        json_mode=True,
                        json_target_type=Dict[str, str],
                    )
                )["code"]
                return code
            except json.decoder.JSONDecodeError:
                pass
        else:
            return ""  # return empty code if failed to get code after 10 attempts

    def assign_code_list_to_evo(self, code_list, evo):
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                evo.sub_workspace_list[index] = FactorFBWorkspace(target_task=evo.sub_tasks[index])
            evo.sub_workspace_list[index].inject_files(**{"factor.py": code_list[index]})
        return evo



================================================
File: rdagent/components/coder/factor_coder/factor.py
================================================
from __future__ import annotations

import subprocess
import uuid
from pathlib import Path
from typing import Tuple, Union

import pandas as pd
from filelock import FileLock

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.components.coder.CoSTEER.task import CoSTEERTask
from rdagent.components.coder.factor_coder.config import FACTOR_COSTEER_SETTINGS
from rdagent.core.exception import CodeFormatError, CustomRuntimeError, NoOutputError
from rdagent.core.experiment import Experiment, FBWorkspace
from rdagent.core.utils import cache_with_pickle
from rdagent.oai.llm_utils import md5_hash


class FactorTask(CoSTEERTask):
    # TODO:  generalized the attributes into the Task
    # - factor_* -> *
    def __init__(
        self,
        factor_name,
        factor_description,
        factor_formulation,
        *args,
        variables: dict = {},
        resource: str = None,
        factor_implementation: bool = False,
        **kwargs,
    ) -> None:
        self.factor_name = (
            factor_name  # TODO: remove it in the later version. Keep it only for pickle version compatibility
        )
        self.factor_formulation = factor_formulation
        self.variables = variables
        self.factor_resources = resource
        self.factor_implementation = factor_implementation
        super().__init__(name=factor_name, description=factor_description, *args, **kwargs)

    @property
    def factor_description(self):
        """for compatibility"""
        return self.description

    def get_task_information(self):
        return f"""factor_name: {self.factor_name}
factor_description: {self.factor_description}
factor_formulation: {self.factor_formulation}
variables: {str(self.variables)}"""

    def get_task_information_and_implementation_result(self):
        return {
            "factor_name": self.factor_name,
            "factor_description": self.factor_description,
            "factor_formulation": self.factor_formulation,
            "variables": str(self.variables),
            "factor_implementation": str(self.factor_implementation),
        }

    @staticmethod
    def from_dict(dict):
        return FactorTask(**dict)

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}[{self.factor_name}]>"


class FactorFBWorkspace(FBWorkspace):
    """
    This class is used to implement a factor by writing the code to a file.
    Input data and output factor value are also written to files.
    """

    # TODO: (Xiao) think raising errors may get better information for processing
    FB_EXEC_SUCCESS = "Execution succeeded without error."
    FB_CODE_NOT_SET = "code is not set."
    FB_EXECUTION_SUCCEEDED = "Execution succeeded without error."
    FB_OUTPUT_FILE_NOT_FOUND = "\nExpected output file not found."
    FB_OUTPUT_FILE_FOUND = "\nExpected output file found."

    def __init__(
        self,
        *args,
        raise_exception: bool = False,
        **kwargs,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.raise_exception = raise_exception

    def hash_func(self, data_type: str = "Debug") -> str:
        return (
            md5_hash(data_type + self.file_dict["factor.py"])
            if ("factor.py" in self.file_dict and not self.raise_exception)
            else None
        )

    @cache_with_pickle(hash_func)
    def execute(self, data_type: str = "Debug") -> Tuple[str, pd.DataFrame]:
        """
        execute the implementation and get the factor value by the following steps:
        1. make the directory in workspace path
        2. write the code to the file in the workspace path
        3. link all the source data to the workspace path folder
        if call_factor_py is True:
            4. execute the code
        else:
            4. generate a script from template to import the factor.py dump get the factor value to result.h5
        5. read the factor value from the output file in the workspace path folder
        returns the execution feedback as a string and the factor value as a pandas dataframe


        Regarding the cache mechanism:
        1. We will store the function's return value to ensure it behaves as expected.
        - The cached information will include a tuple with the following: (execution_feedback, executed_factor_value_dataframe, Optional[Exception])

        """
        self.before_execute()
        if self.file_dict is None or "factor.py" not in self.file_dict:
            if self.raise_exception:
                raise CodeFormatError(self.FB_CODE_NOT_SET)
            else:
                return self.FB_CODE_NOT_SET, None
        with FileLock(self.workspace_path / "execution.lock"):
            if self.target_task.version == 1:
                source_data_path = (
                    Path(
                        FACTOR_COSTEER_SETTINGS.data_folder_debug,
                    )
                    if data_type == "Debug"  # FIXME: (yx) don't think we should use a debug tag for this.
                    else Path(
                        FACTOR_COSTEER_SETTINGS.data_folder,
                    )
                )
            elif self.target_task.version == 2:
                # TODO you can change the name of the data folder for a better understanding
                source_data_path = Path(KAGGLE_IMPLEMENT_SETTING.local_data_path) / KAGGLE_IMPLEMENT_SETTING.competition

            source_data_path.mkdir(exist_ok=True, parents=True)
            code_path = self.workspace_path / f"factor.py"

            self.link_all_files_in_folder_to_workspace(source_data_path, self.workspace_path)

            execution_feedback = self.FB_EXECUTION_SUCCEEDED
            execution_success = False
            execution_error = None

            if self.target_task.version == 1:
                execution_code_path = code_path
            elif self.target_task.version == 2:
                execution_code_path = self.workspace_path / f"{uuid.uuid4()}.py"
                execution_code_path.write_text((Path(__file__).parent / "factor_execution_template.txt").read_text())

            try:
                subprocess.check_output(
                    f"{FACTOR_COSTEER_SETTINGS.python_bin} {execution_code_path}",
                    shell=True,
                    cwd=self.workspace_path,
                    stderr=subprocess.STDOUT,
                    timeout=FACTOR_COSTEER_SETTINGS.file_based_execution_timeout,
                )
                execution_success = True
            except subprocess.CalledProcessError as e:
                import site

                execution_feedback = (
                    e.output.decode()
                    .replace(str(execution_code_path.parent.absolute()), r"/path/to")
                    .replace(str(site.getsitepackages()[0]), r"/path/to/site-packages")
                )
                if len(execution_feedback) > 2000:
                    execution_feedback = (
                        execution_feedback[:1000] + "....hidden long error message...." + execution_feedback[-1000:]
                    )
                if self.raise_exception:
                    raise CustomRuntimeError(execution_feedback)
                else:
                    execution_error = CustomRuntimeError(execution_feedback)
            except subprocess.TimeoutExpired:
                execution_feedback += f"Execution timeout error and the timeout is set to {FACTOR_COSTEER_SETTINGS.file_based_execution_timeout} seconds."
                if self.raise_exception:
                    raise CustomRuntimeError(execution_feedback)
                else:
                    execution_error = CustomRuntimeError(execution_feedback)

            workspace_output_file_path = self.workspace_path / "result.h5"
            if workspace_output_file_path.exists() and execution_success:
                try:
                    executed_factor_value_dataframe = pd.read_hdf(workspace_output_file_path)
                    execution_feedback += self.FB_OUTPUT_FILE_FOUND
                except Exception as e:
                    execution_feedback += f"Error found when reading hdf file: {e}"[:1000]
                    executed_factor_value_dataframe = None
            else:
                execution_feedback += self.FB_OUTPUT_FILE_NOT_FOUND
                executed_factor_value_dataframe = None
                if self.raise_exception:
                    raise NoOutputError(execution_feedback)
                else:
                    execution_error = NoOutputError(execution_feedback)

        return execution_feedback, executed_factor_value_dataframe

    def __str__(self) -> str:
        # NOTE:
        # If the code cache works, the workspace will be None.
        return f"File Factor[{self.target_task.factor_name}]: {self.workspace_path}"

    def __repr__(self) -> str:
        return self.__str__()

    @staticmethod
    def from_folder(task: FactorTask, path: Union[str, Path], **kwargs):
        path = Path(path)
        code_dict = {}
        for file_path in path.iterdir():
            if file_path.suffix == ".py":
                code_dict[file_path.name] = file_path.read_text()
        return FactorFBWorkspace(target_task=task, code_dict=code_dict, **kwargs)


FactorExperiment = Experiment
FeatureExperiment = Experiment



================================================
File: rdagent/components/coder/factor_coder/factor_execution_template.txt
================================================
import os

import numpy as np
import pandas as pd
from factor import feature_engineering_cls

if os.path.exists("X_valid.pkl"):
    valid_df = pd.read_pickle("X_valid.pkl").head(1000)
else:
    raise FileNotFoundError("No valid data found.")

cls = feature_engineering_cls()
cls.fit(valid_df)
new_feat = cls.transform(valid_df)
new_feat.to_hdf("result.h5", key="data", mode="w")



================================================
File: rdagent/components/coder/factor_coder/prompts.yaml
================================================

evaluator_code_feedback_v1_system: |-
  User is trying to implement some factors in the following scenario:
  {{ scenario }}
  User will provide you the information of the factor.

  Your job is to check whether user's code is align with the factor and the scenario.
  The user will provide the source python code and the execution error message if execution failed.
  The user might provide you the ground truth code for you to provide the critic. You should not leak the ground truth code to the user in any form but you can use it to provide the critic.

  User has also compared the factor values calculated by the user's code and the ground truth code. The user will provide you some analyze result comparing two output. You may find some error in the code which caused the difference between the two output.

  If the ground truth code is provided, your critic should only consider checking whether the user's code is align with the ground truth code since the ground truth is definitely correct.
  If the ground truth code is not provided, your critic should consider checking whether the user's code is reasonable and correct.

  Notice that your critics are not for user to debug the code. They are sent to the coding agent to correct the code. So don't give any following items for the user to check like "Please check the code line XXX".

  You suggestion should not include any code, just some clear and short suggestions. Please point out very critical issues in your response, ignore non-important issues to avoid confusion. If no big issue found in the code, you can response "No critics found".
  
  You should provide the suggestion to each of your critic to help the user improve the code. Please response the critic in the following format. Here is an example structure for the output:
  critic 1: The critic message to critic 1
  critic 2: The critic message to critic 2

evaluator_code_feedback_v1_user: |-
  --------------Factor information:---------------
  {{ factor_information }}
  --------------Python code:---------------
  {{ code }}
  --------------Execution feedback:---------------
  {{ execution_feedback }}
  {% if value_feedback is not none %}
  --------------Factor value feedback:---------------
  {{ value_feedback }}
  {% endif %}
  {% if gt_code is not none %}
  --------------Ground truth Python code:---------------
  {{ gt_code }}
  {% endif %}

evolving_strategy_factor_implementation_v1_system: |-
  User is trying to implement some factors in the following scenario:
  {{ scenario }}
  Your code is expected to align the scenario in any form which means The user needs to get the exact factor values with your code as expected.

  To help you write the correct code, the user might provide multiple information that helps you write the correct code:
  1. The user might provide you the correct code to similar factors. Your should learn from these code to write the correct code.
  2. The user might provide you the failed former code and the corresponding feedback to the code. The feedback contains to the execution, the code and the factor value. You should analyze the feedback and try to correct the latest code.
  3. The user might provide you the suggestion to the latest fail code and some similar fail to correct pairs. Each pair contains the fail code with similar error and the corresponding corrected version code. You should learn from these suggestion to write the correct code.
  
  Your must write your code based on your former latest attempt below which consists of your former code and code feedback, you should read the former attempt carefully and must not modify the right part of your former code.

  {% if queried_former_failed_knowledge|length != 0 %}
  --------------Your former latest attempt:---------------
  =====Code to the former implementation=====
  {{ queried_former_failed_knowledge[-1].implementation.all_codes }}
  =====Feedback to the former implementation=====
  {{ queried_former_failed_knowledge[-1].feedback }}
  {% endif %}

  Please response the code in the following json format. Here is an example structure for the JSON output:
  {
      "code": "The Python code as a string."
  }

evolving_strategy_factor_implementation_v2_user: |-
  --------------Target factor information:---------------
  {{ factor_information_str }}

  {% if queried_similar_error_knowledge|length != 0 %}
  {% if error_summary_critics is none %}
  Recall your last failure, your implementation met some errors.
  When doing other tasks, you met some similar errors but you finally solve them. Here are some examples:
  {% for error_content, similar_error_knowledge in queried_similar_error_knowledge %} 
  --------------Factor information to similar error ({{error_content}}):---------------
  {{ similar_error_knowledge[0].target_task.get_task_information() }}
  =====Code with similar error ({{error_content}}):=====
  {{ similar_error_knowledge[0].implementation.all_codes }}
  =====Success code to former code with similar error ({{error_content}}):=====
  {{ similar_error_knowledge[1].implementation.all_codes }}
  {% endfor %}
  {% else %}
  Recall your last failure, your implementation met some errors.
  After reviewing some similar errors and their solutions, here are some suggestions for you to correct your code:
  {{error_summary_critics}}
  {% endif %}
  {% endif %}
  {% if queried_similar_successful_knowledge|length != 0 %}
  Here are some success implements of similar component tasks, take them as references:
  --------------Correct code to similar factors:---------------
  {% for similar_successful_knowledge in queried_similar_successful_knowledge %}
  =====Factor {{loop.index}}:=====
  {{ similar_successful_knowledge.target_task.get_task_information() }}
  =====Code:=====
  {{ similar_successful_knowledge.implementation.all_codes }}
  {% endfor %}
  {% endif %}
  {% if latest_attempt_to_latest_successful_execution is not none %}
  You have tried to correct your former failed code but still met some errors. Here is the latest attempt to the latest successful execution, try not to get the same error to your new code:
  =====Your latest attempt=====
  {{ latest_attempt_to_latest_successful_execution.implementation.all_codes }}
  =====Feedback to your latest attempt=====
  {{ latest_attempt_to_latest_successful_execution.feedback }}
  {% endif %}

evolving_strategy_error_summary_v2_system: |-
  User is trying to implement some factors in the following scenario:
  {{ scenario }}
  User is doing the following task: 
  {{factor_information_str}}

  You have written some code but it meets errors like the following:
  {{code_and_feedback}}

  The user has found some tasks that met similar errors, and their final correct solutions.
  Please refer to these similar errors and their solutions, provide some clear, short and accurate critics that might help you solve the issues in your code.

  You suggestion should not include any code, just some clear and short suggestions. Please point out very critical issues in your response, ignore non-important issues to avoid confusion. If no big issue found in the code, you can response "No critics found".

  Please response the critic in the following format. Here is an example structure for the output:
  critic 1: The critic message to critic 1
  critic 2: The critic message to critic 2
  
evolving_strategy_error_summary_v2_user: |-
  {% if queried_similar_error_knowledge|length != 0 %}
  {% for error_content, similar_error_knowledge in queried_similar_error_knowledge %} 
  --------------Factor information to similar error ({{error_content}}):---------------
  {{ similar_error_knowledge[0].target_task.get_task_information() }}
  =====Code with similar error ({{error_content}}):=====
  {{ similar_error_knowledge[0].implementation.all_codes }}
  =====Success code to former code with similar error ({{error_content}}):=====
  {{ similar_error_knowledge[1].implementation.all_codes }}
  {% endfor %}
  {% endif %}


select_implementable_factor_system: |-
  User is trying to implement some factors in the following scenario:
  {{ scenario }}
  Your job is to help the user select the easiest-to-implement factors. Some factors may be difficult to implement due to a lack of information or excessive complexity. The user will provide the number of factors you should pick and information about the factors, including their descriptions, formulas, and variable explanations.
  User will provide you the former attempt to implement the factor and the feedback to the implementation. You need to carefully review your previous attempts. Some factors have been repeatedly tried without success. You should consider discarding these factors.
  Please analyze the difficulties of the each factors and provide the reason and response the indices of selected implementable factor in the json format. Here is an example structure for the JSON output:
  {
      "Analysis": "Analyze the difficulties of the each factors and provide the reason why the factor can be implemented or not."
      "selected_factor": "The indices of selected factor index in the list, like [0, 2, 3].The length should be the number of factor left after filtering.",
  }

select_implementable_factor_user: |-
  Number of factor you should pick: {{ factor_num }}
  {% for factor_info in sub_tasks %} 
  =============Factor index:{{factor_info[0]}}:=============
  =====Factor name:=====
  {{ factor_info[1].factor_name }}
  =====Factor description:=====
  {{ factor_info[1].factor_description }}
  =====Factor formulation:=====
  {{ factor_info[1].factor_formulation }}
  {% if factor_info[2]|length != 0 %}
  --------------Your former attempt:---------------
  {% for former_attempt in factor_info[2] %}
  =====Code to attempt {{ loop.index }}=====
  {{ former_attempt.implementation.all_codes }}
  =====Feedback to attempt {{ loop.index }}=====
  {{ former_attempt.feedback }}
  {% endfor %}
  {% endif %}
  {% endfor %}

evaluator_output_format_system: |-
  User is trying to implement some factors in the following scenario:
  {{ scenario }}
  User will provide you the format of the output. Please help to check whether the output is align with the format.
  Please respond in the JSON format. Here is an example structure for the JSON output:
  {
      "output_format_decision": True,
      "output_format_feedback": "The output format is correct."
  }


evaluator_final_decision_v1_system: |-
  User is trying to implement some factors in the following scenario:
  {{ scenario }}
  User has finished evaluation and got some feedback from the evaluator.
  The evaluator run the code and get the factor value dataframe and provide several feedback regarding user's code and code output. You should analyze the feedback and considering the scenario and factor description to give a final decision about the evaluation result. The final decision concludes whether the factor is implemented correctly and if not, detail feedback containing reason and suggestion if the final decision is False.

  The implementation final decision is considered in the following logic:
  1. If the value and the ground truth value are exactly the same under a small tolerance, the implementation is considered correct.
  2. If the value and the ground truth value have a high correlation on ic or rank ic, the implementation is considered correct.
  3. If no ground truth value is provided, the implementation is considered correct if the code executes successfully (assuming the data provided is correct). Any exceptions, including those actively raised, are considered faults of the code. Additionally, the code feedback must align with the scenario and factor description.

  Please response the critic in the json format. Here is an example structure for the JSON output, please strictly follow the format:
  {
      "final_decision": True,
      "final_feedback": "The final feedback message",
  }

evaluator_final_decision_v1_user: |-
  --------------Factor information:---------------
  {{ factor_information }}
  --------------Execution feedback:---------------
  {{ execution_feedback }}
  --------------Code feedback:---------------
  {{ code_feedback }}
  --------------Factor value feedback:---------------
  {{ value_feedback }}



================================================
File: rdagent/components/coder/model_coder/__init__.py
================================================
from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.config import CoSTEER_SETTINGS
from rdagent.components.coder.CoSTEER.evaluators import CoSTEERMultiEvaluator
from rdagent.components.coder.model_coder.evaluators import ModelCoSTEEREvaluator
from rdagent.components.coder.model_coder.evolving_strategy import (
    ModelMultiProcessEvolvingStrategy,
)
from rdagent.core.scenario import Scenario


class ModelCoSTEER(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        eva = CoSTEERMultiEvaluator(ModelCoSTEEREvaluator(scen=scen), scen=scen)
        es = ModelMultiProcessEvolvingStrategy(scen=scen, settings=CoSTEER_SETTINGS)

        super().__init__(*args, settings=CoSTEER_SETTINGS, eva=eva, es=es, evolving_version=2, scen=scen, **kwargs)



================================================
File: rdagent/components/coder/model_coder/eva_utils.py
================================================
import json
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.CoSTEER.evaluators import CoSTEEREvaluator
from rdagent.components.coder.model_coder.model import ModelFBWorkspace, ModelTask
from rdagent.core.experiment import Task, Workspace
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.oai.llm_utils import APIBackend

evaluate_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


# This shape evaluator is also used in data_science
def shape_evaluator(prediction: np.ndarray, target_shape: Tuple = None) -> Tuple[str, bool]:
    if target_shape is None or prediction is None:
        return (
            "No output generated from the model. No shape evaluation conducted.",
            False,
        )
    pre_shape = prediction.shape

    if pre_shape == target_shape:
        return "The shape of the output is correct.", True
    else:
        return (
            f"The shape of the output is incorrect. Expected {target_shape}, but got {pre_shape}.",
            False,
        )


def value_evaluator(
    prediction: np.ndarray,
    target: np.ndarray,
) -> Tuple[np.ndarray, bool]:
    if prediction is None:
        return "No output generated from the model. Skip value evaluation", False
    elif target is None:
        return (
            "No ground truth output provided. Value evaluation not impractical",
            False,
        )
    else:
        # Calculate the mean absolute difference
        diff = np.mean(np.abs(target - prediction))
        return (
            f"The value of the output is correct. The mean absolute difference is {diff}.",
            diff < 0.1,
        )


class ModelCodeEvaluator(CoSTEEREvaluator):
    def evaluate(
        self,
        target_task: Task,
        implementation: Workspace,
        gt_implementation: Workspace,
        model_execution_feedback: str = "",
        model_value_feedback: str = "",
    ):
        assert isinstance(target_task, ModelTask)
        assert isinstance(implementation, ModelFBWorkspace)
        if gt_implementation is not None:
            assert isinstance(gt_implementation, ModelFBWorkspace)

        model_task_information = target_task.get_task_information()
        code = implementation.all_codes

        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(evaluate_prompts["evaluator_code_feedback"]["system"])
            .render(
                scenario=(
                    self.scen.get_scenario_all_desc(target_task, filtered_tag=target_task.model_type)
                    if self.scen is not None
                    else "No scenario description."
                )
            )
        )

        execution_feedback_to_render = model_execution_feedback
        for _ in range(10):  # 10 times to split the content is enough
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(
                    evaluate_prompts["evaluator_code_feedback"]["user"],
                )
                .render(
                    model_information=model_task_information,
                    code=code,
                    model_execution_feedback=execution_feedback_to_render,
                    model_value_feedback=model_value_feedback,
                    gt_code=gt_implementation.all_codes if gt_implementation else None,
                )
            )
            if (
                APIBackend().build_messages_and_calculate_token(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                )
                > LLM_SETTINGS.chat_token_limit
            ):
                execution_feedback_to_render = execution_feedback_to_render[len(execution_feedback_to_render) // 2 :]
            else:
                break

        critic_response = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            json_mode=False,
        )

        return critic_response, None


class ModelFinalEvaluator(CoSTEEREvaluator):
    def evaluate(
        self,
        target_task: Task,
        implementation: Workspace,
        gt_implementation: Workspace,
        model_execution_feedback: str,
        model_shape_feedback: str,
        model_value_feedback: str,
        model_code_feedback: str,
    ):
        assert isinstance(target_task, ModelTask)
        assert isinstance(implementation, ModelFBWorkspace)
        if gt_implementation is not None:
            assert isinstance(gt_implementation, ModelFBWorkspace)

        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(evaluate_prompts["evaluator_final_feedback"]["system"])
            .render(
                scenario=(
                    self.scen.get_scenario_all_desc(target_task, filtered_tag=target_task.model_type)
                    if self.scen is not None
                    else "No scenario description."
                )
            )
        )

        execution_feedback_to_render = model_execution_feedback

        for _ in range(10):  # 10 times to split the content is enough
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(
                    evaluate_prompts["evaluator_final_feedback"]["user"],
                )
                .render(
                    model_information=target_task.get_task_information(),
                    model_execution_feedback=execution_feedback_to_render,
                    model_shape_feedback=model_shape_feedback,
                    model_code_feedback=model_code_feedback,
                    model_value_feedback=model_value_feedback,
                )
            )
            if (
                APIBackend().build_messages_and_calculate_token(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                )
                > LLM_SETTINGS.chat_token_limit
            ):
                execution_feedback_to_render = execution_feedback_to_render[len(execution_feedback_to_render) // 2 :]
            else:
                break

        final_evaluation_dict = json.loads(
            APIBackend().build_messages_and_create_chat_completion(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                json_mode=True,
                json_target_type=Dict[str, str | bool | int],
            ),
        )
        if isinstance(final_evaluation_dict["final_decision"], str) and final_evaluation_dict[
            "final_decision"
        ].lower() in ("true", "false"):
            final_evaluation_dict["final_decision"] = bool(final_evaluation_dict["final_decision"])
        return (
            final_evaluation_dict["final_feedback"],
            final_evaluation_dict["final_decision"],
        )



================================================
File: rdagent/components/coder/model_coder/evaluators.py
================================================
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERMultiFeedback,
    CoSTEERSingleFeedbackDeprecated,
)
from rdagent.components.coder.model_coder.eva_utils import (
    ModelCodeEvaluator,
    ModelFinalEvaluator,
    shape_evaluator,
    value_evaluator,
)
from rdagent.components.coder.model_coder.model import ModelFBWorkspace, ModelTask
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import Task, Workspace

ModelSingleFeedback = CoSTEERSingleFeedbackDeprecated
ModelMultiFeedback = CoSTEERMultiFeedback


class ModelCoSTEEREvaluator(CoSTEEREvaluator):
    def evaluate(
        self,
        target_task: Task,
        implementation: Workspace,
        gt_implementation: Workspace,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> ModelSingleFeedback:
        target_task_information = target_task.get_task_information()
        if (
            queried_knowledge is not None
            and target_task_information in queried_knowledge.success_task_to_knowledge_dict
        ):
            return queried_knowledge.success_task_to_knowledge_dict[target_task_information].feedback
        elif queried_knowledge is not None and target_task_information in queried_knowledge.failed_task_info_set:
            return ModelSingleFeedback(
                execution_feedback="This task has failed too many times, skip implementation.",
                shape_feedback="This task has failed too many times, skip implementation.",
                value_feedback="This task has failed too many times, skip implementation.",
                code_feedback="This task has failed too many times, skip implementation.",
                final_feedback="This task has failed too many times, skip implementation.",
                final_decision=False,
            )
        assert isinstance(target_task, ModelTask)

        # NOTE: Use fixed input to test the model to avoid randomness
        batch_size = 8
        num_features = 30
        num_timesteps = 40
        input_value = 0.4
        param_init_value = 0.6

        assert isinstance(implementation, ModelFBWorkspace)
        model_execution_feedback, gen_np_array = implementation.execute(
            batch_size=batch_size,
            num_features=num_features,
            num_timesteps=num_timesteps,
            input_value=input_value,
            param_init_value=param_init_value,
        )
        if gt_implementation is not None:
            assert isinstance(gt_implementation, ModelFBWorkspace)
            _, gt_np_array = gt_implementation.execute(
                batch_size=batch_size,
                num_features=num_features,
                num_timesteps=num_timesteps,
                input_value=input_value,
                param_init_value=param_init_value,
            )
        else:
            gt_np_array = None

        shape_feedback, shape_decision = shape_evaluator(
            gen_np_array,
            (batch_size, self.scen.model_output_channel if hasattr(self.scen, "model_output_channel") else 1),
        )
        value_feedback, value_decision = value_evaluator(gen_np_array, gt_np_array)
        code_feedback, _ = ModelCodeEvaluator(scen=self.scen).evaluate(
            target_task=target_task,
            implementation=implementation,
            gt_implementation=gt_implementation,
            model_execution_feedback=model_execution_feedback,
            model_value_feedback="\n".join([shape_feedback, value_feedback]),
        )
        final_feedback, final_decision = ModelFinalEvaluator(scen=self.scen).evaluate(
            target_task=target_task,
            implementation=implementation,
            gt_implementation=gt_implementation,
            model_execution_feedback=model_execution_feedback,
            model_shape_feedback=shape_feedback,
            model_value_feedback=value_feedback,
            model_code_feedback=code_feedback,
        )

        return ModelSingleFeedback(
            execution_feedback=model_execution_feedback,
            shape_feedback=shape_feedback,
            value_feedback=value_feedback,
            code_feedback=code_feedback,
            final_feedback=final_feedback,
            final_decision=final_decision,
            value_generated_flag=(gen_np_array is not None),
            final_decision_based_on_gt=(gt_implementation is not None),
        )



================================================
File: rdagent/components/coder/model_coder/evolving_strategy.py
================================================
import json
from pathlib import Path
from typing import Dict

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.CoSTEER.config import CoSTEER_SETTINGS
from rdagent.components.coder.CoSTEER.evaluators import CoSTEERSingleFeedback
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.knowledge_management import (
    CoSTEERQueriedKnowledge,
    CoSTEERQueriedKnowledgeV2,
)
from rdagent.components.coder.model_coder.model import (
    ModelExperiment,
    ModelFBWorkspace,
    ModelTask,
)
from rdagent.core.experiment import FBWorkspace
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.oai.llm_utils import APIBackend

coder_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class ModelMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: ModelTask,
        queried_knowledge: CoSTEERQueriedKnowledge = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> str:
        model_information_str = target_task.get_task_information()

        queried_similar_successful_knowledge = (
            queried_knowledge.task_to_similar_task_successful_knowledge[model_information_str]
            if queried_knowledge is not None
            else []
        )
        queried_former_failed_knowledge = (
            queried_knowledge.task_to_former_failed_traces[model_information_str]
            if queried_knowledge is not None
            else []
        )

        queried_former_failed_knowledge_to_render = (
            queried_former_failed_knowledge[0]
            if isinstance(queried_knowledge, CoSTEERQueriedKnowledgeV2)
            else queried_former_failed_knowledge
        )

        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(
                coder_prompts["evolving_strategy_model_coder"]["system"],
            )
            .render(
                scenario=self.scen.get_scenario_all_desc(filtered_tag=target_task.model_type),
                queried_former_failed_knowledge=queried_former_failed_knowledge_to_render,
                current_code=target_task.base_code,
            )
        )

        queried_similar_successful_knowledge_to_render = queried_similar_successful_knowledge
        for _ in range(10):  # max attempt to reduce the length of user_prompt
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(
                    coder_prompts["evolving_strategy_model_coder"]["user"],
                )
                .render(
                    model_information_str=model_information_str,
                    queried_similar_successful_knowledge=queried_similar_successful_knowledge_to_render,
                    queried_former_failed_knowledge=queried_former_failed_knowledge_to_render,
                )
                .strip("\n")
            )
            if (
                APIBackend().build_messages_and_calculate_token(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                )
                < LLM_SETTINGS.chat_token_limit
            ):
                break
            elif len(queried_former_failed_knowledge_to_render) > 1:
                queried_former_failed_knowledge_to_render = queried_former_failed_knowledge_to_render[1:]
            elif len(queried_similar_successful_knowledge_to_render) > 1:
                queried_similar_successful_knowledge_to_render = queried_similar_successful_knowledge_to_render[1:]

        code = json.loads(
            APIBackend(use_chat_cache=CoSTEER_SETTINGS.coder_use_cache).build_messages_and_create_chat_completion(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                json_mode=True,
                json_target_type=Dict[str, str],
            ),
        )["code"]
        return code

    def assign_code_list_to_evo(self, code_list, evo):
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                evo.sub_workspace_list[index] = ModelFBWorkspace(target_task=evo.sub_tasks[index])
            evo.sub_workspace_list[index].inject_files(**{"model.py": code_list[index]})
        return evo



================================================
File: rdagent/components/coder/model_coder/gt_code.py
================================================
"""
This is just an exmaple.
It will be replaced wtih a list of ground truth tasks.
"""

import math
from typing import Any, Callable, Dict, Optional, Union

import torch
from torch import Tensor
from torch.nn import Parameter
from torch_geometric.nn.conv import GCNConv, MessagePassing
from torch_geometric.nn.inits import zeros
from torch_geometric.nn.resolver import activation_resolver
from torch_geometric.typing import Adj


class AntiSymmetricConv(torch.nn.Module):
    r"""The anti-symmetric graph convolutional operator from the
    `"Anti-Symmetric DGN: a stable architecture for Deep Graph Networks"
    <https://openreview.net/forum?id=J3Y7cgZOOS>`_ paper.

    .. math::
        \mathbf{x}^{\prime}_i = \mathbf{x}_i + \epsilon \cdot \sigma \left(
            (\mathbf{W}-\mathbf{W}^T-\gamma \mathbf{I}) \mathbf{x}_i +
            \Phi(\mathbf{X}, \mathcal{N}_i) + \mathbf{b}\right),

    where :math:`\Phi(\mathbf{X}, \mathcal{N}_i)` denotes a
    :class:`~torch.nn.conv.MessagePassing` layer.

    Args:
        in_channels (int): Size of each input sample.
        phi (MessagePassing, optional): The message passing module
            :math:`\Phi`. If set to :obj:`None`, will use a
            :class:`~torch_geometric.nn.conv.GCNConv` layer as default.
            (default: :obj:`None`)
        num_iters (int, optional): The number of times the anti-symmetric deep
            graph network operator is called. (default: :obj:`1`)
        epsilon (float, optional): The discretization step size
            :math:`\epsilon`. (default: :obj:`0.1`)
        gamma (float, optional): The strength of the diffusion :math:`\gamma`.
            It regulates the stability of the method. (default: :obj:`0.1`)
        act (str, optional): The non-linear activation function :math:`\sigma`,
            *e.g.*, :obj:`"tanh"` or :obj:`"relu"`. (default: :class:`"tanh"`)
        act_kwargs (Dict[str, Any], optional): Arguments passed to the
            respective activation function defined by :obj:`act`.
            (default: :obj:`None`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)

    Shapes:
        - **input:**
          node features :math:`(|\mathcal{V}|, F_{in})`,
          edge indices :math:`(2, |\mathcal{E}|)`,
          edge weights :math:`(|\mathcal{E}|)` *(optional)*
        - **output:** node features :math:`(|\mathcal{V}|, F_{in})`
    """

    def __init__(
        self,
        in_channels: int,
        phi: Optional[MessagePassing] = None,
        num_iters: int = 1,
        epsilon: float = 0.1,
        gamma: float = 0.1,
        act: Union[str, Callable, None] = "tanh",
        act_kwargs: Optional[Dict[str, Any]] = None,
        bias: bool = True,
    ):
        super().__init__()

        self.in_channels = in_channels
        self.num_iters = num_iters
        self.gamma = gamma
        self.epsilon = epsilon
        self.act = activation_resolver(act, **(act_kwargs or {}))

        if phi is None:
            phi = GCNConv(in_channels, in_channels, bias=False)

        self.W = Parameter(torch.empty(in_channels, in_channels))
        self.register_buffer("eye", torch.eye(in_channels))
        self.phi = phi

        if bias:
            self.bias = Parameter(torch.empty(in_channels))
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets all learnable parameters of the module."""
        torch.nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))
        self.phi.reset_parameters()
        zeros(self.bias)

    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:
        r"""Runs the forward pass of the module."""
        antisymmetric_W = self.W - self.W.t() - self.gamma * self.eye

        for _ in range(self.num_iters):
            h = self.phi(x, edge_index, *args, **kwargs)
            h = x @ antisymmetric_W.t() + h

            if self.bias is not None:
                h += self.bias

            if self.act is not None:
                h = self.act(h)

            x = x + self.epsilon * h

        return x

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}("
            f"{self.in_channels}, "
            f"phi={self.phi}, "
            f"num_iters={self.num_iters}, "
            f"epsilon={self.epsilon}, "
            f"gamma={self.gamma})"
        )


if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = AntiSymmetricConv(in_channels=node_features.size(-1))
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/model.py
================================================
import pickle
import site
import traceback
from pathlib import Path
from typing import Dict, Optional

from rdagent.components.coder.CoSTEER.task import CoSTEERTask
from rdagent.core.experiment import Experiment, FBWorkspace
from rdagent.core.utils import cache_with_pickle
from rdagent.oai.llm_utils import md5_hash
from rdagent.utils.env import KGDockerEnv, QTDockerEnv


class ModelTask(CoSTEERTask):
    def __init__(
        self,
        name: str,
        description: str,
        architecture: str,
        *args,
        hyperparameters: Dict[str, str],
        formulation: str = None,
        variables: Dict[str, str] = None,
        model_type: Optional[str] = None,
        **kwargs,
    ) -> None:
        self.formulation: str = formulation
        self.architecture: str = architecture
        self.variables: str = variables
        self.hyperparameters: str = hyperparameters
        self.model_type: str = (
            model_type  # Tabular for tabular model, TimesSeries for time series model, Graph for graph model, XGBoost for XGBoost model
        )
        super().__init__(name=name, description=description, *args, **kwargs)

    def get_task_information(self):
        task_desc = f"""name: {self.name}
description: {self.description}
"""
        task_desc += f"formulation: {self.formulation}\n" if self.formulation else ""
        task_desc += f"architecture: {self.architecture}\n"
        task_desc += f"variables: {self.variables}\n" if self.variables else ""
        task_desc += f"hyperparameters: {self.hyperparameters}\n"
        task_desc += f"model_type: {self.model_type}\n"
        return task_desc

    @staticmethod
    def from_dict(dict):
        return ModelTask(**dict)

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} {self.name}>"


class ModelFBWorkspace(FBWorkspace):
    """
    It is a Pytorch model implementation task;
    All the things are placed in a folder.

    Folder
    - data source and documents prepared by `prepare`
        - Please note that new data may be passed in dynamically in `execute`
    - code (file `model.py` ) injected by `inject_code`
        - the `model.py` that contains a variable named `model_cls` which indicates the implemented model structure
            - `model_cls` is a instance of `torch.nn.Module`;

    We support two ways of interface:
        (version 1) for qlib we'll make a script to import the model in the implementation in file `model.py` after setting the cwd into the directory
            - from model import model_cls
            - initialize the model by initializing it `model_cls(input_dim=INPUT_DIM)`
            - And then verify the model.

        (version 2) for kaggle we'll make a script to call the fit and predict function in the implementation in file `model.py` after setting the cwd into the directory
    """

    def hash_func(
        self,
        batch_size: int = 8,
        num_features: int = 10,
        num_timesteps: int = 4,
        num_edges: int = 20,
        input_value: float = 1.0,
        param_init_value: float = 1.0,
    ) -> str:
        target_file_name = f"{batch_size}_{num_features}_{num_timesteps}_{input_value}_{param_init_value}"
        for code_file_name in sorted(list(self.file_dict.keys())):
            target_file_name = f"{target_file_name}_{self.file_dict[code_file_name]}"
        return md5_hash(target_file_name)

    @cache_with_pickle(hash_func)
    def execute(
        self,
        batch_size: int = 8,
        num_features: int = 10,
        num_timesteps: int = 4,
        num_edges: int = 20,
        input_value: float = 1.0,
        param_init_value: float = 1.0,
    ):
        self.before_execute()
        try:
            qtde = QTDockerEnv() if self.target_task.version == 1 else KGDockerEnv()
            qtde.prepare()

            if self.target_task.version == 1:
                dump_code = f"""
MODEL_TYPE = "{self.target_task.model_type}"
BATCH_SIZE = {batch_size}
NUM_FEATURES = {num_features}
NUM_TIMESTEPS = {num_timesteps}
NUM_EDGES = {num_edges}
INPUT_VALUE = {input_value}
PARAM_INIT_VALUE = {param_init_value}
{(Path(__file__).parent / 'model_execute_template_v1.txt').read_text()}
"""
            elif self.target_task.version == 2:
                dump_code = (Path(__file__).parent / "model_execute_template_v2.txt").read_text()

            log, results = qtde.dump_python_code_run_and_get_results(
                code=dump_code,
                dump_file_names=["execution_feedback_str.pkl", "execution_model_output.pkl"],
                local_path=str(self.workspace_path),
                env={},
                code_dump_file_py_name="model_test",
            )
            if len(results) == 0:
                raise RuntimeError(f"Error in running the model code: {log}")
            [execution_feedback_str, execution_model_output] = results

        except Exception as e:
            execution_feedback_str = f"Execution error: {e}\nTraceback: {traceback.format_exc()}"
            execution_model_output = None

        if len(execution_feedback_str) > 2000:
            execution_feedback_str = (
                execution_feedback_str[:1000] + "....hidden long error message...." + execution_feedback_str[-1000:]
            )
        return execution_feedback_str, execution_model_output


ModelExperiment = Experiment



================================================
File: rdagent/components/coder/model_coder/model_execute_template_v1.txt
================================================
# MODEL_TYPE = "Tabular"
# BATCH_SIZE = 32
# NUM_FEATURES = 10
# NUM_TIMESTEPS = 4
# NUM_EDGES = 20
# INPUT_VALUE = 1.0
# PARAM_INIT_VALUE = 1.0

import pickle

import torch
from model import model_cls

if MODEL_TYPE == "Tabular":
    input_shape = (BATCH_SIZE, NUM_FEATURES)
    m = model_cls(num_features=input_shape[1])
    data = torch.full(input_shape, INPUT_VALUE)
elif MODEL_TYPE == "TimeSeries":
    input_shape = (BATCH_SIZE, NUM_FEATURES, NUM_TIMESTEPS)
    m = model_cls(num_features=input_shape[1], num_timesteps=input_shape[2])
    data = torch.full(input_shape, INPUT_VALUE)
elif MODEL_TYPE == "Graph":
    node_feature = torch.randn(BATCH_SIZE, NUM_FEATURES)
    edge_index = torch.randint(0, BATCH_SIZE, (2, NUM_EDGES))
    m = model_cls(num_features=NUM_FEATURES)
    data = (node_feature, edge_index)
else:
    raise ValueError(f"Unsupported model type: {MODEL_TYPE}")

# Initialize all parameters of `m` to `param_init_value`
for _, param in m.named_parameters():
    param.data.fill_(PARAM_INIT_VALUE)

# Execute the model
if MODEL_TYPE == "Graph":
    out = m(*data)
else:
    out = m(data)

execution_model_output = out.cpu().detach().numpy()
execution_feedback_str = f"Execution successful, output tensor shape: {execution_model_output.shape}"

pickle.dump(execution_model_output, open("execution_model_output.pkl", "wb"))
pickle.dump(execution_feedback_str, open("execution_feedback_str.pkl", "wb"))



================================================
File: rdagent/components/coder/model_coder/model_execute_template_v2.txt
================================================
import os
import pickle

import numpy as np
import pandas as pd
import torch
from model import fit, predict

train_X = pd.DataFrame(np.random.randn(8, 30), columns=[f"{i}" for i in range(30)])
train_y = pd.Series(np.random.randint(0, 2, 8))
valid_X = pd.DataFrame(np.random.randn(8, 30), columns=[f"{i}" for i in range(30)])
valid_y = pd.Series(np.random.randint(0, 2, 8))

model = fit(train_X, train_y, valid_X, valid_y)
execution_model_output = predict(model, valid_X)

if isinstance(execution_model_output, torch.Tensor):
    execution_model_output = execution_model_output.cpu().detach().numpy()


execution_feedback_str = f"Execution successful, output numpy ndarray shape: {execution_model_output.shape}"

pickle.dump(execution_model_output, open("execution_model_output.pkl", "wb"))
pickle.dump(execution_feedback_str, open("execution_feedback_str.pkl", "wb"))



================================================
File: rdagent/components/coder/model_coder/prompts.yaml
================================================
extract_model_formulation_system: |-
    offer description of the proposed model in this paper, write a latex formula with variable as well as the architecture of the model. the format should be like 
    {
    "model_name (The name of the model)": {
        "description": "A detailed description of the model",
        "formulation": "A LaTeX formula representing the model's formulation",
        "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "variable_name_2": "Description of variable 2",
            "variable_name_3": "Description of variable 3"
        },
        "hyperparameters": {
            "hyperparameter_name_1": "value of hyperparameter 1",
            "hyperparameter_name_2": "value of hyperparameter 2",
            "hyperparameter_name_3": "value of hyperparameter 3"
        },
        "model_type": "Tabular or TimeSeries or Graph or XGBoost"  # Should be one of "Tabular", "TimeSeries", "Graph", or "XGBoost"
    }
    }
    Eg. 
    {
    "ABC Model": {
        "description": "A detailed description of the model",
        "formulation": "A LaTeX formula representing the model's formulation",
        "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "variable_name_2": "Description of variable 2",
            "variable_name_3": "Description of variable 3"
        },
        "hyperparameters": {
            "hyperparameter_name_1": "value of hyperparameter 1",
            "hyperparameter_name_2": "value of hyperparameter 2",
            "hyperparameter_name_3": "value of hyperparameter 3"
        },
        "model_type": "Tabular or TimeSeries or Graph or RandomForest or XGBoost"  # If torch & Neural network models are required, the choice should be one of "Tabular", "TimeSeries", or "Graph" 
    }
    }
    such format content should be begin with ```json and end with ``` and the content should be in json format.

evolving_strategy_model_coder:
    system: |-
        User is trying to implement some pytorch models in the following scenario:
        {{ scenario }}
        Your code is expected to align the scenario in any form which means The user needs to get the prediction of the model based on the input data.

        To help you write the correct code, the user might provide multiple information that helps you write the correct code:
        1. The user might provide you the correct code to similar models. Your should learn from these code to write the correct code.
        2. The user might provide you the failed former code and the corresponding feedback to the code. The feedback contains to the execution, the code and the model output value. You should analyze the feedback and try to correct the latest code.
        3. The user might provide you the suggestion to the latest fail code and some similar fail to correct pairs. Each pair contains the fail code with similar error and the corresponding corrected version code. You should learn from these suggestion to write the correct code.

        Your must write your code based on your former latest attempt below which consists of your former code and code feedback, you should read the former attempt carefully and must not modify the right part of your former code.

        {% if current_code is not none %}
        User has write some code before. You should write the new code based on this code. Here is the latest code:
        ```python
        {{ current_code }}
        ```
        Your code should be very similar to the former code which means your code should be ninety more percent same as the former code! You should not modify the right part of the code.
        {% else %}
        User has not write any code before. You should write the new code from scratch.
        {% endif %}

        {% if queried_former_failed_knowledge|length != 0 %}
        --------------Your former latest attempt:---------------
        =====Code to the former implementation=====
        {{ queried_former_failed_knowledge[-1].implementation.all_codes }}
        =====Feedback to the former implementation=====
        {{ queried_former_failed_knowledge[-1].feedback }}
        {% endif %}
        
        Please response the code in the following json format. Here is an example structure for the JSON output:
        {
            "code": "The Python code as a string."
        }

    user: |-
        --------------Target model information:---------------
        {{ model_information_str }}

        {% if queried_similar_successful_knowledge|length != 0 %}
        --------------Correct code to similar models:---------------
        {% for similar_successful_knowledge in queried_similar_successful_knowledge %}
        =====Model {{loop.index}}:=====
        {{ similar_successful_knowledge.target_task.get_task_information() }}
        =====Code:=====
        {{ similar_successful_knowledge.implementation.all_codes }}
        {% endfor %}
        {% endif %}

        {% if queried_former_failed_knowledge|length != 0 %}
        --------------Former failed code:---------------
        {% for former_failed_knowledge in queried_former_failed_knowledge %}
        =====Code to implementation {{ loop.index }}=====
        {{ former_failed_knowledge.implementation.all_codes }}
        =====Feedback to implementation {{ loop.index }}=====
        {{ former_failed_knowledge.feedback }}
        {% endfor %}
        {% endif %}

evaluator_code_feedback:
    system: |-
        User is trying to implement some models in the following scenario:
        {{ scenario }}
        User will provide you the information of the model.

        Your job is to check whether user's code is align with the model information and the scenario.
        The user will provide the source python code and the execution error message if execution failed.
        The user might provide you the ground truth code for you to provide the critic. You should not leak the ground truth code to the user in any form but you can use it to provide the critic.

        User has also compared the output generated by the user's code and the ground truth code. The user will provide you some analysis results comparing two output. You may find some error in the code which caused the difference between the two output.

        If the ground truth code is provided, your critic should only consider checking whether the user's code is align with the ground truth code since the ground truth is definitely correct.
        If the ground truth code is not provided, your critic should consider checking whether the user's code is reasonable and correct to the description and to the scenario.

        Notice that your critics are not for user to debug the code. They are sent to the coding agent to correct the code. So don't give any following items for the user to check like "Please check the code line XXX".

        You suggestion should not include any code, just some clear and short suggestions. Please point out very critical issues in your response, ignore non-important issues to avoid confusion. If no big issue found in the code, you can response "No critics found".

        You should provide the suggestion to each of your critic to help the user improve the code. Please response the critic in the following format. Here is an example structure for the output:
        critic 1: The critic message to critic 1
        critic 2: The critic message to critic 2
    
    user: |-
        --------------Model information:---------------
        {{ model_information }}
        --------------Python code:---------------
        {{ code }}
        --------------Execution feedback:---------------
        {{ model_execution_feedback }}
        {% if model_value_feedback is not none %}
        --------------Model value feedback:---------------
        {{ model_value_feedback }}
        {% endif %}
        {% if gt_code is not none %}
        --------------Ground truth Python code:---------------
        {{ gt_code }}
        {% endif %}


evaluator_final_feedback:
    system: |-
        User is trying to implement a model in the following scenario:
        {{ scenario }}
        User has finished evaluation and got some feedback from the evaluator.
        The evaluator run the code and get the output and provide several feedback regarding user's code and code output. You should analyze the feedback and considering the scenario and model description to give a final decision about the evaluation result. The final decision concludes whether the model is implemented correctly and if not, detail feedback containing reason and suggestion if the final decision is False.

        The implementation final decision is considered in the following logic:
        1. If the value and the ground truth value are exactly the same under a small tolerance, the implementation is considered correct.
        2. If no ground truth value is not provided, the implementation is considered correct if the code execution is successful and the code feedback is align with the scenario and model description.

        Please response the critic in the json format. Here is an example structure for the JSON output, please strictly follow the format:
        {
            "final_decision": True,
            "final_feedback": "The final feedback message",
        }
    user: |-
        --------------Model information:---------------
        {{ model_information }}
        --------------Model Execution feedback:---------------
        {{ model_execution_feedback }}
        --------------Model shape feedback:---------------
        {{ model_shape_feedback }}
        --------------Model Code feedback:---------------
        {{ model_code_feedback }}
        --------------Model value feedback:---------------
        {{ model_value_feedback }}


================================================
File: rdagent/components/coder/model_coder/task_loader.py
================================================
from __future__ import annotations

import json
import re
from pathlib import Path

from rdagent.components.coder.model_coder.model import ModelExperiment, ModelTask
from rdagent.components.document_reader.document_reader import (
    load_and_process_pdfs_by_langchain,
)
from rdagent.components.loader.task_loader import ModelTaskLoader
from rdagent.core.prompts import Prompts
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelExperiment

document_process_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


def extract_model_from_doc(doc_content: str) -> dict:
    """
    Extract model information from document content.

    Parameters
    ----------
    doc_content : str
        Document content.

    Returns
    -------
    dict
        {model_name: dict{description, formulation, variables}}
    """
    session = APIBackend().build_chat_session(
        session_system_prompt=document_process_prompts["extract_model_formulation_system"],
    )
    current_user_prompt = doc_content

    # Extract model information from document content.
    model_dict = {}

    for _ in range(10):
        # try to extract model information from the document content, retry at most 10 times.
        extract_result_resp = session.build_chat_completion(
            user_prompt=current_user_prompt,
            json_mode=False,
        )
        re_search_res = re.search(r"```json(.*)```", extract_result_resp, re.S)
        ret_json_str = re_search_res.group(1) if re_search_res is not None else ""
        try:
            ret_dict = json.loads(ret_json_str)
            parse_success = bool(isinstance(ret_dict, dict))
        except json.JSONDecodeError:
            parse_success = False
        if ret_json_str is None or not parse_success:
            current_user_prompt = "Your response didn't follow the instruction might be wrong json format. Try again."
        else:
            for name, formulation_and_description in ret_dict.items():
                if name not in model_dict:
                    model_dict[name] = formulation_and_description
            if len(model_dict) == 0:
                current_user_prompt = "No model extracted. Please try again."
            else:
                break

    logger.info(f"已经完成{len(model_dict)}个模型的提取")

    return model_dict


def merge_file_to_model_dict_to_model_dict(
    file_to_model_dict: dict[str, dict],
) -> dict:
    model_dict = {}
    for file_name in file_to_model_dict:
        for model_name in file_to_model_dict[file_name]:
            model_dict.setdefault(model_name, [])
            model_dict[model_name].append(file_to_model_dict[file_name][model_name])

    model_dict_simple_deduplication = {}
    for model_name in model_dict:
        if len(model_dict[model_name]) > 1:
            model_dict_simple_deduplication[model_name] = max(
                model_dict[model_name],
                key=lambda x: len(x["formulation"]),
            )
        else:
            model_dict_simple_deduplication[model_name] = model_dict[model_name][0]
    return model_dict_simple_deduplication


def extract_model_from_docs(docs_dict):
    model_dict = {}
    for doc_name, doc_content in docs_dict.items():
        model_dict[doc_name] = extract_model_from_doc(doc_content)
    return model_dict


class ModelExperimentLoaderFromDict(ModelTaskLoader):
    def load(self, model_dict: dict) -> list:
        """Load data from a dict."""
        task_l = []
        for model_name, model_data in model_dict.items():
            task = ModelTask(
                name=model_name,
                description=model_data["description"],
                formulation=model_data["formulation"],
                architecture=model_data["architecture"],
                variables=model_data["variables"],
                hyperparameters=model_data["hyperparameters"],
                model_type=model_data["model_type"],
            )
            task_l.append(task)
        return QlibModelExperiment(sub_tasks=task_l)


class ModelExperimentLoaderFromPDFfiles(ModelTaskLoader):
    def load(self, file_or_folder_path: str) -> dict:
        docs_dict = load_and_process_pdfs_by_langchain(file_or_folder_path)  # dict{file_path:content}
        model_dict = extract_model_from_docs(
            docs_dict
        )  # dict{file_name: dict{model_name: dict{description, formulation, variables}}}
        model_dict = merge_file_to_model_dict_to_model_dict(
            model_dict
        )  # dict {model_name: dict{description, formulation, variables}}
        return ModelExperimentLoaderFromDict().load(model_dict)



================================================
File: rdagent/components/coder/model_coder/benchmark/eval.py
================================================
# TODO: inherent from the benchmark base class
import torch

from rdagent.components.coder.model_coder.model import ModelFBWorkspace


def get_data_conf(init_val):
    # TODO: design this step in the workflow
    in_dim = 1000
    in_channels = 128
    exec_config = {"model_eval_param_init": init_val}
    node_feature = torch.randn(in_dim, in_channels)
    edge_index = torch.randint(0, in_dim, (2, 2000))
    return (node_feature, edge_index), exec_config


class ModelImpValEval:
    """
    Evaluate the similarity of the model structure by changing the input and observe the output.

    Assumption:
    - If the model structure is similar, the output will change in similar way when we change the input.

    Challenge:
    - The key difference between it and implementing models is that we have parameters in the layers (Model operators often have no parameters or are given parameters).
    - we try to initialize the model param in similar value. So only the model structure is different.

    Comparing the correlation of following sequences
    - modelA[init1](input1).hidden_out1, modelA[init1](input2).hidden_out1, ...
    - modelB[init1](input1).hidden_out1, modelB[init1](input2).hidden_out1, ...

    For each hidden output, we can calculate a correlation. The average correlation will be the metrics.
    """

    def evaluate(self, gt: ModelFBWorkspace, gen: ModelFBWorkspace):
        round_n = 10

        eval_pairs: list[tuple] = []

        # run different input value
        for _ in range(round_n):
            # run different model initial parameters.
            for init_val in [-0.2, -0.1, 0.1, 0.2]:
                _, gt_res = gt.execute(input_value=init_val, param_init_value=init_val)
                _, res = gen.execute(input_value=init_val, param_init_value=init_val)
                eval_pairs.append((res, gt_res))

        # flat and concat the output
        res_batch, gt_res_batch = [], []
        for res, gt_res in eval_pairs:
            res_batch.append(res.reshape(-1))
            gt_res_batch.append(gt_res.reshape(-1))
        res_batch = torch.stack(res_batch)
        gt_res_batch = torch.stack(gt_res_batch)

        res_batch = res_batch.detach().numpy()
        gt_res_batch = gt_res_batch.detach().numpy()

        # pearson correlation of each hidden output
        def norm(x):
            return (x - x.mean(axis=0)) / x.std(axis=0)

        dim_corr = (norm(res_batch) * norm(gt_res_batch)).mean(axis=0)  # the correlation of each hidden output

        # aggregate all the correlation
        avr_corr = dim_corr.mean()
        # FIXME:
        # It is too high(e.g. 0.944) .
        # Check if it is not a good evaluation!!
        # Maybe all the same initial params will results in extreamly high correlation without regard to the model structure.
        return avr_corr



================================================
File: rdagent/components/coder/model_coder/benchmark/model_dict.json
================================================
{
    "PMLP": {
        "description": "`PMLP` is identical to a standard MLP during training, but then adopts a GNN architecture (add message passing) during testing.",
        "formulation": "\\hat{y}_u = \\psi(\\text{MP}(\\{h^{(l-1)}_v\\}_{v \\in N_u \\cup \\{u\\}}))",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "\\psi": "A function representing the feed-forward process, consisting of a linear feature transformation followed by a non-linear activation",
            "\\text{MP}": "Message Passing operation that aggregates neighbored information",
            "h^{(l-1)}_v": "The feature representation of node v at layer (l-1)",
            "N_u": "The set of neighbored nodes centered at node u"
        },
        "key": "pmlp",
        "model_type": "TimeSeries"
    },
    "LINKX": {
        "description": "A scalable model for node classification that separately embeds adjacency and node features, combines them with MLPs, and applies simple transformations.",
        "formulation": "Y = MLP_f(\\sigma(W[h_A; h_X] + h_A + h_X))",
        "variables": {
            "Y": "The output predictions",
            "\\sigma": "Non-linear activation function",
            "W": "Learned weight matrix",
            "h_A": "Embedding of the adjacency matrix",
            "h_X": "Embedding of the node features",
            "MLP_f": "Final multilayer perceptron for prediction"
        },
        "key": "linkx",
        "model_type": "TimeSeries"
    },
    "GPSConv": {
        "description": "A scalable and powerful graph transformer with linear complexity, capable of handling large graphs with state-of-the-art results across diverse benchmarks.",
        "formulation": "X^{(l+1)} = \\text{MPNN}^{(l)}(X^{(l)}, A) + \\text{GlobalAttn}^{(l)}(X^{(l)})",
        "variables": {
            "X^{(l)}": "The node features at layer l",
            "A": "The adjacency matrix of the graph",
            "X^{(l+1)}": "The updated node features at layer l+1",
            "MPNN^{(l)}": "The message-passing neural network function at layer l",
            "GlobalAttn^{(l)}": "The global attention function at layer l"
        },
        "key": "gpsconv",
        "model_type": "TimeSeries"
    },
    "ViSNet": {
        "description": "ViSNet is an equivariant geometry-enhanced graph neural network designed for efficient molecular modeling[^1^][1][^2^][2]. It utilizes a Vector-Scalar interactive message passing mechanism to extract and utilize geometric features with low computational costs, achieving state-of-the-art performance on multiple molecular dynamics benchmarks.",
        "formulation": "\\text{ViSNet}(G) = \\sum_{u \\in G} f(\\mathbf{h}_u, \\mathbf{e}_u, \\mathbf{v}_u)",
        "variables": {
            "\\mathbf{h}_u": "Node embedding for atom u",
            "\\mathbf{e}_u": "Edge embedding associated with atom u",
            "\\mathbf{v}_u": "Direction unit vector for atom u"
        },
        "key": "visnet",
        "model_type": "TimeSeries"
    },
    "Dir-GNN": {
        "description": "A framework for deep learning on directed graphs that extends MPNNs to incorporate edge directionality.",
        "formulation": "x^{(k)}_i = COM^{(k)}\\left(x^{(k-1)}_i, m^{(k)}_{i,\\leftarrow}, m^{(k)}_{i,\\rightarrow}\\right)",
        "variables": {
            "x^{(k)}_i": "The feature representation of node i at layer k",
            "m^{(k)}_{i,\\leftarrow}": "The aggregated incoming messages to node i at layer k",
            "m^{(k)}_{i,\\rightarrow}": "The aggregated outgoing messages from node i at layer k"
        },
        "key": "dirgnn",
        "model_type": "TimeSeries"
    },
    "A-DGN": {
        "description": "A framework for stable and non-dissipative DGN design, conceived through the lens of ordinary differential equations (ODEs). It ensures long-range information preservation between nodes and prevents gradient vanishing or explosion during training.",
        "formulation": "\\frac{\\partial x_u(t)}{\\partial t} = \\sigma(W^T x_u(t) + \\Phi(X(t), N_u) + b)",
        "variables": {
            "x_u(t)": "The state of node u at time t",
            "\\frac{\\partial x_u(t)}{\\partial t}": "The rate of change of the state of node u at time t",
            "\\sigma": "A monotonically non-decreasing activation function",
            "W": "A weight matrix",
            "b": "A bias vector",
            "\\Phi(X(t), N_u)": "The aggregation function for the states of the nodes in the neighborhood of u",
            "X(t)": "The node feature matrix of the whole graph at time t",
            "N_u": "The set of neighboring nodes of u"
        },
        "key": "A-DGN",
        "model_type": "TimeSeries"
    }
}


================================================
File: rdagent/components/coder/model_coder/benchmark/gt_code/A-DGN.py
================================================
import math
from typing import Any, Callable, Dict, Optional, Union

import torch
from torch import Tensor
from torch.nn import Parameter
from torch_geometric.nn.conv import GCNConv, MessagePassing
from torch_geometric.nn.inits import zeros
from torch_geometric.nn.resolver import activation_resolver
from torch_geometric.typing import Adj


class AntiSymmetricConv(torch.nn.Module):
    r"""The anti-symmetric graph convolutional operator from the
    `"Anti-Symmetric DGN: a stable architecture for Deep Graph Networks"
    <https://openreview.net/forum?id=J3Y7cgZOOS>`_ paper.

    .. math::
        \mathbf{x}^{\prime}_i = \mathbf{x}_i + \epsilon \cdot \sigma \left(
            (\mathbf{W}-\mathbf{W}^T-\gamma \mathbf{I}) \mathbf{x}_i +
            \Phi(\mathbf{X}, \mathcal{N}_i) + \mathbf{b}\right),

    where :math:`\Phi(\mathbf{X}, \mathcal{N}_i)` denotes a
    :class:`~torch.nn.conv.MessagePassing` layer.

    Args:
        in_channels (int): Size of each input sample.
        phi (MessagePassing, optional): The message passing module
            :math:`\Phi`. If set to :obj:`None`, will use a
            :class:`~torch_geometric.nn.conv.GCNConv` layer as default.
            (default: :obj:`None`)
        num_iters (int, optional): The number of times the anti-symmetric deep
            graph network operator is called. (default: :obj:`1`)
        epsilon (float, optional): The discretization step size
            :math:`\epsilon`. (default: :obj:`0.1`)
        gamma (float, optional): The strength of the diffusion :math:`\gamma`.
            It regulates the stability of the method. (default: :obj:`0.1`)
        act (str, optional): The non-linear activation function :math:`\sigma`,
            *e.g.*, :obj:`"tanh"` or :obj:`"relu"`. (default: :class:`"tanh"`)
        act_kwargs (Dict[str, Any], optional): Arguments passed to the
            respective activation function defined by :obj:`act`.
            (default: :obj:`None`)
        bias (bool, optional): If set to :obj:`False`, the layer will not learn
            an additive bias. (default: :obj:`True`)

    Shapes:
        - **input:**
          node features :math:`(|\mathcal{V}|, F_{in})`,
          edge indices :math:`(2, |\mathcal{E}|)`,
          edge weights :math:`(|\mathcal{E}|)` *(optional)*
        - **output:** node features :math:`(|\mathcal{V}|, F_{in})`
    """

    def __init__(
        self,
        in_channels: int,
        phi: Optional[MessagePassing] = None,
        num_iters: int = 1,
        epsilon: float = 0.1,
        gamma: float = 0.1,
        act: Union[str, Callable, None] = "tanh",
        act_kwargs: Optional[Dict[str, Any]] = None,
        bias: bool = True,
    ):
        super().__init__()

        self.in_channels = in_channels
        self.num_iters = num_iters
        self.gamma = gamma
        self.epsilon = epsilon
        self.act = activation_resolver(act, **(act_kwargs or {}))

        if phi is None:
            phi = GCNConv(in_channels, in_channels, bias=False)

        self.W = Parameter(torch.empty(in_channels, in_channels))
        self.register_buffer("eye", torch.eye(in_channels))
        self.phi = phi

        if bias:
            self.bias = Parameter(torch.empty(in_channels))
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets all learnable parameters of the module."""
        torch.nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))
        self.phi.reset_parameters()
        zeros(self.bias)

    def forward(self, x: Tensor, edge_index: Adj, *args, **kwargs) -> Tensor:
        r"""Runs the forward pass of the module."""
        antisymmetric_W = self.W - self.W.t() - self.gamma * self.eye

        for _ in range(self.num_iters):
            h = self.phi(x, edge_index, *args, **kwargs)
            h = x @ antisymmetric_W.t() + h

            if self.bias is not None:
                h += self.bias

            if self.act is not None:
                h = self.act(h)

            x = x + self.epsilon * h

        return x

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}("
            f"{self.in_channels}, "
            f"phi={self.phi}, "
            f"num_iters={self.num_iters}, "
            f"epsilon={self.epsilon}, "
            f"gamma={self.gamma})"
        )


model_cls = AntiSymmetricConv


if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = AntiSymmetricConv(in_channels=node_features.size(-1))
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/benchmark/gt_code/dirgnn.py
================================================
import copy

import torch
from torch import Tensor
from torch_geometric.nn.conv import MessagePassing


class DirGNNConv(torch.nn.Module):
    r"""A generic wrapper for computing graph convolution on directed
    graphs as described in the `"Edge Directionality Improves Learning on
    Heterophilic Graphs" <https://arxiv.org/abs/2305.10498>`_ paper.
    :class:`DirGNNConv` will pass messages both from source nodes to target
    nodes and from target nodes to source nodes.

    Args:
        conv (MessagePassing): The underlying
            :class:`~torch_geometric.nn.conv.MessagePassing` layer to use.
        alpha (float, optional): The alpha coefficient used to weight the
            aggregations of in- and out-edges as part of a convex combination.
            (default: :obj:`0.5`)
        root_weight (bool, optional): If set to :obj:`True`, the layer will add
            transformed root node features to the output.
            (default: :obj:`True`)
    """

    def __init__(
        self,
        conv: MessagePassing,
        alpha: float = 0.5,
        root_weight: bool = True,
    ):
        super().__init__()

        self.alpha = alpha
        self.root_weight = root_weight

        self.conv_in = copy.deepcopy(conv)
        self.conv_out = copy.deepcopy(conv)

        if hasattr(conv, "add_self_loops"):
            self.conv_in.add_self_loops = False
            self.conv_out.add_self_loops = False
        if hasattr(conv, "root_weight"):
            self.conv_in.root_weight = False
            self.conv_out.root_weight = False

        if root_weight:
            self.lin = torch.nn.Linear(conv.in_channels, conv.out_channels)
        else:
            self.lin = None

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets all learnable parameters of the module."""
        self.conv_in.reset_parameters()
        self.conv_out.reset_parameters()
        if self.lin is not None:
            self.lin.reset_parameters()

    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:
        """"""  # noqa: D419
        x_in = self.conv_in(x, edge_index)
        x_out = self.conv_out(x, edge_index.flip([0]))

        out = self.alpha * x_out + (1 - self.alpha) * x_in

        if self.root_weight:
            out = out + self.lin(x)

        return out

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}({self.conv_in}, alpha={self.alpha})"


model_cls = DirGNNConv


if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = DirGNNConv(MessagePassing())
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/benchmark/gt_code/gpsconv.py
================================================
import inspect
from typing import Any, Dict, Optional

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn import Dropout, Linear, Sequential
from torch_geometric.nn.attention import PerformerAttention
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.nn.inits import reset
from torch_geometric.nn.resolver import activation_resolver, normalization_resolver
from torch_geometric.typing import Adj
from torch_geometric.utils import to_dense_batch


class GPSConv(torch.nn.Module):
    r"""The general, powerful, scalable (GPS) graph transformer layer from the
    `"Recipe for a General, Powerful, Scalable Graph Transformer"
    <https://arxiv.org/abs/2205.12454>`_ paper.

    The GPS layer is based on a 3-part recipe:

    1. Inclusion of positional (PE) and structural encodings (SE) to the input
       features (done in a pre-processing step via
       :class:`torch_geometric.transforms`).
    2. A local message passing layer (MPNN) that operates on the input graph.
    3. A global attention layer that operates on the entire graph.

    .. note::

        For an example of using :class:`GPSConv`, see
        `examples/graph_gps.py
        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/
        graph_gps.py>`_.

    Args:
        channels (int): Size of each input sample.
        conv (MessagePassing, optional): The local message passing layer.
        heads (int, optional): Number of multi-head-attentions.
            (default: :obj:`1`)
        dropout (float, optional): Dropout probability of intermediate
            embeddings. (default: :obj:`0.`)
        act (str or Callable, optional): The non-linear activation function to
            use. (default: :obj:`"relu"`)
        act_kwargs (Dict[str, Any], optional): Arguments passed to the
            respective activation function defined by :obj:`act`.
            (default: :obj:`None`)
        norm (str or Callable, optional): The normalization function to
            use. (default: :obj:`"batch_norm"`)
        norm_kwargs (Dict[str, Any], optional): Arguments passed to the
            respective normalization function defined by :obj:`norm`.
            (default: :obj:`None`)
        attn_type (str): Global attention type, :obj:`multihead` or
            :obj:`performer`. (default: :obj:`multihead`)
        attn_kwargs (Dict[str, Any], optional): Arguments passed to the
            attention layer. (default: :obj:`None`)
    """

    def __init__(
        self,
        channels: int,
        conv: Optional[MessagePassing],
        heads: int = 1,
        dropout: float = 0.0,
        act: str = "relu",
        act_kwargs: Optional[Dict[str, Any]] = None,
        norm: Optional[str] = "batch_norm",
        norm_kwargs: Optional[Dict[str, Any]] = None,
        attn_type: str = "multihead",
        attn_kwargs: Optional[Dict[str, Any]] = None,
    ):
        super().__init__()

        self.channels = channels
        self.conv = conv
        self.heads = heads
        self.dropout = dropout
        self.attn_type = attn_type

        attn_kwargs = attn_kwargs or {}
        if attn_type == "multihead":
            self.attn = torch.nn.MultiheadAttention(
                channels,
                heads,
                batch_first=True,
                **attn_kwargs,
            )
        elif attn_type == "performer":
            self.attn = PerformerAttention(
                channels=channels,
                heads=heads,
                **attn_kwargs,
            )
        else:
            # TODO: Support BigBird
            raise ValueError(f"{attn_type} is not supported")

        self.mlp = Sequential(
            Linear(channels, channels * 2),
            activation_resolver(act, **(act_kwargs or {})),
            Dropout(dropout),
            Linear(channels * 2, channels),
            Dropout(dropout),
        )

        norm_kwargs = norm_kwargs or {}
        self.norm1 = normalization_resolver(norm, channels, **norm_kwargs)
        self.norm2 = normalization_resolver(norm, channels, **norm_kwargs)
        self.norm3 = normalization_resolver(norm, channels, **norm_kwargs)

        self.norm_with_batch = False
        if self.norm1 is not None:
            signature = inspect.signature(self.norm1.forward)
            self.norm_with_batch = "batch" in signature.parameters

    def reset_parameters(self):
        r"""Resets all learnable parameters of the module."""
        if self.conv is not None:
            self.conv.reset_parameters()
        self.attn._reset_parameters()
        reset(self.mlp)
        if self.norm1 is not None:
            self.norm1.reset_parameters()
        if self.norm2 is not None:
            self.norm2.reset_parameters()
        if self.norm3 is not None:
            self.norm3.reset_parameters()

    def forward(
        self,
        x: Tensor,
        edge_index: Adj,
        batch: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Tensor:
        r"""Runs the forward pass of the module."""
        hs = []
        if self.conv is not None:  # Local MPNN.
            h = self.conv(x, edge_index, **kwargs)
            h = F.dropout(h, p=self.dropout, training=self.training)
            h = h + x
            if self.norm1 is not None:
                if self.norm_with_batch:
                    h = self.norm1(h, batch=batch)
                else:
                    h = self.norm1(h)
            hs.append(h)

        # Global attention transformer-style model.
        h, mask = to_dense_batch(x, batch)

        if isinstance(self.attn, torch.nn.MultiheadAttention):
            h, _ = self.attn(h, h, h, key_padding_mask=~mask, need_weights=False)
        elif isinstance(self.attn, PerformerAttention):
            h = self.attn(h, mask=mask)

        h = h[mask]
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = h + x  # Residual connection.
        if self.norm2 is not None:
            if self.norm_with_batch:
                h = self.norm2(h, batch=batch)
            else:
                h = self.norm2(h)
        hs.append(h)

        out = sum(hs)  # Combine local and global outputs.

        out = out + self.mlp(out)
        if self.norm3 is not None:
            if self.norm_with_batch:
                out = self.norm3(out, batch=batch)
            else:
                out = self.norm3(out)

        return out

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}({self.channels}, "
            f"conv={self.conv}, heads={self.heads}, "
            f"attn_type={self.attn_type})"
        )


model_cls = GPSConv


if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = GPSConv(channels=node_features.size(-1), conv=MessagePassing())
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/benchmark/gt_code/linkx.py
================================================
import math

import torch
from torch import Tensor
from torch.nn import BatchNorm1d, Parameter
from torch_geometric.nn import inits
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.nn.models import MLP
from torch_geometric.typing import Adj, OptTensor
from torch_geometric.utils import spmm


class SparseLinear(MessagePassing):
    def __init__(self, in_channels: int, out_channels: int, bias: bool = True):
        super().__init__(aggr="add")
        self.in_channels = in_channels
        self.out_channels = out_channels

        self.weight = Parameter(torch.empty(in_channels, out_channels))
        if bias:
            self.bias = Parameter(torch.empty(out_channels))
        else:
            self.register_parameter("bias", None)

        self.reset_parameters()

    def reset_parameters(self):
        inits.kaiming_uniform(self.weight, fan=self.in_channels, a=math.sqrt(5))
        inits.uniform(self.in_channels, self.bias)

    def forward(
        self,
        edge_index: Adj,
        edge_weight: OptTensor = None,
    ) -> Tensor:
        # propagate_type: (weight: Tensor, edge_weight: OptTensor)
        out = self.propagate(edge_index, weight=self.weight, edge_weight=edge_weight)

        if self.bias is not None:
            out = out + self.bias

        return out

    def message(self, weight_j: Tensor, edge_weight: OptTensor) -> Tensor:
        if edge_weight is None:
            return weight_j
        else:
            return edge_weight.view(-1, 1) * weight_j

    def message_and_aggregate(self, adj_t: Adj, weight: Tensor) -> Tensor:
        return spmm(adj_t, weight, reduce=self.aggr)


class LINKX(torch.nn.Module):
    r"""The LINKX model from the `"Large Scale Learning on Non-Homophilous
    Graphs: New Benchmarks and Strong Simple Methods"
    <https://arxiv.org/abs/2110.14446>`_ paper.

    .. math::
        \mathbf{H}_{\mathbf{A}} &= \textrm{MLP}_{\mathbf{A}}(\mathbf{A})

        \mathbf{H}_{\mathbf{X}} &= \textrm{MLP}_{\mathbf{X}}(\mathbf{X})

        \mathbf{Y} &= \textrm{MLP}_{f} \left( \sigma \left( \mathbf{W}
        [\mathbf{H}_{\mathbf{A}}, \mathbf{H}_{\mathbf{X}}] +
        \mathbf{H}_{\mathbf{A}} + \mathbf{H}_{\mathbf{X}} \right) \right)

    .. note::

        For an example of using LINKX, see `examples/linkx.py <https://
        github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py>`_.

    Args:
        num_nodes (int): The number of nodes in the graph.
        in_channels (int): Size of each input sample, or :obj:`-1` to derive
            the size from the first input(s) to the forward method.
        hidden_channels (int): Size of each hidden sample.
        out_channels (int): Size of each output sample.
        num_layers (int): Number of layers of :math:`\textrm{MLP}_{f}`.
        num_edge_layers (int, optional): Number of layers of
            :math:`\textrm{MLP}_{\mathbf{A}}`. (default: :obj:`1`)
        num_node_layers (int, optional): Number of layers of
            :math:`\textrm{MLP}_{\mathbf{X}}`. (default: :obj:`1`)
        dropout (float, optional): Dropout probability of each hidden
            embedding. (default: :obj:`0.0`)
    """

    def __init__(
        self,
        num_nodes: int,
        in_channels: int,
        hidden_channels: int,
        out_channels: int,
        num_layers: int,
        num_edge_layers: int = 1,
        num_node_layers: int = 1,
        dropout: float = 0.0,
    ):
        super().__init__()

        self.num_nodes = num_nodes
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_edge_layers = num_edge_layers

        self.edge_lin = SparseLinear(num_nodes, hidden_channels)

        if self.num_edge_layers > 1:
            self.edge_norm = BatchNorm1d(hidden_channels)
            channels = [hidden_channels] * num_edge_layers
            self.edge_mlp = MLP(channels, dropout=0.0, act_first=True)
        else:
            self.edge_norm = None
            self.edge_mlp = None

        channels = [in_channels] + [hidden_channels] * num_node_layers
        self.node_mlp = MLP(channels, dropout=0.0, act_first=True)

        self.cat_lin1 = torch.nn.Linear(hidden_channels, hidden_channels)
        self.cat_lin2 = torch.nn.Linear(hidden_channels, hidden_channels)

        channels = [hidden_channels] * num_layers + [out_channels]
        self.final_mlp = MLP(channels, dropout=dropout, act_first=True)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets all learnable parameters of the module."""
        self.edge_lin.reset_parameters()
        if self.edge_norm is not None:
            self.edge_norm.reset_parameters()
        if self.edge_mlp is not None:
            self.edge_mlp.reset_parameters()
        self.node_mlp.reset_parameters()
        self.cat_lin1.reset_parameters()
        self.cat_lin2.reset_parameters()
        self.final_mlp.reset_parameters()

    def forward(
        self,
        x: OptTensor,
        edge_index: Adj,
        edge_weight: OptTensor = None,
    ) -> Tensor:
        """"""  # noqa: D419
        out = self.edge_lin(edge_index, edge_weight)

        if self.edge_norm is not None and self.edge_mlp is not None:
            out = out.relu_()
            out = self.edge_norm(out)
            out = self.edge_mlp(out)

        out = out + self.cat_lin1(out)

        if x is not None:
            x = self.node_mlp(x)
            out = out + x
            out = out + self.cat_lin2(x)

        return self.final_mlp(out.relu_())

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}(num_nodes={self.num_nodes}, "
            f"in_channels={self.in_channels}, "
            f"out_channels={self.out_channels})"
        )


model_cls = LINKX

if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = LINKX(
        num_nodes=node_features.size(0),
        in_channels=node_features.size(1),
        hidden_channels=node_features.size(1),
        out_channels=node_features.size(1),
        num_layers=1,
    )
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/benchmark/gt_code/pmlp.py
================================================
from typing import Optional

import torch
import torch.nn.functional as F
from torch import Tensor
from torch_geometric.nn import SimpleConv
from torch_geometric.nn.dense.linear import Linear


class PMLP(torch.nn.Module):
    r"""The P(ropagational)MLP model from the `"Graph Neural Networks are
    Inherently Good Generalizers: Insights by Bridging GNNs and MLPs"
    <https://arxiv.org/abs/2212.09034>`_ paper.
    :class:`PMLP` is identical to a standard MLP during training, but then
    adopts a GNN architecture during testing.

    Args:
        in_channels (int): Size of each input sample.
        hidden_channels (int): Size of each hidden sample.
        out_channels (int): Size of each output sample.
        num_layers (int): The number of layers.
        dropout (float, optional): Dropout probability of each hidden
            embedding. (default: :obj:`0.`)
        norm (bool, optional): If set to :obj:`False`, will not apply batch
            normalization. (default: :obj:`True`)
        bias (bool, optional): If set to :obj:`False`, the module
            will not learn additive biases. (default: :obj:`True`)
    """

    def __init__(
        self,
        in_channels: int,
        hidden_channels: int,
        out_channels: int,
        num_layers: int,
        dropout: float = 0.0,
        norm: bool = True,
        bias: bool = True,
    ):
        super().__init__()

        self.in_channels = in_channels
        self.hidden_channels = hidden_channels
        self.out_channels = out_channels
        self.num_layers = num_layers
        self.dropout = dropout
        self.bias = bias

        self.lins = torch.nn.ModuleList()
        self.lins.append(Linear(in_channels, hidden_channels, self.bias))
        for _ in range(self.num_layers - 2):
            lin = Linear(hidden_channels, hidden_channels, self.bias)
            self.lins.append(lin)
        self.lins.append(Linear(hidden_channels, out_channels, self.bias))

        self.norm = None
        if norm:
            self.norm = torch.nn.BatchNorm1d(
                hidden_channels,
                affine=False,
                track_running_stats=False,
            )

        self.conv = SimpleConv(aggr="mean", combine_root="self_loop")

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets all learnable parameters of the module."""
        for lin in self.lins:
            torch.nn.init.xavier_uniform_(lin.weight, gain=1.414)
            if self.bias:
                torch.nn.init.zeros_(lin.bias)

    def forward(
        self,
        x: torch.Tensor,
        edge_index: Optional[Tensor] = None,
    ) -> torch.Tensor:
        """"""  # noqa: D419
        if not self.training and edge_index is None:
            raise ValueError(f"'edge_index' needs to be present during " f"inference in '{self.__class__.__name__}'")

        for i in range(self.num_layers):
            x = x @ self.lins[i].weight.t()
            if not self.training:
                x = self.conv(x, edge_index)
            if self.bias:
                x = x + self.lins[i].bias
            if i != self.num_layers - 1:
                if self.norm is not None:
                    x = self.norm(x)
                x = x.relu()
                x = F.dropout(x, p=self.dropout, training=self.training)

        return x

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}({self.in_channels}, " f"{self.out_channels}, num_layers={self.num_layers})"


model_cls = PMLP

if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = PMLP(
        in_channels=node_features.size(-1),
        hidden_channels=node_features.size(-1),
        out_channels=node_features.size(-1),
        num_layers=1,
    )
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/benchmark/gt_code/visnet.py
================================================
import math
from typing import Optional, Tuple

import torch
from torch import Tensor
from torch.autograd import grad
from torch.nn import Embedding, LayerNorm, Linear, Parameter
from torch_geometric.nn import MessagePassing, radius_graph
from torch_geometric.utils import scatter


class CosineCutoff(torch.nn.Module):
    r"""Appies a cosine cutoff to the input distances.

    .. math::
        \text{cutoffs} =
        \begin{cases}
        0.5 * (\cos(\frac{\text{distances} * \pi}{\text{cutoff}}) + 1.0),
        & \text{if } \text{distances} < \text{cutoff} \\
        0, & \text{otherwise}
        \end{cases}

    Args:
        cutoff (float): A scalar that determines the point at which the cutoff
            is applied.
    """

    def __init__(self, cutoff: float) -> None:
        super().__init__()
        self.cutoff = cutoff

    def forward(self, distances: Tensor) -> Tensor:
        r"""Applies a cosine cutoff to the input distances.

        Args:
            distances (torch.Tensor): A tensor of distances.

        Returns:
            cutoffs (torch.Tensor): A tensor where the cosine function
                has been applied to the distances,
                but any values that exceed the cutoff are set to 0.
        """
        cutoffs = 0.5 * ((distances * math.pi / self.cutoff).cos() + 1.0)
        cutoffs = cutoffs * (distances < self.cutoff).float()
        return cutoffs


class ExpNormalSmearing(torch.nn.Module):
    r"""Applies exponential normal smearing to the input distances.

    .. math::
        \text{smeared\_dist} = \text{CosineCutoff}(\text{dist})
        * e^{-\beta * (e^{\alpha * (-\text{dist})} - \text{means})^2}

    Args:
        cutoff (float, optional): A scalar that determines the point at which
            the cutoff is applied. (default: :obj:`5.0`)
        num_rbf (int, optional): The number of radial basis functions.
            (default: :obj:`128`)
        trainable (bool, optional): If set to :obj:`False`, the means and betas
            of the RBFs will not be trained. (default: :obj:`True`)
    """

    def __init__(
        self,
        cutoff: float = 5.0,
        num_rbf: int = 128,
        trainable: bool = True,
    ) -> None:
        super().__init__()
        self.cutoff = cutoff
        self.num_rbf = num_rbf
        self.trainable = trainable

        self.cutoff_fn = CosineCutoff(cutoff)
        self.alpha = 5.0 / cutoff

        means, betas = self._initial_params()
        if trainable:
            self.register_parameter("means", Parameter(means))
            self.register_parameter("betas", Parameter(betas))
        else:
            self.register_buffer("means", means)
            self.register_buffer("betas", betas)

    def _initial_params(self) -> Tuple[Tensor, Tensor]:
        r"""Initializes the means and betas for the radial basis functions."""
        start_value = torch.exp(torch.tensor(-self.cutoff))
        means = torch.linspace(start_value, 1, self.num_rbf)
        betas = torch.tensor([(2 / self.num_rbf * (1 - start_value)) ** -2] * self.num_rbf)
        return means, betas

    def reset_parameters(self):
        r"""Resets the means and betas to their initial values."""
        means, betas = self._initial_params()
        self.means.data.copy_(means)
        self.betas.data.copy_(betas)

    def forward(self, dist: Tensor) -> Tensor:
        r"""Applies the exponential normal smearing to the input distance.

        Args:
            dist (torch.Tensor): A tensor of distances.
        """
        dist = dist.unsqueeze(-1)
        smeared_dist = self.cutoff_fn(dist) * (-self.betas * ((self.alpha * (-dist)).exp() - self.means) ** 2).exp()
        return smeared_dist


class Sphere(torch.nn.Module):
    r"""Computes spherical harmonics of the input data.

    This module computes the spherical harmonics up to a given degree
    :obj:`lmax` for the input tensor of 3D vectors.
    The vectors are assumed to be given in Cartesian coordinates.
    See `here <https://en.wikipedia.org/wiki/Table_of_spherical_harmonics>`_
    for mathematical details.

    Args:
        lmax (int, optional): The maximum degree of the spherical harmonics.
            (default: :obj:`2`)
    """

    def __init__(self, lmax: int = 2) -> None:
        super().__init__()
        self.lmax = lmax

    def forward(self, edge_vec: Tensor) -> Tensor:
        r"""Computes the spherical harmonics of the input tensor.

        Args:
            edge_vec (torch.Tensor): A tensor of 3D vectors.
        """
        return self._spherical_harmonics(
            self.lmax,
            edge_vec[..., 0],
            edge_vec[..., 1],
            edge_vec[..., 2],
        )

    @staticmethod
    def _spherical_harmonics(
        lmax: int,
        x: Tensor,
        y: Tensor,
        z: Tensor,
    ) -> Tensor:
        r"""Computes the spherical harmonics up to degree :obj:`lmax` of the
        input vectors.

        Args:
            lmax (int): The maximum degree of the spherical harmonics.
            x (torch.Tensor): The x coordinates of the vectors.
            y (torch.Tensor): The y coordinates of the vectors.
            z (torch.Tensor): The z coordinates of the vectors.
        """
        sh_1_0, sh_1_1, sh_1_2 = x, y, z

        if lmax == 1:
            return torch.stack([sh_1_0, sh_1_1, sh_1_2], dim=-1)

        sh_2_0 = math.sqrt(3.0) * x * z
        sh_2_1 = math.sqrt(3.0) * x * y
        y2 = y.pow(2)
        x2z2 = x.pow(2) + z.pow(2)
        sh_2_2 = y2 - 0.5 * x2z2
        sh_2_3 = math.sqrt(3.0) * y * z
        sh_2_4 = math.sqrt(3.0) / 2.0 * (z.pow(2) - x.pow(2))

        if lmax == 2:
            return torch.stack(
                [
                    sh_1_0,
                    sh_1_1,
                    sh_1_2,
                    sh_2_0,
                    sh_2_1,
                    sh_2_2,
                    sh_2_3,
                    sh_2_4,
                ],
                dim=-1,
            )

        raise ValueError(f"'lmax' needs to be 1 or 2 (got {lmax})")


class VecLayerNorm(torch.nn.Module):
    r"""Applies layer normalization to the input data.

    This module applies a custom layer normalization to a tensor of vectors.
    The normalization can either be :obj:`"max_min"` normalization, or no
    normalization.

    Args:
        hidden_channels (int): The number of hidden channels in the input.
        trainable (bool): If set to :obj:`True`, the normalization weights are
            trainable parameters.
        norm_type (str, optional): The type of normalization to apply, one of
            :obj:`"max_min"` or :obj:`None`. (default: :obj:`"max_min"`)
    """

    def __init__(
        self,
        hidden_channels: int,
        trainable: bool,
        norm_type: Optional[str] = "max_min",
    ) -> None:
        super().__init__()

        self.hidden_channels = hidden_channels
        self.norm_type = norm_type
        self.eps = 1e-12

        weight = torch.ones(self.hidden_channels)
        if trainable:
            self.register_parameter("weight", Parameter(weight))
        else:
            self.register_buffer("weight", weight)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the normalization weights to their initial values."""
        torch.nn.init.ones_(self.weight)

    def max_min_norm(self, vec: Tensor) -> Tensor:
        r"""Applies max-min normalization to the input tensor.

        .. math::
            \text{dist} = ||\text{vec}||_2
            \text{direct} = \frac{\text{vec}}{\text{dist}}
            \text{max\_val} = \max(\text{dist})
            \text{min\_val} = \min(\text{dist})
            \text{delta} = \text{max\_val} - \text{min\_val}
            \text{dist} = \frac{\text{dist} - \text{min\_val}}{\text{delta}}
            \text{normed\_vec} = \max(0, \text{dist}) \cdot \text{direct}

        Args:
            vec (torch.Tensor): The input tensor.
        """
        dist = torch.norm(vec, dim=1, keepdim=True)

        if (dist == 0).all():
            return torch.zeros_like(vec)

        dist = dist.clamp(min=self.eps)
        direct = vec / dist

        max_val, _ = dist.max(dim=-1)
        min_val, _ = dist.min(dim=-1)
        delta = (max_val - min_val).view(-1)
        delta = torch.where(delta == 0, torch.ones_like(delta), delta)
        dist = (dist - min_val.view(-1, 1, 1)) / delta.view(-1, 1, 1)

        return dist.relu() * direct

    def forward(self, vec: Tensor) -> Tensor:
        r"""Applies the layer normalization to the input tensor.

        Args:
            vec (torch.Tensor): The input tensor.
        """
        if vec.size(1) == 3:
            if self.norm_type == "max_min":
                vec = self.max_min_norm(vec)
            return vec * self.weight.unsqueeze(0).unsqueeze(0)
        elif vec.size(1) == 8:
            vec1, vec2 = torch.split(vec, [3, 5], dim=1)
            if self.norm_type == "max_min":
                vec1 = self.max_min_norm(vec1)
                vec2 = self.max_min_norm(vec2)
            vec = torch.cat([vec1, vec2], dim=1)
            return vec * self.weight.unsqueeze(0).unsqueeze(0)

        raise ValueError(f"'{self.__class__.__name__}' only support 3 or 8 " f"channels (got {vec.size(1)})")


class Distance(torch.nn.Module):
    r"""Computes the pairwise distances between atoms in a molecule.

    This module computes the pairwise distances between atoms in a molecule,
    represented by their positions :obj:`pos`.
    The distances are computed only between points that are within a certain
    cutoff radius.

    Args:
        cutoff (float): The cutoff radius beyond
            which distances are not computed.
        max_num_neighbors (int, optional): The maximum number of neighbors
            considered for each point. (default: :obj:`32`)
        add_self_loops (bool, optional): If set to :obj:`False`, will not
            include self-loops. (default: :obj:`True`)
    """

    def __init__(
        self,
        cutoff: float,
        max_num_neighbors: int = 32,
        add_self_loops: bool = True,
    ) -> None:
        super().__init__()
        self.cutoff = cutoff
        self.max_num_neighbors = max_num_neighbors
        self.add_self_loops = add_self_loops

    def forward(
        self,
        pos: Tensor,
        batch: Tensor,
    ) -> Tuple[Tensor, Tensor, Tensor]:
        r"""Computes the pairwise distances between atoms in the molecule.

        Args:
            pos (torch.Tensor): The positions of the atoms in the molecule.
            batch (torch.Tensor): A batch vector, which assigns each node to a
                specific example.

        Returns:
            edge_index (torch.Tensor): The indices of the edges in the graph.
            edge_weight (torch.Tensor): The distances between connected nodes.
            edge_vec (torch.Tensor): The vector differences between connected
                nodes.
        """
        edge_index = radius_graph(
            pos,
            r=self.cutoff,
            batch=batch,
            loop=self.add_self_loops,
            max_num_neighbors=self.max_num_neighbors,
        )
        edge_vec = pos[edge_index[0]] - pos[edge_index[1]]

        if self.add_self_loops:
            mask = edge_index[0] != edge_index[1]
            edge_weight = torch.zeros(edge_vec.size(0), device=edge_vec.device)
            edge_weight[mask] = torch.norm(edge_vec[mask], dim=-1)
        else:
            edge_weight = torch.norm(edge_vec, dim=-1)

        return edge_index, edge_weight, edge_vec


class NeighborEmbedding(MessagePassing):
    r"""The :class:`NeighborEmbedding` module from the `"Enhancing Geometric
    Representations for Molecules with Equivariant Vector-Scalar Interactive
    Message Passing" <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        hidden_channels (int): The number of hidden channels in the node
            embeddings.
        num_rbf (int): The number of radial basis functions.
        cutoff (float): The cutoff distance.
        max_z (int, optional): The maximum atomic numbers.
            (default: :obj:`100`)
    """

    def __init__(
        self,
        hidden_channels: int,
        num_rbf: int,
        cutoff: float,
        max_z: int = 100,
    ) -> None:
        super().__init__(aggr="add")
        self.embedding = Embedding(max_z, hidden_channels)
        self.distance_proj = Linear(num_rbf, hidden_channels)
        self.combine = Linear(hidden_channels * 2, hidden_channels)
        self.cutoff = CosineCutoff(cutoff)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        self.embedding.reset_parameters()
        torch.nn.init.xavier_uniform_(self.distance_proj.weight)
        torch.nn.init.xavier_uniform_(self.combine.weight)
        self.distance_proj.bias.data.zero_()
        self.combine.bias.data.zero_()

    def forward(
        self,
        z: Tensor,
        x: Tensor,
        edge_index: Tensor,
        edge_weight: Tensor,
        edge_attr: Tensor,
    ) -> Tensor:
        r"""Computes the neighborhood embedding of the nodes in the graph.

        Args:
            z (torch.Tensor): The atomic numbers.
            x (torch.Tensor): The node features.
            edge_index (torch.Tensor): The indices of the edges.
            edge_weight (torch.Tensor): The weights of the edges.
            edge_attr (torch.Tensor): The edge features.

        Returns:
            x_neighbors (torch.Tensor): The neighborhood embeddings of the
                nodes.
        """
        mask = edge_index[0] != edge_index[1]
        if not mask.all():
            edge_index = edge_index[:, mask]
            edge_weight = edge_weight[mask]
            edge_attr = edge_attr[mask]

        C = self.cutoff(edge_weight)
        W = self.distance_proj(edge_attr) * C.view(-1, 1)

        x_neighbors = self.embedding(z)
        x_neighbors = self.propagate(edge_index, x=x_neighbors, W=W)
        x_neighbors = self.combine(torch.cat([x, x_neighbors], dim=1))
        return x_neighbors

    def message(self, x_j: Tensor, W: Tensor) -> Tensor:
        return x_j * W


class EdgeEmbedding(torch.nn.Module):
    r"""The :class:`EdgeEmbedding` module from the `"Enhancing Geometric
    Representations for Molecules with Equivariant Vector-Scalar Interactive
    Message Passing" <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        num_rbf (int): The number of radial basis functions.
        hidden_channels (int): The number of hidden channels in the node
            embeddings.
    """

    def __init__(self, num_rbf: int, hidden_channels: int) -> None:
        super().__init__()
        self.edge_proj = Linear(num_rbf, hidden_channels)
        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        torch.nn.init.xavier_uniform_(self.edge_proj.weight)
        self.edge_proj.bias.data.zero_()

    def forward(
        self,
        edge_index: Tensor,
        edge_attr: Tensor,
        x: Tensor,
    ) -> Tensor:
        r"""Computes the edge embeddings of the graph.

        Args:
            edge_index (torch.Tensor): The indices of the edges.
            edge_attr (torch.Tensor): The edge features.
            x (torch.Tensor): The node features.

        Returns:
            out_edge_attr (torch.Tensor): The edge embeddings.
        """
        x_j = x[edge_index[0]]
        x_i = x[edge_index[1]]
        return (x_i + x_j) * self.edge_proj(edge_attr)


class ViS_MP(MessagePassing):
    r"""The message passing module without vertex geometric features of the
    equivariant vector-scalar interactive graph neural network (ViSNet)
    from the `"Enhancing Geometric Representations for Molecules with
    Equivariant Vector-Scalar Interactive Message Passing"
    <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        num_heads (int): The number of attention heads.
        hidden_channels (int): The number of hidden channels in the node
            embeddings.
        cutoff (float): The cutoff distance.
        vecnorm_type (str, optional): The type of normalization to apply to the
            vectors.
        trainable_vecnorm (bool): Whether the normalization weights are
            trainable.
        last_layer (bool, optional): Whether this is the last layer in the
            model. (default: :obj:`False`)
    """

    def __init__(
        self,
        num_heads: int,
        hidden_channels: int,
        cutoff: float,
        vecnorm_type: Optional[str],
        trainable_vecnorm: bool,
        last_layer: bool = False,
    ) -> None:
        super().__init__(aggr="add", node_dim=0)

        if hidden_channels % num_heads != 0:
            raise ValueError(
                f"The number of hidden channels (got {hidden_channels}) must "
                f"be evenly divisible by the number of attention heads "
                f"(got {num_heads})"
            )

        self.num_heads = num_heads
        self.hidden_channels = hidden_channels
        self.head_dim = hidden_channels // num_heads
        self.last_layer = last_layer

        self.layernorm = LayerNorm(hidden_channels)
        self.vec_layernorm = VecLayerNorm(
            hidden_channels,
            trainable=trainable_vecnorm,
            norm_type=vecnorm_type,
        )

        self.act = torch.nn.SiLU()
        self.attn_activation = torch.nn.SiLU()

        self.cutoff = CosineCutoff(cutoff)

        self.vec_proj = Linear(hidden_channels, hidden_channels * 3, False)

        self.q_proj = Linear(hidden_channels, hidden_channels)
        self.k_proj = Linear(hidden_channels, hidden_channels)
        self.v_proj = Linear(hidden_channels, hidden_channels)
        self.dk_proj = Linear(hidden_channels, hidden_channels)
        self.dv_proj = Linear(hidden_channels, hidden_channels)

        self.s_proj = Linear(hidden_channels, hidden_channels * 2)
        if not self.last_layer:
            self.f_proj = Linear(hidden_channels, hidden_channels)
            self.w_src_proj = Linear(hidden_channels, hidden_channels, False)
            self.w_trg_proj = Linear(hidden_channels, hidden_channels, False)

        self.o_proj = Linear(hidden_channels, hidden_channels * 3)

        self.reset_parameters()

    @staticmethod
    def vector_rejection(vec: Tensor, d_ij: Tensor) -> Tensor:
        r"""Computes the component of :obj:`vec` orthogonal to :obj:`d_ij`.

        Args:
            vec (torch.Tensor): The input vector.
            d_ij (torch.Tensor): The reference vector.
        """
        vec_proj = (vec * d_ij.unsqueeze(2)).sum(dim=1, keepdim=True)
        return vec - vec_proj * d_ij.unsqueeze(2)

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        self.layernorm.reset_parameters()
        self.vec_layernorm.reset_parameters()
        torch.nn.init.xavier_uniform_(self.q_proj.weight)
        self.q_proj.bias.data.zero_()
        torch.nn.init.xavier_uniform_(self.k_proj.weight)
        self.k_proj.bias.data.zero_()
        torch.nn.init.xavier_uniform_(self.v_proj.weight)
        self.v_proj.bias.data.zero_()
        torch.nn.init.xavier_uniform_(self.o_proj.weight)
        self.o_proj.bias.data.zero_()
        torch.nn.init.xavier_uniform_(self.s_proj.weight)
        self.s_proj.bias.data.zero_()

        if not self.last_layer:
            torch.nn.init.xavier_uniform_(self.f_proj.weight)
            self.f_proj.bias.data.zero_()
            torch.nn.init.xavier_uniform_(self.w_src_proj.weight)
            torch.nn.init.xavier_uniform_(self.w_trg_proj.weight)

        torch.nn.init.xavier_uniform_(self.vec_proj.weight)
        torch.nn.init.xavier_uniform_(self.dk_proj.weight)
        self.dk_proj.bias.data.zero_()
        torch.nn.init.xavier_uniform_(self.dv_proj.weight)
        self.dv_proj.bias.data.zero_()

    def forward(
        self,
        x: Tensor,
        vec: Tensor,
        edge_index: Tensor,
        r_ij: Tensor,
        f_ij: Tensor,
        d_ij: Tensor,
    ) -> Tuple[Tensor, Tensor, Optional[Tensor]]:
        r"""Computes the residual scalar and vector features of the nodes and
        scalar featues of the edges.

        Args:
            x (torch.Tensor): The scalar features of the nodes.
            vec (torch.Tensor):The vector features of the nodes.
            edge_index (torch.Tensor): The indices of the edges.
            r_ij (torch.Tensor): The distances between connected nodes.
            f_ij (torch.Tensor): The scalar features of the edges.
            d_ij (torch.Tensor): The unit vectors of the edges

        Returns:
            dx (torch.Tensor): The residual scalar features of the nodes.
            dvec (torch.Tensor): The residual vector features of the nodes.
            df_ij (torch.Tensor, optional): The residual scalar features of the
                edges, or :obj:`None` if this is the last layer.
        """
        x = self.layernorm(x)
        vec = self.vec_layernorm(vec)

        q = self.q_proj(x).reshape(-1, self.num_heads, self.head_dim)
        k = self.k_proj(x).reshape(-1, self.num_heads, self.head_dim)
        v = self.v_proj(x).reshape(-1, self.num_heads, self.head_dim)
        dk = self.act(self.dk_proj(f_ij))
        dk = dk.reshape(-1, self.num_heads, self.head_dim)
        dv = self.act(self.dv_proj(f_ij))
        dv = dv.reshape(-1, self.num_heads, self.head_dim)

        vec1, vec2, vec3 = torch.split(self.vec_proj(vec), self.hidden_channels, dim=-1)
        vec_dot = (vec1 * vec2).sum(dim=1)

        x, vec_out = self.propagate(edge_index, q=q, k=k, v=v, dk=dk, dv=dv, vec=vec, r_ij=r_ij, d_ij=d_ij)

        o1, o2, o3 = torch.split(self.o_proj(x), self.hidden_channels, dim=1)
        dx = vec_dot * o2 + o3
        dvec = vec3 * o1.unsqueeze(1) + vec_out
        if not self.last_layer:
            df_ij = self.edge_updater(edge_index, vec=vec, d_ij=d_ij, f_ij=f_ij)
            return dx, dvec, df_ij
        else:
            return dx, dvec, None

    def message(
        self, q_i: Tensor, k_j: Tensor, v_j: Tensor, vec_j: Tensor, dk: Tensor, dv: Tensor, r_ij: Tensor, d_ij: Tensor
    ) -> Tuple[Tensor, Tensor]:
        attn = (q_i * k_j * dk).sum(dim=-1)
        attn = self.attn_activation(attn) * self.cutoff(r_ij).unsqueeze(1)

        v_j = v_j * dv
        v_j = (v_j * attn.unsqueeze(2)).view(-1, self.hidden_channels)

        s1, s2 = torch.split(self.act(self.s_proj(v_j)), self.hidden_channels, dim=1)
        vec_j = vec_j * s1.unsqueeze(1) + s2.unsqueeze(1) * d_ij.unsqueeze(2)

        return v_j, vec_j

    def edge_update(self, vec_i: Tensor, vec_j: Tensor, d_ij: Tensor, f_ij: Tensor) -> Tensor:
        w1 = self.vector_rejection(self.w_trg_proj(vec_i), d_ij)
        w2 = self.vector_rejection(self.w_src_proj(vec_j), -d_ij)
        w_dot = (w1 * w2).sum(dim=1)
        df_ij = self.act(self.f_proj(f_ij)) * w_dot
        return df_ij

    def aggregate(
        self,
        features: Tuple[Tensor, Tensor],
        index: Tensor,
        ptr: Optional[torch.Tensor],
        dim_size: Optional[int],
    ) -> Tuple[Tensor, Tensor]:
        x, vec = features
        x = scatter(x, index, dim=self.node_dim, dim_size=dim_size)
        vec = scatter(vec, index, dim=self.node_dim, dim_size=dim_size)
        return x, vec


class ViS_MP_Vertex(ViS_MP):
    r"""The message passing module with vertex geometric features of the
    equivariant vector-scalar interactive graph neural network (ViSNet)
    from the `"Enhancing Geometric Representations for Molecules with
    Equivariant Vector-Scalar Interactive Message Passing"
    <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        num_heads (int): The number of attention heads.
        hidden_channels (int): The number of hidden channels in the node
            embeddings.
        cutoff (float): The cutoff distance.
        vecnorm_type (str, optional): The type of normalization to apply to the
            vectors.
        trainable_vecnorm (bool): Whether the normalization weights are
            trainable.
        last_layer (bool, optional): Whether this is the last layer in the
            model. (default: :obj:`False`)
    """

    def __init__(
        self,
        num_heads: int,
        hidden_channels: int,
        cutoff: float,
        vecnorm_type: Optional[str],
        trainable_vecnorm: bool,
        last_layer: bool = False,
    ) -> None:
        super().__init__(num_heads, hidden_channels, cutoff, vecnorm_type, trainable_vecnorm, last_layer)

        if not self.last_layer:
            self.f_proj = Linear(hidden_channels, hidden_channels * 2)
            self.t_src_proj = Linear(hidden_channels, hidden_channels, False)
            self.t_trg_proj = Linear(hidden_channels, hidden_channels, False)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        super().reset_parameters()

        if not self.last_layer:
            if hasattr(self, "t_src_proj"):
                torch.nn.init.xavier_uniform_(self.t_src_proj.weight)
            if hasattr(self, "t_trg_proj"):
                torch.nn.init.xavier_uniform_(self.t_trg_proj.weight)

    def edge_update(self, vec_i: Tensor, vec_j: Tensor, d_ij: Tensor, f_ij: Tensor) -> Tensor:
        w1 = self.vector_rejection(self.w_trg_proj(vec_i), d_ij)
        w2 = self.vector_rejection(self.w_src_proj(vec_j), -d_ij)
        w_dot = (w1 * w2).sum(dim=1)

        t1 = self.vector_rejection(self.t_trg_proj(vec_i), d_ij)
        t2 = self.vector_rejection(self.t_src_proj(vec_i), -d_ij)
        t_dot = (t1 * t2).sum(dim=1)

        f1, f2 = torch.split(self.act(self.f_proj(f_ij)), self.hidden_channels, dim=-1)

        return f1 * w_dot + f2 * t_dot


class ViSNetBlock(torch.nn.Module):
    r"""The representation module of the equivariant vector-scalar
    interactive graph neural network (ViSNet) from the `"Enhancing Geometric
    Representations for Molecules with Equivariant Vector-Scalar Interactive
    Message Passing" <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        lmax (int, optional): The maximum degree of the spherical harmonics.
            (default: :obj:`1`)
        vecnorm_type (str, optional): The type of normalization to apply to the
            vectors. (default: :obj:`None`)
        trainable_vecnorm (bool, optional):  Whether the normalization weights
            are trainable. (default: :obj:`False`)
        num_heads (int, optional): The number of attention heads.
            (default: :obj:`8`)
        num_layers (int, optional): The number of layers in the network.
            (default: :obj:`6`)
        hidden_channels (int, optional): The number of hidden channels in the
            node embeddings. (default: :obj:`128`)
        num_rbf (int, optional): The number of radial basis functions.
            (default: :obj:`32`)
        trainable_rbf (bool, optional): Whether the radial basis function
            parameters are trainable. (default: :obj:`False`)
        max_z (int, optional): The maximum atomic numbers.
            (default: :obj:`100`)
        cutoff (float, optional): The cutoff distance. (default: :obj:`5.0`)
        max_num_neighbors (int, optional): The maximum number of neighbors
            considered for each atom. (default: :obj:`32`)
        vertex (bool, optional): Whether to use vertex geometric features.
            (default: :obj:`False`)
    """

    def __init__(
        self,
        lmax: int = 1,
        vecnorm_type: Optional[str] = None,
        trainable_vecnorm: bool = False,
        num_heads: int = 8,
        num_layers: int = 6,
        hidden_channels: int = 128,
        num_rbf: int = 32,
        trainable_rbf: bool = False,
        max_z: int = 100,
        cutoff: float = 5.0,
        max_num_neighbors: int = 32,
        vertex: bool = False,
    ) -> None:
        super().__init__()

        self.lmax = lmax
        self.vecnorm_type = vecnorm_type
        self.trainable_vecnorm = trainable_vecnorm
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.hidden_channels = hidden_channels
        self.num_rbf = num_rbf
        self.trainable_rbf = trainable_rbf
        self.max_z = max_z
        self.cutoff = cutoff
        self.max_num_neighbors = max_num_neighbors

        self.embedding = Embedding(max_z, hidden_channels)
        self.distance = Distance(cutoff, max_num_neighbors=max_num_neighbors)
        self.sphere = Sphere(lmax=lmax)
        self.distance_expansion = ExpNormalSmearing(cutoff, num_rbf, trainable_rbf)
        self.neighbor_embedding = NeighborEmbedding(hidden_channels, num_rbf, cutoff, max_z)
        self.edge_embedding = EdgeEmbedding(num_rbf, hidden_channels)

        self.vis_mp_layers = torch.nn.ModuleList()
        vis_mp_kwargs = dict(
            num_heads=num_heads,
            hidden_channels=hidden_channels,
            cutoff=cutoff,
            vecnorm_type=vecnorm_type,
            trainable_vecnorm=trainable_vecnorm,
        )
        vis_mp_class = ViS_MP if not vertex else ViS_MP_Vertex
        for _ in range(num_layers - 1):
            layer = vis_mp_class(last_layer=False, **vis_mp_kwargs)
            self.vis_mp_layers.append(layer)
        self.vis_mp_layers.append(vis_mp_class(last_layer=True, **vis_mp_kwargs))

        self.out_norm = LayerNorm(hidden_channels)
        self.vec_out_norm = VecLayerNorm(
            hidden_channels,
            trainable=trainable_vecnorm,
            norm_type=vecnorm_type,
        )

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        self.embedding.reset_parameters()
        self.distance_expansion.reset_parameters()
        self.neighbor_embedding.reset_parameters()
        self.edge_embedding.reset_parameters()
        for layer in self.vis_mp_layers:
            layer.reset_parameters()
        self.out_norm.reset_parameters()
        self.vec_out_norm.reset_parameters()

    def forward(
        self,
        z: Tensor,
        pos: Tensor,
        batch: Tensor,
    ) -> Tuple[Tensor, Tensor]:
        r"""Computes the scalar and vector features of the nodes.

        Args:
            z (torch.Tensor): The atomic numbers.
            pos (torch.Tensor): The coordinates of the atoms.
            batch (torch.Tensor): A batch vector, which assigns each node to a
                specific example.

        Returns:
            x (torch.Tensor): The scalar features of the nodes.
            vec (torch.Tensor): The vector features of the nodes.
        """
        x = self.embedding(z)
        edge_index, edge_weight, edge_vec = self.distance(pos, batch)
        edge_attr = self.distance_expansion(edge_weight)
        mask = edge_index[0] != edge_index[1]
        edge_vec[mask] = edge_vec[mask] / torch.norm(edge_vec[mask], dim=1).unsqueeze(1)
        edge_vec = self.sphere(edge_vec)
        x = self.neighbor_embedding(z, x, edge_index, edge_weight, edge_attr)
        vec = torch.zeros(x.size(0), ((self.lmax + 1) ** 2) - 1, x.size(1), dtype=x.dtype, device=x.device)
        edge_attr = self.edge_embedding(edge_index, edge_attr, x)

        for attn in self.vis_mp_layers[:-1]:
            dx, dvec, dedge_attr = attn(x, vec, edge_index, edge_weight, edge_attr, edge_vec)
            x = x + dx
            vec = vec + dvec
            edge_attr = edge_attr + dedge_attr

        dx, dvec, _ = self.vis_mp_layers[-1](x, vec, edge_index, edge_weight, edge_attr, edge_vec)
        x = x + dx
        vec = vec + dvec

        x = self.out_norm(x)
        vec = self.vec_out_norm(vec)

        return x, vec


class GatedEquivariantBlock(torch.nn.Module):
    r"""Applies a gated equivariant operation to scalar features and vector
    features from the `"Enhancing Geometric Representations for Molecules with
    Equivariant Vector-Scalar Interactive Message Passing"
    <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        hidden_channels (int): The number of hidden channels in the node
            embeddings.
        out_channels (int): The number of output channels.
        intermediate_channels (int, optional): The number of channels in the
            intermediate layer, or :obj:`None` to use the same number as
            :obj:`hidden_channels`. (default: :obj:`None`)
        scalar_activation (bool, optional): Whether to apply a scalar
            activation function to the output node features.
            (default: obj:`False`)
    """

    def __init__(
        self,
        hidden_channels: int,
        out_channels: int,
        intermediate_channels: Optional[int] = None,
        scalar_activation: bool = False,
    ) -> None:
        super().__init__()
        self.out_channels = out_channels

        if intermediate_channels is None:
            intermediate_channels = hidden_channels

        self.vec1_proj = Linear(hidden_channels, hidden_channels, bias=False)
        self.vec2_proj = Linear(hidden_channels, out_channels, bias=False)

        self.update_net = torch.nn.Sequential(
            Linear(hidden_channels * 2, intermediate_channels),
            torch.nn.SiLU(),
            Linear(intermediate_channels, out_channels * 2),
        )

        self.act = torch.nn.SiLU() if scalar_activation else None

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        torch.nn.init.xavier_uniform_(self.vec1_proj.weight)
        torch.nn.init.xavier_uniform_(self.vec2_proj.weight)
        torch.nn.init.xavier_uniform_(self.update_net[0].weight)
        self.update_net[0].bias.data.zero_()
        torch.nn.init.xavier_uniform_(self.update_net[2].weight)
        self.update_net[2].bias.data.zero_()

    def forward(self, x: Tensor, v: Tensor) -> Tuple[Tensor, Tensor]:
        r"""Applies a gated equivariant operation to node features and vector
        features.

        Args:
            x (torch.Tensor): The scalar features of the nodes.
            v (torch.Tensor): The vector features of the nodes.
        """
        vec1 = torch.norm(self.vec1_proj(v), dim=-2)
        vec2 = self.vec2_proj(v)

        x = torch.cat([x, vec1], dim=-1)
        x, v = torch.split(self.update_net(x), self.out_channels, dim=-1)
        v = v.unsqueeze(1) * vec2

        if self.act is not None:
            x = self.act(x)

        return x, v


class EquivariantScalar(torch.nn.Module):
    r"""Computes final scalar outputs based on node features and vector
    features.

    Args:
        hidden_channels (int): The number of hidden channels in the node
            embeddings.
    """

    def __init__(self, hidden_channels: int) -> None:
        super().__init__()

        self.output_network = torch.nn.ModuleList(
            [
                GatedEquivariantBlock(
                    hidden_channels,
                    hidden_channels // 2,
                    scalar_activation=True,
                ),
                GatedEquivariantBlock(
                    hidden_channels // 2,
                    1,
                    scalar_activation=False,
                ),
            ]
        )

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        for layer in self.output_network:
            layer.reset_parameters()

    def pre_reduce(self, x: Tensor, v: Tensor) -> Tensor:
        r"""Computes the final scalar outputs.

        Args:
            x (torch.Tensor): The scalar features of the nodes.
            v (torch.Tensor): The vector features of the nodes.

        Returns:
            out (torch.Tensor): The final scalar outputs of the nodes.
        """
        for layer in self.output_network:
            x, v = layer(x, v)

        return x + v.sum() * 0


class Atomref(torch.nn.Module):
    r"""Adds atom reference values to atomic energies.

    Args:
        atomref (torch.Tensor, optional):  A tensor of atom reference values,
            or :obj:`None` if not provided. (default: :obj:`None`)
        max_z (int, optional): The maximum atomic numbers.
            (default: :obj:`100`)
    """

    def __init__(
        self,
        atomref: Optional[Tensor] = None,
        max_z: int = 100,
    ) -> None:
        super().__init__()

        if atomref is None:
            atomref = torch.zeros(max_z, 1)
        else:
            atomref = torch.as_tensor(atomref)

        if atomref.ndim == 1:
            atomref = atomref.view(-1, 1)

        self.register_buffer("initial_atomref", atomref)
        self.atomref = Embedding(len(atomref), 1)

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        self.atomref.weight.data.copy_(self.initial_atomref)

    def forward(self, x: Tensor, z: Tensor) -> Tensor:
        r"""Adds atom reference values to atomic energies.

        Args:
            x (torch.Tensor): The atomic energies.
            z (torch.Tensor): The atomic numbers.
        """
        return x + self.atomref(z)


class ViSNet(torch.nn.Module):
    r"""A :pytorch:`PyTorch` module that implements the equivariant
    vector-scalar interactive graph neural network (ViSNet) from the
    `"Enhancing Geometric Representations for Molecules with Equivariant
    Vector-Scalar Interactive Message Passing"
    <https://arxiv.org/abs/2210.16518>`_ paper.

    Args:
        lmax (int, optional): The maximum degree of the spherical harmonics.
            (default: :obj:`1`)
        vecnorm_type (str, optional): The type of normalization to apply to the
            vectors. (default: :obj:`None`)
        trainable_vecnorm (bool, optional):  Whether the normalization weights
            are trainable. (default: :obj:`False`)
        num_heads (int, optional): The number of attention heads.
            (default: :obj:`8`)
        num_layers (int, optional): The number of layers in the network.
            (default: :obj:`6`)
        hidden_channels (int, optional): The number of hidden channels in the
            node embeddings. (default: :obj:`128`)
        num_rbf (int, optional): The number of radial basis functions.
            (default: :obj:`32`)
        trainable_rbf (bool, optional): Whether the radial basis function
            parameters are trainable. (default: :obj:`False`)
        max_z (int, optional): The maximum atomic numbers.
            (default: :obj:`100`)
        cutoff (float, optional): The cutoff distance. (default: :obj:`5.0`)
        max_num_neighbors (int, optional): The maximum number of neighbors
            considered for each atom. (default: :obj:`32`)
        vertex (bool, optional): Whether to use vertex geometric features.
            (default: :obj:`False`)
        atomref (torch.Tensor, optional): A tensor of atom reference values,
            or :obj:`None` if not provided. (default: :obj:`None`)
        reduce_op (str, optional): The type of reduction operation to apply
            (:obj:`"sum"`, :obj:`"mean"`). (default: :obj:`"sum"`)
        mean (float, optional): The mean of the output distribution.
            (default: :obj:`0.0`)
        std (float, optional): The standard deviation of the output
            distribution. (default: :obj:`1.0`)
        derivative (bool, optional): Whether to compute the derivative of the
            output with respect to the positions. (default: :obj:`False`)
    """

    def __init__(
        self,
        lmax: int = 1,
        vecnorm_type: Optional[str] = None,
        trainable_vecnorm: bool = False,
        num_heads: int = 8,
        num_layers: int = 6,
        hidden_channels: int = 128,
        num_rbf: int = 32,
        trainable_rbf: bool = False,
        max_z: int = 100,
        cutoff: float = 5.0,
        max_num_neighbors: int = 32,
        vertex: bool = False,
        atomref: Optional[Tensor] = None,
        reduce_op: str = "sum",
        mean: float = 0.0,
        std: float = 1.0,
        derivative: bool = False,
    ) -> None:
        super().__init__()

        self.representation_model = ViSNetBlock(
            lmax=lmax,
            vecnorm_type=vecnorm_type,
            trainable_vecnorm=trainable_vecnorm,
            num_heads=num_heads,
            num_layers=num_layers,
            hidden_channels=hidden_channels,
            num_rbf=num_rbf,
            trainable_rbf=trainable_rbf,
            max_z=max_z,
            cutoff=cutoff,
            max_num_neighbors=max_num_neighbors,
            vertex=vertex,
        )

        self.output_model = EquivariantScalar(hidden_channels=hidden_channels)
        self.prior_model = Atomref(atomref=atomref, max_z=max_z)
        self.reduce_op = reduce_op
        self.derivative = derivative

        self.register_buffer("mean", torch.tensor(mean))
        self.register_buffer("std", torch.tensor(std))

        self.reset_parameters()

    def reset_parameters(self):
        r"""Resets the parameters of the module."""
        self.representation_model.reset_parameters()
        self.output_model.reset_parameters()
        if self.prior_model is not None:
            self.prior_model.reset_parameters()

    def forward(
        self,
        z: Tensor,
        pos: Tensor,
        batch: Tensor,
    ) -> Tuple[Tensor, Optional[Tensor]]:
        r"""Computes the energies or properties (forces) for a batch of
        molecules.

        Args:
            z (torch.Tensor): The atomic numbers.
            pos (torch.Tensor): The coordinates of the atoms.
            batch (torch.Tensor): A batch vector,
                which assigns each node to a specific example.

        Returns:
            y (torch.Tensor): The energies or properties for each molecule.
            dy (torch.Tensor, optional): The negative derivative of energies.
        """
        if self.derivative:
            pos.requires_grad_(True)

        x, v = self.representation_model(z, pos, batch)
        x = self.output_model.pre_reduce(x, v)
        x = x * self.std

        if self.prior_model is not None:
            x = self.prior_model(x, z)

        y = scatter(x, batch, dim=0, reduce=self.reduce_op)
        y = y + self.mean

        if self.derivative:
            grad_outputs = [torch.ones_like(y)]
            dy = grad(
                [y],
                [pos],
                grad_outputs=grad_outputs,
                create_graph=True,
                retain_graph=True,
            )[0]
            if dy is None:
                raise RuntimeError("Autograd returned None for the force prediction.")
            return y, -dy

        return y, None


model_cls = ViSNet


if __name__ == "__main__":
    node_features = torch.load("node_features.pt")
    edge_index = torch.load("edge_index.pt")

    # Model instantiation and forward pass
    model = ViSNet()
    output = model(node_features, edge_index)

    # Save output to a file
    torch.save(output, "gt_output.pt")



================================================
File: rdagent/components/coder/model_coder/one_shot/__init__.py
================================================
import re
from pathlib import Path

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.model_coder.model import ModelExperiment, ModelFBWorkspace
from rdagent.core.developer import Developer
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_utils import APIBackend

DIRNAME = Path(__file__).absolute().resolve().parent


class ModelCodeWriter(Developer[ModelExperiment]):
    def develop(self, exp: ModelExperiment) -> ModelExperiment:
        mti_l = []
        for t in exp.sub_tasks:
            mti = ModelFBWorkspace(t)
            mti.prepare()
            pr = Prompts(file_path=DIRNAME / "prompt.yaml")

            user_prompt_tpl = Environment(undefined=StrictUndefined).from_string(pr["code_implement_user"])
            sys_prompt_tpl = Environment(undefined=StrictUndefined).from_string(pr["code_implement_sys"])

            user_prompt = user_prompt_tpl.render(
                name=t.name,
                description=t.description,
                formulation=t.formulation,
                variables=t.variables,
            )
            system_prompt = sys_prompt_tpl.render()

            resp = APIBackend().build_messages_and_create_chat_completion(user_prompt, system_prompt)

            # Extract the code part from the response
            match = re.search(r".*```[Pp]ython\n(.*)\n```.*", resp, re.DOTALL)
            code = match.group(1)
            mti.inject_files(**{"model.py": code})
            mti_l.append(mti)
        exp.sub_workspace_list = mti_l
        return exp



================================================
File: rdagent/components/coder/model_coder/one_shot/prompt.yaml
================================================


code_implement_sys: |-
  You are an assistant whose job is to answer user's question.
code_implement_user: |-
  With the following given information, write a python code using pytorch and torch_geometric to implement the model.
  This model is in the graph learning field, only have one layer.
  The input will be node_feature [num_nodes, dim_feature] and edge_index [2, num_edges]  (It would be the input of the forward model)
  There is not edge attribute or edge weight as input. The model should detect the node_feature and edge_index shape, if there is Linear transformation layer in the model, the input and output shape should be consistent. The in_channels is the dimension of the node features.
  Implement the model forward function based on the following information:model formula information.
  1. model name:{{name}}
  2. model description:{{description}}
  3. model formulation:{{formulation}}
  4. model variables:{{variables}}.
  You must complete the forward function as far as you can do.
  Execution Your implemented code will be executed in the follow way:
  The the implemented code will be placed in a file like [uuid]/model.py
  We'll import the model in the implementation in file `model.py` after setting the cwd into the directory
  - from model import model_cls (So you must have a variable named `model_cls` in the file)
    - So your implemented code could follow the following pattern
      ```Python
      class XXXLayer(torch.nn.Module):
          ...
      model_cls = XXXLayer
      ```
  - initialize the model by initializing it `model_cls(input_dim=INPUT_DIM)`
  - And then verify the model by comparing the output tensors by feeding specific input tensor.



================================================
File: rdagent/components/document_reader/document_reader.py
================================================
from __future__ import annotations

import io
from pathlib import Path
from typing import TYPE_CHECKING

import fitz
import requests
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
from langchain_community.document_loaders import PyPDFDirectoryLoader, PyPDFLoader
from PIL import Image

if TYPE_CHECKING:
    from langchain_core.documents import Document

from rdagent.core.conf import RD_AGENT_SETTINGS


def load_documents_by_langchain(path: str) -> list:
    """Load documents from the specified path.

    Args:
        path (str): The path to the directory or file containing the documents.

    Returns:
        list: A list of loaded documents.
    """
    if Path(path).is_dir():
        loader = PyPDFDirectoryLoader(path, silent_errors=True)
    else:
        loader = PyPDFLoader(path)
    return loader.load()


def process_documents_by_langchain(docs: list[Document]) -> dict[str, str]:
    """Process a list of documents and group them by document name.

    Args:
        docs (list): A list of documents.

    Returns:
        dict: A dictionary where the keys are document names and the values are
        the concatenated content of the documents.
    """
    content_dict = {}

    for doc in docs:
        if Path(doc.metadata["source"]).exists():
            doc_name = str(Path(doc.metadata["source"]).resolve())
        else:
            doc_name = doc.metadata["source"]
        doc_content = doc.page_content

        if doc_name not in content_dict:
            content_dict[str(doc_name)] = doc_content
        else:
            content_dict[str(doc_name)] += doc_content

    return content_dict


def load_and_process_pdfs_by_langchain(path: str) -> dict[str, str]:
    return process_documents_by_langchain(load_documents_by_langchain(path))


def load_and_process_one_pdf_by_azure_document_intelligence(
    path: Path,
    key: str,
    endpoint: str,
) -> str:
    pages = len(PyPDFLoader(str(path)).load())
    document_analysis_client = DocumentAnalysisClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(key),
    )

    with path.open("rb") as file:
        result = document_analysis_client.begin_analyze_document(
            "prebuilt-document",
            file,
            pages=f"1-{pages}",
        ).result()
    return result.content


def load_and_process_pdfs_by_azure_document_intelligence(path: Path) -> dict[str, str]:
    assert RD_AGENT_SETTINGS.azure_document_intelligence_key is not None
    assert RD_AGENT_SETTINGS.azure_document_intelligence_endpoint is not None

    content_dict = {}
    ab_path = path.resolve()
    if ab_path.is_file():
        assert ".pdf" in ab_path.suffixes, "The file must be a PDF file."
        proc = load_and_process_one_pdf_by_azure_document_intelligence
        content_dict[str(ab_path)] = proc(
            ab_path,
            RD_AGENT_SETTINGS.azure_document_intelligence_key,
            RD_AGENT_SETTINGS.azure_document_intelligence_endpoint,
        )
    else:
        for file_path in ab_path.rglob("*"):
            if file_path.is_file() and ".pdf" in file_path.suffixes:
                content_dict[str(file_path)] = load_and_process_one_pdf_by_azure_document_intelligence(
                    file_path,
                    RD_AGENT_SETTINGS.azure_document_intelligence_key,
                    RD_AGENT_SETTINGS.azure_document_intelligence_endpoint,
                )
    return content_dict


def extract_first_page_screenshot_from_pdf(pdf_path: str) -> Image:
    if not Path(pdf_path).exists():
        doc = fitz.open(stream=io.BytesIO(requests.get(pdf_path).content), filetype="pdf")
    else:
        doc = fitz.open(pdf_path)
    page = doc.load_page(0)
    pix = page.get_pixmap()
    image = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)

    return image



================================================
File: rdagent/components/knowledge_management/graph.py
================================================
from __future__ import annotations

import pickle
import random
from collections import deque
from pathlib import Path
from typing import Any, NoReturn

from rdagent.components.knowledge_management.vector_base import (
    KnowledgeMetaData,
    PDVectorBase,
    VectorBase,
    cosine,
)
from rdagent.core.knowledge_base import KnowledgeBase
from rdagent.oai.llm_utils import APIBackend

Node = KnowledgeMetaData


class UndirectedNode(Node):
    def __init__(self, content: str = "", label: str = "", embedding: Any = None) -> None:
        super().__init__(content, label, embedding)
        self.neighbors: set[UndirectedNode] = set()
        assert isinstance(content, str), "content must be a string"

    def add_neighbor(self, node: UndirectedNode) -> None:
        self.neighbors.add(node)
        node.neighbors.add(self)

    def remove_neighbor(self, node: UndirectedNode) -> None:
        if node in self.neighbors:
            self.neighbors.remove(node)
            node.neighbors.remove(self)

    def get_neighbors(self) -> set[UndirectedNode]:
        return self.neighbors

    def __str__(self) -> str:
        return (
            f"UndirectedNode(id={self.id}, label={self.label}, content={self.content[:100]}, "
            f"neighbors={self.neighbors})"
        )

    def __repr__(self) -> str:
        return (
            f"UndirectedNode(id={self.id}, label={self.label}, content={self.content[:100]}, "
            f"neighbors={self.neighbors})"
        )


class Graph(KnowledgeBase):
    """
    base Graph class for Knowledge Graph Search
    """

    def __init__(self, path: str | Path | None = None) -> None:
        self.nodes = {}
        super().__init__(path=path)

    def size(self) -> int:
        return len(self.nodes)

    def get_node(self, node_id: str) -> Node | None:
        return self.nodes.get(node_id)

    def add_node(self, **kwargs: Any) -> NoReturn:
        raise NotImplementedError

    def get_all_nodes(self) -> list[Node]:
        return list(self.nodes.values())

    def get_all_nodes_by_label_list(self, label_list: list[str]) -> list[Node]:
        return [node for node in self.nodes.values() if node.label in label_list]

    def find_node(self, content: str, label: str) -> Node | None:
        for node in self.nodes.values():
            if node.content == content and node.label == label:
                return node
        return None

    @staticmethod
    def batch_embedding(nodes: list[Node]) -> list[Node]:
        contents = [node.content for node in nodes]
        # openai create embedding API input's max length is 16
        size = 16
        embeddings = []
        for i in range(0, len(contents), size):
            embeddings.extend(
                APIBackend().create_embedding(input_content=contents[i : i + size]),
            )

        assert len(nodes) == len(embeddings), "nodes' length must equals embeddings' length"
        for node, embedding in zip(nodes, embeddings):
            node.embedding = embedding
        return nodes

    def __str__(self) -> str:
        return f"Graph(nodes={self.nodes})"


class UndirectedGraph(Graph):
    """
    Undirected Graph which edges have no relationship
    """

    def __init__(self, path: str | Path | None = None) -> None:
        self.vector_base: VectorBase = PDVectorBase()
        super().__init__(path=path)

    def __str__(self) -> str:
        return f"UndirectedGraph(nodes={self.nodes})"

    def add_node(
        self,
        node: UndirectedNode,
        neighbor: UndirectedNode = None,
        same_node_threshold: float = 0.95,  # noqa: ARG002
    ) -> None:
        """
        add node and neighbor to the Graph
        Parameters
        ----------
        same_node_threshold: 0.95 is an empirical value. When two strings only differ in case, the similarity is greater
         than 0.95.
        node
        neighbor

        Returns
        -------

        """
        if self.get_node(node.id):
            node = self.get_node(node.id)
        elif self.find_node(content=node.content, label=node.label):
            node = self.find_node(content=node.content, label=node.label)
        else:
            # same_node = self.semantic_search(node=node.content, similarity_threshold=same_node_threshold, topk_k=1)
            # if len(same_node):
            #     node = same_node[0]
            # else:
            node.create_embedding()
            self.vector_base.add(document=node)
            self.nodes.update({node.id: node})

        if neighbor is not None:
            if self.get_node(neighbor.id):
                neighbor = self.get_node(neighbor.id)
            elif self.find_node(content=neighbor.content, label=node.label):
                neighbor = self.find_node(content=neighbor.content, label=node.label)
            else:
                # same_node = self.semantic_search(node=neighbor.content,
                #                                  similarity_threshold=same_node_threshold, topk_k=1)
                # if len(same_node):
                #     neighbor = same_node[0]
                # else:
                neighbor.create_embedding()
                self.vector_base.add(document=neighbor)
                self.nodes.update({neighbor.id: neighbor})

            node.add_neighbor(neighbor)

    def add_nodes(self, node: UndirectedNode, neighbors: list[UndirectedNode]) -> None:
        if not neighbors:
            self.add_node(node)
        else:
            for neighbor in neighbors:
                self.add_node(node, neighbor=neighbor)

    def get_node(self, node_id: str) -> UndirectedNode:
        return self.nodes.get(node_id)

    def get_node_by_content(self, content: str) -> UndirectedNode | None:
        """
        Get node by semantic distance
        Parameters
        ----------
        content

        Returns
        -------

        """
        if content == "Model":
            pass
        match = self.semantic_search(node=content, similarity_threshold=0.999)
        if match:
            return match[0]
        return None

    def get_nodes_within_steps(
        self,
        start_node: UndirectedNode,
        steps: int = 1,
        constraint_labels: list[str] | None = None,
        *,
        block: bool = False,
    ) -> list[UndirectedNode]:
        """
        Returns the nodes in the graph whose distance from node is less than or equal to step
        """
        visited = set()
        queue = deque([(start_node, 0)])
        result = []

        while queue:
            node, current_steps = queue.popleft()

            if current_steps > steps:
                break

            if node not in visited:
                visited.add(node)
                result.append(node)

                for neighbor in sorted(
                    self.get_node(node.id).neighbors,
                    key=lambda x: x.content,
                ):  # to make sure the result is deterministic
                    if neighbor not in visited and not (block and neighbor.label not in constraint_labels):
                        queue.append((neighbor, current_steps + 1))

        if constraint_labels:
            result = [node for node in result if node.label in constraint_labels]
        if start_node in result:
            result.remove(start_node)
        return result

    def get_nodes_intersection(
        self,
        nodes: list[UndirectedNode],
        steps: int = 1,
        constraint_labels: list[str] | None = None,
    ) -> list[UndirectedNode]:
        """
        Get the intersection with nodes connected within n steps of nodes

        Parameters
        ----------
        nodes
        steps
        constraint_labels

        Returns
        -------

        """
        min_nodes_count = 2
        assert len(nodes) >= min_nodes_count, "nodes length must >=2"
        intersection = None

        for node in nodes:
            if intersection is None:
                intersection = self.get_nodes_within_steps(
                    node,
                    steps=steps,
                    constraint_labels=constraint_labels,
                )
            intersection = self.intersection(
                nodes1=intersection,
                nodes2=self.get_nodes_within_steps(
                    node,
                    steps=steps,
                    constraint_labels=constraint_labels,
                ),
            )
        return intersection

    def semantic_search(
        self,
        node: UndirectedNode | str,
        similarity_threshold: float = 0.0,
        topk_k: int = 5,
        constraint_labels: list[str] | None = None,
    ) -> list[UndirectedNode]:
        """
        Semantic search by node's embedding.

        Parameters
        ----------
        node : UndirectedNode | str
            The node to search for.
        similarity_threshold : float, optional
            The minimum similarity score for a node to be included in the results.
            Nodes with a similarity score less than or equal to this threshold will be excluded.
        topk_k : int, optional
            The maximum number of similar nodes to return.
        constraint_labels : list[str], optional
            If provided, only nodes with matching labels will be considered.

        Returns
        -------
        list[UndirectedNode]
            A list of `topk_k` nodes that are semantically similar to the input node, sorted by similarity score.
            All nodes shall meet the `similarity_threshold` and `constraint_labels` criteria.
        """
        # Question: why do we need to convert to Node object first?
        if isinstance(node, str):
            node = UndirectedNode(content=node)
        docs, scores = self.vector_base.search(
            content=node.content,
            topk_k=topk_k,
            similarity_threshold=similarity_threshold,
            constraint_labels=constraint_labels,
        )
        return [self.get_node(doc.id) for doc in docs]

    def clear(self) -> None:
        self.nodes.clear()
        self.vector_base: VectorBase = PDVectorBase()

    def query_by_node(
        self,
        node: UndirectedNode,
        step: int = 1,
        constraint_labels: list[str] | None = None,
        constraint_node: UndirectedNode | None = None,
        constraint_distance: float = 0,
        *,
        block: bool = False,
    ) -> list[UndirectedNode]:
        """
        search graph by connection, return empty list if nodes' chain without node near to constraint_node
        Parameters
        ----------
        node
        step
        constraint_labels
        constraint_node
        constraint_distance
        block: despite the start node, the search can only flow through the constraint_label type nodes

        Returns
        -------

        """
        nodes = self.get_nodes_within_steps(
            start_node=node,
            steps=step,
            constraint_labels=constraint_labels,
            block=block,
        )
        if constraint_node is not None:
            for n in nodes:
                if self.cal_distance(n, constraint_node) > constraint_distance:
                    return nodes
            return []
        return nodes

    def query_by_content(
        self,
        content: str | list[str],
        topk_k: int = 5,
        step: int = 1,
        constraint_labels: list[str] | None = None,
        constraint_node: UndirectedNode | None = None,
        similarity_threshold: float = 0.0,
        constraint_distance: float = 0,
        *,
        block: bool = False,
    ) -> list[UndirectedNode]:
        """
        Search graph by content similarity and connection relationship, return empty
        list if nodes' chain without node near to constraint_node.

        Parameters
        ----------
        constraint_distance : float
            The distance between the node and the constraint_node.
        content : Union[str, List[str]]
            Content to search for.
        topk_k: int
            The upper number of output for each query. If the number of fit nodes is
            less than topk_k, returns all fit nodes' content.
        step : int
            The maximum distance between the start node and the result node.
        constraint_labels : List[str]
            The type of nodes that the search can only flow through.
        constraint_node : UndirectedNode, optional
            The node that the search can only flow through.
        similarity_threshold : float
            The similarity threshold of the content.
        block: bool
            Despite the start node, the search can only flow through the constraint_label type nodes.

        Returns
        -------

        """

        if isinstance(content, str):
            content = [content]

        res_list = []
        for query in content:
            similar_nodes = self.semantic_search(
                content=query,
                topk_k=topk_k,
                similarity_threshold=similarity_threshold,
            )

            connected_nodes = []
            for node in similar_nodes:
                graph_query_node_res = self.query_by_node(
                    node,
                    step=step,
                    constraint_labels=constraint_labels,
                    constraint_node=constraint_node,
                    constraint_distance=constraint_distance,
                    block=block,
                )
                connected_nodes.extend(
                    [node for node in graph_query_node_res if node not in connected_nodes],
                )
                if len(connected_nodes) >= topk_k:
                    break

            res_list.extend(
                [node for node in connected_nodes[:topk_k] if node not in res_list],
            )
        return res_list

    @staticmethod
    def intersection(nodes1: list[UndirectedNode], nodes2: list[UndirectedNode]) -> list[UndirectedNode]:
        return [node for node in nodes1 if node in nodes2]

    @staticmethod
    def different(nodes1: list[UndirectedNode], nodes2: list[UndirectedNode]) -> list[UndirectedNode]:
        return list(set(nodes1).symmetric_difference(set(nodes2)))

    @staticmethod
    def cal_distance(node1: UndirectedNode, node2: UndirectedNode) -> float:
        return cosine(node1.embedding, node2.embedding)

    @staticmethod
    def filter_label(nodes: list[UndirectedNode], labels: list[str]) -> list[UndirectedNode]:
        return [node for node in nodes if node.label in labels]


def graph_to_edges(graph: dict[str, list[str]]) -> list[tuple[str, str]]:
    edges = []

    for node, neighbors in graph.items():
        for neighbor in neighbors:
            if (node, neighbor) in edges or (neighbor, node) in edges:
                continue
            edges.append((node, neighbor))

    return edges


def assign_random_coordinate_to_node(
    nodes: list[str],
    scope: float = 1.0,
    origin: tuple[float, float] = (0.0, 0.0),
) -> dict[str, tuple[float, float]]:
    coordinates = {}
    for node in nodes:
        x = random.SystemRandom().uniform(0, scope) + origin[0]
        y = random.SystemRandom().uniform(0, scope) + origin[1]
        coordinates[node] = (x, y)

    return coordinates


def assign_isometric_coordinate_to_node(
    nodes: list,
    x_step: float = 1.0,
    x_origin: float = 0.0,
    y_origin: float = 0.0,
) -> dict:
    coordinates = {}

    for i, node in enumerate(nodes):
        x = x_origin + i * x_step
        y = y_origin
        coordinates[node] = (x, y)

    return coordinates


def curly_node_coordinate(
    coordinates: dict,
    center_y: float = 1.0,
    r: float = 1.0,
) -> dict:
    # noto: this method can only curly < 90 degree, and the curl line is circle.
    # the original function is: x**2 + (y-m)**2 = r**2
    for node, coordinate in coordinates.items():
        coordinates[node] = (coordinate[0], center_y + (r**2 - coordinate[0] ** 2) ** 0.5)
    return coordinates



================================================
File: rdagent/components/knowledge_management/vector_base.py
================================================
import uuid
from pathlib import Path
from typing import List, Tuple, Union

import pandas as pd
from scipy.spatial.distance import cosine

from rdagent.core.knowledge_base import KnowledgeBase
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend


class KnowledgeMetaData:
    def __init__(self, content: str = "", label: str = None, embedding=None, identity=None):
        self.label = label
        self.content = content
        self.id = str(uuid.uuid3(uuid.NAMESPACE_DNS, str(self.content))) if identity is None else identity
        self.embedding = embedding
        self.trunks = []
        self.trunks_embedding = []

    def split_into_trunk(self, size: int = 1000, overlap: int = 0):
        """
        split content into trunks and create embedding by trunk
        Returns
        -------

        """

        def split_string_into_chunks(string: str, chunk_size: int):
            chunks = []
            for i in range(0, len(string), chunk_size):
                chunk = string[i : i + chunk_size]
                chunks.append(chunk)
            return chunks

        self.trunks = split_string_into_chunks(self.content, chunk_size=size)
        self.trunks_embedding = APIBackend().create_embedding(input_content=self.trunks)

    def create_embedding(self):
        """
        create content's embedding
        Returns
        -------

        """
        if self.embedding is None:
            self.embedding = APIBackend().create_embedding(input_content=self.content)

    def from_dict(self, data: dict):
        for key, value in data.items():
            setattr(self, key, value)
        return self

    def __repr__(self):
        return f"Document(id={self.id}, label={self.label}, data={self.content})"


Document = KnowledgeMetaData


def contents_to_documents(contents: List[str], label: str = None) -> List[Document]:
    # openai create embedding API input's max length is 16
    size = 16
    embedding = []
    for i in range(0, len(contents), size):
        embedding.extend(APIBackend().create_embedding(input_content=contents[i : i + size]))
    docs = [Document(content=c, label=label, embedding=e) for c, e in zip(contents, embedding)]
    return docs


class VectorBase(KnowledgeBase):
    """
    This class is used for handling vector storage and query
    """

    def add(self, document: Union[Document, List[Document]]):
        """
        add new node to vector_df
        Parameters
        ----------
        document

        Returns
        -------

        """
        pass

    def search(self, content: str, topk_k: int = 5, similarity_threshold: float = 0) -> List[Document]:
        """
        search vector_df by node
        Parameters
        ----------
        similarity_threshold
        content
        topk_k: return topk_k nearest vector_df

        Returns
        -------

        """
        pass


class PDVectorBase(VectorBase):
    """
    Implement of VectorBase using Pandas
    """

    def __init__(self, path: Union[str, Path] = None):
        self.vector_df = pd.DataFrame(columns=["id", "label", "content", "embedding"])
        super().__init__(path)

    def shape(self):
        return self.vector_df.shape

    def add(self, document: Union[Document, List[Document]]):
        """
        add new node to vector_df
        Parameters
        ----------
        document

        Returns
        -------

        """
        if isinstance(document, Document):
            if document.embedding is None:
                document.create_embedding()
            docs = [
                {
                    "id": document.id,
                    "label": document.label,
                    "content": document.content,
                    "trunk": document.content,
                    "embedding": document.embedding,
                }
            ]
            docs.extend(
                [
                    {
                        "id": document.id,
                        "label": document.label,
                        "content": document.content,
                        "trunk": trunk,
                        "embedding": embedding,
                    }
                    for trunk, embedding in zip(document.trunks, document.trunks_embedding)
                ]
            )
            self.vector_df = pd.concat([self.vector_df, pd.DataFrame(docs)], ignore_index=True)
        else:
            for doc in document:
                self.add(document=doc)

    def search(
        self, content: str, topk_k: int = 5, similarity_threshold: float = 0, constraint_labels: list[str] | None = None
    ) -> Tuple[List[Document], List]:
        """
        Search vector by node's embedding.

        Parameters
        ----------
        content : str
            The content to search for.
        topk_k : int, optional
            The number of nearest vectors to return.
        similarity_threshold : float, optional
            The minimum similarity score for a vector to be considered.
        constraint_labels : List[str], optional
            If provided, only nodes with matching labels will be considered.

        Returns
        -------
        Tuple[List[Document], List]
            A list of `topk_k` nodes that are semantically similar to the input node, sorted by similarity score.
            All nodes shall meet the `similarity_threshold` and `constraint_labels` criteria.
        """
        if not self.vector_df.shape[0]:
            return [], []

        document = Document(content=content)
        document.create_embedding()

        filtered_df = self.vector_df
        if constraint_labels is not None:
            filtered_df = self.vector_df[self.vector_df["label"].isin(constraint_labels)]

        similarities = filtered_df["embedding"].apply(
            lambda x: 1 - cosine(x, document.embedding)
        )  # cosine is cosine distance, 1-similarity

        searched_similarities = similarities[similarities > similarity_threshold].nlargest(topk_k)
        most_similar_docs = filtered_df.loc[searched_similarities.index]

        docs = []
        for _, similar_docs in most_similar_docs.iterrows():
            docs.append(Document().from_dict(similar_docs.to_dict()))

        return docs, searched_similarities.to_list()



================================================
File: rdagent/components/loader/experiment_loader.py
================================================
from rdagent.components.coder.factor_coder.factor import FactorExperiment
from rdagent.core.experiment import Loader


class FactorExperimentLoader(Loader[FactorExperiment]):
    pass


class ModelExperimentLoader(Loader[FactorExperiment]):
    pass



================================================
File: rdagent/components/loader/task_loader.py
================================================
import json
from pathlib import Path
from typing import Sequence

from rdagent.components.coder.factor_coder.factor import FactorTask
from rdagent.components.coder.model_coder.model import ModelFBWorkspace, ModelTask
from rdagent.core.experiment import Loader, WsLoader


class FactorTaskLoader(Loader[FactorTask]):
    pass


class ModelTaskLoader(Loader[ModelTask]):
    pass


class ModelTaskLoaderJson(ModelTaskLoader):
    # def __init__(self, json_uri: str, select_model: Optional[str] = None) -> None:
    #     super().__init__()
    #     self.json_uri = json_uri
    #     self.select_model = 'A-DGN'

    # def load(self, *argT, **kwargs) -> Sequence[ModelImplTask]:
    #     # json is supposed to be in the format of {model_name: dict{model_data}}
    #     model_dict = json.load(open(self.json_uri, "r"))
    #     if self.select_model is not None:
    #         assert self.select_model in model_dict
    #         model_name = self.select_model
    #         model_data = model_dict[self.select_model]
    #     else:
    #         model_name, model_data = list(model_dict.items())[0]

    #     model_impl_task = ModelImplTask(
    #         name=model_name,
    #         description=model_data["description"],
    #         formulation=model_data["formulation"],
    #         variables=model_data["variables"],
    #         key=model_name
    #     )

    #     return [model_impl_task]

    def __init__(self, json_uri: str) -> None:
        super().__init__()
        self.json_uri = json_uri

    def load(self, *argT, **kwargs) -> Sequence[ModelTask]:
        # json is supposed to be in the format of {model_name: dict{model_data}}
        model_dict = json.load(open(self.json_uri, "r"))
        # FIXME: the model in the json file is not right due to extraction error
        #       We should fix them case by case in the future
        #
        # formula_info = {
        #     "name": "Anti-Symmetric Deep Graph Network (A-DGN)",
        #     "description": "A framework for stable and non-dissipative DGN design. It ensures long-range information preservation between nodes and prevents gradient vanishing or explosion during training.",
        #     "formulation": r"\mathbf{x}^{\prime}_i = \mathbf{x}_i + \epsilon \cdot \sigma \left( (\mathbf{W}-\mathbf{W}^T-\gamma \mathbf{I}) \mathbf{x}_i + \Phi(\mathbf{X}, \mathcal{N}_i) + \mathbf{b}\right),",
        #     "variables": {
        #         r"\mathbf{x}_i": "The state of node i at previous layer",
        #         r"\epsilon": "The step size in the Euler discretization",
        #         r"\sigma": "A monotonically non-decreasing activation function",
        #         r"\Phi": "A graph convolutional operator",
        #         r"W": "An anti-symmetric weight matrix",
        #         r"\mathbf{x}^{\prime}_i": "The node feature matrix at layer l-1",
        #         r"\mathcal{N}_i": "The set of neighbors of node u",
        #         r"\mathbf{b}": "A bias vector",
        #     },
        #     "key": "A-DGN",
        # }
        model_impl_task_list = []
        for model_name, model_data in model_dict.items():
            model_impl_task = ModelTask(
                name=model_name,
                description=model_data["description"],
                formulation=model_data["formulation"],
                variables=model_data["variables"],
                model_type=model_data["model_type"],
                architecture="",
                hyperparameters="",
            )
            model_impl_task_list.append(model_impl_task)
        return model_impl_task_list


class ModelWsLoader(WsLoader[ModelTask, ModelFBWorkspace]):
    def __init__(self, path: Path) -> None:
        self.path = Path(path)

    def load(self, task: ModelTask) -> ModelFBWorkspace:
        assert task.name is not None
        mti = ModelFBWorkspace(task)
        mti.prepare()
        with open(self.path / f"{task.name}.py", "r") as f:
            code = f.read()
        mti.inject_files(**{"model.py": code})
        return mti



================================================
File: rdagent/components/proposal/__init__.py
================================================
from abc import abstractmethod
from pathlib import Path
from typing import Tuple

from jinja2 import Environment, StrictUndefined

from rdagent.core.experiment import Experiment
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import (
    Hypothesis,
    Hypothesis2Experiment,
    HypothesisGen,
    Scenario,
    Trace,
)
from rdagent.oai.llm_utils import APIBackend

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class LLMHypothesisGen(HypothesisGen):
    def __init__(self, scen: Scenario):
        super().__init__(scen)

    # The following methods are scenario related so they should be implemented in the subclass
    @abstractmethod
    def prepare_context(self, trace: Trace) -> Tuple[dict, bool]: ...

    @abstractmethod
    def convert_response(self, response: str) -> Hypothesis: ...

    def gen(self, trace: Trace) -> Hypothesis:
        context_dict, json_flag = self.prepare_context(trace)

        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["hypothesis_gen"]["system_prompt"])
            .render(
                targets=self.targets,
                scenario=self.scen.get_scenario_all_desc(filtered_tag="hypothesis_and_experiment"),
                hypothesis_output_format=context_dict["hypothesis_output_format"],
                hypothesis_specification=context_dict["hypothesis_specification"],
            )
        )
        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["hypothesis_gen"]["user_prompt"])
            .render(
                targets=self.targets,
                hypothesis_and_feedback=context_dict["hypothesis_and_feedback"],
                RAG=context_dict["RAG"],
            )
        )

        resp = APIBackend().build_messages_and_create_chat_completion(user_prompt, system_prompt, json_mode=json_flag)

        hypothesis = self.convert_response(resp)

        return hypothesis


class FactorHypothesisGen(LLMHypothesisGen):
    def __init__(self, scen: Scenario):
        super().__init__(scen)
        self.targets = "factors"


class ModelHypothesisGen(LLMHypothesisGen):
    def __init__(self, scen: Scenario):
        super().__init__(scen)
        self.targets = "model tuning"


class FactorAndModelHypothesisGen(LLMHypothesisGen):
    def __init__(self, scen: Scenario):
        super().__init__(scen)
        self.targets = "feature engineering and model building"


class LLMHypothesis2Experiment(Hypothesis2Experiment[Experiment]):
    @abstractmethod
    def prepare_context(self, hypothesis: Hypothesis, trace: Trace) -> Tuple[dict, bool]: ...

    @abstractmethod
    def convert_response(self, response: str, hypothesis: Hypothesis, trace: Trace) -> Experiment: ...

    def convert(self, hypothesis: Hypothesis, trace: Trace) -> Experiment:
        context, json_flag = self.prepare_context(hypothesis, trace)
        system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["hypothesis2experiment"]["system_prompt"])
            .render(
                targets=self.targets,
                scenario=trace.scen.get_scenario_all_desc(filtered_tag="hypothesis_and_experiment"),
                experiment_output_format=context["experiment_output_format"],
            )
        )
        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["hypothesis2experiment"]["user_prompt"])
            .render(
                targets=self.targets,
                target_hypothesis=context["target_hypothesis"],
                hypothesis_and_feedback=context["hypothesis_and_feedback"],
                target_list=context["target_list"],
                RAG=context["RAG"],
            )
        )

        resp = APIBackend().build_messages_and_create_chat_completion(user_prompt, system_prompt, json_mode=json_flag)

        return self.convert_response(resp, hypothesis, trace)


class FactorHypothesis2Experiment(LLMHypothesis2Experiment):
    def __init__(self):
        super().__init__()
        self.targets = "factors"


class ModelHypothesis2Experiment(LLMHypothesis2Experiment):
    def __init__(self):
        super().__init__()
        self.targets = "model tuning"


class FactorAndModelHypothesis2Experiment(LLMHypothesis2Experiment):
    def __init__(self):
        super().__init__()
        self.targets = "feature engineering and model building"



================================================
File: rdagent/components/proposal/prompts.yaml
================================================
hypothesis_gen:
  system_prompt: |-
    The user is working on generating new hypotheses for the {{targets}} in a data-driven research and development process. 
    The {{targets}} are used in the following scenario:
    {{scenario}}
    The user has already proposed several hypotheses and conducted evaluations on them. This information will be provided to you. Your task is to check whether a similar hypothesis has already been generated. 
    If one exists and you agree with it, feel free to use it. If you disagree, please generate an improved version.
    {% if hypothesis_specification %}
    To assist you in formulating new hypotheses, the user has provided some additional information: {{hypothesis_specification}}.
    **Important:** If the hypothesis_specification outlines the next steps you need to follow, ensure you adhere to those instructions.
    {% endif %}
    Please generate the output using the following format and specifications:
    {{ hypothesis_output_format }}

  user_prompt: |-
    {% if hypothesis_and_feedback|length == 0 %}It is the first round of hypothesis generation. The user has no hypothesis on this scenario yet.
    {% else %}It is not the first round, the user has made several hypothesis on this scenario and did several evaluation on them.
    The former hypothesis and the corresponding feedbacks are as follows (focus on the last one & the new hypothesis that it provides and reasoning to see if you agree):
    {{ hypothesis_and_feedback }}
    {% endif %}
    {% if RAG %}
    To assist you in generating new {{targets}}, we have provided the following information: {{RAG}}.
    **Note:** The provided RAG is for reference only. 
    You must carefully assess whether the RAG aligns with the {{targets}}. 
    If it does not, it should not be used. Exercise caution and make your own judgment.
    {% endif %}
    Also generate the relevant keys for the reasoning and the distilled knowledge that follows. For those keys, in particular for knowledge, explain in the context of the specific scenario to build up domain knowledge in the specific field rather than general knowledge.

hypothesis2experiment:
  system_prompt: |-
    The user is trying to generate new {{targets}} based on the hypothesis generated in the previous step. 
    The {{targets}} are used in certain scenario, the scenario is as follows:
    {{ scenario }}
    The user will use the {{targets}} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{targets}} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{targets}} on similar hypothesis.
    4. Some additional information to help you generate new {{targets}}.
    Please generate the output following the format below:
    {{ experiment_output_format }}
    
  user_prompt: |-
    The user has made several hypothesis on this scenario and did several evaluation on them.
    The target hypothesis you are targeting to generate {{targets}} for is as follows:
    {{ target_hypothesis }}
    The former hypothesis and the corresponding feedbacks are as follows:
    {{ hypothesis_and_feedback }}
    Please generate the new {{targets}} based on the information above.



================================================
File: rdagent/components/runner/__init__.py
================================================
from rdagent.core.developer import Developer
from rdagent.core.experiment import ASpecificExp, Experiment
from rdagent.oai.llm_utils import md5_hash


class CachedRunner(Developer[ASpecificExp]):
    def get_cache_key(self, exp: Experiment) -> str:
        all_tasks = []
        for based_exp in exp.based_experiments:
            all_tasks.extend(based_exp.sub_tasks)
        all_tasks.extend(exp.sub_tasks)
        task_info_list = [task.get_task_information() for task in all_tasks]
        task_info_str = "\n".join(task_info_list)
        return md5_hash(task_info_str)

    def assign_cached_result(self, exp: Experiment, cached_res: Experiment) -> Experiment:
        if exp.based_experiments and exp.based_experiments[-1].result is None:
            exp.based_experiments[-1].result = cached_res.based_experiments[-1].result
        exp.result = cached_res.result
        return exp



================================================
File: rdagent/components/workflow/conf.py
================================================
from rdagent.core.conf import ExtendedBaseSettings


class BasePropSetting(ExtendedBaseSettings):
    """
    The common part of the config for RD Loop to propose and development
    You can add following config in the subclass to distinguish the environment variables.
    """

    scen: str = ""
    knowledge_base: str = ""
    knowledge_base_path: str = ""
    hypothesis_gen: str = ""
    hypothesis2experiment: str = ""
    coder: str = ""
    runner: str = ""
    summarizer: str = ""

    evolving_n: int = 10



================================================
File: rdagent/components/workflow/rd_loop.py
================================================
"""
Model workflow with session control
It is from `rdagent/app/qlib_rd_loop/model.py` and try to replace `rdagent/app/qlib_rd_loop/RDAgent.py`
"""

from typing import Any

from rdagent.components.workflow.conf import BasePropSetting
from rdagent.core.developer import Developer
from rdagent.core.proposal import (
    Experiment2Feedback,
    Hypothesis,
    Hypothesis2Experiment,
    HypothesisFeedback,
    HypothesisGen,
    Trace,
)
from rdagent.core.scenario import Scenario
from rdagent.core.utils import import_class
from rdagent.log import rdagent_logger as logger
from rdagent.utils.workflow import LoopBase, LoopMeta


class RDLoop(LoopBase, metaclass=LoopMeta):

    def __init__(self, PROP_SETTING: BasePropSetting):
        with logger.tag("init"):
            scen: Scenario = import_class(PROP_SETTING.scen)()
            logger.log_object(scen, tag="scenario")

            self.hypothesis_gen: HypothesisGen = import_class(PROP_SETTING.hypothesis_gen)(scen)
            logger.log_object(self.hypothesis_gen, tag="hypothesis generator")

            self.hypothesis2experiment: Hypothesis2Experiment = import_class(PROP_SETTING.hypothesis2experiment)()
            logger.log_object(self.hypothesis2experiment, tag="hypothesis2experiment")

            self.coder: Developer = import_class(PROP_SETTING.coder)(scen)
            logger.log_object(self.coder, tag="coder")
            self.runner: Developer = import_class(PROP_SETTING.runner)(scen)
            logger.log_object(self.runner, tag="runner")

            self.summarizer: Experiment2Feedback = import_class(PROP_SETTING.summarizer)(scen)
            logger.log_object(self.summarizer, tag="summarizer")
            self.trace = Trace(scen=scen)
            super().__init__()

    # excluded steps
    def _propose(self):
        hypothesis = self.hypothesis_gen.gen(self.trace)
        logger.log_object(hypothesis, tag="hypothesis generation")
        return hypothesis

    def _exp_gen(self, hypothesis: Hypothesis):
        exp = self.hypothesis2experiment.convert(hypothesis, self.trace)
        logger.log_object(exp.sub_tasks, tag="experiment generation")
        return exp

    # included steps
    def direct_exp_gen(self, prev_out: dict[str, Any]):
        with logger.tag("r"):  # research
            hypo = self._propose()
            exp = self._exp_gen(hypo)
        return {"propose": hypo, "exp_gen": exp}

    def coding(self, prev_out: dict[str, Any]):
        with logger.tag("d"):  # develop
            exp = self.coder.develop(prev_out["direct_exp_gen"]["exp_gen"])
            logger.log_object(exp.sub_workspace_list, tag="coder result")
        return exp

    def running(self, prev_out: dict[str, Any]):
        with logger.tag("ef"):  # evaluate and feedback
            exp = self.runner.develop(prev_out["coding"])
            logger.log_object(exp, tag="runner result")
        return exp

    def feedback(self, prev_out: dict[str, Any]):
        e = prev_out.get(self.EXCEPTION_KEY, None)
        if e is not None:
            feedback = HypothesisFeedback(
                observations="Error occurred in loop, skip this loop",
                hypothesis_evaluation="",
                new_hypothesis="",
                reason="",
                decision=False,
            )
            self.trace.hist.append((prev_out["direct_exp_gen"]["exp_gen"], feedback))
        else:
            feedback = self.summarizer.generate_feedback(prev_out["running"], self.trace)
            with logger.tag("ef"):  # evaluate and feedback
                logger.log_object(feedback, tag="feedback")
            self.trace.hist.append((prev_out["running"], feedback))



================================================
File: rdagent/core/conf.py
================================================
from __future__ import annotations

# TODO: use pydantic for other modules in Qlib
from pathlib import Path
from typing import cast

from pydantic_settings import (
    BaseSettings,
    EnvSettingsSource,
    PydanticBaseSettingsSource,
)


class ExtendedBaseSettings(BaseSettings):

    @classmethod
    def settings_customise_sources(
        cls,
        settings_cls: type[BaseSettings],
        init_settings: PydanticBaseSettingsSource,
        env_settings: PydanticBaseSettingsSource,
        dotenv_settings: PydanticBaseSettingsSource,
        file_secret_settings: PydanticBaseSettingsSource,
    ) -> tuple[PydanticBaseSettingsSource, ...]:
        # 1) walk from base class
        def base_iter(settings_cls: type[ExtendedBaseSettings]) -> list[type[ExtendedBaseSettings]]:
            bases = []
            for cl in settings_cls.__bases__:
                if issubclass(cl, ExtendedBaseSettings) and cl is not ExtendedBaseSettings:
                    bases.append(cl)
                    bases.extend(base_iter(cl))
            return bases

        # 2) Build EnvSettingsSource from base classes, so we can add parent Env Sources
        parent_env_settings = [
            EnvSettingsSource(
                base_cls,
                case_sensitive=base_cls.model_config.get("case_sensitive"),
                env_prefix=base_cls.model_config.get("env_prefix"),
                env_nested_delimiter=base_cls.model_config.get("env_nested_delimiter"),
            )
            for base_cls in base_iter(cast(type[ExtendedBaseSettings], settings_cls))
        ]
        return init_settings, env_settings, *parent_env_settings, dotenv_settings, file_secret_settings


class RDAgentSettings(ExtendedBaseSettings):
    # TODO: (xiao) I think LLMSetting may be a better name.
    # TODO: (xiao) I think most of the config should be in oai.config
    # Log configs
    # TODO: (xiao) think it can be a separate config.
    log_trace_path: str | None = None

    # azure document intelligence configs
    azure_document_intelligence_key: str = ""
    azure_document_intelligence_endpoint: str = ""
    # factor extraction conf
    max_input_duplicate_factor_group: int = 300
    max_output_duplicate_factor_group: int = 20
    max_kmeans_group_number: int = 40

    # workspace conf
    workspace_path: Path = Path.cwd() / "git_ignore_folder" / "RD-Agent_workspace"

    # multi processing conf
    multi_proc_n: int = 1

    # pickle cache conf
    cache_with_pickle: bool = True  # whether to use pickle cache
    pickle_cache_folder_path_str: str = str(
        Path.cwd() / "pickle_cache/",
    )  # the path of the folder to store the pickle cache
    use_file_lock: bool = (
        True  # when calling the function with same parameters, whether to use file lock to avoid
        # executing the function multiple times
    )

    # misc
    """The limitation of context stdout"""
    stdout_context_len: int = 400


RD_AGENT_SETTINGS = RDAgentSettings()



================================================
File: rdagent/core/developer.py
================================================
from __future__ import annotations

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Generic

from rdagent.core.experiment import ASpecificExp

if TYPE_CHECKING:
    from rdagent.core.scenario import Scenario


class Developer(ABC, Generic[ASpecificExp]):
    def __init__(self, scen: Scenario) -> None:
        self.scen: Scenario = scen

    @abstractmethod
    def develop(self, exp: ASpecificExp) -> ASpecificExp:  # TODO: remove return value
        """
        Task Generator should take in an experiment.

        Because the schedule of different tasks is crucial for the final performance
        due to it affects the learning process.

        Current constraints:
        - The developer should **inplace** edit the exp instead of returning value;
            - because we have a lot of use cases to raise errors, but we need the intermediate results in exp.
        - So we should remove the return value in the future.

        Responsibilities:
        - Generate a new experiment after developing on it.
        - If it tries to deliver message for future development, it should set a ExperimentFeedback
        """
        error_message = "generate method is not implemented."
        raise NotImplementedError(error_message)



================================================
File: rdagent/core/evaluation.py
================================================
"""
It is expected to be shared among different frameworks.
"""

from abc import ABC, abstractmethod


class Feedback:
    """
    Design Principle:
        It will be more like a **dataclass**.
        The building process of feedback will should be in evaluator
    """

    def __bool__(self) -> bool:
        return True


class EvaluableObj:
    """
    A set of information that is evaluable. Following things can be included.
    - Task
    - Solution
    - Ground Truth
    """


class Evaluator(ABC):
    """
    Design Principle:

        It should cover the building process of feedback from raw information.
            Typically the building of feedback will be two phases.
            1. raw information including stdout & workspace  (feedback itself will handle this)
            2. advanced/summarized feedback information. (evaluate will handle this)
    """

    @abstractmethod
    def evaluate(
        self,
        eo: EvaluableObj,
    ) -> Feedback:
        raise NotImplementedError



================================================
File: rdagent/core/evolving_agent.py
================================================
from __future__ import annotations

from abc import ABC, abstractmethod
from collections.abc import Generator
from typing import TYPE_CHECKING, Any, Generic, TypeVar

from tqdm import tqdm

if TYPE_CHECKING:
    from rdagent.core.evolving_framework import EvolvableSubjects

from rdagent.core.evaluation import EvaluableObj, Evaluator, Feedback
from rdagent.core.evolving_framework import EvolvingStrategy, EvoStep
from rdagent.log import rdagent_logger as logger

ASpecificEvaluator = TypeVar("ASpecificEvaluator", bound=Evaluator)


class EvoAgent(ABC, Generic[ASpecificEvaluator]):

    def __init__(self, max_loop: int, evolving_strategy: EvolvingStrategy) -> None:
        self.max_loop = max_loop
        self.evolving_strategy = evolving_strategy

    @abstractmethod
    def multistep_evolve(
        self,
        evo: EvolvableSubjects,
        eva: ASpecificEvaluator | Feedback,
    ) -> Generator[EvolvableSubjects, None, None]:
        """
        yield EvolvableSubjects for caller for easier process control and logging.
        """


class RAGEvaluator(Evaluator):

    @abstractmethod
    def evaluate(
        self,
        eo: EvaluableObj,
        queried_knowledge: object = None,
    ) -> Feedback:
        raise NotImplementedError


class RAGEvoAgent(EvoAgent[RAGEvaluator]):

    def __init__(
        self,
        max_loop: int,
        evolving_strategy: EvolvingStrategy,
        rag: Any,
        *,
        with_knowledge: bool = False,
        with_feedback: bool = True,
        knowledge_self_gen: bool = False,
    ) -> None:
        super().__init__(max_loop, evolving_strategy)
        self.rag = rag
        self.evolving_trace: list[EvoStep] = []
        self.with_knowledge = with_knowledge
        self.with_feedback = with_feedback
        self.knowledge_self_gen = knowledge_self_gen

    def multistep_evolve(
        self,
        evo: EvolvableSubjects,
        eva: RAGEvaluator | Feedback,
    ) -> Generator[EvolvableSubjects, None, None]:
        for evo_loop_id in tqdm(range(self.max_loop), "Implementing"):
            with logger.tag(f"evo_loop_{evo_loop_id}"):
                # 1. knowledge self-evolving
                if self.knowledge_self_gen and self.rag is not None:
                    self.rag.generate_knowledge(self.evolving_trace)
                # 2. RAG
                queried_knowledge = None
                if self.with_knowledge and self.rag is not None:
                    # TODO: Putting the evolving trace in here doesn't actually work
                    queried_knowledge = self.rag.query(evo, self.evolving_trace)

                # 3. evolve
                evo = self.evolving_strategy.evolve(
                    evo=evo,
                    evolving_trace=self.evolving_trace,
                    queried_knowledge=queried_knowledge,
                )

                # 4. Pack evolve results
                es = EvoStep(evo, queried_knowledge)

                # 5. Evaluation
                if self.with_feedback:
                    es.feedback = (
                        eva if isinstance(eva, Feedback) else eva.evaluate(evo, queried_knowledge=queried_knowledge)
                    )
                    logger.log_object(es.feedback, tag="evolving feedback")

                # 6. update trace
                self.evolving_trace.append(es)

                yield evo  # yield the control to caller for process control and logging.

                # 7. check if all tasks are completed
                if self.with_feedback and es.feedback:
                    logger.info("All tasks in evolving subject have been completed.")
                    break



================================================
File: rdagent/core/evolving_framework.py
================================================
from __future__ import annotations

import copy
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any

from rdagent.core.evaluation import EvaluableObj
from rdagent.core.knowledge_base import KnowledgeBase

if TYPE_CHECKING:
    from rdagent.core.evaluation import Feedback
    from rdagent.core.scenario import Scenario


class Knowledge:
    pass


class QueriedKnowledge:
    pass


class EvolvingKnowledgeBase(KnowledgeBase):
    @abstractmethod
    def query(
        self,
    ) -> QueriedKnowledge | None:
        raise NotImplementedError


class EvolvableSubjects(EvaluableObj):
    """The target object to be evolved"""

    def clone(self) -> EvolvableSubjects:
        return copy.deepcopy(self)


@dataclass
class EvoStep:
    """At a specific step,
    based on
    - previous trace
    - newly RAG knowledge `QueriedKnowledge`

    the EvolvableSubjects is evolved to a new one `EvolvableSubjects`.

    (optional) After evaluation, we get feedback `feedback`.
    """

    evolvable_subjects: EvolvableSubjects
    queried_knowledge: QueriedKnowledge | None = None
    feedback: Feedback | None = None


class EvolvingStrategy(ABC):
    def __init__(self, scen: Scenario) -> None:
        self.scen = scen

    @abstractmethod
    def evolve(
        self,
        *evo: EvolvableSubjects,
        evolving_trace: list[EvoStep] | None = None,
        queried_knowledge: QueriedKnowledge | None = None,
        **kwargs: Any,
    ) -> EvolvableSubjects:
        """The evolving trace is a list of (evolvable_subjects, feedback) ordered
        according to the time.

        The reason why the parameter is important for the evolving.
        - evolving_trace: the historical feedback is important.
        - queried_knowledge: queried knowledge
        """


class RAGStrategy(ABC):
    """Retrieval Augmentation Generation Strategy"""

    def __init__(self, knowledgebase: EvolvingKnowledgeBase) -> None:
        self.knowledgebase: EvolvingKnowledgeBase = knowledgebase

    @abstractmethod
    def query(
        self,
        evo: EvolvableSubjects,
        evolving_trace: list[EvoStep],
        **kwargs: Any,
    ) -> QueriedKnowledge | None:
        pass

    @abstractmethod
    def generate_knowledge(
        self,
        evolving_trace: list[EvoStep],
        *,
        return_knowledge: bool = False,
        **kwargs: Any,
    ) -> Knowledge | None:
        """Generating new knowledge based on the evolving trace.
        - It is encouraged to query related knowledge before generating new knowledge.

        RAGStrategy should maintain the new knowledge all by itself.
        """



================================================
File: rdagent/core/exception.py
================================================
class WorkflowError(Exception):
    """
    Exception indicating an error that the current loop cannot handle, preventing further progress.
    """


class FormatError(WorkflowError):
    """
    After multiple attempts, we are unable to obtain the answer in the correct format to proceed.
    """


class CoderError(WorkflowError):
    """
    Exceptions raised when Implementing and running code.
    - start: FactorTask => FactorGenerator
    - end: Get dataframe after execution

    The more detailed evaluation in dataframe values are managed by the evaluator.
    """

    # NOTE: it corresponds to the error of **component**


class CodeFormatError(CoderError):
    """
    The generated code is not found due format error.
    """


class CustomRuntimeError(CoderError):
    """
    The generated code fail to execute the script.
    """


class NoOutputError(CoderError):
    """
    The code fail to generate output file.
    """


class RunnerError(Exception):
    """
    Exceptions raised when running the code output.
    """

    # NOTE: it corresponds to the error of whole **project**


FactorEmptyError = CoderError  # Exceptions raised when no factor is generated correctly

ModelEmptyError = CoderError  # Exceptions raised when no model is generated correctly


class KaggleError(Exception):
    """
    Exceptions raised when calling Kaggle API
    """



================================================
File: rdagent/core/experiment.py
================================================
from __future__ import annotations

import os
import platform
import re
import shutil
import typing
import uuid
from abc import ABC, abstractmethod
from collections.abc import Sequence
from copy import deepcopy
from pathlib import Path
from typing import Any, Generic, TypeVar

from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.evaluation import Feedback
from rdagent.utils import filter_progress_bar
from rdagent.utils.fmt import shrink_text

if typing.TYPE_CHECKING:
    from rdagent.core.proposal import Hypothesis
    from rdagent.utils.env import Env

"""
This file contains the all the class about organizing the task in RD-Agent.
"""


class AbsTask(ABC):
    def __init__(self, name: str, version: int = 1) -> None:
        """
        The version of the task, default is 1
        Because qlib tasks execution and kaggle tasks execution are different, we need to distinguish them.
        TODO: We may align them in the future.
        """
        self.version = version
        self.name = name

    @abstractmethod
    def get_task_information(self) -> str:
        """
        Get the task information string to build the unique key
        """


class Task(AbsTask):
    def __init__(self, name: str, version: int = 1, description: str = "") -> None:
        super().__init__(name, version)
        self.description = description

    def get_task_information(self) -> str:
        return f"Task Name: {self.name}\nDescription: {self.description}"

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} {self.name}>"


ASpecificTask = TypeVar("ASpecificTask", bound=Task)
ASpecificFeedback = TypeVar("ASpecificFeedback", bound=Feedback)


class Workspace(ABC, Generic[ASpecificTask, ASpecificFeedback]):
    """
    A workspace is a place to store the task implementation. It evolves as the developer implements the task.
    To get a snapshot of the workspace, make sure call `copy` to get a copy of the workspace.
    """

    def __init__(self, target_task: ASpecificTask | None = None) -> None:
        self.target_task: ASpecificTask | None = target_task
        self.feedback: ASpecificFeedback | None = None

    @abstractmethod
    def execute(self, *args: Any, **kwargs: Any) -> object | None:
        error_message = "execute method is not implemented."
        raise NotImplementedError(error_message)

    @abstractmethod
    def copy(self) -> Workspace:
        error_message = "copy method is not implemented."
        raise NotImplementedError(error_message)

    @property
    @abstractmethod
    def all_codes(self) -> str:
        """
        Get all the code files in the workspace as a single string.
        """


ASpecificWS = TypeVar("ASpecificWS", bound=Workspace)


class WsLoader(ABC, Generic[ASpecificTask, ASpecificWS]):
    @abstractmethod
    def load(self, task: ASpecificTask) -> ASpecificWS:
        error_message = "load method is not implemented."
        raise NotImplementedError(error_message)


class FBWorkspace(Workspace):
    """
    File-based task workspace

    The implemented task will be a folder which contains related elements.
    - Data
    - Code Workspace
    - Output
        - After execution, it will generate the final output as file.

    A typical way to run the pipeline of FBWorkspace will be:
    (We didn't add it as a method due to that we may pass arguments into
    `prepare` or `execute` based on our requirements.)

    .. code-block:: python

        def run_pipeline(self, **files: str):
            self.prepare()
            self.inject_files(**files)
            self.execute()

    """

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.file_dict: dict[str, Any] = (
            {}
        )  # The code injected into the folder, store them in the variable to reproduce the former result
        self.workspace_path: Path = RD_AGENT_SETTINGS.workspace_path / uuid.uuid4().hex

    @staticmethod
    def _format_code_dict(code_dict: dict[str, str]) -> str:
        """
        Helper function to format the code dictionary into a string.
        """
        code_string = ""
        for file_name in sorted(code_dict.keys()):
            code_string += f"\nFile Path: {file_name}\n```\n{code_dict[file_name]}\n```"
        return code_string

    @property
    def all_codes(self) -> str:
        """
        Get all the code files in the workspace as a single string, excluding test files.
        """
        filtered_dict = {k: v for k, v in self.file_dict.items() if k.endswith(".py") and "test" not in k}
        return self._format_code_dict(filtered_dict)

    def get_codes(self, pattern: str) -> str:
        """
        Get code files matching a specific pattern as a single string, excluding test files.
        """
        filtered_dict = {
            k: v for k, v in self.file_dict.items() if re.search(pattern, k) and k.endswith(".py") and "test" not in k
        }
        return self._format_code_dict(filtered_dict)

    def prepare(self) -> None:
        """
        Prepare the workspace except the injected code
        - Data
        - Documentation
            typical usage of `*args, **kwargs`:
                Different methods shares the same data. The data are passed by the arguments.
        """
        self.workspace_path.mkdir(parents=True, exist_ok=True)

    @staticmethod
    def link_all_files_in_folder_to_workspace(data_path: Path, workspace_path: Path) -> None:
        data_path = Path(data_path).absolute()  # in case of relative path that will be invalid when we change cwd.
        workspace_path = Path(workspace_path)
        for data_file_path in data_path.iterdir():
            workspace_data_file_path = workspace_path / data_file_path.name
            if workspace_data_file_path.exists():
                workspace_data_file_path.unlink()
            if platform.system() == "Linux":
                os.symlink(data_file_path, workspace_data_file_path)
            if platform.system() == "Windows":
                os.link(data_file_path, workspace_data_file_path)

    DEL_KEY = "__DEL__"

    def inject_files(self, **files: str) -> None:
        """
        Inject the code into the folder.
        {
            <file name1>: <code>,  // indicate writing <code> into <file name>
                          (create new file or replace existing file)
            <file name2>: "__DEL__"  // indicate removing file name2. When we want to replace a file to a new one,
                          we usually use this
        }
        """
        self.prepare()
        for k, v in files.items():
            target_file_path = self.workspace_path / k  # Define target_file_path before using it
            if v == self.DEL_KEY:  # Use self.DEL_KEY to access the class variable
                if target_file_path.exists():
                    target_file_path.unlink()  # Unlink the file if it exists
                self.file_dict.pop(k, None)  # Safely remove the key from file_dict
            else:
                self.file_dict[k] = v
                target_file_path.parent.mkdir(parents=True, exist_ok=True)
                target_file_path.write_text(v)

    def get_files(self) -> list[Path]:
        """
        Get the environment description.

        To be general, we only return a list of filenames.
        How to summarize the environment is the responsibility of the Developer.
        """
        return list(self.workspace_path.iterdir())

    def inject_code_from_folder(self, folder_path: Path) -> None:
        """
        Load the workspace from the folder
        """
        for file_path in folder_path.rglob("*"):
            if file_path.suffix in (".py", ".yaml", ".md"):
                relative_path = file_path.relative_to(folder_path)
                self.inject_files(**{str(relative_path): file_path.read_text()})

    def inject_code_from_file_dict(self, workspace: FBWorkspace) -> None:
        """
        Load the workspace from the file_dict
        """
        for name, code in workspace.file_dict.items():
            self.inject_files(**{name: code})

    def copy(self) -> FBWorkspace:
        """
        copy the workspace from the original one
        """
        return deepcopy(self)

    def clear(self) -> None:
        """
        Clear the workspace
        """
        shutil.rmtree(self.workspace_path, ignore_errors=True)
        self.file_dict = {}

    def before_execute(self) -> None:
        """
        Before executing the code, we need to prepare the workspace and inject code into the workspace.
        """
        self.prepare()
        self.inject_files(**self.file_dict)

    def execute(self, env: Env, entry: str) -> str:
        """
        Before each execution, make sure to prepare and inject code.
        """
        stdout, _ = self.execute_ret_code(env, entry)
        return stdout

    def execute_ret_code(self, env: Env, entry: str) -> tuple[str, int]:
        """
        Execute the code in the environment and return both the stdout and the exit code.

        Before each execution, make sure to prepare and inject code.
        """
        self.prepare()
        self.inject_files(**self.file_dict)
        stdout, return_code = env.run_ret_code(entry, str(self.workspace_path))
        return (
            shrink_text(
                filter_progress_bar(stdout),
                context_lines=RD_AGENT_SETTINGS.stdout_context_len,
            ),
            return_code,
        )

    def __str__(self) -> str:
        return f"Workspace[{self.workspace_path=}" + (
            "]" if self.target_task is None else f",{self.target_task.name=}]"
        )


ASpecificWSForExperiment = TypeVar("ASpecificWSForExperiment", bound=Workspace)
ASpecificWSForSubTasks = TypeVar("ASpecificWSForSubTasks", bound=Workspace)


class Experiment(
    ABC,
    Generic[ASpecificTask, ASpecificWSForExperiment, ASpecificWSForSubTasks],
):
    """
    The experiment is a sequence of tasks and the implementations of the tasks after generated by the Developer.
    """

    def __init__(
        self,
        sub_tasks: Sequence[ASpecificTask],
        based_experiments: Sequence[ASpecificWSForExperiment] = [],
        hypothesis: Hypothesis | None = None,
    ) -> None:
        self.hypothesis: Hypothesis | None = hypothesis  # Experiment is optionally generated by hypothesis
        self.sub_tasks: Sequence[ASpecificTask] = sub_tasks
        self.sub_workspace_list: list[ASpecificWSForSubTasks | None] = [None] * len(self.sub_tasks)
        # TODO:
        # It will be used in runner in history
        # If we implement the whole workflow, we don't have to use it, then we remove it.
        self.based_experiments: Sequence[ASpecificWSForExperiment] = based_experiments

        self.experiment_workspace: ASpecificWSForExperiment | None = None

        # The experiment may be developed by different developers.
        # Last feedback is used to propagate info to the next developer.
        # Life cycle:
        # - Developer assigns feedback for next component;
        # - Workflow control clears feedback.
        self.prop_dev_feedback: Feedback | None = None

        # TODO: (xiao) I think this is too concrete; we should move it into
        # NOTE: Assumption
        # - only runner will assign this variable
        # - We will always create a new Experiment without copying previous results when we goto the next new loop.
        self.result: object = None  # The result of the experiment, can be different types in different scenarios.
        self.sub_results: dict[str, float] = (
            {}
        )  # TODO: in Kaggle, now sub results are all saved in self.result, remove this in the future.


ASpecificExp = TypeVar("ASpecificExp", bound=Experiment)

TaskOrExperiment = TypeVar("TaskOrExperiment", Task, Experiment)


class Loader(ABC, Generic[TaskOrExperiment]):
    @abstractmethod
    def load(self, *args: Any, **kwargs: Any) -> TaskOrExperiment:
        err_msg = "load method is not implemented."
        raise NotImplementedError(err_msg)



================================================
File: rdagent/core/knowledge_base.py
================================================
from pathlib import Path

import dill as pickle  # type: ignore[import-untyped]

from rdagent.log import rdagent_logger as logger


class KnowledgeBase:
    def __init__(self, path: str | Path | None = None) -> None:
        self.path = Path(path) if path else None
        self.load()

    def load(self) -> None:
        if self.path is not None and self.path.exists():
            with self.path.open("rb") as f:
                loaded = pickle.load(f)
                if isinstance(loaded, dict):
                    self.__dict__.update({k: v for k, v in loaded.items() if k != "path"})
                else:
                    self.__dict__.update({k: v for k, v in loaded.__dict__.items() if k != "path"})

    def dump(self) -> None:
        if self.path is not None:
            self.path.parent.mkdir(parents=True, exist_ok=True)
            pickle.dump(self.__dict__, self.path.open("wb"))
        else:
            logger.warning("KnowledgeBase path is not set, dump failed.")



================================================
File: rdagent/core/prompts.py
================================================
from pathlib import Path

import yaml

from rdagent.core.utils import SingletonBaseClass


class Prompts(SingletonBaseClass, dict[str, str]):
    def __init__(self, file_path: Path) -> None:
        super().__init__()
        with file_path.open(encoding="utf8") as file:
            prompt_yaml_dict = yaml.safe_load(file)

        if prompt_yaml_dict is None:
            error_message = f"Failed to load prompts from {file_path}"
            raise ValueError(error_message)

        for key, value in prompt_yaml_dict.items():
            self[key] = value



================================================
File: rdagent/core/proposal.py
================================================
""" """

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Generic, TypeVar

from rdagent.core.evaluation import Feedback
from rdagent.core.experiment import ASpecificExp, Experiment
from rdagent.core.knowledge_base import KnowledgeBase
from rdagent.core.scenario import Scenario

if TYPE_CHECKING:
    from rdagent.core.prompts import Prompts

# class data_ana: XXX


class Hypothesis:
    """
    TODO: We may have better name for it.

    Name Candidates:
    - Belief
    """

    def __init__(
        self,
        hypothesis: str,
        reason: str,
        concise_reason: str,
        concise_observation: str,
        concise_justification: str,
        concise_knowledge: str,
    ) -> None:
        self.hypothesis: str = hypothesis
        self.reason: str = reason
        self.concise_reason: str = concise_reason
        self.concise_observation: str = concise_observation
        self.concise_justification: str = concise_justification
        self.concise_knowledge: str = concise_knowledge

    def __str__(self) -> str:
        return f"""Hypothesis: {self.hypothesis}
                Reason: {self.reason}
                Concise Reason & Knowledge: {self.concise_reason}
                Concise Observation: {self.concise_observation}
                Concise Justification: {self.concise_justification}
                Concise Knowledge: {self.concise_knowledge}
                """

    # source: data_ana | model_nan = None


# Origin(path of repo/data/feedback) => view/summarization => generated Hypothesis


class ExperimentFeedback(Feedback):
    def __init__(
        self,
        reason: str,
        *,
        decision: bool,
        exception: Exception | None = None,
    ) -> None:
        self.decision = decision
        self.reason = reason
        # Exception is not None means failing to generate runnable experiments due to exception.
        # Runable reuslts are not always good.
        self.exception: Exception | None = (
            exception  # if the experiment raises exception, it will be integrated into part of the feedback.
        )

    def __bool__(self) -> bool:
        return self.decision

    def __str__(self) -> str:
        return f"Decision: {self.decision}\nReason: {self.reason}"

    @classmethod
    def from_exception(cls, e: Exception) -> ExperimentFeedback:
        """
        A convenient method to create Feedback from an exception.
        """
        return cls(decision=False, reason=f"The experiment fails due to {e!s}", exception=e)


class HypothesisFeedback(ExperimentFeedback):
    def __init__(
        self,
        observations: str,
        hypothesis_evaluation: str,
        new_hypothesis: str,
        reason: str,
        *,
        decision: bool,
    ) -> None:
        super().__init__(reason, decision=decision)
        self.observations = observations
        self.hypothesis_evaluation = hypothesis_evaluation
        self.new_hypothesis = new_hypothesis

    def __str__(self) -> str:
        return f"""{super().__str__()}
Observations: {self.observations}
Hypothesis Evaluation: {self.hypothesis_evaluation}
New Hypothesis: {self.new_hypothesis}"""


ASpecificScen = TypeVar("ASpecificScen", bound=Scenario)
ASpecificKB = TypeVar("ASpecificKB", bound=KnowledgeBase)


class Trace(Generic[ASpecificScen, ASpecificKB]):
    def __init__(self, scen: ASpecificScen, knowledge_base: ASpecificKB | None = None) -> None:
        self.scen: ASpecificScen = scen
        self.hist: list[tuple[Experiment, ExperimentFeedback]] = []
        # TODO: self.hist is 2-tuple now, remove hypothesis from it, change old code for this later.
        self.knowledge_base: ASpecificKB | None = knowledge_base

    def get_sota_hypothesis_and_experiment(self) -> tuple[Hypothesis | None, Experiment | None]:
        """Access the last experiment result, sub-task, and the corresponding hypothesis."""
        # TODO: The return value does not align with the signature.
        for experiment, feedback in self.hist[::-1]:
            if feedback.decision:
                return experiment.hypothesis, experiment

        return None, None


class ExpGen(ABC):

    def __init__(self, scen: Scenario) -> None:
        self.scen = scen

    @abstractmethod
    def gen(self, trace: Trace) -> Experiment:
        """
        Generate the experiment based on the trace.

        `ExpGen().gen()` play a role like

        .. code-block:: python

            # ExpGen().gen() ==
            Hypothesis2Experiment().convert(
                HypothesisGen().gen(trace)
            )
        """


class HypothesisGen(ABC):
    # NOTE: the design is a little wierd
    # - Sometimes we want accurate access the prompts in a specific level
    #   - It renders the prompt to a specific abstract level
    # - Sometimes we want to access the most recent level prompts
    prompts: Prompts  # this is a class level prompt.

    def __init__(self, scen: Scenario) -> None:
        self.scen = scen

    @abstractmethod
    def gen(self, trace: Trace) -> Hypothesis:
        # def gen(self, scenario_desc: str, ) -> Hypothesis:
        """
        Motivation of the variable `scenario_desc`:
            - Mocking a data-scientist is observing the scenario.

        scenario_desc may include:
            - data observation:
                - Original or derivative
            - Task information:
        """


class Hypothesis2Experiment(ABC, Generic[ASpecificExp]):
    """
    [Abstract description => concrete description] => Code implementation Card
    """

    @abstractmethod
    def convert(self, hypothesis: Hypothesis, trace: Trace) -> ASpecificExp:
        """Connect the idea proposal to implementation"""
        ...


# Boolean, Reason, Confidence, etc.


class Experiment2Feedback(ABC):
    """ "Generated feedbacks on the hypothesis from **Executed** Implementations of different tasks
    & their comparisons with previous performances"""

    def __init__(self, scen: Scenario) -> None:
        self.scen = scen

    @abstractmethod
    def generate_feedback(self, exp: Experiment, trace: Trace) -> ExperimentFeedback:
        """
        The `exp` should be executed and the results should be included, as well as the comparison
        between previous results (done by LLM).
        For example: `mlflow` of Qlib will be included.
        """
        error_message = "generate_feedback method is not implemented."
        raise NotImplementedError(error_message)



================================================
File: rdagent/core/scenario.py
================================================
from abc import ABC, abstractmethod

from rdagent.core.experiment import Task


class Scenario(ABC):
    """
    We should include scenario information here. Following inform should not be included
    - method related (e.g. rag... config for a concrete module)
    """

    @property
    @abstractmethod
    def background(self) -> str:
        """Background information"""

    # TODO: We have to change all the sub classes to override get_source_data_desc instead of `source_data`
    def get_source_data_desc(self, task: Task | None = None) -> str:  # noqa: ARG002
        """
        Source data description

        The choice of data may vary based on the specific task at hand.
        """
        return ""

    @property
    def source_data(self) -> str:
        """
        A convenient shortcut for describing source data
        """
        return self.get_source_data_desc()

    # NOTE: we should keep the interface simpler. So some previous interfaces are deleted.
    # If we need some specific function only used in the subclass(no external usage).
    # We should not set them in the base class

    @property
    @abstractmethod
    def rich_style_description(self) -> str:
        """Rich style description to present"""

    @abstractmethod
    def get_scenario_all_desc(
        self,
        task: Task | None = None,
        filtered_tag: str | None = None,
        simple_background: bool | None = None,
    ) -> str:
        """
        Combine all descriptions together

        The scenario description varies based on the task being performed.
        """

    @property
    def experiment_setting(self) -> str | None:
        """Get experiment setting and return as rich text string"""
        return None



================================================
File: rdagent/core/utils.py
================================================
from __future__ import annotations

import functools
import importlib
import json
import multiprocessing as mp
import pickle
import random
from collections.abc import Callable
from pathlib import Path
from typing import Any, ClassVar, NoReturn, cast

from filelock import FileLock
from fuzzywuzzy import fuzz  # type: ignore[import-untyped]

from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.oai.llm_conf import LLM_SETTINGS


class RDAgentException(Exception):  # noqa: N818
    pass


class SingletonBaseClass:
    """
    Because we try to support defining Singleton with `class A(SingletonBaseClass)`
    instead of `A(metaclass=SingletonMeta)` this class becomes necessary.
    """

    _instance_dict: ClassVar[dict] = {}

    def __new__(cls, *args: Any, **kwargs: Any) -> Any:
        # Since it's hard to align the difference call using args and kwargs, we strictly ask to use kwargs in Singleton
        if args:
            # TODO: this restriction can be solved.
            exception_message = "Please only use kwargs in Singleton to avoid misunderstanding."
            raise RDAgentException(exception_message)
        class_name = [(-1, f"{cls.__module__}.{cls.__name__}")]
        args_l = [(i, args[i]) for i in args]
        kwargs_l = sorted(kwargs.items())
        all_args = class_name + args_l + kwargs_l
        kwargs_hash = hash(tuple(all_args))
        if kwargs_hash not in cls._instance_dict:
            cls._instance_dict[kwargs_hash] = super().__new__(cls)  # Corrected call
        return cls._instance_dict[kwargs_hash]

    def __reduce__(self) -> NoReturn:
        """
        NOTE:
        When loading an object from a pickle, the __new__ method does not receive the `kwargs`
        it was initialized with. This makes it difficult to retrieve the correct singleton object.
        Therefore, we have made it unpicklable.
        """
        msg = f"Instances of {self.__class__.__name__} cannot be pickled"
        raise pickle.PicklingError(msg)


def parse_json(response: str) -> Any:
    try:
        return json.loads(response)
    except json.decoder.JSONDecodeError:
        pass
    error_message = f"Failed to parse response: {response}, please report it or help us to fix it."
    raise ValueError(error_message)


def similarity(text1: str, text2: str) -> int:
    text1 = text1 if isinstance(text1, str) else ""
    text2 = text2 if isinstance(text2, str) else ""

    # Maybe we can use other similarity algorithm such as tfidf
    return cast(int, fuzz.ratio(text1, text2))  # mypy does not regard it as int


def import_class(class_path: str) -> Any:
    """
    Parameters
    ----------
    class_path : str
        class path like"scripts.factor_implementation.baselines.naive.one_shot.OneshotFactorGen"

    Returns
    -------
        class of `class_path`
    """
    module_path, class_name = class_path.rsplit(".", 1)
    module = importlib.import_module(module_path)
    return getattr(module, class_name)


class CacheSeedGen:
    """
    It is a global seed generator to generate a sequence of seeds.
    This will support the feature `use_auto_chat_cache_seed_gen` claim

    NOTE:
    - This seed is specifically for the cache and is different from a regular seed.
    - If the cache is removed, setting the same seed will not produce the same QA trace.
    """

    def __init__(self) -> None:
        self.set_seed(LLM_SETTINGS.init_chat_cache_seed)

    def set_seed(self, seed: int) -> None:
        random.seed(seed)

    def get_next_seed(self) -> int:
        """generate next random int"""
        return random.randint(0, 10000)  # noqa: S311


LLM_CACHE_SEED_GEN = CacheSeedGen()


def _subprocess_wrapper(f: Callable, seed: int, args: list) -> Any:
    """
    It is a function wrapper. To ensure the subprocess has a fixed start seed.
    """

    LLM_CACHE_SEED_GEN.set_seed(seed)
    return f(*args)


def multiprocessing_wrapper(func_calls: list[tuple[Callable, tuple]], n: int) -> list:
    """It will use multiprocessing to call the functions in func_calls with the given parameters.
    The results equals to `return  [f(*args) for f, args in func_calls]`
    It will not call multiprocessing if `n=1`

    NOTE:
    We cooperate with chat_cache_seed feature
    We ensure get the same seed trace even we have multiple number of seed

    Parameters
    ----------
    func_calls : List[Tuple[Callable, Tuple]]
        the list of functions and their parameters
    n : int
        the number of subprocesses

    Returns
    -------
    list

    """
    if n == 1 or max(1, min(n, len(func_calls))) == 1:
        return [f(*args) for f, args in func_calls]

    with mp.Pool(processes=max(1, min(n, len(func_calls)))) as pool:
        results = [
            pool.apply_async(_subprocess_wrapper, args=(f, LLM_CACHE_SEED_GEN.get_next_seed(), args))
            for f, args in func_calls
        ]
        return [result.get() for result in results]


def cache_with_pickle(hash_func: Callable, post_process_func: Callable | None = None, force: bool = False) -> Callable:
    """
    This decorator will cache the return value of the function with pickle.
    The cache key is generated by the hash_func. The hash function returns a string or None.
    If it returns None, the cache will not be used. The cache will be stored in the folder
    specified by RD_AGENT_SETTINGS.pickle_cache_folder_path_str with name hash_key.pkl.
    The post_process_func will be called with the original arguments and the cached result
    to give each caller a chance to process the cached result. The post_process_func should
    return the final result.

    Parameters
    ----------
    hash_func : Callable
        The function to generate the hash key for the cache.
    post_process_func : Callable | None, optional
        The function to process the cached result, by default None.
    force : bool, optional
        If True, the cache will be used even if RD_AGENT_SETTINGS.cache_with_pickle is False, by default False.
    """

    def cache_decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def cache_wrapper(*args: Any, **kwargs: Any) -> Any:
            if not RD_AGENT_SETTINGS.cache_with_pickle and not force:
                return func(*args, **kwargs)

            target_folder = Path(RD_AGENT_SETTINGS.pickle_cache_folder_path_str) / f"{func.__module__}.{func.__name__}"
            target_folder.mkdir(parents=True, exist_ok=True)
            hash_key = hash_func(*args, **kwargs)

            if hash_key is None:
                return func(*args, **kwargs)

            cache_file = target_folder / f"{hash_key}.pkl"
            lock_file = target_folder / f"{hash_key}.lock"

            if cache_file.exists():
                with cache_file.open("rb") as f:
                    cached_res = pickle.load(f)
                return post_process_func(*args, cached_res=cached_res, **kwargs) if post_process_func else cached_res

            if RD_AGENT_SETTINGS.use_file_lock:
                with FileLock(lock_file):
                    result = func(*args, **kwargs)
            else:
                result = func(*args, **kwargs)

            with cache_file.open("wb") as f:
                pickle.dump(result, f)

            return result

        return cache_wrapper

    return cache_decorator



================================================
File: rdagent/log/__init__.py
================================================
from rdagent.log.logger import RDAgentLog
from rdagent.log.utils import LogColors

rdagent_logger: RDAgentLog = RDAgentLog()



================================================
File: rdagent/log/base.py
================================================
from __future__ import annotations

from abc import abstractmethod
from collections.abc import Generator
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Literal, Optional, Union


@dataclass
class Message:
    """The info unit of the storage"""

    tag: str  # namespace like like a.b.c
    level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]  # The level of the logging
    timestamp: datetime  # The time when the message is generated
    caller: Optional[
        str
    ]  # The caller of the logging like `rdagent.oai.llm_utils:_create_chat_completion_inner_function:55`(file:func:line)
    pid_trace: Optional[str]  # The process id trace;  A-B-C represents A create B, B create C
    content: object  # The content


class Storage:
    """
    Basic storage to support saving objects;

    # Usage:

    The storage has mainly two kind of users:
    - The logging end: you can choose any of the following method to use the object
        - We can use it directly with the native logging storage
        - We can use it with other logging tools; For example, serve as a handler for loggers
    - The view end:
        - Mainly for the subclass of `logging.base.View`
        - It should provide two kind of ways to provide content
            - offline content provision.
            - online content preovision.
    """

    @abstractmethod
    def log(
        self,
        obj: object,
        name: str = "",
        save_type: Literal["json", "text", "pkl"] = "text",
        timestamp: datetime | None = None,
        **kwargs: dict,
    ) -> str | Path:
        """

        Parameters
        ----------
        obj : object
            The object for logging.
        name : str
            The name of the object.  For example "a.b.c"
            We may log a lot of objects to a same name

        Returns
        -------
        str | Path
            The storage identifier of the object.
        """
        ...

    @abstractmethod
    def iter_msg(self, watch: bool = False) -> Generator[Message, None, None]:
        """
        Parameters
        ----------
        watch : bool
            should we watch the new content and display them
        """
        ...


class View:
    """
    Motivation:

    Display the content in the storage
    """

    # TODO: pleas fix me
    @abstractmethod
    def display(self, s: Storage, watch: bool = False) -> None:
        """

        Parameters
        ----------
        s : Storage

        watch : bool
            should we watch the new content and display them
        """
        ...



================================================
File: rdagent/log/logger.py
================================================
import json
import os
import pickle
import sys
from contextlib import contextmanager
from datetime import datetime, timezone
from functools import partial
from logging import LogRecord
from multiprocessing import Pipe
from multiprocessing.connection import Connection
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, Generator, Union

from loguru import logger

if TYPE_CHECKING:
    from loguru import Record

from psutil import Process

from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.utils import SingletonBaseClass

from .storage import FileStorage
from .utils import LogColors, get_caller_info


class RDAgentLog(SingletonBaseClass):
    """
    The files are organized based on the tag & PID
    Here is an example tag

    .. code-block::

        a
        - b
        - c
            - 123
              - common_logs.log
            - 1322
              - common_logs.log
            - 1233
              - <timestamp>.pkl
            - d
                - 1233-673 ...
                - 1233-4563 ...
                - 1233-365 ...

    """

    # TODO: Simplify it to introduce less concepts ( We may merge RDAgentLog, Storage &)
    # Solution:  Storage => PipeLog, View => PipeLogView, RDAgentLog is an instance of PipeLogger
    # PipeLogger.info(...) ,  PipeLogger.get_resp() to get feedback from frontend.
    # def f():
    #   logger = PipeLog()
    #   logger.info("<code>")
    #   feedback = logger.get_reps()
    _tag: str = ""

    def __init__(self, log_trace_path: Union[str, None] = RD_AGENT_SETTINGS.log_trace_path) -> None:
        if log_trace_path is None:
            timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M-%S-%f")
            self.log_trace_path = Path.cwd() / "log" / timestamp
        else:
            self.log_trace_path = Path(log_trace_path)

        self.log_trace_path.mkdir(parents=True, exist_ok=True)

        self.storage = FileStorage(self.log_trace_path)

        self.main_pid = os.getpid()

    def set_trace_path(self, log_trace_path: str | Path) -> None:
        self.log_trace_path = Path(log_trace_path)
        self.storage = FileStorage(log_trace_path)

    @contextmanager
    def tag(self, tag: str) -> Generator[None, None, None]:
        if tag.strip() == "":
            raise ValueError("Tag cannot be empty.")
        if self._tag != "":
            tag = "." + tag

        # TODO: It may result in error in mutithreading or co-routine
        self._tag = self._tag + tag
        try:
            yield
        finally:
            self._tag = self._tag[: -len(tag)]

    def get_pids(self) -> str:
        """
        Returns a string of pids from the current process to the main process.
        Split by '-'.
        """
        pid = os.getpid()
        process = Process(pid)
        pid_chain = f"{pid}"
        while process.pid != self.main_pid:
            parent_pid = process.ppid()
            parent_process = Process(parent_pid)
            pid_chain = f"{parent_pid}-{pid_chain}"
            process = parent_process
        return pid_chain

    def file_format(self, record: "Record", raw: bool = False) -> str:
        # FIXME: the formmat is tightly coupled with the message reading in storage.
        record["message"] = LogColors.remove_ansi_codes(record["message"])
        if raw:
            return "{message}"
        return "{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}\n"

    def log_object(self, obj: object, *, tag: str = "") -> None:
        # TODO: I think we can merge the log_object function with other normal log methods to make the interface simpler.
        caller_info = get_caller_info()
        tag = f"{self._tag}.{tag}.{self.get_pids()}".strip(".")

        # FIXME: it looks like a hacking... We should redesign it...
        if "debug_" in tag:
            debug_log_path = self.log_trace_path / "debug_llm.pkl"
            debug_data = {"tag": tag, "obj": obj}
            if debug_log_path.exists():
                with debug_log_path.open("rb") as f:
                    existing_data = pickle.load(f)
                existing_data.append(debug_data)
                with debug_log_path.open("wb") as f:
                    pickle.dump(existing_data, f)
            else:
                with debug_log_path.open("wb") as f:
                    pickle.dump([debug_data], f)
            return

        logp = self.storage.log(obj, name=tag, save_type="pkl")

        file_handler_id = logger.add(
            self.log_trace_path / tag.replace(".", "/") / "common_logs.log", format=self.file_format
        )
        logger.patch(lambda r: r.update(caller_info)).info(f"Logging object in {Path(logp).absolute()}")
        logger.remove(file_handler_id)

    def info(self, msg: str, *, tag: str = "", raw: bool = False) -> None:
        # TODO: too much duplicated. due to we have no logger with stream context;
        caller_info = get_caller_info()
        if raw:
            logger.remove()
            logger.add(sys.stderr, format=lambda r: "{message}")

        tag = f"{self._tag}.{tag}.{self.get_pids()}".strip(".")
        log_file_path = self.log_trace_path / tag.replace(".", "/") / "common_logs.log"
        if raw:
            file_handler_id = logger.add(log_file_path, format=partial(self.file_format, raw=True))
        else:
            file_handler_id = logger.add(log_file_path, format=self.file_format)

        logger.patch(lambda r: r.update(caller_info)).info(msg)
        logger.remove(file_handler_id)

        if raw:
            logger.remove()
            logger.add(sys.stderr)

    def warning(self, msg: str, *, tag: str = "") -> None:
        # TODO: reuse code
        # _log(self, msg: str, *, tag: str = "", level=Literal["warning", "error", ..]) -> None:
        # getattr(logger.patch(lambda r: r.update(caller_info)), level)(msg)
        caller_info = get_caller_info()

        tag = f"{self._tag}.{tag}.{self.get_pids()}".strip(".")
        file_handler_id = logger.add(
            self.log_trace_path / tag.replace(".", "/") / "common_logs.log", format=self.file_format
        )
        logger.patch(lambda r: r.update(caller_info)).warning(msg)
        logger.remove(file_handler_id)

    def error(self, msg: str, *, tag: str = "") -> None:
        caller_info = get_caller_info()

        tag = f"{self._tag}.{tag}.{self.get_pids()}".strip(".")
        file_handler_id = logger.add(
            self.log_trace_path / tag.replace(".", "/") / "common_logs.log", format=self.file_format
        )
        logger.patch(lambda r: r.update(caller_info)).error(msg)
        logger.remove(file_handler_id)



================================================
File: rdagent/log/mle_summary.py
================================================
import json
import re
from collections import defaultdict
from pathlib import Path

import fire
import pandas as pd

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.experiment import FBWorkspace
from rdagent.core.proposal import ExperimentFeedback
from rdagent.log.storage import FileStorage
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.kaggle.kaggle_crawler import score_rank
from rdagent.utils.env import DockerEnv, MLEBDockerConf

de = get_ds_env("mlebench")
de.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/zip_files": "/mle/data"}
de.prepare()


def extract_mle_json(log_content: str) -> dict | None:
    match = re.search(r"\{.*\}", log_content, re.DOTALL)
    if match:
        return json.loads(match.group(0))
    return None


def extract_loopid_func_name(tag):
    """提取 Loop ID 和函数名称"""
    match = re.search(r"Loop_(\d+)\.([^.]+)", tag)
    return match.groups() if match else (None, None)


def save_grade_info(log_trace_path: Path):
    trace_storage = FileStorage(log_trace_path)
    for msg in trace_storage.iter_msg():
        if "competition" in msg.tag:
            competition = msg.content

        if "running" in msg.tag:
            if isinstance(msg.content, DSExperiment):
                mle_score_str = msg.content.experiment_workspace.execute(
                    env=de,
                    entry=f"mlebench grade-sample submission.csv {competition} --data-dir /mle/data | tee mle_score.txt",
                )
                msg.content.experiment_workspace.execute(env=de, entry="chmod 777 mle_score.txt")
                trace_storage.log(
                    mle_score_str, name=f"{msg.tag}.mle_score.pid", save_type="pkl", timestamp=msg.timestamp
                )


def is_valid_session(p: Path) -> bool:
    return p.is_dir() and p.joinpath("__session__").exists()


def save_all_grade_info(log_folder):
    for log_trace_path in log_folder.iterdir():
        if is_valid_session(log_trace_path):
            save_grade_info(log_trace_path)


def summarize_folder(log_folder: Path):
    log_folder = Path(log_folder)
    stat = defaultdict(dict)
    for log_trace_path in log_folder.iterdir():  # One log trace
        if not is_valid_session(log_trace_path):
            continue
        loop_num = 0
        made_submission_num = 0
        valid_submission_num = 0
        above_median_num = 0
        get_medal_num = 0
        bronze_num = 0
        silver_num = 0
        gold_num = 0
        test_scores = {}
        test_ranks = {}
        valid_scores = {}
        bronze_threshold = 0.0
        silver_threshold = 0.0
        gold_threshold = 0.0
        median_threshold = 0.0
        success_loop_num = 0

        sota_exp_stat = ""
        sota_exp_score = None
        sota_exp_rank = None
        grade_output = None
        for msg in FileStorage(log_trace_path).iter_msg():  # messages in log trace
            if msg.tag and "llm" not in msg.tag and "session" not in msg.tag:
                if "competition" in msg.tag:
                    stat[log_trace_path.name]["competition"] = msg.content

                    # get threshold scores
                    workflowexp = FBWorkspace()
                    stdout = workflowexp.execute(
                        env=de,
                        entry=f"mlebench grade-sample None {stat[log_trace_path.name]['competition']} --data-dir /mle/data",
                    )
                    grade_output = extract_mle_json(stdout)
                    if grade_output:
                        bronze_threshold = grade_output["bronze_threshold"]
                        silver_threshold = grade_output["silver_threshold"]
                        gold_threshold = grade_output["gold_threshold"]
                        median_threshold = grade_output["median_threshold"]

                if "direct_exp_gen" in msg.tag and isinstance(msg.content, DSExperiment):
                    loop_num += 1

                if "running" in msg.tag:
                    if isinstance(msg.content, DSExperiment):
                        submission_path = msg.content.experiment_workspace.workspace_path / "submission.csv"
                        if submission_path.exists():
                            made_submission_num += 1
                            scores_path = msg.content.experiment_workspace.workspace_path / "scores.csv"
                            valid_scores[loop_num - 1] = pd.read_csv(scores_path, index_col=0)
                    elif "mle_score" in msg.tag:
                        loop_id, _ = extract_loopid_func_name(msg.tag)
                        loop_id = int(loop_id)
                        grade_output = extract_mle_json(msg.content)
                        if grade_output:
                            if grade_output["score"] is not None:
                                test_scores[loop_id + 1] = grade_output["score"]
                                _, test_ranks[loop_id + 1] = score_rank(
                                    stat[log_trace_path.name]["competition"], grade_output["score"]
                                )
                            if grade_output["valid_submission"]:
                                valid_submission_num += 1
                            if grade_output["above_median"]:
                                above_median_num += 1
                            if grade_output["any_medal"]:
                                get_medal_num += 1
                            if grade_output["bronze_medal"]:
                                bronze_num += 1
                            if grade_output["silver_medal"]:
                                silver_num += 1
                            if grade_output["gold_medal"]:
                                gold_num += 1

                if "feedback" in msg.tag and "evolving" not in msg.tag:
                    if isinstance(msg.content, ExperimentFeedback) and bool(msg.content):
                        success_loop_num += 1

                        if grade_output:  # sota exp's grade output
                            if grade_output["gold_medal"]:
                                sota_exp_stat = "gold"
                            elif grade_output["silver_medal"]:
                                sota_exp_stat = "silver"
                            elif grade_output["bronze_medal"]:
                                sota_exp_stat = "bronze"
                            elif grade_output["above_median"]:
                                sota_exp_stat = "above_median"
                            elif grade_output["valid_submission"]:
                                sota_exp_stat = "valid_submission"
                            elif grade_output["submission_exists"]:
                                sota_exp_stat = "made_submission"
                            if grade_output["score"] is not None:
                                sota_exp_score = grade_output["score"]
                                _, sota_exp_rank = score_rank(
                                    stat[log_trace_path.name]["competition"], grade_output["score"]
                                )

        stat[log_trace_path.name].update(
            {
                "loop_num": loop_num,
                "made_submission_num": made_submission_num,
                "valid_submission_num": valid_submission_num,
                "above_median_num": above_median_num,
                "get_medal_num": get_medal_num,
                "bronze_num": bronze_num,
                "silver_num": silver_num,
                "gold_num": gold_num,
                "test_scores": test_scores,
                "test_ranks": test_ranks,
                "valid_scores": valid_scores,
                "success_loop_num": success_loop_num,
                "sota_exp_stat": sota_exp_stat,
                "sota_exp_score": sota_exp_score,
                "sota_exp_rank": sota_exp_rank,
                "bronze_threshold": bronze_threshold,
                "silver_threshold": silver_threshold,
                "gold_threshold": gold_threshold,
                "median_threshold": median_threshold,
            }
        )
    if (log_folder / "summary.pkl").exists():
        (log_folder / "summary.pkl").unlink()
        print("Old summary file removed.")
    pd.to_pickle(stat, log_folder / "summary.pkl")


# {
#     "competition_id": "stanford-covid-vaccine",
#     "score": null,
#     "gold_threshold": 0.34728,
#     "silver_threshold": 0.35175,
#     "bronze_threshold": 0.3534,
#     "median_threshold": 0.363095,
#     "any_medal": false,
#     "gold_medal": false,
#     "silver_medal": false,
#     "bronze_medal": false,
#     "above_median": false,
#     "submission_exists": true,
#     "valid_submission": false,
#     "is_lower_better": true,
#     "created_at": "2025-01-21T11:59:33.788201",
#     "submission_path": "submission.csv"
# }


def grade_summary(log_folder):
    log_folder = Path(log_folder)
    save_all_grade_info(log_folder)
    summarize_folder(log_folder)


if __name__ == "__main__":
    fire.Fire(
        {
            "grade": save_all_grade_info,
            "summary": summarize_folder,
            "grade_summary": grade_summary,
        }
    )



================================================
File: rdagent/log/storage.py
================================================
import json
import pickle
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Generator, Literal, Union, cast

from .base import Message, Storage

LOG_LEVEL = Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]


class FileStorage(Storage):
    """
    The info are logginged to the file systems

    TODO: describe the storage format
    """

    def __init__(self, path: str | Path = "./log/") -> None:
        self.path = Path(path)
        self.path.mkdir(parents=True, exist_ok=True)

    def log(
        self,
        obj: object,
        name: str = "",
        save_type: Literal["json", "text", "pkl"] = "text",
        timestamp: datetime | None = None,
        **kwargs: Any,
    ) -> Union[str, Path]:
        # TODO: We can remove the timestamp after we implement PipeLog
        if timestamp is None:
            timestamp = datetime.now(timezone.utc)
        else:
            timestamp = timestamp.astimezone(timezone.utc)

        cur_p = self.path / name.replace(".", "/")
        cur_p.mkdir(parents=True, exist_ok=True)

        path = cur_p / f"{timestamp.strftime('%Y-%m-%d_%H-%M-%S-%f')}.log"

        if save_type == "json":
            path = path.with_suffix(".json")
            with path.open("w") as f:
                try:
                    json.dump(obj, f)
                except TypeError:
                    json.dump(json.loads(str(obj)), f)
            return path
        elif save_type == "pkl":
            path = path.with_suffix(".pkl")
            with path.open("wb") as f:
                pickle.dump(obj, f)
            return path
        elif save_type == "text":
            obj = str(obj)
            with path.open("w") as f:
                f.write(obj)
            return path

    log_pattern = re.compile(
        r"(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}) \| "
        r"(?P<level>DEBUG|INFO|WARNING|ERROR|CRITICAL) *\| "
        r"(?P<caller>.+:.+:\d+) - "
    )

    def iter_msg(self, watch: bool = False) -> Generator[Message, None, None]:
        msg_l = []
        for file in self.path.glob("**/*.log"):
            tag = ".".join(file.relative_to(self.path).as_posix().replace("/", ".").split(".")[:-3])
            pid = file.parent.name

            with file.open("r", encoding="utf-8") as f:
                content = f.read()

            matches, next_matches = self.log_pattern.finditer(content), self.log_pattern.finditer(content)
            next_match = next(next_matches, None)
            # NOTE: the content will be the text between `match` and `next_match`
            for match in matches:
                next_match = next(next_matches, None)

                timestamp_str = match.group("timestamp")
                timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S.%f").replace(tzinfo=timezone.utc)
                level: LOG_LEVEL = cast(LOG_LEVEL, match.group("level"))
                caller = match.group("caller")

                # Extract the message content
                message_start = match.end()
                message_end = next_match.start() if next_match else len(content)
                message_content = content[message_start:message_end].strip()

                if "Logging object in" in message_content:
                    continue

                m = Message(
                    tag=tag, level=level, timestamp=timestamp, caller=caller, pid_trace=pid, content=message_content
                )

                msg_l.append(m)

        for file in self.path.glob("**/*.pkl"):
            if file.name == "debug_llm.pkl":
                continue
            tag = ".".join(file.relative_to(self.path).as_posix().replace("/", ".").split(".")[:-3])
            pid = file.parent.name

            with file.open("rb") as f:
                content = pickle.load(f)

            timestamp = datetime.strptime(file.stem, "%Y-%m-%d_%H-%M-%S-%f").replace(tzinfo=timezone.utc)

            m = Message(tag=tag, level="INFO", timestamp=timestamp, caller="", pid_trace=pid, content=content)

            msg_l.append(m)

        msg_l.sort(key=lambda x: x.timestamp)
        for m in msg_l:
            yield m

    def truncate(self, time: datetime) -> None:
        # any message later than `time` will be removed
        for file in self.path.glob("**/*.log"):
            with file.open("r") as f:
                content = f.read()

            new_content = ""

            matches, next_matches = self.log_pattern.finditer(content), self.log_pattern.finditer(content)

            next_match = next(next_matches, None)
            for match in matches:
                next_match = next(next_matches, None)
                timestamp_str = match.group("timestamp")
                timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S.%f").replace(tzinfo=timezone.utc)

                log_start = match.start()
                log_end = next_match.start() if next_match else len(content)
                msg = content[match.end() : log_end].strip()

                if timestamp > time:
                    if "Logging object in" in msg:
                        absolute_p = msg.split("Logging object in ")[1]
                        p = Path(absolute_p)
                        if p.exists():
                            p.unlink()
                        else:
                            print(f"Missing pickle object: {p}.")
                    continue

                new_content += content[log_start:log_end]
            with file.open("w") as f:
                f.write(new_content)



================================================
File: rdagent/log/utils.py
================================================
import inspect
import re
from typing import Dict, Optional, TypedDict, Union


class LogColors:
    """
    ANSI color codes for use in console output.
    """

    RED = "\033[91m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    MAGENTA = "\033[95m"
    CYAN = "\033[96m"
    WHITE = "\033[97m"
    GRAY = "\033[90m"
    BLACK = "\033[30m"

    BOLD = "\033[1m"
    ITALIC = "\033[3m"

    END = "\033[0m"

    @classmethod
    def get_all_colors(cls: type["LogColors"]) -> list:
        names = dir(cls)
        names = [name for name in names if not name.startswith("__") and not callable(getattr(cls, name))]
        return [getattr(cls, name) for name in names]

    def render(self, text: str, color: str = "", style: str = "") -> str:
        """
        render text by input color and style.
        It's not recommend that input text is already rendered.
        """
        # This method is called too frequently, which is not good.
        colors = self.get_all_colors()
        # Perhaps color and font should be distinguished here.
        if color and color in colors:
            error_message = f"color should be in: {colors} but now is: {color}"
            raise ValueError(error_message)
        if style and style in colors:
            error_message = f"style should be in: {colors} but now is: {style}"
            raise ValueError(error_message)

        text = f"{color}{text}{self.END}"

        return f"{style}{text}{self.END}"

    @staticmethod
    def remove_ansi_codes(s: str) -> str:
        """
        It is for removing ansi ctrl characters in the string(e.g. colored text)
        """
        ansi_escape = re.compile(r"\x1B\[[0-?]*[ -/]*[@-~]")
        return ansi_escape.sub("", s)


class CallerInfo(TypedDict):
    function: str
    line: int
    name: Optional[str]


def get_caller_info() -> CallerInfo:
    # Get the current stack information
    stack = inspect.stack()
    # The second element is usually the caller's information
    caller_info = stack[2]
    frame = caller_info[0]
    info: CallerInfo = {
        "line": caller_info.lineno,
        "name": frame.f_globals["__name__"],  # Get the module name from the frame's globals
        "function": frame.f_code.co_name,  # Get the caller's function name
    }
    return info



================================================
File: rdagent/log/ui/__init__.py
================================================
"""
UI is a kind of view for user.

We are not sure how generality of the UI, we can't make decision among following options:
- in general folder like rdagent/log/ui
- It is for specific scenario rdagent/scenarios/
"""



================================================
File: rdagent/log/ui/app.py
================================================
import argparse
import re
import textwrap
from collections import defaultdict
from datetime import datetime, timezone
from importlib.resources import files as rfiles
from pathlib import Path
from typing import Callable, Type

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from plotly.subplots import make_subplots
from streamlit import session_state as state
from streamlit_theme import st_theme

from rdagent.components.coder.factor_coder.evaluators import FactorSingleFeedback
from rdagent.components.coder.factor_coder.factor import FactorFBWorkspace, FactorTask
from rdagent.components.coder.model_coder.evaluators import ModelSingleFeedback
from rdagent.components.coder.model_coder.model import ModelFBWorkspace, ModelTask
from rdagent.core.proposal import Hypothesis, HypothesisFeedback
from rdagent.core.scenario import Scenario
from rdagent.log.base import Message
from rdagent.log.storage import FileStorage
from rdagent.log.ui.qlib_report_figure import report_figure
from rdagent.scenarios.data_mining.experiment.model_experiment import DMModelScenario
from rdagent.scenarios.general_model.scenario import GeneralModelScenario
from rdagent.scenarios.kaggle.experiment.scenario import KGScenario
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorScenario
from rdagent.scenarios.qlib.experiment.factor_from_report_experiment import (
    QlibFactorFromReportScenario,
)
from rdagent.scenarios.qlib.experiment.model_experiment import (
    QlibModelExperiment,
    QlibModelScenario,
)

st.set_page_config(layout="wide", page_title="RD-Agent", page_icon="🎓", initial_sidebar_state="expanded")


# 获取log_path参数
parser = argparse.ArgumentParser(description="RD-Agent Streamlit App")
parser.add_argument("--log_dir", type=str, help="Path to the log directory")
parser.add_argument("--debug", action="store_true", help="Enable debug mode")
args = parser.parse_args()
if args.log_dir:
    main_log_path = Path(args.log_dir)
    if not main_log_path.exists():
        st.error(f"Log dir `{main_log_path}` does not exist!")
        st.stop()
else:
    main_log_path = None


QLIB_SELECTED_METRICS = [
    "IC",
    "1day.excess_return_without_cost.annualized_return",
    "1day.excess_return_without_cost.information_ratio",
    "1day.excess_return_without_cost.max_drawdown",
]

SIMILAR_SCENARIOS = (QlibModelScenario, DMModelScenario, QlibFactorScenario, QlibFactorFromReportScenario, KGScenario)


def filter_log_folders(main_log_path):
    """
    The webpage only displays valid folders.
    If the __session__ folder exists in a subfolder of the log folder, it is considered a valid folder,
    otherwise it is considered an invalid folder.
    """
    folders = [
        folder.relative_to(main_log_path)
        for folder in main_log_path.iterdir()
        if folder.is_dir() and folder.joinpath("__session__").exists() and folder.joinpath("__session__").is_dir()
    ]
    folders = sorted(folders, key=lambda x: x.name)
    return folders


if "log_path" not in state:
    if main_log_path:
        state.log_path = filter_log_folders(main_log_path)[0]
    else:
        state.log_path = None
        st.toast(":red[**Please Set Log Path!**]", icon="⚠️")

if "scenario" not in state:
    state.scenario = None

if "fs" not in state:
    state.fs = None

if "msgs" not in state:
    state.msgs = defaultdict(lambda: defaultdict(list))

if "last_msg" not in state:
    state.last_msg = None

if "current_tags" not in state:
    state.current_tags = []

if "lround" not in state:
    state.lround = 0  # RD Loop Round

if "times" not in state:
    state.times = defaultdict(lambda: defaultdict(list))

if "erounds" not in state:
    state.erounds = defaultdict(int)  # Evolving Rounds in each RD Loop

if "e_decisions" not in state:
    state.e_decisions = defaultdict(lambda: defaultdict(tuple))

# Summary Info
if "hypotheses" not in state:
    # Hypotheses in each RD Loop
    state.hypotheses = defaultdict(None)

if "h_decisions" not in state:
    state.h_decisions = defaultdict(bool)

if "metric_series" not in state:
    state.metric_series = []

# Factor Task Baseline
if "alpha158_metrics" not in state:
    state.alpha158_metrics = None


def should_display(msg: Message):
    for t in state.excluded_tags:
        if t in msg.tag.split("."):
            return False

    if type(msg.content).__name__ in state.excluded_types:
        return False

    return True


def get_msgs_until(end_func: Callable[[Message], bool] = lambda _: True):
    if state.fs:
        while True:
            try:
                msg = next(state.fs)

                # new scenario gen this tags, old version UI not have these tags.
                msg.tag = re.sub(r"\.evo_loop_\d+", "", msg.tag)
                msg.tag = re.sub(r"Loop_\d+\.[^.]+", "", msg.tag)
                msg.tag = re.sub(r"\.\.", ".", msg.tag)
                msg.tag = msg.tag.strip(".")

                if should_display(msg):
                    tags = msg.tag.split(".")
                    if "r" not in state.current_tags and "r" in tags:
                        state.lround += 1
                    if "evolving code" not in state.current_tags and "evolving code" in tags:
                        state.erounds[state.lround] += 1

                    state.current_tags = tags
                    state.last_msg = msg

                    # Update Summary Info
                    if "model runner result" in tags or "factor runner result" in tags or "runner result" in tags:
                        # factor baseline exp metrics
                        if isinstance(state.scenario, QlibFactorScenario) and state.alpha158_metrics is None:
                            sms = msg.content.based_experiments[0].result.loc[QLIB_SELECTED_METRICS]
                            sms.name = "alpha158"
                            state.alpha158_metrics = sms

                        if (
                            state.lround == 1
                            and len(msg.content.based_experiments) > 0
                            and msg.content.based_experiments[-1].result is not None
                        ):
                            sms = msg.content.based_experiments[-1].result
                            if isinstance(state.scenario, DMModelScenario):
                                sms.index = ["AUROC"]
                            elif isinstance(
                                state.scenario, (QlibModelScenario, QlibFactorFromReportScenario, QlibFactorScenario)
                            ):
                                sms = sms.loc[QLIB_SELECTED_METRICS]
                            sms.name = f"Baseline"
                            state.metric_series.append(sms)

                        # common metrics
                        if msg.content.result is None:
                            if isinstance(state.scenario, DMModelScenario):
                                state.metric_series.append(
                                    pd.Series([None], index=["AUROC"], name=f"Round {state.lround}")
                                )
                        else:
                            sms = msg.content.result
                            if isinstance(state.scenario, DMModelScenario):
                                sms.index = ["AUROC"]
                            elif isinstance(
                                state.scenario, (QlibModelScenario, QlibFactorFromReportScenario, QlibFactorScenario)
                            ):
                                sms = sms.loc[QLIB_SELECTED_METRICS]

                            sms.name = f"Round {state.lround}"
                            state.metric_series.append(sms)
                    elif "hypothesis generation" in tags:
                        state.hypotheses[state.lround] = msg.content
                    elif "ef" in tags and "feedback" in tags:
                        state.h_decisions[state.lround] = msg.content.decision
                    elif "d" in tags:
                        if "evolving code" in tags:
                            msg.content = [i for i in msg.content if i]
                        if "evolving feedback" in tags:
                            total_len = len(msg.content)
                            msg.content = [i for i in msg.content if i]
                            none_num = total_len - len(msg.content)
                            if len(msg.content) != len(state.msgs[state.lround]["d.evolving code"][-1].content):
                                st.toast(":red[**Evolving Feedback Length Error!**]", icon="‼️")
                            right_num = 0
                            for wsf in msg.content:
                                if wsf.final_decision:
                                    right_num += 1
                            wrong_num = len(msg.content) - right_num
                            state.e_decisions[state.lround][state.erounds[state.lround]] = (
                                right_num,
                                wrong_num,
                                none_num,
                            )

                    state.msgs[state.lround][msg.tag].append(msg)

                    # Update Times
                    if "init" in tags:
                        state.times[state.lround]["init"].append(msg.timestamp)
                    if "r" in tags:
                        state.times[state.lround]["r"].append(msg.timestamp)
                    if "d" in tags:
                        state.times[state.lround]["d"].append(msg.timestamp)
                    if "ef" in tags:
                        state.times[state.lround]["ef"].append(msg.timestamp)

                    # Stop Getting Logs
                    if end_func(msg):
                        break
            except StopIteration:
                st.toast(":red[**No More Logs to Show!**]", icon="🛑")
                break


def refresh(same_trace: bool = False):
    if state.log_path is None:
        st.toast(":red[**Please Set Log Path!**]", icon="⚠️")
        return

    if main_log_path:
        state.fs = FileStorage(main_log_path / state.log_path).iter_msg()
    else:
        state.fs = FileStorage(state.log_path).iter_msg()

    # detect scenario
    if not same_trace:
        get_msgs_until(lambda m: not isinstance(m.content, str))
        if state.last_msg is None or not isinstance(state.last_msg.content, Scenario):
            st.toast(":red[**No Scenario Info detected**]", icon="❗")
            state.scenario = None
        else:
            state.scenario = state.last_msg.content
            st.toast(f":green[**Scenario Info detected**] *{type(state.scenario).__name__}*", icon="✅")

    state.msgs = defaultdict(lambda: defaultdict(list))
    state.lround = 0
    state.erounds = defaultdict(int)
    state.e_decisions = defaultdict(lambda: defaultdict(tuple))
    state.hypotheses = defaultdict(None)
    state.h_decisions = defaultdict(bool)
    state.metric_series = []
    state.last_msg = None
    state.current_tags = []
    state.alpha158_metrics = None
    state.times = defaultdict(lambda: defaultdict(list))


def evolving_feedback_window(wsf: FactorSingleFeedback | ModelSingleFeedback):
    if isinstance(wsf, FactorSingleFeedback):
        ffc, efc, cfc, vfc = st.tabs(
            ["**Final Feedback🏁**", "Execution Feedback🖥️", "Code Feedback📄", "Value Feedback🔢"]
        )
        with ffc:
            st.markdown(wsf.final_feedback)
        with efc:
            st.code(wsf.execution_feedback, language="log")
        with cfc:
            st.markdown(wsf.code_feedback)
        with vfc:
            st.markdown(wsf.value_feedback)
    elif isinstance(wsf, ModelSingleFeedback):
        ffc, efc, cfc, msfc, vfc = st.tabs(
            [
                "**Final Feedback🏁**",
                "Execution Feedback🖥️",
                "Code Feedback📄",
                "Model Shape Feedback📐",
                "Value Feedback🔢",
            ]
        )
        with ffc:
            st.markdown(wsf.final_feedback)
        with efc:
            st.code(wsf.execution_feedback, language="log")
        with cfc:
            st.markdown(wsf.code_feedback)
        with msfc:
            st.markdown(wsf.shape_feedback)
        with vfc:
            st.markdown(wsf.value_feedback)


def display_hypotheses(hypotheses: dict[int, Hypothesis], decisions: dict[int, bool], success_only: bool = False):
    name_dict = {
        "hypothesis": "RD-Agent proposes the hypothesis⬇️",
        "concise_justification": "because the reason⬇️",
        "concise_observation": "based on the observation⬇️",
        "concise_knowledge": "Knowledge⬇️ gained after practice",
    }
    if success_only:
        shd = {k: v.__dict__ for k, v in hypotheses.items() if decisions[k]}
    else:
        shd = {k: v.__dict__ for k, v in hypotheses.items()}
    df = pd.DataFrame(shd).T

    if "concise_observation" in df.columns and "concise_justification" in df.columns:
        df["concise_observation"], df["concise_justification"] = df["concise_justification"], df["concise_observation"]
        df.rename(
            columns={"concise_observation": "concise_justification", "concise_justification": "concise_observation"},
            inplace=True,
        )
    if "reason" in df.columns:
        df.drop(["reason"], axis=1, inplace=True)
    if "concise_reason" in df.columns:
        df.drop(["concise_reason"], axis=1, inplace=True)

    df.columns = df.columns.map(lambda x: name_dict.get(x, x))

    def style_rows(row):
        if decisions[row.name]:
            return ["color: green;"] * len(row)
        return [""] * len(row)

    def style_columns(col):
        if col.name != name_dict.get("hypothesis", "hypothesis"):
            return ["font-style: italic;"] * len(col)
        return ["font-weight: bold;"] * len(col)

    # st.dataframe(df.style.apply(style_rows, axis=1).apply(style_columns, axis=0))
    st.markdown(df.style.apply(style_rows, axis=1).apply(style_columns, axis=0).to_html(), unsafe_allow_html=True)


def metrics_window(df: pd.DataFrame, R: int, C: int, *, height: int = 300, colors: list[str] = None):
    fig = make_subplots(rows=R, cols=C, subplot_titles=df.columns)

    def hypothesis_hover_text(h: Hypothesis, d: bool = False):
        color = "green" if d else "black"
        text = h.hypothesis
        lines = textwrap.wrap(text, width=60)
        return f"<span style='color: {color};'>{'<br>'.join(lines)}</span>"

    hover_texts = [
        hypothesis_hover_text(state.hypotheses[int(i[6:])], state.h_decisions[int(i[6:])])
        for i in df.index
        if i != "alpha158" and i != "Baseline"
    ]
    if state.alpha158_metrics is not None:
        hover_texts = ["Baseline: alpha158"] + hover_texts
    for ci, col in enumerate(df.columns):
        row = ci // C + 1
        col_num = ci % C + 1
        fig.add_trace(
            go.Scatter(
                x=df.index,
                y=df[col],
                name=col,
                mode="lines+markers",
                connectgaps=True,
                marker=dict(size=10, color=colors[ci]) if colors else dict(size=10),
                hovertext=hover_texts,
                hovertemplate="%{hovertext}<br><br><span style='color: black'>%{x} Value:</span> <span style='color: blue'>%{y}</span><extra></extra>",
            ),
            row=row,
            col=col_num,
        )
    fig.update_layout(showlegend=False, height=height)

    if state.alpha158_metrics is not None:
        for i in range(1, R + 1):  # 行
            for j in range(1, C + 1):  # 列
                fig.update_xaxes(
                    tickvals=[df.index[0]] + list(df.index[1:]),
                    ticktext=[f'<span style="color:blue; font-weight:bold">{df.index[0]}</span>'] + list(df.index[1:]),
                    row=i,
                    col=j,
                )
    st.plotly_chart(fig)


def summary_window():
    if isinstance(state.scenario, SIMILAR_SCENARIOS):
        st.header("Summary📊", divider="rainbow", anchor="_summary")
        if state.lround == 0:
            return
        with st.container():
            # TODO: not fixed height
            with st.container():
                bc, cc = st.columns([2, 2], vertical_alignment="center")
                with bc:
                    st.subheader("Metrics📈", anchor="_metrics")
                with cc:
                    show_true_only = st.toggle("successful hypotheses", value=False)

            # hypotheses_c, chart_c = st.columns([2, 3])
            chart_c = st.container()
            hypotheses_c = st.container()

            with hypotheses_c:
                st.subheader("Hypotheses🏅", anchor="_hypotheses")
                display_hypotheses(state.hypotheses, state.h_decisions, show_true_only)

            with chart_c:
                if isinstance(state.scenario, QlibFactorScenario) and state.alpha158_metrics is not None:
                    df = pd.DataFrame([state.alpha158_metrics] + state.metric_series)
                else:
                    df = pd.DataFrame(state.metric_series)
                if show_true_only and len(state.hypotheses) >= len(state.metric_series):
                    if state.alpha158_metrics is not None:
                        selected = ["alpha158"] + [i for i in df.index if state.h_decisions[int(i[6:])]]
                    else:
                        selected = [i for i in df.index if i == "Baseline" or state.h_decisions[int(i[6:])]]
                    df = df.loc[selected]
                if df.shape[0] == 1:
                    st.table(df.iloc[0])
                elif df.shape[0] > 1:
                    if df.shape[1] == 1:
                        fig = px.line(df, x=df.index, y=df.columns, markers=True)
                        fig.update_layout(xaxis_title="Loop Round", yaxis_title=None)
                        st.plotly_chart(fig)
                    else:
                        metrics_window(df, 1, 4, height=300, colors=["red", "blue", "orange", "green"])

    elif isinstance(state.scenario, GeneralModelScenario):
        with st.container(border=True):
            st.subheader("Summary📊", divider="rainbow", anchor="_summary")
            if len(state.msgs[state.lround]["d.evolving code"]) > 0:
                # pass
                ws: list[FactorFBWorkspace | ModelFBWorkspace] = state.msgs[state.lround]["d.evolving code"][-1].content
                # All Tasks

                tab_names = [
                    w.target_task.factor_name if isinstance(w.target_task, FactorTask) else w.target_task.name
                    for w in ws
                ]
                for j in range(len(ws)):
                    if state.msgs[state.lround]["d.evolving feedback"][-1].content[j].final_decision:
                        tab_names[j] += "✔️"
                    else:
                        tab_names[j] += "❌"

                wtabs = st.tabs(tab_names)
                for j, w in enumerate(ws):
                    with wtabs[j]:
                        # Evolving Code
                        for k, v in w.file_dict.items():
                            with st.expander(f":green[`{k}`]", expanded=False):
                                st.code(v, language="python")

                        # Evolving Feedback
                        evolving_feedback_window(state.msgs[state.lround]["d.evolving feedback"][-1].content[j])


def tabs_hint():
    st.markdown(
        "<p style='font-size: small; color: #888888;'>You can navigate through the tabs using ⬅️ ➡️ or by holding Shift and scrolling with the mouse wheel🖱️.</p>",
        unsafe_allow_html=True,
    )


def tasks_window(tasks: list[FactorTask | ModelTask]):
    if isinstance(tasks[0], FactorTask):
        st.markdown("**Factor Tasks🚩**")
        tnames = [f.factor_name for f in tasks]
        if sum(len(tn) for tn in tnames) > 100:
            tabs_hint()
        tabs = st.tabs(tnames)
        for i, ft in enumerate(tasks):
            with tabs[i]:
                # st.markdown(f"**Factor Name**: {ft.factor_name}")
                st.markdown(f"**Description**: {ft.factor_description}")
                st.latex("Formulation")
                st.latex(ft.factor_formulation)

                mks = "| Variable | Description |\n| --- | --- |\n"
                if isinstance(ft.variables, dict):
                    for v, d in ft.variables.items():
                        mks += f"| ${v}$ | {d} |\n"
                    st.markdown(mks)

    elif isinstance(tasks[0], ModelTask):
        st.markdown("**Model Tasks🚩**")
        tnames = [m.name for m in tasks]
        if sum(len(tn) for tn in tnames) > 100:
            tabs_hint()
        tabs = st.tabs(tnames)
        for i, mt in enumerate(tasks):
            with tabs[i]:
                # st.markdown(f"**Model Name**: {mt.name}")
                st.markdown(f"**Model Type**: {mt.model_type}")
                st.markdown(f"**Description**: {mt.description}")
                st.latex("Formulation")
                st.latex(mt.formulation)

                mks = "| Variable | Description |\n| --- | --- |\n"
                if mt.variables:
                    for v, d in mt.variables.items():
                        mks += f"| ${v}$ | {d} |\n"
                    st.markdown(mks)


def research_window():
    with st.container(border=True):
        title = "Research🔍" if isinstance(state.scenario, SIMILAR_SCENARIOS) else "Research🔍 (reader)"
        st.subheader(title, divider="blue", anchor="_research")
        if isinstance(state.scenario, SIMILAR_SCENARIOS):
            # pdf image
            if pim := state.msgs[round]["r.extract_factors_and_implement.load_pdf_screenshot"]:
                for i in range(min(2, len(pim))):
                    st.image(pim[i].content, use_container_width=True)

            # Hypothesis
            if hg := state.msgs[round]["r.hypothesis generation"]:
                st.markdown("**Hypothesis💡**")  # 🧠
                h: Hypothesis = hg[0].content
                st.markdown(
                    f"""
- **Hypothesis**: {h.hypothesis}
- **Reason**: {h.reason}"""
                )

            if eg := state.msgs[round]["r.experiment generation"]:
                tasks_window(eg[0].content)

        elif isinstance(state.scenario, GeneralModelScenario):
            # pdf image
            c1, c2 = st.columns([2, 3])
            with c1:
                if pim := state.msgs[round]["r.pdf_image"]:
                    for i in range(len(pim)):
                        st.image(pim[i].content, use_container_width=True)

            # loaded model exp
            with c2:
                if mem := state.msgs[round]["d.load_experiment"]:
                    # 'load_experiment' should in 'r' now, but old version trace may in 'd', so we need to check both
                    # TODO: modify the way to get one message with a specific tag like 'load_experiment' in the future
                    me: QlibModelExperiment = mem[0].content
                    tasks_window(me.sub_tasks)
                elif mem := state.msgs[round]["r.load_experiment"]:
                    me: QlibModelExperiment = mem[0].content
                    tasks_window(me.sub_tasks)


def feedback_window():
    if isinstance(state.scenario, SIMILAR_SCENARIOS):
        with st.container(border=True):
            st.subheader("Feedback📝", divider="orange", anchor="_feedback")

            if state.lround > 0 and isinstance(
                state.scenario, (QlibModelScenario, QlibFactorScenario, QlibFactorFromReportScenario, KGScenario)
            ):
                with st.expander("**Config⚙️**", expanded=True):
                    st.markdown(state.scenario.experiment_setting, unsafe_allow_html=True)

            if fbr := state.msgs[round]["ef.Quantitative Backtesting Chart"]:
                st.markdown("**Returns📈**")
                fig = report_figure(fbr[0].content)
                st.plotly_chart(fig)
            if fb := state.msgs[round]["ef.feedback"]:
                st.markdown("**Hypothesis Feedback🔍**")
                h: HypothesisFeedback = fb[0].content
                st.markdown(
                    f"""
- **Observations**: {h.observations}
- **Hypothesis Evaluation**: {h.hypothesis_evaluation}
- **New Hypothesis**: {h.new_hypothesis}
- **Decision**: {h.decision}
- **Reason**: {h.reason}"""
                )

            if isinstance(state.scenario, KGScenario):
                if fbe := state.msgs[round]["ef.runner result"]:
                    submission_path = fbe[0].content.experiment_workspace.workspace_path / "submission.csv"
                    st.markdown(
                        f":green[**Exp Workspace**]: {str(fbe[0].content.experiment_workspace.workspace_path.absolute())}"
                    )
                    try:
                        data = submission_path.read_bytes()
                        st.download_button(
                            label="**Download** submission.csv",
                            data=data,
                            file_name="submission.csv",
                            mime="text/csv",
                        )
                    except Exception as e:
                        st.markdown(f":red[**Download Button Error**]: {e}")


def evolving_window():
    title = "Development🛠️" if isinstance(state.scenario, SIMILAR_SCENARIOS) else "Development🛠️ (evolving coder)"
    st.subheader(title, divider="green", anchor="_development")

    # Evolving Status
    if state.erounds[round] > 0:
        st.markdown("**☑️ Evolving Status**")
        es = state.e_decisions[round]
        e_status_mks = "".join(f"| {ei} " for ei in range(1, state.erounds[round] + 1)) + "|\n"
        e_status_mks += "|--" * state.erounds[round] + "|\n"
        for ei, estatus in es.items():
            if not estatus:
                estatus = (0, 0, 0)
            e_status_mks += "| " + "🕙<br>" * estatus[2] + "✔️<br>" * estatus[0] + "❌<br>" * estatus[1] + " "
        e_status_mks += "|\n"
        st.markdown(e_status_mks, unsafe_allow_html=True)

    # Evolving Tabs
    if state.erounds[round] > 0:
        if state.erounds[round] > 1:
            evolving_round = st.radio(
                "**🔄️Evolving Rounds**",
                horizontal=True,
                options=range(1, state.erounds[round] + 1),
                index=state.erounds[round] - 1,
                key="show_eround",
            )
        else:
            evolving_round = 1

        ws: list[FactorFBWorkspace | ModelFBWorkspace] = state.msgs[round]["d.evolving code"][
            evolving_round - 1
        ].content
        # All Tasks

        tab_names = [
            w.target_task.factor_name if isinstance(w.target_task, FactorTask) else w.target_task.name for w in ws
        ]
        if len(state.msgs[round]["d.evolving feedback"]) >= evolving_round:
            for j in range(len(ws)):
                if state.msgs[round]["d.evolving feedback"][evolving_round - 1].content[j].final_decision:
                    tab_names[j] += "✔️"
                else:
                    tab_names[j] += "❌"
        if sum(len(tn) for tn in tab_names) > 100:
            tabs_hint()
        wtabs = st.tabs(tab_names)
        for j, w in enumerate(ws):
            with wtabs[j]:
                # Evolving Code
                st.markdown(f"**Workspace Path**: {w.workspace_path}")
                for k, v in w.file_dict.items():
                    with st.expander(f":green[`{k}`]", expanded=True):
                        st.code(v, language="python")

                # Evolving Feedback
                if len(state.msgs[round]["d.evolving feedback"]) >= evolving_round:
                    evolving_feedback_window(state.msgs[round]["d.evolving feedback"][evolving_round - 1].content[j])


toc = """
## [Scenario Description📖](#_scenario)
## [Summary📊](#_summary)
- [**Metrics📈**](#_metrics)
- [**Hypotheses🏅**](#_hypotheses)
## [RD-Loops♾️](#_rdloops)
- [**Research🔍**](#_research)
- [**Development🛠️**](#_development)
- [**Feedback📝**](#_feedback)
"""
if isinstance(state.scenario, GeneralModelScenario):
    toc = """
## [Scenario Description📖](#_scenario)
### [Summary📊](#_summary)
### [Research🔍](#_research)
### [Development🛠️](#_development)
"""
# Config Sidebar
with st.sidebar:
    st.markdown("# RD-Agent🤖  [:grey[@GitHub]](https://github.com/microsoft/RD-Agent)")
    st.subheader(":blue[Table of Content]", divider="blue")
    st.markdown(toc)
    st.subheader(":orange[Control Panel]", divider="red")

    with st.container(border=True):
        if main_log_path:
            lc1, lc2 = st.columns([1, 2], vertical_alignment="center")
            with lc1:
                st.markdown(":blue[**Log Path**]")
            with lc2:
                manually = st.toggle("Manual Input")
            if manually:
                st.text_input("log path", key="log_path", on_change=refresh, label_visibility="collapsed")
            else:
                folders = filter_log_folders(main_log_path)
                st.selectbox(f"**Select from `{main_log_path}`**", folders, key="log_path", on_change=refresh)
        else:
            st.text_input(":blue[**log path**]", key="log_path", on_change=refresh)

    c1, c2 = st.columns([1, 1], vertical_alignment="center")
    with c1:
        if st.button(":green[**All Loops**]", use_container_width=True):
            if not state.fs:
                refresh()
            get_msgs_until(lambda m: False)
        if st.button("**Reset**", use_container_width=True):
            refresh(same_trace=True)
    with c2:
        if st.button(":green[Next Loop]", use_container_width=True):
            if not state.fs:
                refresh()
            get_msgs_until(lambda m: "ef.feedback" in m.tag)

        if st.button("Next Step", use_container_width=True):
            if not state.fs:
                refresh()
            get_msgs_until(lambda m: "d.evolving feedback" in m.tag)

    with st.popover(":orange[**Config⚙️**]", use_container_width=True):
        st.multiselect("excluded log tags", ["llm_messages"], ["llm_messages"], key="excluded_tags")
        st.multiselect("excluded log types", ["str", "dict", "list"], ["str"], key="excluded_types")

    if args.debug:
        debug = st.toggle("debug", value=False)

        if debug:
            if st.button("Single Step Run", use_container_width=True):
                get_msgs_until()
    else:
        debug = False


# Debug Info Window
if debug:
    with st.expander(":red[**Debug Info**]", expanded=True):
        dcol1, dcol2 = st.columns([1, 3])
        with dcol1:
            st.markdown(
                f"**log path**: {state.log_path}\n\n"
                f"**excluded tags**: {state.excluded_tags}\n\n"
                f"**excluded types**: {state.excluded_types}\n\n"
                f":blue[**message id**]: {sum(sum(len(tmsgs) for tmsgs in rmsgs.values()) for rmsgs in state.msgs.values())}\n\n"
                f":blue[**round**]: {state.lround}\n\n"
                f":blue[**evolving round**]: {state.erounds[state.lround]}\n\n"
            )
        with dcol2:
            if state.last_msg:
                st.write(state.last_msg)
                if isinstance(state.last_msg.content, list):
                    st.write(state.last_msg.content[0])
                elif not isinstance(state.last_msg.content, str):
                    st.write(state.last_msg.content.__dict__)


if state.log_path and state.fs is None:
    refresh()

# Main Window
header_c1, header_c3 = st.columns([1, 6], vertical_alignment="center")
with st.container():
    with header_c1:
        st.image("https://img-prod-cms-rt-microsoft-com.akamaized.net/cms/api/am/imageFileData/RE1Mu3b?ver=5c31")
    with header_c3:
        st.markdown(
            """
        <h1>
            RD-Agent:<br>LLM-based autonomous evolving agents for industrial data-driven R&D
        </h1>
        """,
            unsafe_allow_html=True,
        )

# Project Info
with st.container():
    image_c, scen_c = st.columns([3, 3], vertical_alignment="center")
    with image_c:
        img_path = rfiles("rdagent.log.ui").joinpath("flow.png")
        st.image(str(img_path), use_container_width=True)
    with scen_c:
        st.header("Scenario Description📖", divider="violet", anchor="_scenario")
        if state.scenario is not None:
            theme = st_theme()
            if theme:
                theme = theme.get("base", "light")
            css = f"""
<style>
    a[href="#_rdloops"], a[href="#_research"], a[href="#_development"], a[href="#_feedback"], a[href="#_scenario"], a[href="#_summary"], a[href="#_hypotheses"], a[href="#_metrics"] {{
        color: {"black" if theme == "light" else "white"};
    }}
</style>
"""
            st.markdown(state.scenario.rich_style_description + css, unsafe_allow_html=True)


def show_times(round: int):
    for k, v in state.times[round].items():
        if len(v) > 1:
            diff = v[-1] - v[0]
        else:
            diff = v[0] - v[0]
        total_seconds = diff.seconds
        seconds = total_seconds % 60
        minutes = total_seconds // 60
        st.markdown(f"**:blue[{k}]**: :red[**{minutes}**] minutes :orange[**{seconds}**] seconds")


if state.scenario is not None:
    summary_window()

    # R&D Loops Window
    if isinstance(state.scenario, SIMILAR_SCENARIOS):
        st.header("R&D Loops♾️", divider="rainbow", anchor="_rdloops")
        if len(state.msgs) > 1:
            r_options = list(state.msgs.keys())
            if 0 in r_options:
                r_options.remove(0)
            round = st.radio("**Loops**", horizontal=True, options=r_options, index=state.lround - 1)
        else:
            round = 1

        show_times(round)
        rf_c, d_c = st.columns([2, 2])
    elif isinstance(state.scenario, GeneralModelScenario):
        show_times(round)

        rf_c = st.container()
        d_c = st.container()
        round = 1
    else:
        st.error("Unknown Scenario!")
        st.stop()

    with rf_c:
        research_window()
        feedback_window()

    with d_c.container(border=True):
        evolving_window()


st.markdown("<br><br><br>", unsafe_allow_html=True)
st.markdown("#### Disclaimer")
st.markdown(
    "*This content is AI-generated and may not be fully accurate or up-to-date; please verify with a professional for critical matters.*",
    unsafe_allow_html=True,
)



================================================
File: rdagent/log/ui/dsapp.py
================================================
import re
from collections import defaultdict
from datetime import timedelta
from pathlib import Path

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from streamlit import session_state as state

from rdagent.app.data_science.loop import DataScienceRDLoop
from rdagent.log.mle_summary import extract_mle_json, is_valid_session
from rdagent.log.storage import FileStorage
from rdagent.utils import remove_ansi_codes

st.set_page_config(layout="wide", page_title="RD-Agent", page_icon="🎓", initial_sidebar_state="expanded")

# 设置主日志路径
if "log_folder" not in state:
    state.log_folder = Path("./log")
if "log_folders" not in state:
    state.log_folders = ["./log"]
if "log_path" not in state:
    state.log_path = None
if "show_all_summary" not in state:
    state.show_all_summary = True
if "show_stdout" not in state:
    state.show_stdout = False


def load_stdout():
    # FIXME: TODO: 使用配置项来指定stdout文件名
    stdout_path = state.log_folder / f"{state.log_path}.stdout"
    if stdout_path.exists():
        stdout = stdout_path.read_text()
    else:
        stdout = f"Please Set: {stdout_path}"
    return stdout


def extract_loopid_func_name(tag):
    """提取 Loop ID 和函数名称"""
    match = re.search(r"Loop_(\d+)\.([^.]+)", tag)
    return match.groups() if match else (None, None)


def extract_evoid(tag):
    """提取 EVO ID"""
    match = re.search(r"\.evo_loop_(\d+)\.", tag)
    return match.group(1) if match else None


# @st.cache_data
def load_data(log_path: Path):
    state.data = defaultdict(lambda: defaultdict(dict))
    state.times = defaultdict(lambda: defaultdict(dict))
    for msg in FileStorage(log_path).iter_msg():
        if msg.tag and "llm" not in msg.tag and "session" not in msg.tag:
            if msg.tag == "competition":
                state.data["competition"] = msg.content
                continue

            li, fn = extract_loopid_func_name(msg.tag)
            li = int(li)

            # read times
            loop_obj_path = log_path / "__session__" / f"{li}" / "4_record"
            if loop_obj_path.exists():
                state.times[li] = DataScienceRDLoop.load(loop_obj_path).loop_trace[li]

            ei = extract_evoid(msg.tag)
            msg.tag = re.sub(r"\.evo_loop_\d+", "", msg.tag)
            msg.tag = re.sub(r"Loop_\d+\.[^.]+\.?", "", msg.tag)
            msg.tag = msg.tag.strip()

            if ei:
                state.data[li][int(ei)][msg.tag] = msg.content
            else:
                if msg.tag:
                    state.data[li][fn][msg.tag] = msg.content
                else:
                    if not isinstance(msg.content, str):
                        state.data[li][fn] = msg.content


# @st.cache_data
def get_folders_sorted(log_path):
    """缓存并返回排序后的文件夹列表，并加入进度打印"""
    with st.spinner("正在加载文件夹列表..."):
        folders = sorted(
            (folder for folder in log_path.iterdir() if is_valid_session(folder)),
            key=lambda folder: folder.stat().st_mtime,
            reverse=True,
        )
        st.write(f"找到 {len(folders)} 个文件夹")
    return [folder.name for folder in folders]


# UI - Sidebar
with st.sidebar:
    log_folder_str = st.text_area(
        "**Log Folders**(split by ';')", placeholder=state.log_folder, value=";".join(state.log_folders)
    )
    state.log_folders = [folder.strip() for folder in log_folder_str.split(";") if folder.strip()]

    state.log_folder = Path(st.radio(f"Select :blue[**one log folder**]", state.log_folders))
    if not state.log_folder.exists():
        st.warning(f"Path {state.log_folder} does not exist!")

    folders = get_folders_sorted(state.log_folder)
    st.selectbox(f"Select from :blue[**{state.log_folder.absolute()}**]", folders, key="log_path")

    if st.button("Refresh Data"):
        if state.log_path is None:
            st.toast("Please select a log path first!", type="error")
            st.stop()

        load_data(state.log_folder / state.log_path)

    st.toggle("One Trace / Log Folder Summary", key="show_all_summary")
    st.toggle("Show stdout", key="show_stdout")


# UI windows
def task_win(data):
    with st.container(border=True):
        st.markdown(f"**:violet[{data.name}]**")
        st.markdown(data.description)
        if hasattr(data, "architecture"):  # model task
            st.markdown(
                f"""
    | Model_type | Architecture | hyperparameters |
    |------------|--------------|-----------------|
    | {data.model_type} | {data.architecture} | {data.hyperparameters} |
    """
            )


def workspace_win(data):
    show_files = {k: v for k, v in data.file_dict.items() if not "test" in k}
    if len(show_files) > 0:
        with st.expander(f"Files in :blue[{replace_ep_path(data.workspace_path)}]"):
            code_tabs = st.tabs(show_files.keys())
            for ct, codename in zip(code_tabs, show_files.keys()):
                with ct:
                    st.code(
                        show_files[codename],
                        language=("python" if codename.endswith(".py") else "markdown"),
                        wrap_lines=True,
                    )
    else:
        st.markdown("No files in the workspace")


def exp_gen_win(data):
    st.header("Exp Gen", divider="blue")
    st.subheader("Hypothesis")
    st.code(str(data.hypothesis).replace("\n", "\n\n"), wrap_lines=True)

    st.subheader("pending_tasks")
    for tasks in data.pending_tasks_list:
        task_win(tasks[0])
    st.subheader("Exp Workspace", anchor="exp-workspace")
    workspace_win(data.experiment_workspace)


def evolving_win(data):
    st.header("Code Evolving", divider="green")
    if len(data) > 1:
        evo_id = st.slider("Evolving", 0, len(data) - 1, 0)
    else:
        evo_id = 0

    if evo_id in data:
        if data[evo_id]["evolving code"][0] is not None:
            st.subheader("codes")
            workspace_win(data[evo_id]["evolving code"][0])
            fb = data[evo_id]["evolving feedback"][0]
            st.subheader("evolving feedback" + ("✅" if bool(fb) else "❌"), anchor="c_feedback")
            f1, f2, f3 = st.tabs(["execution", "return_checking", "code"])
            f1.code(fb.execution, wrap_lines=True)
            f2.code(fb.return_checking, wrap_lines=True)
            f3.code(fb.code, wrap_lines=True)
        else:
            st.write("data[evo_id]['evolving code'][0] is None.")
            st.write(data[evo_id])
    else:
        st.markdown("No evolving.")


def exp_after_coding_win(data):
    st.header("Exp After Coding", divider="blue")
    st.subheader("Exp Workspace", anchor="eac-exp-workspace")
    workspace_win(data.experiment_workspace)


def exp_after_running_win(data, mle_score):
    st.header("Exp After Running", divider="blue")
    st.subheader("Exp Workspace", anchor="ear-exp-workspace")
    workspace_win(data.experiment_workspace)
    st.subheader("Result")
    st.write(data.result)
    st.subheader("MLE Submission Score" + ("✅" if (isinstance(mle_score, dict) and mle_score["score"]) else "❌"))
    if isinstance(mle_score, dict):
        st.json(mle_score)
    else:
        st.code(mle_score, wrap_lines=True)


def feedback_win(data):
    st.header("Feedback" + ("✅" if bool(data) else "❌"), divider="orange")
    st.code(data, wrap_lines=True)
    if data.exception is not None:
        st.markdown(f"**:red[Exception]**: {data.exception}")


def sota_win(data):
    st.header("SOTA Experiment", divider="rainbow")
    if data:
        st.subheader("Exp Workspace", anchor="sota-exp-workspace")
        workspace_win(data.experiment_workspace)
    else:
        st.markdown("No SOTA experiment.")


def main_win(data):
    exp_gen_win(data["direct_exp_gen"])
    evo_data = {k: v for k, v in data.items() if isinstance(k, int)}
    evolving_win(evo_data)
    if "coding" in data:
        exp_after_coding_win(data["coding"])
    if "running" in data:
        exp_after_running_win(data["running"], data["mle_score"])
    if "feedback" in data:
        feedback_win(data["feedback"])
    sota_win(data["SOTA experiment"])

    with st.sidebar:
        st.markdown(
            f"""
- [Exp Gen](#exp-gen)
    - [Hypothesis](#hypothesis)
    - [pending_tasks](#pending-tasks)
    - [Exp Workspace](#exp-workspace)
- [Code Evolving ({len(evo_data)})](#code-evolving)
    - [codes](#codes)
    - [evolving feedback](#c_feedback)
{"- [Exp After Coding](#exp-after-coding)" if "coding" in data else ""}
{"- [Exp After Running](#exp-after-running)" if "running" in data else ""}
{"- [Feedback](#feedback)" if "feedback" in data else ""}
- [SOTA Experiment](#sota-experiment)
"""
        )


def replace_ep_path(p: Path):
    # 替换workspace path为对应ep机器mount在ep03的path
    # TODO: FIXME: 使用配置项来处理
    match = re.search(r"ep\d+", str(state.log_folder))
    if match:
        ep = match.group(0)
        return Path(
            str(p).replace("repos/RD-Agent-Exp", f"repos/batch_ctrl/all_projects/{ep}").replace("/Data", "/data")
        )
    return p


def summarize_data():
    st.header("Summary", divider="rainbow")
    df = pd.DataFrame(
        columns=["Component", "Running Score", "Feedback", "Time", "Start Time (UTC+8)", "End Time (UTC+8)"],
        index=range(len(state.data) - 1),
    )

    for loop in range(len(state.data) - 1):
        loop_data = state.data[loop]
        df.loc[loop, "Component"] = loop_data["direct_exp_gen"].hypothesis.component
        if state.times[loop]:
            df.loc[loop, "Time"] = str(sum((i.end - i.start for i in state.times[loop]), timedelta())).split(".")[0]
            df.loc[loop, "Start Time (UTC+8)"] = state.times[loop][0].start + timedelta(hours=8)
            df.loc[loop, "End Time (UTC+8)"] = state.times[loop][-1].end + timedelta(hours=8)
        if "running" in loop_data:
            if "mle_score" not in state.data[loop]:
                mle_score_path = (
                    replace_ep_path(loop_data["running"].experiment_workspace.workspace_path) / "mle_score.txt"
                )
                try:
                    mle_score_txt = mle_score_path.read_text()
                    state.data[loop]["mle_score"] = extract_mle_json(mle_score_txt)
                    if state.data[loop]["mle_score"]["score"] is not None:
                        df.loc[loop, "Running Score"] = str(state.data[loop]["mle_score"]["score"])
                    else:
                        state.data[loop]["mle_score"] = mle_score_txt
                        df.loc[loop, "Running Score"] = "❌"
                except Exception as e:
                    state.data[loop]["mle_score"] = str(e)
                    df.loc[loop, "Running Score"] = "❌"
            else:
                if isinstance(state.data[loop]["mle_score"], dict):
                    df.loc[loop, "Running Score"] = str(state.data[loop]["mle_score"]["score"])
                else:
                    df.loc[loop, "Running Score"] = "❌"

        else:
            df.loc[loop, "Running Score"] = "N/A"

        if "feedback" in loop_data:
            df.loc[loop, "Feedback"] = "✅" if bool(loop_data["feedback"]) else "❌"
        else:
            df.loc[loop, "Feedback"] = "N/A"
    st.dataframe(df)


def all_summarize_win():
    summarys = {}
    for lf in state.log_folders:
        if not (Path(lf) / "summary.pkl").exists():
            st.warning(
                f"No summary file found in {lf}\nRun:`dotenv run -- python rdagent/log/mle_summary.py grade_summary --log_folder=<your trace folder>`"
            )
        else:
            summarys[lf] = pd.read_pickle(Path(lf) / "summary.pkl")

    if len(summarys) == 0:
        return

    summary = {}
    for lf, s in summarys.items():
        for k, v in s.items():
            summary[f"{lf[lf.rfind('ep'):]}{k}"] = v

    summary = {k: v for k, v in summary.items() if "competition" in v}
    base_df = pd.DataFrame(
        columns=[
            "Competition",
            "Total Loops",
            "Successful Final Decision",
            "Made Submission",
            "Valid Submission",
            "V/M",
            "Above Median",
            "Bronze",
            "Silver",
            "Gold",
            "Any Medal",
            "SOTA Exp",
            "Ours - Base",
            "SOTA Exp Score",
            "Baseline Score",
            "Bronze Threshold",
            "Silver Threshold",
            "Gold Threshold",
            "Medium Threshold",
        ],
        index=summary.keys(),
    )

    # Read baseline results
    baseline_result_path = ""
    if Path(baseline_result_path).exists():
        baseline_df = pd.read_csv(baseline_result_path)

    for k, v in summary.items():
        loop_num = v["loop_num"]
        base_df.loc[k, "Competition"] = v["competition"]
        base_df.loc[k, "Total Loops"] = loop_num
        if loop_num == 0:
            base_df.loc[k] = "N/A"
        else:
            base_df.loc[k, "Successful Final Decision"] = (
                f"{v['success_loop_num']} ({round(v['success_loop_num'] / loop_num * 100, 2)}%)"
            )
            base_df.loc[k, "Made Submission"] = (
                f"{v['made_submission_num']} ({round(v['made_submission_num'] / loop_num * 100, 2)}%)"
            )
            base_df.loc[k, "Valid Submission"] = (
                f"{v['valid_submission_num']} ({round(v['valid_submission_num'] / loop_num * 100, 2)}%)"
            )
            if v["made_submission_num"] != 0:
                base_df.loc[k, "V/M"] = f"{round(v['valid_submission_num'] / v['made_submission_num'] * 100, 2)}%"
            else:
                base_df.loc[k, "V/M"] = "N/A"
            base_df.loc[k, "Above Median"] = (
                f"{v['above_median_num']} ({round(v['above_median_num'] / loop_num * 100, 2)}%)"
            )
            base_df.loc[k, "Bronze"] = f"{v['bronze_num']} ({round(v['bronze_num'] / loop_num * 100, 2)}%)"
            base_df.loc[k, "Silver"] = f"{v['silver_num']} ({round(v['silver_num'] / loop_num * 100, 2)}%)"
            base_df.loc[k, "Gold"] = f"{v['gold_num']} ({round(v['gold_num'] / loop_num * 100, 2)}%)"
            base_df.loc[k, "Any Medal"] = f"{v['get_medal_num']} ({round(v['get_medal_num'] / loop_num * 100, 2)}%)"

            baseline_score = None
            if Path(baseline_result_path).exists():
                baseline_score = baseline_df.loc[baseline_df["competition_id"] == v["competition"], "score"].item()

            base_df.loc[k, "SOTA Exp"] = v.get("sota_exp_stat", None)
            if (
                baseline_score is not None
                and not pd.isna(baseline_score)
                and not pd.isna(v.get("sota_exp_score", None))
            ):
                base_df.loc[k, "Ours - Base"] = v.get("sota_exp_score", 0.0) - baseline_score
            base_df.loc[k, "SOTA Exp Score"] = v.get("sota_exp_score", None)
            base_df.loc[k, "Baseline Score"] = baseline_score
            base_df.loc[k, "Bronze Threshold"] = v.get("bronze_threshold", None)
            base_df.loc[k, "Silver Threshold"] = v.get("silver_threshold", None)
            base_df.loc[k, "Gold Threshold"] = v.get("gold_threshold", None)
            base_df.loc[k, "Medium Threshold"] = v.get("median_threshold", None)

    base_df["SOTA Exp"].replace("", pd.NA, inplace=True)
    st.dataframe(base_df)
    total_stat = (
        (
            base_df[
                [
                    "Made Submission",
                    "Valid Submission",
                    "Above Median",
                    "Bronze",
                    "Silver",
                    "Gold",
                    "Any Medal",
                ]
            ]
            != "0 (0.0%)"
        ).sum()
        / base_df.shape[0]
        * 100
    )
    total_stat.name = "总体统计(%)"

    # SOTA Exp 统计
    se_counts = base_df["SOTA Exp"].value_counts(dropna=True)
    se_counts.loc["made_submission"] = se_counts.sum()
    se_counts.loc["Any Medal"] = se_counts.get("gold", 0) + se_counts.get("silver", 0) + se_counts.get("bronze", 0)
    se_counts.loc["above_median"] = se_counts.get("above_median", 0) + se_counts.get("Any Medal", 0)
    se_counts.loc["valid_submission"] = se_counts.get("valid_submission", 0) + se_counts.get("above_median", 0)

    sota_exp_stat = pd.Series(index=total_stat.index, dtype=int, name="SOTA Exp 统计(%)")
    sota_exp_stat.loc["Made Submission"] = se_counts.get("made_submission", 0)
    sota_exp_stat.loc["Valid Submission"] = se_counts.get("valid_submission", 0)
    sota_exp_stat.loc["Above Median"] = se_counts.get("above_median", 0)
    sota_exp_stat.loc["Bronze"] = se_counts.get("bronze", 0)
    sota_exp_stat.loc["Silver"] = se_counts.get("silver", 0)
    sota_exp_stat.loc["Gold"] = se_counts.get("gold", 0)
    sota_exp_stat.loc["Any Medal"] = se_counts.get("Any Medal", 0)
    sota_exp_stat = sota_exp_stat / base_df.shape[0] * 100

    stat_df = pd.concat([total_stat, sota_exp_stat], axis=1)
    st.dataframe(stat_df.round(2))

    # write curve
    for k, v in summary.items():
        with st.container(border=True):
            st.markdown(f"**:blue[{k}] - :violet[{v['competition']}]**")
            vscores = {k: v.iloc[:, 0] for k, v in v["valid_scores"].items()}
            tscores = {f"loop {k}": v for k, v in v["test_scores"].items()}
            if len(vscores) > 0:
                metric_name = list(vscores.values())[0].name
            else:
                metric_name = "None"

            fc1, fc2 = st.columns(2)
            try:
                vdf = pd.DataFrame(vscores)
                vdf.columns = [f"loop {i}" for i in vdf.columns]
                f1 = px.line(vdf.T, markers=True, title=f"Valid scores (metric: {metric_name})")
                fc1.plotly_chart(f1, key=f"{k}_v")

                tdf = pd.Series(tscores, name="score")
                f2 = px.line(tdf, markers=True, title="Test scores")
                fc2.plotly_chart(f2, key=k)
            except Exception as e:
                import traceback

                st.markdown("- Error: " + str(e))
                st.code(traceback.format_exc())
                st.markdown("- Valid Scores: ")
                st.json(vscores)
                st.markdown("- Test Scores: ")
                st.json(tscores)


def stdout_win(loop_id: int):
    stdout = load_stdout()
    if stdout.startswith("Please Set"):
        st.toast(stdout, icon="🟡")
        return
    start_index = stdout.find(f"Start Loop {loop_id}")
    end_index = stdout.find(f"Start Loop {loop_id + 1}")
    loop_stdout = remove_ansi_codes(stdout[start_index:end_index])
    with st.container(border=True):
        st.subheader(f"Loop {loop_id} stdout")
        pattern = f"Start Loop {loop_id}, " + r"Step \d+: \w+"
        matches = re.finditer(pattern, loop_stdout)
        step_stdouts = {}
        for match in matches:
            step = match.group(0)
            si = match.start()
            ei = loop_stdout.find(f"Start Loop {loop_id}", match.end())
            step_stdouts[step] = loop_stdout[si:ei].strip()

        for k, v in step_stdouts.items():
            expanded = True if "coding" in k else False
            with st.expander(k, expanded=expanded):
                st.code(v, language="log", wrap_lines=True)


# UI - Main
if state.show_all_summary:
    all_summarize_win()
elif "data" in state:
    st.title(state.data["competition"])
    summarize_data()
    loop_id = st.slider("Loop", 0, len(state.data) - 2, 0)
    if state.show_stdout:
        stdout_win(loop_id)
    main_win(state.data[loop_id])



================================================
File: rdagent/log/ui/llm_st.py
================================================
import argparse
import json
import pickle
import re
import time
from pathlib import Path

import streamlit as st
from streamlit import session_state

st.set_page_config(layout="wide", page_title="debug_llm", page_icon="🎓", initial_sidebar_state="expanded")

# 获取 log_path 参数
parser = argparse.ArgumentParser(description="RD-Agent Streamlit App")
parser.add_argument("--log_dir", type=str, help="Path to the log directory")
args = parser.parse_args()


@st.cache_data
def get_folders_sorted(log_path):
    """缓存并返回排序后的文件夹列表，并加入进度打印"""
    with st.spinner("正在加载文件夹列表..."):
        folders = sorted(
            (folder for folder in log_path.iterdir() if folder.is_dir() and list(folder.iterdir())),
            key=lambda folder: folder.stat().st_mtime,
            reverse=True,
        )
        st.write(f"找到 {len(folders)} 个文件夹")
    return [folder.name for folder in folders]


# 设置主日志路径
main_log_path = Path(args.log_dir) if args.log_dir else Path("./log")
if not main_log_path.exists():
    st.error(f"Log dir {main_log_path} does not exist!")
    st.stop()

if "data" not in session_state:
    session_state.data = []
if "log_path" not in session_state:
    session_state.log_path = None

tlist = []


def load_data():
    """加载数据到 session_state 并显示进度"""
    log_file = main_log_path / session_state.log_path / "debug_llm.pkl"
    try:
        with st.spinner(f"正在加载数据文件 {log_file}..."):
            start_time = time.time()
            with open(log_file, "rb") as f:
                session_state.data = pickle.load(f)
            st.success(f"数据加载完成！耗时 {time.time() - start_time:.2f} 秒")
            st.session_state["current_loop"] = 1
    except Exception as e:
        session_state.data = [{"error": str(e)}]
        st.error(f"加载数据失败: {e}")


# UI - Sidebar
with st.sidebar:
    st.markdown(":blue[**Log Path**]")
    manually = st.toggle("Manual Input")
    if manually:
        st.text_input("log path", key="log_path", label_visibility="collapsed")
    else:
        folders = get_folders_sorted(main_log_path)
        st.selectbox(f"**Select from {main_log_path.absolute()}**", folders, key="log_path")

    if st.button("Refresh Data"):
        load_data()
        st.rerun()


# Helper functions
def show_text(text, lang=None):
    """显示文本代码块"""
    if lang:
        st.code(text, language=lang, wrap_lines=True)
    elif "\n" in text:
        st.code(text, language="python", wrap_lines=True)
    else:
        st.code(text, language="html", wrap_lines=True)


def highlight_prompts_uri(uri):
    """高亮 URI 的格式"""
    parts = uri.split(":")
    return f"**{parts[0]}:**:green[**{parts[1]}**]"


def extract_loopid_func_name(tag):
    """提取 Loop ID 和函数名称"""
    match = re.search(r"Loop_(\d+)\.(\w+)\.", tag)
    return match.groups() if match else (None, None)


def extract_evoid(tag):
    """提取 EVO ID"""
    match = re.search(r"\.evo_loop_(\d+)\.", tag)
    return match.group(1) if match else None


# Display Data
progress_text = st.empty()
progress_bar = st.progress(0)

# 每页展示一个 Loop
LOOPS_PER_PAGE = 1

# 获取所有的 Loop ID
loop_groups = {}
for i, d in enumerate(session_state.data):
    tag = d["tag"]
    loop_id, _ = extract_loopid_func_name(tag)
    if loop_id:
        if loop_id not in loop_groups:
            loop_groups[loop_id] = []
        loop_groups[loop_id].append(d)

# 按 Loop ID 排序
sorted_loop_ids = sorted(loop_groups.keys(), key=int)  # 假设 Loop ID 是数字
total_loops = len(sorted_loop_ids)
total_pages = total_loops  # 每页展示一个 Loop


# simple display
# FIXME: Delete this simple UI if trace have tag(evo_id & loop_id)
# with st.sidebar:
#     start = int(st.text_input("start", 0))
#     end = int(st.text_input("end", 100))
# for m in session_state.data[start:end]:
#     if "tpl" in m["tag"]:
#         obj = m["obj"]
#         uri = obj["uri"]
#         tpl = obj["template"]
#         cxt = obj["context"]
#         rd = obj["rendered"]
#         with st.expander(highlight_prompts_uri(uri), expanded=False, icon="⚙️"):
#             t1, t2, t3 = st.tabs([":green[**Rendered**]", ":blue[**Template**]", ":orange[**Context**]"])
#             with t1:
#                 show_text(rd)
#             with t2:
#                 show_text(tpl, lang="django")
#             with t3:
#                 st.json(cxt)
#     if "llm" in m["tag"]:
#         obj = m["obj"]
#         system = obj.get("system", None)
#         user = obj["user"]
#         resp = obj["resp"]
#         with st.expander(f"**LLM**", expanded=False, icon="🤖"):
#             t1, t2, t3 = st.tabs([":green[**Response**]", ":blue[**User**]", ":orange[**System**]"])
#             with t1:
#                 try:
#                     rdict = json.loads(resp)
#                     if "code" in rdict:
#                         code = rdict["code"]
#                         st.markdown(":red[**Code in response dict:**]")
#                         st.code(code, language="python", wrap_lines=True, line_numbers=True)
#                         rdict.pop("code")
#                     elif "spec" in rdict:
#                         spec = rdict["spec"]
#                         st.markdown(":red[**Spec in response dict:**]")
#                         st.markdown(spec)
#                         rdict.pop("spec")
#                     else:
#                         # show model codes
#                         showed_keys = []
#                         for k, v in rdict.items():
#                             if k.startswith("model_") and k.endswith(".py"):
#                                 st.markdown(f":red[**{k}**]")
#                                 st.code(v, language="python", wrap_lines=True, line_numbers=True)
#                                 showed_keys.append(k)
#                         for k in showed_keys:
#                             rdict.pop(k)
#                     st.write(":red[**Other parts (except for the code or spec) in response dict:**]")
#                     st.json(rdict)
#                 except:
#                     st.json(resp)
#             with t2:
#                 show_text(user)
#             with t3:
#                 show_text(system or "No system prompt available")


if total_pages:
    # 初始化 current_loop
    if "current_loop" not in st.session_state:
        st.session_state["current_loop"] = 1

    # Loop 导航按钮
    col1, col2, col3, col4, col5 = st.sidebar.columns([1.2, 1, 2, 1, 1.2])

    with col1:
        if st.button("|<"):  # 首页
            st.session_state["current_loop"] = 1
    with col2:
        if st.button("<") and st.session_state["current_loop"] > 1:  # 上一页
            st.session_state["current_loop"] -= 1
    with col3:
        # 下拉列表显示所有 Loop
        st.session_state["current_loop"] = st.selectbox(
            "选择 Loop",
            options=list(range(1, total_loops + 1)),
            index=st.session_state["current_loop"] - 1,  # 默认选中当前 Loop
            label_visibility="collapsed",  # 隐藏标签
        )
    with col4:
        if st.button("\>") and st.session_state["current_loop"] < total_loops:  # 下一页
            st.session_state["current_loop"] += 1
    with col5:
        if st.button("\>|"):  # 最后一页
            st.session_state["current_loop"] = total_loops

    # 获取当前 Loop
    current_loop = st.session_state["current_loop"]

    # 渲染当前 Loop 数据
    loop_id = sorted_loop_ids[current_loop - 1]
    progress_text = st.empty()
    progress_text.text(f"正在处理 Loop {loop_id}...")
    progress_bar.progress(current_loop / total_loops, text=f"Loop :green[**{current_loop}**] / {total_loops}")

    # 渲染 Loop Header
    loop_anchor = f"Loop_{loop_id}"
    if loop_anchor not in tlist:
        tlist.append(loop_anchor)
        st.header(loop_anchor, anchor=loop_anchor, divider="blue")

    # 渲染当前 Loop 的所有数据
    loop_data = loop_groups[loop_id]
    for d in loop_data:
        tag = d["tag"]
        obj = d["obj"]
        _, func_name = extract_loopid_func_name(tag)
        evo_id = extract_evoid(tag)

        func_anchor = f"loop_{loop_id}.{func_name}"
        if func_anchor not in tlist:
            tlist.append(func_anchor)
            st.header(f"in *{func_name}*", anchor=func_anchor, divider="green")

        evo_anchor = f"loop_{loop_id}.evo_step_{evo_id}"
        if evo_id and evo_anchor not in tlist:
            tlist.append(evo_anchor)
            st.subheader(f"evo_step_{evo_id}", anchor=evo_anchor, divider="orange")

        # 根据 tag 渲染内容
        if "debug_exp_gen" in tag:
            with st.expander(
                f"Exp in :violet[**{obj.experiment_workspace.workspace_path}**]", expanded=False, icon="🧩"
            ):
                st.write(obj)
        elif "debug_tpl" in tag:
            uri = obj["uri"]
            tpl = obj["template"]
            cxt = obj["context"]
            rd = obj["rendered"]
            with st.expander(highlight_prompts_uri(uri), expanded=False, icon="⚙️"):
                t1, t2, t3 = st.tabs([":green[**Rendered**]", ":blue[**Template**]", ":orange[**Context**]"])
                with t1:
                    show_text(rd)
                with t2:
                    show_text(tpl, lang="django")
                with t3:
                    st.json(cxt)
        elif "debug_llm" in tag:
            system = obj.get("system", None)
            user = obj["user"]
            resp = obj["resp"]
            with st.expander(f"**LLM**", expanded=False, icon="🤖"):
                t1, t2, t3 = st.tabs([":green[**Response**]", ":blue[**User**]", ":orange[**System**]"])
                with t1:
                    try:
                        rdict = json.loads(resp)
                        if "code" in rdict:
                            code = rdict["code"]
                            st.markdown(":red[**Code in response dict:**]")
                            st.code(code, language="python", wrap_lines=True, line_numbers=True)
                            rdict.pop("code")
                        elif "spec" in rdict:
                            spec = rdict["spec"]
                            st.markdown(":red[**Spec in response dict:**]")
                            st.markdown(spec)
                            rdict.pop("spec")
                        else:
                            # show model codes
                            showed_keys = []
                            for k, v in rdict.items():
                                if k.startswith("model_") and k.endswith(".py"):
                                    st.markdown(f":red[**{k}**]")
                                    st.code(v, language="python", wrap_lines=True, line_numbers=True)
                                    showed_keys.append(k)
                            for k in showed_keys:
                                rdict.pop(k)
                        st.write(":red[**Other parts (except for the code or spec) in response dict:**]")
                        st.json(rdict)
                    except:
                        st.json(resp)
                with t2:
                    show_text(user)
                with t3:
                    show_text(system or "No system prompt available")

    progress_text.text("当前 Loop 数据处理完成！")

    # Sidebar TOC
    with st.sidebar:
        toc = "\n".join([f"- [{t}](#{t})" if t.startswith("L") else f"  - [{t.split('.')[1]}](#{t})" for t in tlist])
        st.markdown(toc, unsafe_allow_html=True)



================================================
File: rdagent/log/ui/qlib_report_figure.py
================================================
import importlib
import math

import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots


class BaseGraph:
    _name = None

    def __init__(
        self, df: pd.DataFrame = None, layout: dict = None, graph_kwargs: dict = None, name_dict: dict = None, **kwargs
    ):
        """

        :param df:
        :param layout:
        :param graph_kwargs:
        :param name_dict:
        :param kwargs:
            layout: dict
                go.Layout parameters
            graph_kwargs: dict
                Graph parameters, eg: go.Bar(**graph_kwargs)
        """
        self._df = df

        self._layout = dict() if layout is None else layout
        self._graph_kwargs = dict() if graph_kwargs is None else graph_kwargs
        self._name_dict = name_dict

        self.data = None

        self._init_parameters(**kwargs)
        self._init_data()

    def _init_data(self):
        """

        :return:
        """
        if self._df.empty:
            raise ValueError("df is empty.")

        self.data = self._get_data()

    def _init_parameters(self, **kwargs):
        """

        :param kwargs
        """

        # Instantiate graphics parameters
        self._graph_type = self._name.lower().capitalize()

        # Displayed column name
        if self._name_dict is None:
            self._name_dict = {_item: _item for _item in self._df.columns}

    @staticmethod
    def get_instance_with_graph_parameters(graph_type: str = None, **kwargs):
        """

        :param graph_type:
        :param kwargs:
        :return:
        """
        try:
            _graph_module = importlib.import_module("plotly.graph_objs")
            _graph_class = getattr(_graph_module, graph_type)
        except AttributeError:
            _graph_module = importlib.import_module("qlib.contrib.report.graph")
            _graph_class = getattr(_graph_module, graph_type)
        return _graph_class(**kwargs)

    def _get_layout(self) -> go.Layout:
        """

        :return:
        """
        return go.Layout(**self._layout)

    def _get_data(self) -> list:
        """

        :return:
        """

        _data = [
            self.get_instance_with_graph_parameters(
                graph_type=self._graph_type, x=self._df.index, y=self._df[_col], name=_name, **self._graph_kwargs
            )
            for _col, _name in self._name_dict.items()
        ]
        return _data

    @property
    def figure(self) -> go.Figure:
        """

        :return:
        """
        _figure = go.Figure(data=self.data, layout=self._get_layout())
        # NOTE: Use the default theme from plotly version 3.x, template=None
        _figure["layout"].update(template=None)
        return _figure


class SubplotsGraph:
    """Create subplots same as df.plot(subplots=True)

    Simple package for `plotly.tools.subplots`
    """

    def __init__(
        self,
        df: pd.DataFrame = None,
        kind_map: dict = None,
        layout: dict = None,
        sub_graph_layout: dict = None,
        sub_graph_data: list = None,
        subplots_kwargs: dict = None,
        **kwargs,
    ):
        """

        :param df: pd.DataFrame

        :param kind_map: dict, subplots graph kind and kwargs
            eg: dict(kind='Scatter', kwargs=dict())

        :param layout: `go.Layout` parameters

        :param sub_graph_layout: Layout of each graphic, similar to 'layout'

        :param sub_graph_data: Instantiation parameters for each sub-graphic
            eg: [(column_name, instance_parameters), ]

            column_name: str or go.Figure

            Instance_parameters:

                - row: int, the row where the graph is located

                - col: int, the col where the graph is located

                - name: str, show name, default column_name in 'df'

                - kind: str, graph kind, default `kind` param, eg: bar, scatter, ...

                - graph_kwargs: dict, graph kwargs, default {}, used in `go.Bar(**graph_kwargs)`

        :param subplots_kwargs: `plotly.tools.make_subplots` original parameters

                - shared_xaxes: bool, default False

                - shared_yaxes: bool, default False

                - vertical_spacing: float, default 0.3 / rows

                - subplot_titles: list, default []
                    If `sub_graph_data` is None, will generate 'subplot_titles' according to `df.columns`,
                    this field will be discarded


                - specs: list, see `make_subplots` docs

                - rows: int, Number of rows in the subplot grid, default 1
                    If `sub_graph_data` is None, will generate 'rows' according to `df`, this field will be discarded

                - cols: int, Number of cols in the subplot grid, default 1
                    If `sub_graph_data` is None, will generate 'cols' according to `df`, this field will be discarded


        :param kwargs:

        """

        self._df = df
        self._layout = layout
        self._sub_graph_layout = sub_graph_layout

        self._kind_map = kind_map
        if self._kind_map is None:
            self._kind_map = dict(kind="Scatter", kwargs=dict())

        self._subplots_kwargs = subplots_kwargs
        if self._subplots_kwargs is None:
            self._init_subplots_kwargs()

        self.__cols = self._subplots_kwargs.get("cols", 2)  # pylint: disable=W0238
        self.__rows = self._subplots_kwargs.get(  # pylint: disable=W0238
            "rows", math.ceil(len(self._df.columns) / self.__cols)
        )

        self._sub_graph_data = sub_graph_data
        if self._sub_graph_data is None:
            self._init_sub_graph_data()

        self._init_figure()

    def _init_sub_graph_data(self):
        """

        :return:
        """
        self._sub_graph_data = []
        self._subplot_titles = []

        for i, column_name in enumerate(self._df.columns):
            row = math.ceil((i + 1) / self.__cols)
            _temp = (i + 1) % self.__cols
            col = _temp if _temp else self.__cols
            res_name = column_name.replace("_", " ")
            _temp_row_data = (
                column_name,
                dict(
                    row=row,
                    col=col,
                    name=res_name,
                    kind=self._kind_map["kind"],
                    graph_kwargs=self._kind_map["kwargs"],
                ),
            )
            self._sub_graph_data.append(_temp_row_data)
            self._subplot_titles.append(res_name)

    def _init_subplots_kwargs(self):
        """

        :return:
        """
        # Default cols, rows
        _cols = 2
        _rows = math.ceil(len(self._df.columns) / 2)
        self._subplots_kwargs = dict()
        self._subplots_kwargs["rows"] = _rows
        self._subplots_kwargs["cols"] = _cols
        self._subplots_kwargs["shared_xaxes"] = False
        self._subplots_kwargs["shared_yaxes"] = False
        self._subplots_kwargs["vertical_spacing"] = 0.3 / _rows
        self._subplots_kwargs["print_grid"] = False
        self._subplots_kwargs["subplot_titles"] = self._df.columns.tolist()

    def _init_figure(self):
        """

        :return:
        """
        self._figure = make_subplots(**self._subplots_kwargs)

        for column_name, column_map in self._sub_graph_data:
            if isinstance(column_name, go.Figure):
                _graph_obj = column_name
            elif isinstance(column_name, str):
                temp_name = column_map.get("name", column_name.replace("_", " "))
                kind = column_map.get("kind", self._kind_map.get("kind", "Scatter"))
                _graph_kwargs = column_map.get("graph_kwargs", self._kind_map.get("kwargs", {}))
                _graph_obj = BaseGraph.get_instance_with_graph_parameters(
                    kind,
                    **dict(
                        x=self._df.index,
                        y=self._df[column_name],
                        name=temp_name,
                        **_graph_kwargs,
                    ),
                )
            else:
                raise TypeError()

            row = column_map["row"]
            col = column_map["col"]

            self._figure.add_trace(_graph_obj, row=row, col=col)

        if self._sub_graph_layout is not None:
            for k, v in self._sub_graph_layout.items():
                self._figure["layout"][k].update(v)

        # NOTE: Use the default theme from plotly version 3.x: template=None
        self._figure["layout"].update(template=None)
        self._figure["layout"].update(self._layout)

    @property
    def figure(self):
        return self._figure


def _calculate_maximum(df: pd.DataFrame, is_ex: bool = False):
    """

    :param df:
    :param is_ex:
    :return:
    """
    if is_ex:
        end_date = df["cum_ex_return_wo_cost_mdd"].idxmin()
        start_date = df.loc[df.index <= end_date]["cum_ex_return_wo_cost"].idxmax()
    else:
        end_date = df["return_wo_mdd"].idxmin()
        start_date = df.loc[df.index <= end_date]["cum_return_wo_cost"].idxmax()
    return start_date, end_date


def _calculate_mdd(series):
    """
    Calculate mdd

    :param series:
    :return:
    """
    return series - series.cummax()


def _calculate_report_data(raw_df: pd.DataFrame) -> pd.DataFrame:
    """

    :param df:
    :return:
    """
    df = raw_df.copy(deep=True)
    index_names = df.index.names
    df.index = df.index.strftime("%Y-%m-%d")

    report_df = pd.DataFrame()

    report_df["cum_bench"] = df["bench"].cumsum()
    report_df["cum_return_wo_cost"] = df["return"].cumsum()
    report_df["cum_return_w_cost"] = (df["return"] - df["cost"]).cumsum()
    # report_df['cum_return'] - report_df['cum_return'].cummax()
    report_df["return_wo_mdd"] = _calculate_mdd(report_df["cum_return_wo_cost"])
    report_df["return_w_cost_mdd"] = _calculate_mdd((df["return"] - df["cost"]).cumsum())

    report_df["cum_ex_return_wo_cost"] = (df["return"] - df["bench"]).cumsum()
    report_df["cum_ex_return_w_cost"] = (df["return"] - df["bench"] - df["cost"]).cumsum()
    report_df["cum_ex_return_wo_cost_mdd"] = _calculate_mdd((df["return"] - df["bench"]).cumsum())
    report_df["cum_ex_return_w_cost_mdd"] = _calculate_mdd((df["return"] - df["cost"] - df["bench"]).cumsum())
    # return_wo_mdd , return_w_cost_mdd,  cum_ex_return_wo_cost_mdd, cum_ex_return_w

    report_df["turnover"] = df["turnover"]
    report_df.sort_index(ascending=True, inplace=True)

    report_df.index.names = index_names
    return report_df


def report_figure(df: pd.DataFrame) -> list | tuple:
    """

    :param df:
    :return:
    """

    # Get data
    report_df = _calculate_report_data(df)

    # Maximum Drawdown
    max_start_date, max_end_date = _calculate_maximum(report_df)
    ex_max_start_date, ex_max_end_date = _calculate_maximum(report_df, True)

    index_name = report_df.index.name
    _temp_df = report_df.reset_index()
    _temp_df.loc[-1] = 0
    _temp_df = _temp_df.shift(1)
    _temp_df.loc[0, index_name] = "T0"
    _temp_df.set_index(index_name, inplace=True)
    _temp_df.iloc[0] = 0
    report_df = _temp_df

    # Create figure
    _default_kind_map = dict(kind="Scatter", kwargs={"mode": "lines+markers"})
    _temp_fill_args = {"fill": "tozeroy", "mode": "lines+markers"}
    _column_row_col_dict = [
        ("cum_bench", dict(row=1, col=1)),
        ("cum_return_wo_cost", dict(row=1, col=1)),
        ("cum_return_w_cost", dict(row=1, col=1)),
        ("return_wo_mdd", dict(row=2, col=1, graph_kwargs=_temp_fill_args)),
        ("return_w_cost_mdd", dict(row=3, col=1, graph_kwargs=_temp_fill_args)),
        ("cum_ex_return_wo_cost", dict(row=4, col=1)),
        ("cum_ex_return_w_cost", dict(row=4, col=1)),
        ("turnover", dict(row=5, col=1)),
        ("cum_ex_return_w_cost_mdd", dict(row=6, col=1, graph_kwargs=_temp_fill_args)),
        ("cum_ex_return_wo_cost_mdd", dict(row=7, col=1, graph_kwargs=_temp_fill_args)),
    ]

    _subplot_layout = dict()
    for i in range(1, 8):
        # yaxis
        _subplot_layout.update({"yaxis{}".format(i): dict(zeroline=True, showline=True, showticklabels=True)})
        _show_line = i == 7
        _subplot_layout.update({"xaxis{}".format(i): dict(showline=_show_line, type="category", tickangle=45)})

    _layout_style = dict(
        height=1200,
        title=" ",
        shapes=[
            {
                "type": "rect",
                "xref": "x",
                "yref": "paper",
                "x0": max_start_date,
                "y0": 0.55,
                "x1": max_end_date,
                "y1": 1,
                "fillcolor": "#d3d3d3",
                "opacity": 0.3,
                "line": {
                    "width": 0,
                },
            },
            {
                "type": "rect",
                "xref": "x",
                "yref": "paper",
                "x0": ex_max_start_date,
                "y0": 0,
                "x1": ex_max_end_date,
                "y1": 0.55,
                "fillcolor": "#d3d3d3",
                "opacity": 0.3,
                "line": {
                    "width": 0,
                },
            },
        ],
    )

    _subplot_kwargs = dict(
        shared_xaxes=True,
        vertical_spacing=0.01,
        rows=7,
        cols=1,
        row_width=[1, 1, 1, 3, 1, 1, 3],
        print_grid=False,
    )
    figure = SubplotsGraph(
        df=report_df,
        layout=_layout_style,
        sub_graph_data=_column_row_col_dict,
        subplots_kwargs=_subplot_kwargs,
        kind_map=_default_kind_map,
        sub_graph_layout=_subplot_layout,
    ).figure
    return figure



================================================
File: rdagent/log/ui/st_fixed_container.py
================================================
from typing import Literal

import streamlit as st
from streamlit.components.v1 import html

FIXED_CONTAINER_CSS = """
:root {{
    --background-color: #ffffff; /* Default background color */
}}
div[data-testid="stVerticalBlockBorderWrapper"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) {{
    position: {mode};
    width: inherit;
    background-color: inherit;
    {position}: {margin};
    z-index: 999;
}}
div[data-testid="stVerticalBlockBorderWrapper"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) div[data-testid="stVerticalBlock"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) > div[data-testid="stVerticalBlockBorderWrapper"] {{
    background-color: transparent;
    width: 100%;
}}
div[data-testid="stVerticalBlockBorderWrapper"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) div[data-testid="stVerticalBlock"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) > div[data-testid="stVerticalBlockBorderWrapper"] div[data-testid="stVerticalBlockBorderWrapper"] {{
    background-color: var(--background-color);
}}
div[data-testid="stVerticalBlockBorderWrapper"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) div[data-testid="stVerticalBlock"]:has(div.fixed-container-{id}):not(:has(div.not-fixed-container)) > div[data-testid="element-container"] {{
    display: none;
}}
div[data-testid="stVerticalBlockBorderWrapper"]:has(div.not-fixed-container):not(:has(div[class^='fixed-container-'])) {{
    display: none;
}}
""".strip()

FIXED_CONTAINER_JS = """
const root = parent.document.querySelector('.stApp');
let lastBackgroundColor = null;
function updateContainerBackground(currentBackground) {
    parent.document.documentElement.style.setProperty('--background-color', currentBackground);
    ;
}
function checkForBackgroundColorChange() {
    const style = window.getComputedStyle(root);
    const currentBackgroundColor = style.backgroundColor;
    if (currentBackgroundColor !== lastBackgroundColor) {
        lastBackgroundColor = currentBackgroundColor; // Update the last known value
        updateContainerBackground(lastBackgroundColor);
    }
}
const observerCallback = (mutationsList, observer) => {
    for(let mutation of mutationsList) {
        if (mutation.type === 'attributes' && (mutation.attributeName === 'class' || mutation.attributeName === 'style')) {
            checkForBackgroundColorChange();
        }
    }
};
const main = () => {
    checkForBackgroundColorChange();
    const observer = new MutationObserver(observerCallback);
    observer.observe(root, { attributes: true, childList: false, subtree: false });
}
// main();
document.addEventListener("DOMContentLoaded", main);
""".strip()


MARGINS = {
    "top": "2.875rem",
    "bottom": "0",
}


counter = 0


def st_fixed_container(
    *,
    height: int | None = None,
    border: bool | None = None,
    mode: Literal["fixed", "sticky"] = "fixed",
    position: Literal["top", "bottom"] = "top",
    margin: str | None = None,
    transparent: bool = False,
):
    if margin is None:
        margin = MARGINS[position]
    global counter

    fixed_container = st.container()
    non_fixed_container = st.container()
    css = FIXED_CONTAINER_CSS.format(
        mode=mode,
        position=position,
        margin=margin,
        id=counter,
    )
    with fixed_container:
        html(f"<script>{FIXED_CONTAINER_JS}</script>", scrolling=False, height=0)
        st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)
        st.markdown(
            f"<div class='fixed-container-{counter}'></div>",
            unsafe_allow_html=True,
        )
    with non_fixed_container:
        st.markdown(
            f"<div class='not-fixed-container'></div>",
            unsafe_allow_html=True,
        )
    counter += 1

    parent_container = fixed_container if transparent else fixed_container.container()
    return parent_container.container(height=height, border=border)


if __name__ == "__main__":
    for i in range(30):
        st.write(f"Line {i}")

    # with st_fixed_container(mode="sticky", position="top", border=True):
    # with st_fixed_container(mode="sticky", position="bottom", border=True):
    # with st_fixed_container(mode="fixed", position="top", border=True):
    with st_fixed_container(mode="fixed", position="bottom", border=True):
        st.write("This is a fixed container.")
        st.write("This is a fixed container.")
        st.write("This is a fixed container.")

    st.container(border=True).write("This is a regular container.")
    for i in range(30):
        st.write(f"Line {i}")



================================================
File: rdagent/log/ui/web.py
================================================
import time
from collections import defaultdict
from copy import deepcopy
from datetime import datetime, timezone
from typing import Callable, Type

import pandas as pd
import plotly.express as px
import streamlit as st
from streamlit.delta_generator import DeltaGenerator

from rdagent.components.coder.factor_coder.evaluators import FactorSingleFeedback
from rdagent.components.coder.factor_coder.factor import FactorFBWorkspace, FactorTask
from rdagent.components.coder.model_coder.evaluators import ModelSingleFeedback
from rdagent.components.coder.model_coder.model import ModelFBWorkspace, ModelTask
from rdagent.core.proposal import Hypothesis, HypothesisFeedback, Trace
from rdagent.log.base import Message, Storage, View
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorExperiment
from rdagent.scenarios.qlib.experiment.model_experiment import (
    QlibModelExperiment,
    QlibModelScenario,
)

st.set_page_config(layout="wide")

TIME_DELAY = 0.001


class WebView(View):
    def __init__(self, ui: "StWindow"):
        self.ui = ui
        # Save logs to your desired data structure
        # ...

    def display(self, s: Storage, watch: bool = False):
        for msg in s.iter_msg():  # iterate overtime
            # NOTE:  iter_msg will correctly separate the information.
            # TODO: msg may support streaming mode.
            self.ui.consume_msg(msg)


class StWindow:
    def __init__(self, container: "DeltaGenerator"):
        self.container = container

    def consume_msg(self, msg: Message):
        msg_str = f"{msg.timestamp.astimezone(timezone.utc).isoformat()} | {msg.level} | {msg.caller} - {msg.content}"
        self.container.code(msg_str, language="log")


class LLMWindow(StWindow):
    def __init__(self, container: "DeltaGenerator", session_name: str = "common"):
        self.session_name = session_name
        self.container = container.expander(f"{self.session_name} message")

    def consume_msg(self, msg: Message):
        self.container.chat_message("user").markdown(f"{msg.content}")


class ProgressTabsWindow(StWindow):
    """
    For windows with stream messages, will refresh when a new tab is created.
    """

    def __init__(
        self,
        container: "DeltaGenerator",
        inner_class: Type[StWindow] = StWindow,
        mapper: Callable[[Message], str] = lambda x: x.pid_trace,
    ):
        self.inner_class = inner_class
        self.mapper = mapper

        self.container = container.empty()
        self.tab_windows: dict[str, StWindow] = defaultdict(None)
        self.tab_caches: dict[str, list[Message]] = defaultdict(list)

    def consume_msg(self, msg: Message):
        name = self.mapper(msg)

        if name not in self.tab_windows:
            # new tab need to be created, current streamlit container need to be updated.
            names = list(self.tab_windows.keys()) + [name]

            if len(names) == 1:
                tabs = [self.container.container()]
            else:
                tabs = self.container.tabs(names)

            for id, name in enumerate(names):
                self.tab_windows[name] = self.inner_class(tabs[id])

            # consume the cache
            for name in self.tab_caches:
                for msg in self.tab_caches[name]:
                    self.tab_windows[name].consume_msg(msg)

        self.tab_caches[name].append(msg)
        self.tab_windows[name].consume_msg(msg)


class ObjectsTabsWindow(StWindow):
    def __init__(
        self,
        container: "DeltaGenerator",
        inner_class: Type[StWindow] = StWindow,
        mapper: Callable[[object], str] = lambda x: str(x),
        tab_names: list[str] | None = None,
    ):
        self.inner_class = inner_class
        self.mapper = mapper
        self.container = container
        self.tab_names = tab_names

    def consume_msg(self, msg: Message):
        if isinstance(msg.content, list):
            if self.tab_names:
                assert len(self.tab_names) == len(
                    msg.content
                ), "List of objects should have the same length as provided tab names."
                objs_dict = {self.tab_names[id]: obj for id, obj in enumerate(msg.content)}
            else:
                objs_dict = {self.mapper(obj): obj for obj in msg.content}
        elif not isinstance(msg.content, dict):
            raise ValueError("Message content should be a list or a dict of objects.")

        # two many tabs may cause display problem
        tab_names = list(objs_dict.keys())
        tabs = []
        for i in range(0, len(tab_names), 10):
            tabs.extend(self.container.tabs(tab_names[i : i + 10]))

        for id, obj in enumerate(objs_dict.values()):
            splited_msg = Message(
                tag=msg.tag,
                level=msg.level,
                timestamp=msg.timestamp,
                caller=msg.caller,
                pid_trace=msg.pid_trace,
                content=obj,
            )
            self.inner_class(tabs[id]).consume_msg(splited_msg)


class RoundTabsWindow(StWindow):
    def __init__(
        self,
        container: "DeltaGenerator",
        new_tab_func: Callable[[Message], bool],
        inner_class: Type[StWindow] = StWindow,
        title: str = "Round tabs",
    ):
        container.markdown(f"### **{title}**")
        self.inner_class = inner_class
        self.new_tab_func = new_tab_func
        self.round = 0

        self.current_win = StWindow(container)
        self.tabs_c = container.empty()

    def consume_msg(self, msg: Message):
        if self.new_tab_func(msg):
            self.round += 1
            self.current_win = self.inner_class(self.tabs_c.tabs([str(i) for i in range(1, self.round + 1)])[-1])

        self.current_win.consume_msg(msg)


class HypothesisWindow(StWindow):
    def consume_msg(self, msg: Message | Hypothesis):
        h: Hypothesis = msg.content if isinstance(msg, Message) else msg

        self.container.markdown("#### **Hypothesis💡**")
        self.container.markdown(
            f"""
- **Hypothesis**: {h.hypothesis}
- **Reason**: {h.reason}"""
        )


class HypothesisFeedbackWindow(StWindow):
    def consume_msg(self, msg: Message | HypothesisFeedback):
        h: HypothesisFeedback = msg.content if isinstance(msg, Message) else msg

        self.container.markdown("#### **Hypothesis Feedback🔍**")
        self.container.markdown(
            f"""
- **Observations**: {h.observations}
- **Hypothesis Evaluation**: {h.hypothesis_evaluation}
- **New Hypothesis**: {h.new_hypothesis}
- **Decision**: {h.decision}
- **Reason**: {h.reason}"""
        )


class FactorTaskWindow(StWindow):
    def consume_msg(self, msg: Message | FactorTask):
        ft: FactorTask = msg.content if isinstance(msg, Message) else msg

        self.container.markdown(f"**Factor Name**: {ft.factor_name}")
        self.container.markdown(f"**Description**: {ft.factor_description}")
        self.container.latex(f"Formulation: {ft.factor_formulation}")

        variables_df = pd.DataFrame(ft.variables, index=["Description"]).T
        variables_df.index.name = "Variable"
        self.container.table(variables_df)
        self.container.text(f"Factor resources: {ft.factor_resources}")


class ModelTaskWindow(StWindow):
    def consume_msg(self, msg: Message | ModelTask):
        mt: ModelTask = msg.content if isinstance(msg, Message) else msg

        self.container.markdown(f"**Model Name**: {mt.name}")
        self.container.markdown(f"**Model Type**: {mt.model_type}")
        self.container.markdown(f"**Description**: {mt.description}")
        self.container.latex(f"Formulation: {mt.formulation}")

        variables_df = pd.DataFrame(mt.variables, index=["Value"]).T
        variables_df.index.name = "Variable"
        self.container.table(variables_df)


class FactorFeedbackWindow(StWindow):
    def consume_msg(self, msg: Message | FactorSingleFeedback):
        fb: FactorSingleFeedback = msg.content if isinstance(msg, Message) else msg

        self.container.markdown(
            f"""### :blue[Factor Execution Feedback]
{fb.execution_feedback}
### :blue[Factor Code Feedback]
{fb.code_feedback}
### :blue[Factor Value Feedback]
{fb.value_feedback}
### :blue[Factor Final Feedback]
{fb.final_feedback}
### :blue[Factor Final Decision]
This implementation is {'SUCCESS' if fb.final_decision else 'FAIL'}.
"""
        )


class ModelFeedbackWindow(StWindow):
    def consume_msg(self, msg: Message | ModelSingleFeedback):
        mb: ModelSingleFeedback = msg.content if isinstance(msg, Message) else msg

        self.container.markdown(
            f"""### :blue[Model Execution Feedback]
{mb.execution_feedback}
### :blue[Model Shape Feedback]
{mb.shape_feedback}
### :blue[Model Value Feedback]
{mb.value_feedback}
### :blue[Model Code Feedback]
{mb.code_feedback}
### :blue[Model Final Feedback]
{mb.final_feedback}
### :blue[Model Final Decision]
This implementation is {'SUCCESS' if mb.final_decision else 'FAIL'}.
"""
        )


class WorkspaceWindow(StWindow):
    def __init__(self, container: "DeltaGenerator", show_task_info: bool = False):
        self.container = container
        self.show_task_info = show_task_info

    def consume_msg(self, msg: Message | FactorFBWorkspace | ModelFBWorkspace):
        ws: FactorFBWorkspace | ModelFBWorkspace = msg.content if isinstance(msg, Message) else msg

        # no workspace
        if ws is None:
            return

        # task info
        if self.show_task_info:
            task_msg = deepcopy(msg)
            task_msg.content = ws.target_task
            if isinstance(ws, FactorFBWorkspace):
                self.container.subheader("Factor Info")
                FactorTaskWindow(self.container.container()).consume_msg(task_msg)
            else:
                self.container.subheader("Model Info")
                ModelTaskWindow(self.container.container()).consume_msg(task_msg)

        # task codes
        for k, v in ws.file_dict.items():
            self.container.markdown(f"`{k}`")
            self.container.code(v, language="python")


class QlibFactorExpWindow(StWindow):
    def __init__(self, container: DeltaGenerator, show_task_info: bool = False):
        self.container = container
        self.show_task_info = show_task_info

    def consume_msg(self, msg: Message | QlibFactorExperiment):
        exp: QlibFactorExperiment = msg.content if isinstance(msg, Message) else msg

        # factor tasks
        if self.show_task_info:
            ftm_msg = deepcopy(msg)
            ftm_msg.content = [ws for ws in exp.sub_workspace_list if ws]
            self.container.markdown("**Factor Tasks**")
            ObjectsTabsWindow(
                self.container.container(),
                inner_class=WorkspaceWindow,
                mapper=lambda x: x.target_task.factor_name,
            ).consume_msg(ftm_msg)

        # result
        self.container.markdown("**Results**")
        results = pd.DataFrame({f"base_exp_{id}": e.result for id, e in enumerate(exp.based_experiments)})
        results["now"] = exp.result

        self.container.expander("results table").table(results)

        try:
            bar_chart = px.bar(results, orientation="h", barmode="group")
            self.container.expander("results chart").plotly_chart(bar_chart)
        except:
            self.container.text("Results are incomplete.")


class QlibModelExpWindow(StWindow):
    def __init__(self, container: DeltaGenerator, show_task_info: bool = False):
        self.container = container
        self.show_task_info = show_task_info

    def consume_msg(self, msg: Message | QlibModelExperiment):
        exp: QlibModelExperiment = msg.content if isinstance(msg, Message) else msg

        # model tasks
        if self.show_task_info:
            _msg = deepcopy(msg)
            _msg.content = [ws for ws in exp.sub_workspace_list if ws]
            self.container.markdown("**Model Tasks**")
            ObjectsTabsWindow(
                self.container.container(),
                inner_class=WorkspaceWindow,
                mapper=lambda x: x.target_task.name,
            ).consume_msg(_msg)

        # result
        self.container.subheader("Results", divider=True)
        results = pd.DataFrame({f"base_exp_{id}": e.result for id, e in enumerate(exp.based_experiments)})
        results["now"] = exp.result

        self.container.expander("results table").table(results)


class SimpleTraceWindow(StWindow):
    def __init__(
        self, container: "DeltaGenerator" = st.container(), show_llm: bool = False, show_common_logs: bool = False
    ):
        super().__init__(container)
        self.show_llm = show_llm
        self.show_common_logs = show_common_logs
        self.pid_trace = ""
        self.current_tag = ""

        self.current_win = StWindow(self.container)
        self.evolving_tasks: list[str] = []

    def consume_msg(self, msg: Message):
        # divide tag levels
        if len(msg.tag) > len(self.current_tag):
            # write a header about current task, if it is llm message, not write.
            if not msg.tag.endswith("llm_messages"):
                self.container.header(msg.tag.replace(".", " ➡ "), divider=True)

        self.current_tag = msg.tag

        # set log writer (window) according to msg
        if msg.tag.endswith("llm_messages"):
            # llm messages logs
            if not self.show_llm:
                return
            if not isinstance(self.current_win, LLMWindow):
                self.current_win = LLMWindow(self.container)
        elif isinstance(msg.content, Hypothesis):
            # hypothesis
            self.current_win = HypothesisWindow(self.container)
        elif isinstance(msg.content, HypothesisFeedback):
            # hypothesis feedback
            self.current_win = HypothesisFeedbackWindow(self.container)
        elif isinstance(msg.content, QlibFactorExperiment):
            self.current_win = QlibFactorExpWindow(self.container)
        elif isinstance(msg.content, QlibModelExperiment):
            self.current_win = QlibModelExpWindow(self.container)
        elif isinstance(msg.content, list):
            msg.content = [m for m in msg.content if m]
            if len(msg.content) == 0:
                return
            if isinstance(msg.content[0], FactorTask):
                self.current_win = ObjectsTabsWindow(
                    self.container.expander("Factor Tasks"), FactorTaskWindow, lambda x: x.factor_name
                )
            elif isinstance(msg.content[0], ModelTask):
                self.current_win = ObjectsTabsWindow(
                    self.container.expander("Model Tasks"), ModelTaskWindow, lambda x: x.name
                )

            elif isinstance(msg.content[0], FactorFBWorkspace):
                self.current_win = ObjectsTabsWindow(
                    self.container.expander("Factor Workspaces"),
                    inner_class=WorkspaceWindow,
                    mapper=lambda x: x.target_task.factor_name,
                )
                self.evolving_tasks = [m.target_task.factor_name for m in msg.content]
            elif isinstance(msg.content[0], ModelFBWorkspace):
                self.current_win = ObjectsTabsWindow(
                    self.container.expander("Model Workspaces"),
                    inner_class=WorkspaceWindow,
                    mapper=lambda x: x.target_task.name,
                )
                self.evolving_tasks = [m.target_task.name for m in msg.content]

            elif isinstance(msg.content[0], FactorSingleFeedback):
                self.current_win = ObjectsTabsWindow(
                    self.container.expander("Factor Feedbacks"),
                    inner_class=FactorFeedbackWindow,
                    tab_names=self.evolving_tasks,
                )
            elif isinstance(msg.content[0], ModelSingleFeedback):
                self.current_win = ObjectsTabsWindow(
                    self.container.expander("Model Feedbacks"),
                    inner_class=ModelFeedbackWindow,
                    tab_names=self.evolving_tasks,
                )
        else:
            # common logs
            if not self.show_common_logs:
                return
            self.current_win = StWindow(self.container)

        self.current_win.consume_msg(msg)


def mock_msg(obj) -> Message:
    return Message(tag="mock", level="INFO", timestamp=datetime.now(), pid_trace="000", caller="mock", content=obj)


class TraceObjWindow(StWindow):
    def __init__(self, container: "DeltaGenerator" = st.container()):
        self.container = container

    def consume_msg(self, msg: Message | Trace):
        if isinstance(msg, Message):
            trace: Trace = msg.content
        else:
            trace = msg

        for id, (h, e, hf) in enumerate(trace.hist):
            self.container.header(f"Trace History {id}", divider=True)
            HypothesisWindow(self.container).consume_msg(mock_msg(h))
            if isinstance(e, QlibFactorExperiment):
                QlibFactorExpWindow(self.container).consume_msg(mock_msg(e))
            else:
                QlibModelExpWindow(self.container).consume_msg(mock_msg(e))
            HypothesisFeedbackWindow(self.container).consume_msg(mock_msg(hf))


class ResearchWindow(StWindow):
    def consume_msg(self, msg: Message):
        if msg.tag.endswith("hypothesis generation"):
            HypothesisWindow(self.container.container()).consume_msg(msg)
        elif msg.tag.endswith("experiment generation"):
            if isinstance(msg.content, list):
                if isinstance(msg.content[0], FactorTask):
                    self.container.markdown("**Factor Tasks**")
                    ObjectsTabsWindow(
                        self.container.container(), FactorTaskWindow, lambda x: x.factor_name
                    ).consume_msg(msg)
                elif isinstance(msg.content[0], ModelTask):
                    self.container.markdown("**Model Tasks**")
                    ObjectsTabsWindow(self.container.container(), ModelTaskWindow, lambda x: x.name).consume_msg(msg)
        elif msg.tag.endswith("load_pdf_screenshot"):
            self.container.image(msg.content)
        elif msg.tag.endswith("load_factor_tasks"):
            self.container.json(msg.content)


class EvolvingWindow(StWindow):
    def __init__(self, container: "DeltaGenerator"):
        self.container = container
        self.evolving_tasks: list[str] = []

    def consume_msg(self, msg: Message):
        if msg.tag.endswith("evolving code"):
            if isinstance(msg.content, list):
                msg.content = [m for m in msg.content if m]
                if len(msg.content) == 0:
                    return
                if isinstance(msg.content[0], FactorFBWorkspace):
                    self.container.markdown("**Factor Codes**")
                    ObjectsTabsWindow(
                        self.container.container(),
                        inner_class=WorkspaceWindow,
                        mapper=lambda x: x.target_task.factor_name,
                    ).consume_msg(msg)
                    self.evolving_tasks = [m.target_task.factor_name for m in msg.content]
                elif isinstance(msg.content[0], ModelFBWorkspace):
                    self.container.markdown("**Model Codes**")
                    ObjectsTabsWindow(
                        self.container.container(), inner_class=WorkspaceWindow, mapper=lambda x: x.target_task.name
                    ).consume_msg(msg)
                    self.evolving_tasks = [m.target_task.name for m in msg.content]
        elif msg.tag.endswith("evolving feedback"):
            if isinstance(msg.content, list):
                msg.content = [m for m in msg.content if m]
                if len(msg.content) == 0:
                    return
                if isinstance(msg.content[0], FactorSingleFeedback):
                    self.container.markdown("**Factor Feedbacks🔍**")
                    ObjectsTabsWindow(
                        self.container.container(), inner_class=FactorFeedbackWindow, tab_names=self.evolving_tasks
                    ).consume_msg(msg)
                elif isinstance(msg.content[0], ModelSingleFeedback):
                    self.container.markdown("**Model Feedbacks🔍**")
                    ObjectsTabsWindow(
                        self.container.container(), inner_class=ModelFeedbackWindow, tab_names=self.evolving_tasks
                    ).consume_msg(msg)


class DevelopmentWindow(StWindow):
    def __init__(self, container: "DeltaGenerator"):
        self.E_win = RoundTabsWindow(
            container.container(),
            new_tab_func=lambda x: x.tag.endswith("evolving code"),
            inner_class=EvolvingWindow,
            title="Evolving Loops🔧",
        )

    def consume_msg(self, msg: Message):
        if "evolving" in msg.tag:
            self.E_win.consume_msg(msg)


class FeedbackWindow(StWindow):
    def __init__(self, container: "DeltaGenerator"):
        self.container = container

    def consume_msg(self, msg: Message):
        if msg.tag.endswith("returns"):
            fig = px.line(msg.content)
            self.container.markdown("**Returns📈**")
            self.container.plotly_chart(fig)
        elif isinstance(msg.content, HypothesisFeedback):
            HypothesisFeedbackWindow(self.container.container(border=True)).consume_msg(msg)
        elif isinstance(msg.content, QlibModelExperiment):
            QlibModelExpWindow(self.container.container(border=True)).consume_msg(msg)
        elif isinstance(msg.content, QlibFactorExperiment):
            QlibFactorExpWindow(self.container.container(border=True)).consume_msg(msg)


class SingleRDLoopWindow(StWindow):
    def __init__(self, container: "DeltaGenerator"):
        self.container = container
        col1, col2 = self.container.columns([2, 3])
        self.R_win = ResearchWindow(col1.container(border=True))
        self.F_win = FeedbackWindow(col1.container(border=True))
        self.D_win = DevelopmentWindow(col2.container(border=True))

    def consume_msg(self, msg: Message):
        tags = msg.tag.split(".")
        if "r" in tags:
            self.R_win.consume_msg(msg)
        elif "d" in tags:
            self.D_win.consume_msg(msg)
        elif "ef" in tags:
            self.F_win.consume_msg(msg)


class TraceWindow(StWindow):
    def __init__(
        self, container: "DeltaGenerator" = st.container(), show_llm: bool = False, show_common_logs: bool = False
    ):
        self.show_llm = show_llm
        self.show_common_logs = show_common_logs
        image_c, scen_c = container.columns([2, 3], vertical_alignment="center")
        image_c.image("scen.png")
        scen_c.container(border=True).markdown(QlibModelScenario().rich_style_description)
        top_container = container.container()
        col1, col2 = top_container.columns([2, 3])
        chart_c = col2.container(border=True, height=500)
        chart_c.markdown("**Metrics📈**")
        self.chart_c = chart_c.empty()
        hypothesis_status_c = col1.container(border=True, height=500)
        hypothesis_status_c.markdown("**Hypotheses🏅**")
        self.summary_c = hypothesis_status_c.empty()

        self.RDL_win = RoundTabsWindow(
            container.container(),
            new_tab_func=lambda x: x.tag.endswith("hypothesis generation"),
            inner_class=SingleRDLoopWindow,
            title="R&D Loops♾️",
        )

        self.hypothesis_decisions = defaultdict(bool)
        self.hypotheses: list[Hypothesis] = []

        self.results = []

    def consume_msg(self, msg: Message):
        if not self.show_llm and "llm_messages" in msg.tag:
            return
        if not self.show_common_logs and isinstance(msg.content, str):
            return
        if isinstance(msg.content, dict):
            return
        if msg.tag.endswith("hypothesis generation"):
            self.hypotheses.append(msg.content)
        elif msg.tag.endswith("ef.feedback"):
            self.hypothesis_decisions[self.hypotheses[-1]] = msg.content.decision
            self.summary_c.markdown(
                "\n".join(
                    (
                        f"{id+1}. :green[{self.hypotheses[id].hypothesis}]\n\t>*{self.hypotheses[id].concise_reason}*"
                        if d
                        else f"{id+1}. {self.hypotheses[id].hypothesis}\n\t>*{self.hypotheses[id].concise_reason}*"
                    )
                    for id, (h, d) in enumerate(self.hypothesis_decisions.items())
                )
            )
        elif msg.tag.endswith("ef.model runner result") or msg.tag.endswith("ef.factor runner result"):
            self.results.append(msg.content.result)
            if len(self.results) == 1:
                self.chart_c.table(self.results[0])
            else:
                df = pd.DataFrame(self.results, index=range(1, len(self.results) + 1))
                fig = px.line(df, x=df.index, y=df.columns, markers=True)
                self.chart_c.plotly_chart(fig)

        self.RDL_win.consume_msg(msg)
        # time.sleep(TIME_DELAY)



================================================
File: rdagent/oai/llm_conf.py
================================================
from __future__ import annotations

from pathlib import Path

from pydantic import Field

from rdagent.core.conf import ExtendedBaseSettings


class LLMSettings(ExtendedBaseSettings):
    # backend
    backend: str = "rdagent.oai.backend.DeprecBackend"

    # TODO: most of the settings are only used on deprec.DeprecBackend.
    # So they should move the settings to that folder.

    log_llm_chat_content: bool = True

    use_azure: bool = Field(default=False, deprecated=True)
    chat_use_azure: bool = False
    embedding_use_azure: bool = False

    chat_use_azure_token_provider: bool = False
    embedding_use_azure_token_provider: bool = False
    managed_identity_client_id: str | None = None
    max_retry: int = 10
    retry_wait_seconds: int = 1
    dump_chat_cache: bool = False
    use_chat_cache: bool = False
    dump_embedding_cache: bool = False
    use_embedding_cache: bool = False
    prompt_cache_path: str = str(Path.cwd() / "prompt_cache.db")
    max_past_message_include: int = 10

    # Behavior of returning answers to the same question when caching is enabled
    use_auto_chat_cache_seed_gen: bool = False
    """
    `_create_chat_completion_inner_function` provides a feature to pass in a seed to affect the cache hash key
    We want to enable a auto seed generator to get different default seed for `_create_chat_completion_inner_function`
    if seed is not given.
    So the cache will only not miss you ask the same question on same round.
    """
    init_chat_cache_seed: int = 42

    # Chat configs
    openai_api_key: str = ""  # TODO: simplify the key design.
    chat_openai_api_key: str | None = None
    chat_openai_base_url: str | None = None  #
    chat_azure_api_base: str = ""
    chat_azure_api_version: str = ""
    chat_model: str = "gpt-4-turbo"
    chat_max_tokens: int = 3000
    chat_temperature: float = 0.5
    chat_stream: bool = True
    chat_seed: int | None = None
    chat_frequency_penalty: float = 0.0
    chat_presence_penalty: float = 0.0
    chat_token_limit: int = (
        100000  # 100000 is the maximum limit of gpt4, which might increase in the future version of gpt
    )
    default_system_prompt: str = "You are an AI assistant who helps to answer user's questions."
    system_prompt_role: str = "system"
    """Some models (like o1) do not support the 'system' role.
    Therefore, we make the system_prompt_role customizable to ensure successful calls."""

    # Embedding configs
    embedding_openai_api_key: str = ""
    embedding_openai_base_url: str = ""
    embedding_azure_api_base: str = ""
    embedding_azure_api_version: str = ""
    embedding_model: str = ""
    embedding_max_str_num: int = 50

    # offline llama2 related config
    use_llama2: bool = False
    llama2_ckpt_dir: str = "Llama-2-7b-chat"
    llama2_tokenizer_path: str = "Llama-2-7b-chat/tokenizer.model"
    llams2_max_batch_size: int = 8

    # server served endpoints
    use_gcr_endpoint: bool = False
    gcr_endpoint_type: str = "llama2_70b"  # or "llama3_70b", "phi2", "phi3_4k", "phi3_128k"

    llama2_70b_endpoint: str = ""
    llama2_70b_endpoint_key: str = ""
    llama2_70b_endpoint_deployment: str = ""

    llama3_70b_endpoint: str = ""
    llama3_70b_endpoint_key: str = ""
    llama3_70b_endpoint_deployment: str = ""

    phi2_endpoint: str = ""
    phi2_endpoint_key: str = ""
    phi2_endpoint_deployment: str = ""

    phi3_4k_endpoint: str = ""
    phi3_4k_endpoint_key: str = ""
    phi3_4k_endpoint_deployment: str = ""

    phi3_128k_endpoint: str = ""
    phi3_128k_endpoint_key: str = ""
    phi3_128k_endpoint_deployment: str = ""

    gcr_endpoint_temperature: float = 0.7
    gcr_endpoint_top_p: float = 0.9
    gcr_endpoint_do_sample: bool = False
    gcr_endpoint_max_token: int = 100

    chat_use_azure_deepseek: bool = False
    chat_azure_deepseek_endpoint: str = ""
    chat_azure_deepseek_key: str = ""

    chat_model_map: str = "{}"


LLM_SETTINGS = LLMSettings()



================================================
File: rdagent/oai/llm_utils.py
================================================
from __future__ import annotations

from typing import Any, Type

import numpy as np

from rdagent.core.utils import import_class
from rdagent.oai.backend.base import APIBackend as BaseAPIBackend
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.utils import md5_hash  # for compatible with previous import


def calculate_embedding_distance_between_str_list(
    source_str_list: list[str],
    target_str_list: list[str],
) -> list[list[float]]:
    if not source_str_list or not target_str_list:
        return [[]]

    embeddings = APIBackend().create_embedding(source_str_list + target_str_list)

    source_embeddings = embeddings[: len(source_str_list)]
    target_embeddings = embeddings[len(source_str_list) :]

    source_embeddings_np = np.array(source_embeddings)
    target_embeddings_np = np.array(target_embeddings)

    source_embeddings_np = source_embeddings_np / np.linalg.norm(source_embeddings_np, axis=1, keepdims=True)
    target_embeddings_np = target_embeddings_np / np.linalg.norm(target_embeddings_np, axis=1, keepdims=True)
    similarity_matrix = np.dot(source_embeddings_np, target_embeddings_np.T)

    return similarity_matrix.tolist()  # type: ignore[no-any-return]


def get_api_backend(*args: Any, **kwargs: Any) -> BaseAPIBackend:  # TODO: import it from base.py
    """
    get llm api backend based on settings dynamically.
    """
    api_backend_cls: Type[BaseAPIBackend] = import_class(LLM_SETTINGS.backend)
    return api_backend_cls(*args, **kwargs)


# Alias
APIBackend = get_api_backend



================================================
File: rdagent/oai/backend/__init__.py
================================================
from .deprec import DeprecBackend
from .litellm import LiteLLMAPIBackend



================================================
File: rdagent/oai/backend/base.py
================================================
from __future__ import annotations

import json
import re
import sqlite3
import time
import uuid
from abc import ABC, abstractmethod
from copy import deepcopy
from pathlib import Path
from typing import Any, Optional, cast

from pydantic import TypeAdapter

from rdagent.core.utils import LLM_CACHE_SEED_GEN, SingletonBaseClass
from rdagent.log import LogColors
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.utils import md5_hash


class SQliteLazyCache(SingletonBaseClass):
    def __init__(self, cache_location: str) -> None:
        super().__init__()
        self.cache_location = cache_location
        db_file_exist = Path(cache_location).exists()
        # TODO: sqlite3 does not support multiprocessing.
        self.conn = sqlite3.connect(cache_location, timeout=20)
        self.c = self.conn.cursor()
        if not db_file_exist:
            self.c.execute(
                """
                CREATE TABLE chat_cache (
                    md5_key TEXT PRIMARY KEY,
                    chat TEXT
                )
                """,
            )
            self.c.execute(
                """
                CREATE TABLE embedding_cache (
                    md5_key TEXT PRIMARY KEY,
                    embedding TEXT
                )
                """,
            )
            self.c.execute(
                """
                CREATE TABLE message_cache (
                    conversation_id TEXT PRIMARY KEY,
                    message TEXT
                )
                """,
            )
            self.conn.commit()

    def chat_get(self, key: str) -> str | None:
        md5_key = md5_hash(key)
        self.c.execute("SELECT chat FROM chat_cache WHERE md5_key=?", (md5_key,))
        result = self.c.fetchone()
        return None if result is None else result[0]

    def embedding_get(self, key: str) -> list | dict | str | None:
        md5_key = md5_hash(key)
        self.c.execute("SELECT embedding FROM embedding_cache WHERE md5_key=?", (md5_key,))
        result = self.c.fetchone()
        return None if result is None else json.loads(result[0])

    def chat_set(self, key: str, value: str) -> None:
        md5_key = md5_hash(key)
        self.c.execute(
            "INSERT OR REPLACE INTO chat_cache (md5_key, chat) VALUES (?, ?)",
            (md5_key, value),
        )
        self.conn.commit()
        return None

    def embedding_set(self, content_to_embedding_dict: dict) -> None:
        for key, value in content_to_embedding_dict.items():
            md5_key = md5_hash(key)
            self.c.execute(
                "INSERT OR REPLACE INTO embedding_cache (md5_key, embedding) VALUES (?, ?)",
                (md5_key, json.dumps(value)),
            )
        self.conn.commit()

    def message_get(self, conversation_id: str) -> list[dict[str, Any]]:
        self.c.execute("SELECT message FROM message_cache WHERE conversation_id=?", (conversation_id,))
        result = self.c.fetchone()
        return [] if result is None else cast(list[dict[str, Any]], json.loads(result[0]))

    def message_set(self, conversation_id: str, message_value: list[dict[str, Any]]) -> None:
        self.c.execute(
            "INSERT OR REPLACE INTO message_cache (conversation_id, message) VALUES (?, ?)",
            (conversation_id, json.dumps(message_value)),
        )
        self.conn.commit()
        return None


class SessionChatHistoryCache(SingletonBaseClass):
    def __init__(self) -> None:
        """load all history conversation json file from self.session_cache_location"""
        self.cache = SQliteLazyCache(cache_location=LLM_SETTINGS.prompt_cache_path)

    def message_get(self, conversation_id: str) -> list[dict[str, Any]]:
        return self.cache.message_get(conversation_id)

    def message_set(self, conversation_id: str, message_value: list[dict[str, Any]]) -> None:
        self.cache.message_set(conversation_id, message_value)


class ChatSession:
    def __init__(self, api_backend: Any, conversation_id: str | None = None, system_prompt: str | None = None) -> None:
        self.conversation_id = str(uuid.uuid4()) if conversation_id is None else conversation_id
        self.system_prompt = system_prompt if system_prompt is not None else LLM_SETTINGS.default_system_prompt
        self.api_backend = api_backend

    def build_chat_completion_message(self, user_prompt: str) -> list[dict[str, Any]]:
        history_message = SessionChatHistoryCache().message_get(self.conversation_id)
        messages = history_message
        if not messages:
            messages.append({"role": LLM_SETTINGS.system_prompt_role, "content": self.system_prompt})
        messages.append(
            {
                "role": "user",
                "content": user_prompt,
            },
        )
        return messages

    def build_chat_completion_message_and_calculate_token(self, user_prompt: str) -> Any:
        messages = self.build_chat_completion_message(user_prompt)
        return self.api_backend._calculate_token_from_messages(messages)

    def build_chat_completion(self, user_prompt: str, *args, **kwargs) -> str:  # type: ignore[no-untyped-def]
        """
        this function is to build the session messages
        user prompt should always be provided
        """
        messages = self.build_chat_completion_message(user_prompt)

        with logger.tag(f"session_{self.conversation_id}"):
            response: str = self.api_backend._try_create_chat_completion_or_embedding(  # noqa: SLF001
                *args,
                messages=messages,
                chat_completion=True,
                **kwargs,
            )
            logger.log_object({"user": user_prompt, "resp": response}, tag="debug_llm")

        messages.append(
            {
                "role": "assistant",
                "content": response,
            },
        )
        SessionChatHistoryCache().message_set(self.conversation_id, messages)
        return response

    def get_conversation_id(self) -> str:
        return self.conversation_id

    def display_history(self) -> None:
        # TODO: Realize a beautiful presentation format for history messages
        pass


class APIBackend(ABC):
    """
    Abstract base class for LLM API backends
    supporting auto retry, cache and auto continue
    Inner api call should be implemented in the subclass
    """

    def __init__(
        self,
        use_chat_cache: bool | None = None,
        dump_chat_cache: bool | None = None,
        use_embedding_cache: bool | None = None,
        dump_embedding_cache: bool | None = None,
    ):
        self.dump_chat_cache = LLM_SETTINGS.dump_chat_cache if dump_chat_cache is None else dump_chat_cache
        self.use_chat_cache = LLM_SETTINGS.use_chat_cache if use_chat_cache is None else use_chat_cache
        self.dump_embedding_cache = (
            LLM_SETTINGS.dump_embedding_cache if dump_embedding_cache is None else dump_embedding_cache
        )
        self.use_embedding_cache = (
            LLM_SETTINGS.use_embedding_cache if use_embedding_cache is None else use_embedding_cache
        )
        if self.dump_chat_cache or self.use_chat_cache or self.dump_embedding_cache or self.use_embedding_cache:
            self.cache_file_location = LLM_SETTINGS.prompt_cache_path
            self.cache = SQliteLazyCache(cache_location=self.cache_file_location)

        self.retry_wait_seconds = LLM_SETTINGS.retry_wait_seconds

    def build_chat_session(
        self,
        conversation_id: str | None = None,
        session_system_prompt: str | None = None,
    ) -> ChatSession:
        """
        conversation_id is a 256-bit string created by uuid.uuid4() and is also
        the file name under session_cache_folder/ for each conversation
        """
        return ChatSession(self, conversation_id, session_system_prompt)

    def _build_messages(
        self,
        user_prompt: str,
        system_prompt: str | None = None,
        former_messages: list[dict[str, Any]] | None = None,
        *,
        shrink_multiple_break: bool = False,
    ) -> list[dict[str, Any]]:
        """
        build the messages to avoid implementing several redundant lines of code

        """
        if former_messages is None:
            former_messages = []
        # shrink multiple break will recursively remove multiple breaks(more than 2)
        if shrink_multiple_break:
            while "\n\n\n" in user_prompt:
                user_prompt = user_prompt.replace("\n\n\n", "\n\n")
            if system_prompt is not None:
                while "\n\n\n" in system_prompt:
                    system_prompt = system_prompt.replace("\n\n\n", "\n\n")
        system_prompt = LLM_SETTINGS.default_system_prompt if system_prompt is None else system_prompt
        messages = [
            {
                "role": LLM_SETTINGS.system_prompt_role,
                "content": system_prompt,
            },
        ]
        messages.extend(former_messages[-1 * LLM_SETTINGS.max_past_message_include :])
        messages.append(
            {
                "role": "user",
                "content": user_prompt,
            },
        )
        return messages

    def _build_log_messages(self, messages: list[dict[str, Any]]) -> str:
        log_messages = ""
        for m in messages:
            log_messages += (
                f"\n{LogColors.MAGENTA}{LogColors.BOLD}Role:{LogColors.END}"
                f"{LogColors.CYAN}{m['role']}{LogColors.END}\n"
                f"{LogColors.MAGENTA}{LogColors.BOLD}Content:{LogColors.END} "
                f"{LogColors.CYAN}{m['content']}{LogColors.END}\n"
            )
        return log_messages

    def build_messages_and_create_chat_completion(  # type: ignore[no-untyped-def]
        self,
        user_prompt: str,
        system_prompt: str | None = None,
        former_messages: list | None = None,
        chat_cache_prefix: str = "",
        shrink_multiple_break: bool = False,
        *args,
        **kwargs,
    ) -> str:
        if former_messages is None:
            former_messages = []
        messages = self._build_messages(
            user_prompt,
            system_prompt,
            former_messages,
            shrink_multiple_break=shrink_multiple_break,
        )

        resp = self._try_create_chat_completion_or_embedding(  # type: ignore[misc]
            *args,
            messages=messages,
            chat_completion=True,
            chat_cache_prefix=chat_cache_prefix,
            **kwargs,
        )
        if isinstance(resp, list):
            raise ValueError("The response of _try_create_chat_completion_or_embedding should be a string.")
        logger.log_object({"system": system_prompt, "user": user_prompt, "resp": resp}, tag="debug_llm")
        return resp

    def create_embedding(self, input_content: str | list[str], *args, **kwargs) -> list[float] | list[list[float]]:  # type: ignore[no-untyped-def]
        input_content_list = [input_content] if isinstance(input_content, str) else input_content
        resp = self._try_create_chat_completion_or_embedding(  # type: ignore[misc]
            input_content_list=input_content_list,
            embedding=True,
            *args,
            **kwargs,
        )
        if isinstance(input_content, str):
            return resp[0]  # type: ignore[return-value]
        return resp  # type: ignore[return-value]

    def build_messages_and_calculate_token(
        self,
        user_prompt: str,
        system_prompt: str | None,
        former_messages: list[dict[str, Any]] | None = None,
        *,
        shrink_multiple_break: bool = False,
    ) -> int:
        if former_messages is None:
            former_messages = []
        messages = self._build_messages(
            user_prompt, system_prompt, former_messages, shrink_multiple_break=shrink_multiple_break
        )
        return self._calculate_token_from_messages(messages)

    def _try_create_chat_completion_or_embedding(  # type: ignore[no-untyped-def]
        self,
        max_retry: int = 10,
        chat_completion: bool = False,
        embedding: bool = False,
        *args,
        **kwargs,
    ) -> str | list[list[float]]:
        assert not (chat_completion and embedding), "chat_completion and embedding cannot be True at the same time"
        max_retry = LLM_SETTINGS.max_retry if LLM_SETTINGS.max_retry is not None else max_retry
        for i in range(max_retry):
            try:
                if embedding:
                    return self._create_embedding_with_cache(*args, **kwargs)
                if chat_completion:
                    return self._create_chat_completion_auto_continue(*args, **kwargs)
            except Exception as e:  # noqa: BLE001
                if hasattr(e, "message") and (
                    "'messages' must contain the word 'json' in some form" in e.message
                    or "\\'messages\\' must contain the word \\'json\\' in some form" in e.message
                ):
                    kwargs["add_json_in_prompt"] = True
                elif hasattr(e, "message") and embedding and "maximum context length" in e.message:
                    kwargs["input_content_list"] = [
                        content[: len(content) // 2] for content in kwargs.get("input_content_list", [])
                    ]
                else:
                    time.sleep(self.retry_wait_seconds)
                logger.warning(str(e))
                logger.warning(f"Retrying {i+1}th time...")
        error_message = f"Failed to create chat completion after {max_retry} retries."
        raise RuntimeError(error_message)

    def _create_chat_completion_add_json_in_prompt(
        self,
        messages: list[dict[str, Any]],
        add_json_in_prompt: bool = False,
        json_mode: bool = False,
        *args: Any,
        **kwargs: Any,
    ) -> tuple[str, str | None]:
        """
        add json related content in the prompt if add_json_in_prompt is True
        """
        if json_mode and add_json_in_prompt:
            for message in messages[::-1]:
                message["content"] = message["content"] + "\nPlease respond in json format."
                if message["role"] == LLM_SETTINGS.system_prompt_role:
                    # NOTE: assumption: systemprompt is always the first message
                    break
        return self._create_chat_completion_inner_function(messages=messages, json_mode=json_mode, *args, **kwargs)  # type: ignore[misc]

    def _create_chat_completion_auto_continue(
        self,
        messages: list[dict[str, Any]],
        *args: Any,
        json_mode: bool = False,
        chat_cache_prefix: str = "",
        seed: Optional[int] = None,
        json_target_type: Optional[str] = None,
        **kwargs: Any,
    ) -> str:
        """
        Call the chat completion function and automatically continue the conversation if the finish_reason is length.
        """
        if seed is None and LLM_SETTINGS.use_auto_chat_cache_seed_gen:
            seed = LLM_CACHE_SEED_GEN.get_next_seed()
        input_content_json = json.dumps(messages)
        input_content_json = (
            chat_cache_prefix + input_content_json + f"<seed={seed}/>"
        )  # FIXME this is a hack to make sure the cache represents the round index
        if self.use_chat_cache:
            cache_result = self.cache.chat_get(input_content_json)
            if cache_result is not None:
                if LLM_SETTINGS.log_llm_chat_content:
                    logger.info(f"{LogColors.CYAN}Response:{cache_result}{LogColors.END}", tag="llm_messages")
                return cache_result

        all_response = ""
        new_messages = deepcopy(messages)
        for _ in range(6):  # for some long code, 3 times may not enough for reasoning models
            if "json_mode" in kwargs:
                del kwargs["json_mode"]
            response, finish_reason = self._create_chat_completion_add_json_in_prompt(
                new_messages, json_mode=json_mode, *args, **kwargs
            )  # type: ignore[misc]
            all_response += response
            if finish_reason is None or finish_reason != "length":
                if json_mode:
                    try:
                        json.loads(all_response)
                    except:
                        match = re.search(r"```json(.*?)```", all_response, re.DOTALL)
                        all_response = match.groups()[0] if match else all_response
                        json.loads(all_response)
                if json_target_type is not None:
                    TypeAdapter(json_target_type).validate_json(all_response)
                if self.dump_chat_cache:
                    self.cache.chat_set(input_content_json, all_response)
                return all_response
            new_messages.append({"role": "assistant", "content": response})
        raise RuntimeError("Failed to continue the conversation after 3 retries.")

    def _create_embedding_with_cache(
        self, input_content_list: list[str], *args: Any, **kwargs: Any
    ) -> list[list[float]]:
        content_to_embedding_dict = {}
        filtered_input_content_list = []
        if self.use_embedding_cache:
            for content in input_content_list:
                cache_result = self.cache.embedding_get(content)
                if cache_result is not None:
                    content_to_embedding_dict[content] = cache_result
                else:
                    filtered_input_content_list.append(content)
        else:
            filtered_input_content_list = input_content_list

        if len(filtered_input_content_list) > 0:
            resp = self._create_embedding_inner_function(input_content_list=filtered_input_content_list)
            for index, data in enumerate(resp):
                content_to_embedding_dict[filtered_input_content_list[index]] = data
            if self.dump_embedding_cache:
                self.cache.embedding_set(content_to_embedding_dict)
        return [content_to_embedding_dict[content] for content in input_content_list]  # type: ignore[misc]

    @abstractmethod
    def _calculate_token_from_messages(self, messages: list[dict[str, Any]]) -> int:
        """
        Calculate the token count from messages
        """
        raise NotImplementedError("Subclasses must implement this method")

    @abstractmethod
    def _create_embedding_inner_function(  # type: ignore[no-untyped-def]
        self, input_content_list: list[str], *args, **kwargs
    ) -> list[list[float]]:  # noqa: ARG002
        """
        Call the embedding function
        """
        raise NotImplementedError("Subclasses must implement this method")

    @abstractmethod
    def _create_chat_completion_inner_function(  # type: ignore[no-untyped-def] # noqa: C901, PLR0912, PLR0915
        self,
        messages: list[dict[str, Any]],
        json_mode: bool = False,
        *args,
        **kwargs,
    ) -> tuple[str, str | None]:
        """
        Call the chat completion function
        """
        raise NotImplementedError("Subclasses must implement this method")



================================================
File: rdagent/oai/backend/deprec.py
================================================
from __future__ import annotations

import inspect
import json
import os
import random
import re
import sqlite3
import ssl
import time
import urllib.request
import uuid
from copy import deepcopy
from pathlib import Path
from typing import Any, Optional, cast

import numpy as np
import openai
import tiktoken

from rdagent.core.utils import LLM_CACHE_SEED_GEN, SingletonBaseClass, import_class
from rdagent.log import LogColors
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.utils import md5_hash

DEFAULT_QLIB_DOT_PATH = Path("./")

from rdagent.oai.backend.base import APIBackend

try:
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider
except ImportError:
    logger.warning("azure.identity is not installed.")

try:
    import openai
except ImportError:
    logger.warning("openai is not installed.")

try:
    from llama import Llama
except ImportError:
    if LLM_SETTINGS.use_llama2:
        logger.warning("llama is not installed.")

try:
    from azure.ai.inference import ChatCompletionsClient
    from azure.ai.inference.models import (
        AssistantMessage,
        ChatRequestMessage,
        SystemMessage,
        UserMessage,
    )
    from azure.core.credentials import AzureKeyCredential
except ImportError:
    if LLM_SETTINGS.chat_use_azure_deepseek:
        logger.warning("azure.ai.inference or azure.core.credentials is not installed.")


class ConvManager:
    """
    This is a conversation manager of LLM
    It is for convenience of exporting conversation for debugging.
    """

    def __init__(
        self,
        path: Path | str = DEFAULT_QLIB_DOT_PATH / "llm_conv",
        recent_n: int = 10,
    ) -> None:
        self.path = Path(path)
        self.path.mkdir(parents=True, exist_ok=True)
        self.recent_n = recent_n

    def _rotate_files(self) -> None:
        pairs = []
        for f in self.path.glob("*.json"):
            m = re.match(r"(\d+).json", f.name)
            if m is not None:
                n = int(m.group(1))
                pairs.append((n, f))
        pairs.sort(key=lambda x: x[0])
        for n, f in pairs[: self.recent_n][::-1]:
            if (self.path / f"{n+1}.json").exists():
                (self.path / f"{n+1}.json").unlink()
            f.rename(self.path / f"{n+1}.json")

    def append(self, conv: tuple[list, str]) -> None:
        self._rotate_files()
        with (self.path / "0.json").open("w") as file:
            json.dump(conv, file)
        # TODO: reseve line breaks to make it more convient to edit file directly.


class DeprecBackend(APIBackend):
    """
    This is a unified interface for different backends.

    (xiao) thinks integrate all kinds of API in a single class is not a good design.
    So we should split them into different classes in `oai/backends/` in the future.
    """

    # FIXME: (xiao) We should avoid using self.xxxx.
    # Instead, we can use LLM_SETTINGS directly. If it's difficult to support different backend settings, we can split them into multiple BaseSettings.
    def __init__(  # noqa: C901, PLR0912, PLR0915
        self,
        *args: Any,
        **kwargs: Any,
    ) -> None:
        super().__init__(*args, **kwargs)
        if LLM_SETTINGS.use_llama2:
            self.generator = Llama.build(
                ckpt_dir=LLM_SETTINGS.llama2_ckpt_dir,
                tokenizer_path=LLM_SETTINGS.llama2_tokenizer_path,
                max_seq_len=LLM_SETTINGS.chat_max_tokens,
                max_batch_size=LLM_SETTINGS.llams2_max_batch_size,
            )
            self.encoder = None
        elif LLM_SETTINGS.use_gcr_endpoint:
            gcr_endpoint_type = LLM_SETTINGS.gcr_endpoint_type
            if gcr_endpoint_type == "llama2_70b":
                self.gcr_endpoint_key = LLM_SETTINGS.llama2_70b_endpoint_key
                self.gcr_endpoint_deployment = LLM_SETTINGS.llama2_70b_endpoint_deployment
                self.gcr_endpoint = LLM_SETTINGS.llama2_70b_endpoint
            elif gcr_endpoint_type == "llama3_70b":
                self.gcr_endpoint_key = LLM_SETTINGS.llama3_70b_endpoint_key
                self.gcr_endpoint_deployment = LLM_SETTINGS.llama3_70b_endpoint_deployment
                self.gcr_endpoint = LLM_SETTINGS.llama3_70b_endpoint
            elif gcr_endpoint_type == "phi2":
                self.gcr_endpoint_key = LLM_SETTINGS.phi2_endpoint_key
                self.gcr_endpoint_deployment = LLM_SETTINGS.phi2_endpoint_deployment
                self.gcr_endpoint = LLM_SETTINGS.phi2_endpoint
            elif gcr_endpoint_type == "phi3_4k":
                self.gcr_endpoint_key = LLM_SETTINGS.phi3_4k_endpoint_key
                self.gcr_endpoint_deployment = LLM_SETTINGS.phi3_4k_endpoint_deployment
                self.gcr_endpoint = LLM_SETTINGS.phi3_4k_endpoint
            elif gcr_endpoint_type == "phi3_128k":
                self.gcr_endpoint_key = LLM_SETTINGS.phi3_128k_endpoint_key
                self.gcr_endpoint_deployment = LLM_SETTINGS.phi3_128k_endpoint_deployment
                self.gcr_endpoint = LLM_SETTINGS.phi3_128k_endpoint
            else:
                error_message = f"Invalid gcr_endpoint_type: {gcr_endpoint_type}"
                raise ValueError(error_message)
            self.headers = {
                "Content-Type": "application/json",
                "Authorization": ("Bearer " + self.gcr_endpoint_key),
            }
            self.gcr_endpoint_temperature = LLM_SETTINGS.gcr_endpoint_temperature
            self.gcr_endpoint_top_p = LLM_SETTINGS.gcr_endpoint_top_p
            self.gcr_endpoint_do_sample = LLM_SETTINGS.gcr_endpoint_do_sample
            self.gcr_endpoint_max_token = LLM_SETTINGS.gcr_endpoint_max_token
            if not os.environ.get("PYTHONHTTPSVERIFY", "") and hasattr(ssl, "_create_unverified_context"):
                ssl._create_default_https_context = ssl._create_unverified_context  # noqa: SLF001
            self.chat_model_map = json.loads(LLM_SETTINGS.chat_model_map)
            self.chat_model = LLM_SETTINGS.chat_model
            self.encoder = None
        elif LLM_SETTINGS.chat_use_azure_deepseek:
            self.client = ChatCompletionsClient(
                endpoint=LLM_SETTINGS.chat_azure_deepseek_endpoint,
                credential=AzureKeyCredential(LLM_SETTINGS.chat_azure_deepseek_key),
            )
            self.chat_model_map = json.loads(LLM_SETTINGS.chat_model_map)
            self.encoder = None
            self.chat_model = "deepseek-R1"
            self.chat_stream = LLM_SETTINGS.chat_stream
        else:
            self.chat_use_azure = LLM_SETTINGS.chat_use_azure or LLM_SETTINGS.use_azure
            self.embedding_use_azure = LLM_SETTINGS.embedding_use_azure or LLM_SETTINGS.use_azure
            self.chat_use_azure_token_provider = LLM_SETTINGS.chat_use_azure_token_provider
            self.embedding_use_azure_token_provider = LLM_SETTINGS.embedding_use_azure_token_provider
            self.managed_identity_client_id = LLM_SETTINGS.managed_identity_client_id

            # Priority: chat_api_key/embedding_api_key > openai_api_key > os.environ.get("OPENAI_API_KEY")
            # TODO: Simplify the key design. Consider Pandatic's field alias & priority.
            self.chat_api_key = (
                LLM_SETTINGS.chat_openai_api_key or LLM_SETTINGS.openai_api_key or os.environ.get("OPENAI_API_KEY")
            )
            self.embedding_api_key = (
                LLM_SETTINGS.embedding_openai_api_key or LLM_SETTINGS.openai_api_key or os.environ.get("OPENAI_API_KEY")
            )

            self.chat_model = LLM_SETTINGS.chat_model
            self.chat_model_map = json.loads(LLM_SETTINGS.chat_model_map)
            self.encoder = self._get_encoder()
            self.chat_openai_base_url = LLM_SETTINGS.chat_openai_base_url
            self.embedding_openai_base_url = LLM_SETTINGS.embedding_openai_base_url
            self.chat_api_base = LLM_SETTINGS.chat_azure_api_base
            self.chat_api_version = LLM_SETTINGS.chat_azure_api_version
            self.chat_stream = LLM_SETTINGS.chat_stream
            self.chat_seed = LLM_SETTINGS.chat_seed

            self.embedding_model = LLM_SETTINGS.embedding_model
            self.embedding_api_base = LLM_SETTINGS.embedding_azure_api_base
            self.embedding_api_version = LLM_SETTINGS.embedding_azure_api_version

            if (self.chat_use_azure or self.embedding_use_azure) and (
                self.chat_use_azure_token_provider or self.embedding_use_azure_token_provider
            ):
                dac_kwargs = {}
                if self.managed_identity_client_id is not None:
                    dac_kwargs["managed_identity_client_id"] = self.managed_identity_client_id
                credential = DefaultAzureCredential(**dac_kwargs)
                token_provider = get_bearer_token_provider(
                    credential,
                    "https://cognitiveservices.azure.com/.default",
                )
            self.chat_client: openai.OpenAI = (
                openai.AzureOpenAI(
                    azure_ad_token_provider=token_provider if self.chat_use_azure_token_provider else None,
                    api_key=self.chat_api_key if not self.chat_use_azure_token_provider else None,
                    api_version=self.chat_api_version,
                    azure_endpoint=self.chat_api_base,
                )
                if self.chat_use_azure
                else openai.OpenAI(api_key=self.chat_api_key, base_url=self.chat_openai_base_url)
            )

            self.embedding_client: openai.OpenAI = (
                openai.AzureOpenAI(
                    azure_ad_token_provider=token_provider if self.embedding_use_azure_token_provider else None,
                    api_key=self.embedding_api_key if not self.embedding_use_azure_token_provider else None,
                    api_version=self.embedding_api_version,
                    azure_endpoint=self.embedding_api_base,
                )
                if self.embedding_use_azure
                else openai.OpenAI(api_key=self.embedding_api_key, base_url=self.embedding_openai_base_url)
            )

        # transfer the config to the class if the config is not supposed to change during the runtime
        self.use_llama2 = LLM_SETTINGS.use_llama2
        self.use_gcr_endpoint = LLM_SETTINGS.use_gcr_endpoint
        self.chat_use_azure_deepseek = LLM_SETTINGS.chat_use_azure_deepseek

    def _get_encoder(self) -> tiktoken.Encoding:
        """
        tiktoken.encoding_for_model(self.chat_model) does not cover all cases it should consider.

        This function attempts to handle several edge cases.
        """

        # 1) cases
        def _azure_patch(model: str) -> str:
            """
            When using Azure API, self.chat_model is the deployment name that can be any string.
            For example, it may be `gpt-4o_2024-08-06`. But tiktoken.encoding_for_model can't handle this.
            """
            return model.replace("_", "-")

        model = self.chat_model
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            logger.warning(f"Failed to get encoder. Trying to patch the model name")
            for patch_func in [_azure_patch]:
                try:
                    encoding = tiktoken.encoding_for_model(patch_func(model))
                except KeyError:
                    logger.error(f"Failed to get encoder even after patching with {patch_func.__name__}")
                    raise
        return encoding

    def _create_embedding_inner_function(  # type: ignore[no-untyped-def]
        self, input_content_list: list[str], *args, **kwargs
    ) -> list[list[float]]:  # noqa: ARG002
        content_to_embedding_dict = {}
        for sliced_filtered_input_content_list in [
            input_content_list[i : i + LLM_SETTINGS.embedding_max_str_num]
            for i in range(0, len(input_content_list), LLM_SETTINGS.embedding_max_str_num)
        ]:
            if self.embedding_use_azure:
                response = self.embedding_client.embeddings.create(
                    model=self.embedding_model,
                    input=sliced_filtered_input_content_list,
                )
            else:
                response = self.embedding_client.embeddings.create(
                    model=self.embedding_model,
                    input=sliced_filtered_input_content_list,
                )
            for index, data in enumerate(response.data):
                content_to_embedding_dict[sliced_filtered_input_content_list[index]] = data.embedding

        return [content_to_embedding_dict[content] for content in input_content_list]

    def _create_chat_completion_inner_function(  # type: ignore[no-untyped-def] # noqa: C901, PLR0912, PLR0915
        self,
        messages: list[dict[str, Any]],
        json_mode: bool = False,
        add_json_in_prompt: bool = False,
        *args,
        **kwargs,
    ) -> tuple[str, str | None]:
        """
        seed : Optional[int]
            When retrying with cache enabled, it will keep returning the same results.
            To make retries useful, we need to enable a seed.
            This seed is different from `self.chat_seed` for GPT. It is for the local cache mechanism enabled by RD-Agent locally.
        """

        # TODO: we can add this function back to avoid so much `self.cfg.log_llm_chat_content`
        if LLM_SETTINGS.log_llm_chat_content:
            logger.info(self._build_log_messages(messages), tag="llm_messages")
        # TODO: fail to use loguru adaptor due to stream response

        temperature = LLM_SETTINGS.chat_temperature
        max_tokens = LLM_SETTINGS.chat_max_tokens
        frequency_penalty = LLM_SETTINGS.chat_frequency_penalty
        presence_penalty = LLM_SETTINGS.chat_presence_penalty

        # Use index 4 to skip the current function and intermediate calls,
        # and get the locals of the caller's frame.
        caller_locals = inspect.stack()[4].frame.f_locals
        if "self" in caller_locals:
            tag = caller_locals["self"].__class__.__name__
        else:
            tag = inspect.stack()[4].function
        model = self.chat_model_map.get(tag, self.chat_model)

        finish_reason = None
        if self.use_llama2:
            response = self.generator.chat_completion(
                messages,
                max_gen_len=max_tokens,
                temperature=temperature,
            )
            resp = response[0]["generation"]["content"]
            if LLM_SETTINGS.log_llm_chat_content:
                logger.info(f"{LogColors.CYAN}Response:{resp}{LogColors.END}", tag="llm_messages")
        elif self.use_gcr_endpoint:
            body = str.encode(
                json.dumps(
                    {
                        "input_data": {
                            "input_string": messages,
                            "parameters": {
                                "temperature": self.gcr_endpoint_temperature,
                                "top_p": self.gcr_endpoint_top_p,
                                "max_new_tokens": self.gcr_endpoint_max_token,
                            },
                        },
                    },
                ),
            )

            req = urllib.request.Request(self.gcr_endpoint, body, self.headers)  # noqa: S310
            response = urllib.request.urlopen(req)  # noqa: S310
            resp = json.loads(response.read().decode())["output"]
            if LLM_SETTINGS.log_llm_chat_content:
                logger.info(f"{LogColors.CYAN}Response:{resp}{LogColors.END}", tag="llm_messages")
        elif self.chat_use_azure_deepseek:
            azure_style_message: list[ChatRequestMessage] = []
            for message in messages:
                if message["role"] == "system":
                    azure_style_message.append(SystemMessage(content=message["content"]))
                elif message["role"] == "user":
                    azure_style_message.append(UserMessage(content=message["content"]))
                elif message["role"] == "assistant":
                    azure_style_message.append(AssistantMessage(content=message["content"]))

            response = self.client.complete(
                messages=azure_style_message,
                stream=self.chat_stream,
                temperature=temperature,
                max_tokens=max_tokens,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
            )
            if self.chat_stream:
                resp = ""
                # TODO: with logger.config(stream=self.chat_stream): and add a `stream_start` flag to add timestamp for first message.
                if LLM_SETTINGS.log_llm_chat_content:
                    logger.info(f"{LogColors.CYAN}Response:{LogColors.END}", tag="llm_messages")

                for chunk in response:
                    content = (
                        chunk.choices[0].delta.content
                        if len(chunk.choices) > 0 and chunk.choices[0].delta.content is not None
                        else ""
                    )
                    if LLM_SETTINGS.log_llm_chat_content:
                        logger.info(LogColors.CYAN + content + LogColors.END, raw=True, tag="llm_messages")
                    resp += content
                    if len(chunk.choices) > 0 and chunk.choices[0].finish_reason is not None:
                        finish_reason = chunk.choices[0].finish_reason
            else:
                resp = response.choices[0].message.content
                finish_reason = response.choices[0].finish_reason
                if LLM_SETTINGS.log_llm_chat_content:
                    logger.info(f"{LogColors.CYAN}Response:{resp}{LogColors.END}", tag="llm_messages")
            match = re.search(r"<think>(.*?)</think>(.*)", resp, re.DOTALL)
            think_part, resp = match.groups() if match else ("", resp)
            if LLM_SETTINGS.log_llm_chat_content:
                logger.info(f"{LogColors.CYAN}Think:{think_part}{LogColors.END}", tag="llm_messages")
                logger.info(f"{LogColors.CYAN}Response:{resp}{LogColors.END}", tag="llm_messages")
        else:
            call_kwargs = dict(
                model=model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=self.chat_stream,
                seed=self.chat_seed,
                frequency_penalty=frequency_penalty,
                presence_penalty=presence_penalty,
            )
            if json_mode:
                if add_json_in_prompt:
                    for message in messages[::-1]:
                        message["content"] = message["content"] + "\nPlease respond in json format."
                        if message["role"] == LLM_SETTINGS.system_prompt_role:
                            # NOTE: assumption: systemprompt is always the first message
                            break
                call_kwargs["response_format"] = {"type": "json_object"}
            response = self.chat_client.chat.completions.create(**call_kwargs)

            if self.chat_stream:
                resp = ""
                # TODO: with logger.config(stream=self.chat_stream): and add a `stream_start` flag to add timestamp for first message.
                if LLM_SETTINGS.log_llm_chat_content:
                    logger.info(f"{LogColors.CYAN}Response:{LogColors.END}", tag="llm_messages")

                for chunk in response:
                    content = (
                        chunk.choices[0].delta.content
                        if len(chunk.choices) > 0 and chunk.choices[0].delta.content is not None
                        else ""
                    )
                    if LLM_SETTINGS.log_llm_chat_content:
                        logger.info(LogColors.CYAN + content + LogColors.END, raw=True, tag="llm_messages")
                    resp += content
                    if len(chunk.choices) > 0 and chunk.choices[0].finish_reason is not None:
                        finish_reason = chunk.choices[0].finish_reason

                if LLM_SETTINGS.log_llm_chat_content:
                    logger.info("\n", raw=True, tag="llm_messages")

            else:
                resp = response.choices[0].message.content
                finish_reason = response.choices[0].finish_reason
                if LLM_SETTINGS.log_llm_chat_content:
                    logger.info(f"{LogColors.CYAN}Response:{resp}{LogColors.END}", tag="llm_messages")
                    logger.info(
                        json.dumps(
                            {
                                "tag": tag,
                                "total_tokens": response.usage.total_tokens,
                                "prompt_tokens": response.usage.prompt_tokens,
                                "completion_tokens": response.usage.completion_tokens,
                                "model": model,
                            }
                        ),
                        tag="llm_messages",
                    )
        return resp, finish_reason

    def _calculate_token_from_messages(self, messages: list[dict[str, Any]]) -> int:
        if self.chat_use_azure_deepseek:
            return 0
        if self.encoder is None:
            raise ValueError("Encoder is not initialized.")
        if self.use_llama2 or self.use_gcr_endpoint:
            logger.warning("num_tokens_from_messages() is not implemented for model llama2.")
            return 0  # TODO implement this function for llama2

        if "gpt4" in self.chat_model or "gpt-4" in self.chat_model:
            tokens_per_message = 3
            tokens_per_name = 1
        else:
            tokens_per_message = 4  # every message follows <start>{role/name}\n{content}<end>\n
            tokens_per_name = -1  # if there's a name, the role is omitted
        num_tokens = 0
        for message in messages:
            num_tokens += tokens_per_message
            for key, value in message.items():
                num_tokens += len(self.encoder.encode(value))
                if key == "name":
                    num_tokens += tokens_per_name
        num_tokens += 3  # every reply is primed with <start>assistant<message>
        return num_tokens



================================================
File: rdagent/oai/backend/litellm.py
================================================
from typing import Any

from litellm import completion, embedding, token_counter

from rdagent.log import LogColors
from rdagent.log import rdagent_logger as logger
from rdagent.oai.backend.base import APIBackend
from rdagent.oai.llm_conf import LLMSettings


class LiteLLMSettings(LLMSettings):

    class Config:
        env_prefix = "LITELLM_"
        """Use `LITELLM_` as prefix for environment variables"""

    # Placeholder for LiteLLM specific settings, so far it's empty


LITELLM_SETTINGS = LiteLLMSettings()


class LiteLLMAPIBackend(APIBackend):
    """LiteLLM implementation of APIBackend interface"""

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)

    def _calculate_token_from_messages(self, messages: list[dict[str, Any]]) -> int:
        """
        Calculate the token count from messages
        """
        num_tokens = token_counter(
            model=LITELLM_SETTINGS.chat_model,
            messages=messages,
        )
        logger.info(f"{LogColors.CYAN}Token count: {LogColors.END} {num_tokens}", tag="debug_litellm_token")
        return num_tokens

    def _create_embedding_inner_function(
        self, input_content_list: list[str], *args: Any, **kwargs: Any
    ) -> list[list[float]]:  # noqa: ARG002
        """
        Call the embedding function
        """
        response_list = []
        for input_content_iter in input_content_list:
            model_name = LITELLM_SETTINGS.embedding_model or "azure/text-embedding-3-small"
            logger.info(f"{LogColors.GREEN}Using emb model{LogColors.END} {model_name}", tag="debug_litellm_emb")
            logger.info(f"Creating embedding for: {input_content_iter}", tag="debug_litellm_emb")
            if not isinstance(input_content_iter, str):
                raise ValueError("Input content must be a string")
            response = embedding(
                model=model_name,
                input=input_content_iter,
                *args,
                **kwargs,
            )
            response_list.append(response.data[0]["embedding"])
        return response_list

    def _create_chat_completion_inner_function(  # type: ignore[no-untyped-def] # noqa: C901, PLR0912, PLR0915
        self,
        messages: list[dict[str, Any]],
        json_mode: bool = False,
        *args,
        **kwargs,
    ) -> tuple[str, str | None]:
        """
        Call the chat completion function
        """
        if json_mode:
            kwargs["response_format"] = {"type": "json_object"}

        # Call LiteLLM completion
        response = completion(
            model=LITELLM_SETTINGS.chat_model,
            messages=messages,
            stream=LITELLM_SETTINGS.chat_stream,
            temperature=LITELLM_SETTINGS.chat_temperature,
            max_tokens=LITELLM_SETTINGS.chat_max_tokens,
            **kwargs,
        )
        logger.info(
            f"{LogColors.GREEN}Using chat model{LogColors.END} {LITELLM_SETTINGS.chat_model}", tag="llm_messages"
        )

        logger.info(self._build_log_messages(messages), tag="llm_messages")
        if LITELLM_SETTINGS.chat_stream:
            logger.info(f"{LogColors.BLUE}assistant:{LogColors.END}", tag="llm_messages")
            content = ""
            finish_reason = None
            for message in response:
                if message["choices"][0]["finish_reason"]:
                    finish_reason = message["choices"][0]["finish_reason"]
                if "content" in message["choices"][0]["delta"]:
                    chunk = (
                        message["choices"][0]["delta"]["content"] or ""
                    )  # when finish_reason is "stop", content is None
                    content += chunk
                    logger.info(LogColors.CYAN + chunk + LogColors.END, raw=True, tag="llm_messages")

            logger.info("\n", raw=True, tag="llm_messages")
        else:
            content = str(response.choices[0].message.content)
            finish_reason = response.choices[0].finish_reason
            logger.info(f"{LogColors.BLUE}assistant:{LogColors.END} {content}", tag="llm_messages")

        return content, finish_reason



================================================
File: rdagent/scenarios/data_mining/developer/feedback.py
================================================
# TODO:
# Implement to feedback.

import json
from pathlib import Path
from typing import Dict

from jinja2 import Environment, StrictUndefined

from rdagent.core.experiment import Experiment
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import (
    Experiment2Feedback,
    Hypothesis,
    HypothesisFeedback,
    Trace,
)
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils import convert2bool

feedback_prompts = Prompts(file_path=Path(__file__).parent.parent.parent / "qlib" / "prompts.yaml")
DIRNAME = Path(__file__).absolute().resolve().parent


class DMModelExperiment2Feedback(Experiment2Feedback):
    """Generated feedbacks on the hypothesis from **Executed** Implementations of different tasks & their comparisons with previous performances"""

    def generate_feedback(self, exp: Experiment, trace: Trace) -> HypothesisFeedback:
        """
        The `ti` should be executed and the results should be included, as well as the comparison between previous results (done by LLM).
        For example: `mlflow` of Qlib will be included.
        """
        hypothesis = exp.hypothesis

        logger.info("Generating feedback...")
        # Define the system prompt for hypothesis feedback
        system_prompt = feedback_prompts["model_feedback_generation"]["system"]

        # Define the user prompt for hypothesis feedback
        context = trace.scen
        SOTA_hypothesis, SOTA_experiment = trace.get_sota_hypothesis_and_experiment()

        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(feedback_prompts["model_feedback_generation"]["user"])
            .render(
                context=context,
                last_hypothesis=SOTA_hypothesis,
                last_task=SOTA_experiment.sub_tasks[0].get_task_information() if SOTA_hypothesis else None,
                last_code=SOTA_experiment.sub_workspace_list[0].file_dict.get("model.py") if SOTA_hypothesis else None,
                last_result=SOTA_experiment.result if SOTA_hypothesis else None,
                hypothesis=hypothesis,
                exp=exp,
            )
        )

        # Call the APIBackend to generate the response for hypothesis feedback
        response_hypothesis = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            json_mode=True,
            json_target_type=Dict[str, str | bool | int],
        )

        # Parse the JSON response to extract the feedback
        response_json_hypothesis = json.loads(response_hypothesis)
        return HypothesisFeedback(
            observations=response_json_hypothesis.get("Observations", "No observations provided"),
            hypothesis_evaluation=response_json_hypothesis.get("Feedback for Hypothesis", "No feedback provided"),
            new_hypothesis=response_json_hypothesis.get("New Hypothesis", "No new hypothesis provided"),
            reason=response_json_hypothesis.get("Reasoning", "No reasoning provided"),
            decision=convert2bool(response_json_hypothesis.get("Decision", "false")),
        )



================================================
File: rdagent/scenarios/data_mining/developer/model_coder.py
================================================
from rdagent.components.coder.model_coder import ModelCoSTEER

DMModelCoSTEER = ModelCoSTEER



================================================
File: rdagent/scenarios/data_mining/developer/model_runner.py
================================================
from rdagent.components.runner import CachedRunner
from rdagent.core.exception import ModelEmptyError
from rdagent.core.utils import cache_with_pickle
from rdagent.scenarios.data_mining.experiment.model_experiment import DMModelExperiment


class DMModelRunner(CachedRunner[DMModelExperiment]):
    @cache_with_pickle(CachedRunner.get_cache_key, CachedRunner.assign_cached_result)
    def develop(self, exp: DMModelExperiment) -> DMModelExperiment:
        if exp.sub_workspace_list[0].file_dict.get("model.py") is None:
            raise ModelEmptyError("model.py is empty")
        # to replace & inject code
        exp.experiment_workspace.inject_files(**{"model.py": exp.sub_workspace_list[0].file_dict["model.py"]})

        env_to_use = {"PYTHONPATH": "./"}

        result = exp.experiment_workspace.execute(run_env=env_to_use)

        exp.result = result

        return exp



================================================
File: rdagent/scenarios/data_mining/docker/Dockerfile
================================================
FROM pytorch/pytorch:2.2.1-cuda12.1-cudnn8-runtime
# For GPU support, please choose the proper tag from https://hub.docker.com/r/pytorch/pytorch/tags

RUN apt-get clean && apt-get update && apt-get install -y \  
    curl \  
    vim \  
    git \  
    build-essential \
    && rm -rf /var/lib/apt/lists/* 

WORKDIR /workspace

RUN python -m pip install numpy
RUN python -m pip install --upgrade cython
# RUN python -m pip install -e .

RUN python -m pip install pandas
# RUN pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.3.0%2Bcu121.html
RUN pip install torch_geometric
RUN pip install ogb
RUN pip install networkx
RUN pip install scikit-learn
RUN pip install catboost
RUN pip install xgboost
RUN pip install sparse



================================================
File: rdagent/scenarios/data_mining/experiment/model_experiment.py
================================================
from pathlib import Path

from rdagent.components.coder.model_coder.model import (
    ModelExperiment,
    ModelFBWorkspace,
    ModelTask,
)
from rdagent.core.experiment import Task
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario
from rdagent.scenarios.data_mining.experiment.workspace import DMFBWorkspace

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class DMModelExperiment(ModelExperiment[ModelTask, DMFBWorkspace, ModelFBWorkspace]):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.experiment_workspace = DMFBWorkspace(template_folder_path=Path(__file__).parent / "model_template")


class DMModelScenario(Scenario):
    @property
    def background(self) -> str:
        return prompt_dict["dm_model_background"]

    @property
    def source_data(self) -> str:
        raise NotImplementedError("source_data is not implemented")

    @property
    def output_format(self) -> str:
        return prompt_dict["dm_model_output_format"]

    @property
    def interface(self) -> str:
        return prompt_dict["dm_model_interface"]

    @property
    def simulator(self) -> str:
        return prompt_dict["dm_model_simulator"]

    @property
    def rich_style_description(self) -> str:
        return """
### MIMIC-III Model Evolving Automatic R&D Demo
 
#### [Overview](#_summary)
 
The demo showcases the iterative process of hypothesis generation, knowledge construction, and decision-making in model construction in a clinical prediction task. The model should predict whether a patient would suffer from Acute Respiratory Failure (ARF) based on first 12 hours ICU monitoring data. 
 
#### [Automated R&D](#_rdloops)
 
- **[R (Research)](#_research)**
  - Iteration of ideas and hypotheses.
  - Continuous learning and knowledge construction.
 
- **[D (Development)](#_development)**
  - Evolving code generation and model refinement.
  - Automated implementation and testing of models.
 
#### [Objective](#_summary)
 
To demonstrate the dynamic evolution of models through the R&D loop, emphasizing how each iteration enhances the model performance and reliability. The performane is measured by the AUROC score (Area Under the Receiver Operating Characteristic), which is a commonly used metric for binary classification.   """

    def get_scenario_all_desc(
        self, task: Task | None = None, filtered_tag: str | None = None, simple_background: bool | None = None
    ) -> str:
        return f"""Background of the scenario:
{self.background}
The interface you should follow to write the runnable code:
{self.interface}
The output of your code should be in the format:
{self.output_format}
The simulator user can use to test your model:
{self.simulator}
"""



================================================
File: rdagent/scenarios/data_mining/experiment/prompts.yaml
================================================
dm_model_background: |-
  The model is a machine learning or deep learning structure used in clinical settings to predict whether the patient will suffer from acute respiratory failure (ARF) based on their vital signs monitored in ICU.
  The data is extracted from MIMIC-III using FIDDLE pipeline, and we focus on the ARF_12h prediction task, which means we use the first 12 hours data to predict the onset of ARF on discharge.
  The model is defined in the following parts:
  1. Name: The name of the model.
  2. Description: The description of the model.
  3. Architecture: The detailed architecture of the model, such as neural network layers or tree structures.
  The model should provide clear and detailed documentation of its architecture and hyperparameters. 

dm_model_interface: |-
  Your python code should follow the interface to better interact with the user's system.
  You code should contain several parts:
  1. The import part: import the necessary libraries.
  2. A class which is a sub-class of pytorch.nn.Module. This class should should have a init function and a forward function which inputs a tensor and outputs a tensor.
  3. Set a variable called "model_cls" to the class you defined.

  The user will save your code into a python file called "model.py". Then the user imports model_cls in file "model.py" after setting the cwd into the directory:
  ```python
  from model import model_cls
  ```
  So your python code should follow the pattern:
  ```python
  class XXXModel(torch.nn.Module):
      ...
  model_cls = XXXModel
  ```

  The model has one type, "TimeSeries". The input shape to a time series model is (batch_size, num_features, num_timesteps). The output shape of the model should be (batch_size, 1).
  The "batch_size" is a dynamic value which is determined by the input of forward function.
  The "num_features" and "num_timesteps" are static which will be provided to the model through init function.
  User will initialize the time series model with the following code:
  ```python
  model = model_cls(num_features=num_features, num_timesteps=num_timesteps)
  ```
  No other parameters will be passed to the model so give other parameters a default value or just make them static.

  The input tensor shape is (batch_size, num_features, num_timesteps) which is different from the normal time series input shape of (batch_size, num_timesteps, num_features). Please write code accordingly. 
  
  Note that for nn.Conv1d() layers, please do not permute the input tensor as the in_channel dimension should match the num_feature dimension.

  The output shape should be (batch_size, 1) with sigmoid activation since we have binary labels. 

  Don't write any try-except block in your python code. The user will catch the exception message and provide the feedback to you. Also, don't write main function in your python code. The user will call the forward method in the model_cls to get the output tensor.

  Please notice that your model should only use current features as input. The user will provide the input tensor to the model's forward function.


dm_model_output_format: |-
  Your output should be a tensor with shape (batch_size, 1). 
  The output tensor should be saved in a file named "output.pth" in the same directory as your python file.
  The user will evaluate the shape of the output tensor so the tensor read from "output.pth" should be 8 numbers.

dm_model_simulator: |-
  The models will be put to train on MIMIC-III dataset and evaluate their performance in terms of roc score (Area Under Curve Receiver Operating Characteristics Curve). Hypothesis is improved upon checking the feedback on the results. 


================================================
File: rdagent/scenarios/data_mining/experiment/workspace.py
================================================
from pathlib import Path

import pandas as pd

from rdagent.app.data_mining.conf import MED_PROP_SETTING
from rdagent.core.experiment import FBWorkspace
from rdagent.log import rdagent_logger as logger
from rdagent.utils.env import DMDockerEnv


class DMFBWorkspace(FBWorkspace):
    def __init__(self, template_folder_path: Path, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.inject_code_from_folder(template_folder_path)

    def execute(self, run_env: dict = {}, *args, **kwargs) -> str:
        qtde = DMDockerEnv()
        qtde.prepare(MED_PROP_SETTING.username, MED_PROP_SETTING.password)

        execute_log = qtde.run(
            local_path=str(self.workspace_path),
            entry=f"python train.py",
            env=run_env,
        )

        csv_path = self.workspace_path / "submission.csv"

        if not csv_path.exists():
            logger.error(f"File {csv_path} does not exist.")
            return None
        return pd.read_csv(csv_path, index_col=0).iloc[:, 0]



================================================
File: rdagent/scenarios/data_mining/experiment/model_template/README.md
================================================
## This folder is a template to be copied from for each model implementation & running process. 

Components: Dummy model.py, versatile conf.yaml, and a result reader. 



================================================
File: rdagent/scenarios/data_mining/experiment/model_template/train.py
================================================
import os
import random
from pathlib import Path

import numpy as np
import pandas as pd
import sparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from model import model_cls
from sklearn.metrics import accuracy_score, roc_auc_score
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, transforms

# Set device for training
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# device = torch.device("cpu")


class MyDataset(Dataset):
    def __init__(self, x, label, device):
        self.x1 = x
        self.label = label
        self.device = device

    def __len__(self):
        return len(self.label)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        return torch.FloatTensor(self.x1[idx]).to(self.device), torch.tensor(self.label[idx], dtype=torch.float).to(
            self.device
        )


def collate_fn(batch):
    x, label = [], []
    for data in batch:
        x.append(data[0])
        label.append(data[1])
    return torch.stack(x, 0), torch.stack(label, 0)


datapath = "/root/.data"
# datapath = '/home/v-suhancui/RD-Agent/physionet.org/files/mimic-eicu-fiddle-feature/1.0.0/FIDDLE_mimic3'


X = sparse.load_npz(datapath + "/features/ARF_12h/X.npz").todense()
df_pop = pd.read_csv(datapath + "/population/ARF_12h.csv")["ARF_LABEL"]

X = X.transpose(0, 2, 1)

indices = [i for i in range(len(df_pop))]
random.shuffle(indices)
split_point = int(0.7 * len(df_pop))

X_train, y_train = X[indices[:split_point]], np.array(df_pop[indices[:split_point]])
X_test, y_test = X[indices[split_point:]], np.array(df_pop[indices[split_point:]])


train_dataloader = DataLoader(
    MyDataset(X_train, y_train, device), collate_fn=collate_fn, shuffle=True, drop_last=True, batch_size=64
)
test_dataloader = DataLoader(
    MyDataset(X_test, y_test, device), collate_fn=collate_fn, shuffle=False, drop_last=False, batch_size=64
)

num_features = 4816
num_timesteps = 12
# Define the optimizer and loss function
model = model_cls(num_features=num_features, num_timesteps=num_timesteps).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.CrossEntropyLoss()


# Train the model
def eval_auc(model):
    y_pred = []
    for data in test_dataloader:
        x, y = data
        out = model(x)
        y_pred.append(out.cpu().detach().numpy())
    return roc_auc_score(y_test, np.concatenate(y_pred))


best = 0.0
best_model = None

for i in range(15):
    for data in train_dataloader:
        x, y = data
        out = model(x)
        optimizer.zero_grad()
        loss = criterion(out.squeeze(), y)
        loss.backward()
        optimizer.step()
    roc = eval_auc(model)
    if roc > best:
        best = roc
        best_model = model

y_pred = []
for data in test_dataloader:
    x, y = data
    out = best_model(x)
    y_pred.append(out.cpu().detach().numpy())

acc = roc_auc_score(y_test, np.concatenate(y_pred))

print(acc)

res = pd.Series(data=[acc], index=["AUROC"])
res.to_csv("./submission.csv")



================================================
File: rdagent/scenarios/data_mining/proposal/model_proposal.py
================================================
import json
from pathlib import Path
from typing import List, Tuple

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.model_coder.model import ModelExperiment, ModelTask
from rdagent.components.proposal import (
    Hypothesis,
    ModelHypothesis2Experiment,
    ModelHypothesisGen,
)
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import Hypothesis, Scenario, Trace
from rdagent.scenarios.data_mining.experiment.model_experiment import DMModelExperiment

prompt_dict = Prompts(file_path=Path(__file__).parent.parent.parent / "qlib" / "prompts.yaml")

DMModelHypothesis = Hypothesis


class DMModelHypothesisGen(ModelHypothesisGen):
    """
    # NOTE: we can share this class across different data mining scenarios
    # It may better to move the class into components folder like `rdagent/components/proposal/model_proposal.py`
    # Here is the use case:

    .. code-block:: python

        class XXXDMModelHypothesisGen(DMModelHypothesisGen):
            prompts: Prompts = a_specifc_prompt_dict
    """

    def __init__(self, scen: Scenario) -> Tuple[dict, bool]:
        super().__init__(scen)

    def prepare_context(self, trace: Trace) -> Tuple[dict, bool]:
        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )
        context_dict = {
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "RAG": None,
            "hypothesis_output_format": prompt_dict["hypothesis_output_format"],
            "hypothesis_specification": prompt_dict["model_hypothesis_specification"],
        }
        return context_dict, True

    def convert_response(self, response: str) -> Hypothesis:
        response_dict = json.loads(response)
        hypothesis = DMModelHypothesis(
            hypothesis=response_dict["hypothesis"],
            reason=response_dict["reason"],
            concise_reason=response_dict["concise_reason"],
            concise_observation=response_dict["concise_observation"],
            concise_justification=response_dict["concise_justification"],
            concise_knowledge=response_dict["concise_knowledge"],
        )
        return hypothesis


class DMModelHypothesis2Experiment(ModelHypothesis2Experiment):
    def prepare_context(self, hypothesis: Hypothesis, trace: Trace) -> Tuple[dict, bool]:
        scenario = trace.scen.get_scenario_all_desc()
        experiment_output_format = prompt_dict["model_experiment_output_format"]

        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )

        experiment_list: List[ModelExperiment] = [t[0] for t in trace.hist]

        model_list = []
        for experiment in experiment_list:
            model_list.extend(experiment.sub_tasks)

        return {
            "target_hypothesis": str(hypothesis),
            "scenario": scenario,
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "experiment_output_format": experiment_output_format,
            "target_list": model_list,
            "RAG": None,
        }, True

    def convert_response(self, response: str, hypothesis: Hypothesis, trace: Trace) -> ModelExperiment:
        response_dict = json.loads(response)
        tasks = []
        for model_name in response_dict:
            description = response_dict[model_name]["description"]
            formulation = response_dict[model_name]["formulation"]
            architecture = response_dict[model_name]["architecture"]
            variables = response_dict[model_name]["variables"]
            hyperparameters = response_dict[model_name]["hyperparameters"]
            model_type = response_dict[model_name]["model_type"]
            tasks.append(
                ModelTask(
                    name=model_name,
                    description=description,
                    formulation=formulation,
                    architecture=architecture,
                    variables=variables,
                    hyperparameters=hyperparameters,
                    model_type=model_type,
                )
            )
        exp = DMModelExperiment(tasks, hypothesis=hypothesis)
        exp.based_experiments = [t[0] for t in trace.hist if t[1]]
        return exp



================================================
File: rdagent/scenarios/data_science/__init__.py
================================================



================================================
File: rdagent/scenarios/data_science/share.yaml
================================================
describe: # some template to describe some object
  # exp is a template used fo
  exp: |-
    ## {{ heading | default('Best solution of previous exploration of the scenario') }}
    {% if exp %}
    ### Code
    Here is the complete code of the solution.
    {{ exp.experiment_workspace.all_codes }}

    {% if exp.hypothesis is not none %}
    ### Hypothesis for the experiment
    the experiment is designed based on hypothesis: {{exp.hypothesis}}
    {% endif %}

    ### Results
    {% if exp.result is none %}
    There are no according evaluation results
    {% else %}
    Evaluated results on validation are:
    {{ exp.result }}
    {% if exp.format_check_result is not none %}
    Submission format check result is:
    {{ exp.format_check_result }}
    {% endif %}
    {% endif %}

    {% else %}
    No previous complete experiment available.
    {% endif %}

  feedback: |-
    {% if exp_and_feedback and exp_and_feedback|length > 1 %}
    ## {{heading | default('Previous trial and feedback')}}
    Before current trial, a previous recent trial is listed below.
    {% if exp_and_feedback[0].hypothesis %}
    the experiment is designed based on hypothesis: {{ exp_and_feedback[0].hypothesis }}
    {% endif %}
    ### Task of previous trial
    {{ exp_and_feedback[0].pending_tasks_list[0][0].get_task_information() }}
    feedback decision: {{ exp_and_feedback[1].decision }}
    reason: {{ exp_and_feedback[1].reason }}
    {% endif %}

  trace: |-
    {% if exp_and_feedback_list|length == 0 %}
    No previous {% if success %}successful{% else %}failed{% endif %} trial available.
    {% else %}
    {% if success %}
    ## {{ heading | default('Trace of the successful trial') }}
    {% else %}
    ## {{ heading | default('Trace of the failed trial') }}
    {% endif %}
    Before current trial, several {% if success %}successful{% else %}failed{% endif %} trials are listed below. {% if success %}The current SOTA method is the combination of the best solutions of these trials.{% endif %} The trace order is from the earliest to the latest please focus more on the later trials.
    {% for exp_and_feedback in exp_and_feedback_list %}
    ### Experiment index: {{ loop.index }}
    The experiment is designed based on hypothesis: {{ exp_and_feedback[0].hypothesis }}
    ### Task of experiment
    {{ exp_and_feedback[0].pending_tasks_list[0][0].get_task_information() }}
    Experiment feedback decision: {{ exp_and_feedback[1].decision }}
    Reason: {{ exp_and_feedback[1].reason }}
    {% endfor %}
    {% endif %}



================================================
File: rdagent/scenarios/data_science/debug/data.py
================================================
import os
import platform
import shutil
from collections import Counter, defaultdict
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

try:
    import bson  # pip install pymongo
except:
    pass

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING


class DataHandler:
    """Base DataHandler interface."""

    def load(self, path) -> pd.DataFrame:
        raise NotImplementedError

    def dump(self, df: pd.DataFrame, path):
        raise NotImplementedError


class GenericDataHandler(DataHandler):
    """
    A generic data handler that automatically detects file type based on suffix
    and uses the correct pandas method for load/dump.
    """

    def load(self, path) -> pd.DataFrame:
        path = Path(path)
        suffix = path.suffix.lower()

        if suffix == ".csv":
            return pd.read_csv(path, encoding="utf-8")
        elif suffix == ".pkl":
            return pd.read_pickle(path)
        elif suffix == ".parquet":
            return pd.read_parquet(path)
        elif suffix in [".h5", ".hdf", ".hdf5"]:
            # Note: for HDF, you need a 'key' in read_hdf. If you expect a single key,
            # you might do: pd.read_hdf(path, key='df') or something similar.
            # Adjust as needed based on your HDF structure.
            return pd.read_hdf(path, key="data")
        elif suffix == ".jsonl":
            # Read JSON Lines file
            return pd.read_json(path, lines=True)
        elif suffix == ".bson":
            data = bson.decode_file_iter(open(path, "rb"))
            df = pd.DataFrame(data)
            return df
        else:
            raise ValueError(f"Unsupported file type: {suffix}")

    def dump(self, df: pd.DataFrame, path):
        path = Path(path)
        suffix = path.suffix.lower()

        if suffix == ".csv":
            df.to_csv(path, index=False, encoding="utf-8")
        elif suffix == ".pkl":
            df.to_pickle(path)
        elif suffix == ".parquet":
            df.to_parquet(path, index=True)
        elif suffix in [".h5", ".hdf", ".hdf5"]:
            # Similarly, you need a key for HDF.
            df.to_hdf(path, key="data", mode="w")
        elif suffix == ".jsonl":
            # Save DataFrame to JSON Lines file
            df.to_json(path, orient="records", lines=True)
        elif suffix == ".bson":
            data = df.to_dict(orient="records")
            with open(path, "wb") as file:
                # Write each record in the list to the BSON file
                for record in data:
                    file.write(bson.BSON.encode(record))
        else:
            raise ValueError(f"Unsupported file type: {suffix}")


class DataReducer:
    """Base DataReducer interface."""

    def reduce(self, df: pd.DataFrame) -> pd.DataFrame:
        raise NotImplementedError


class RandDataReducer(DataReducer):
    """
    Example random sampler: ensures at least `min_num` rows
    or at least `min_frac` fraction of the data (whichever is larger).
    """

    def __init__(self, min_frac=0.02, min_num=5):
        self.min_frac = min_frac
        self.min_num = min_num

    def reduce(self, df: pd.DataFrame, frac: float = None) -> pd.DataFrame:
        frac = max(self.min_frac, self.min_num / len(df)) if frac is None else frac
        # print(f"Sampling {frac * 100:.2f}% of the data ({len(df)} rows)")
        if frac >= 1:
            return df
        return df.sample(frac=frac, random_state=1)


class UniqueIDDataReducer(DataReducer):
    def __init__(self, min_frac=0.02, min_num=5):
        self.min_frac = min_frac
        self.min_num = min_num
        self.random_reducer = RandDataReducer(min_frac, min_num)

    def reduce(self, df: pd.DataFrame) -> pd.DataFrame:
        if not len(df):
            return df

        if not isinstance(df, pd.DataFrame):
            return self.random_reducer.reduce(df)

        def is_valid_label(column):
            if not isinstance(column.iloc[0], (int, float, str, tuple, frozenset, bytes, complex, type(None))):
                return False

            if not (0 < column.nunique() < df.shape[0] * 0.5):
                return False

            if pd.api.types.is_numeric_dtype(column) and all(isinstance(x, float) for x in column.dropna()):
                return False

            return True

        label_col = df.iloc[:, -1]
        if not is_valid_label(label_col) and df.shape[1] > 2:
            label_col = df.iloc[:, 1]

        if not is_valid_label(label_col):
            return self.random_reducer.reduce(df)

        unique_labels = label_col.unique()
        unique_count = len(unique_labels)
        print(f"Unique labels: {unique_count} / {df.shape[0]}")

        sampled_rows = df.groupby(label_col, group_keys=False).apply(lambda x: x.sample(n=1, random_state=1))
        frac = max(self.min_frac, self.min_num / len(df))

        if int(len(df) * frac) < unique_count:
            return sampled_rows.reset_index(drop=True)

        remain_df = df.drop(index=sampled_rows.index)
        remaining_frac = frac - unique_count / len(df)

        remaining_sampled = self.random_reducer.reduce(remain_df, remaining_frac)
        result_df = pd.concat([sampled_rows, remaining_sampled]).sort_index()
        return result_df


def count_files_in_folder(folder: Path) -> int:
    """
    Count the total number of files in a folder, including files in subfolders.
    """
    return sum(1 for _ in folder.rglob("*") if _.is_file())


def copy_file(src_fp, target_folder, data_folder):
    """
    Construct the target file path based on the file's relative location from data_folder,
    then copy the file if it doesn't already exist.
    """
    target_fp = target_folder / src_fp.relative_to(data_folder)
    if not target_fp.exists():
        target_fp.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy(src_fp, target_fp)


def create_debug_data(
    competition: str,
    dr_cls: type[DataReducer] = UniqueIDDataReducer,
    min_frac=0.01,
    min_num=5,
    dataset_path=None,
    sample_path=None,
):
    """
    Reads the original data file, creates a reduced sample,
    and renames/moves files for easier debugging.
    Automatically detects file type (csv, pkl, parquet, hdf, etc.).
    """
    if dataset_path is None:
        dataset_path = KAGGLE_IMPLEMENT_SETTING.local_data_path  # FIXME: don't hardcode this KAGGLE_IMPLEMENT_SETTING

    if sample_path is None:
        sample_path = Path(dataset_path) / "sample"

    data_folder = Path(dataset_path) / competition
    sample_folder = Path(sample_path) / competition

    # Traverse the folder and exclude specific file types
    included_extensions = {".csv", ".pkl", ".parquet", ".h5", ".hdf", ".hdf5", ".jsonl", ".bson"}
    files_to_process = [file for file in data_folder.rglob("*") if file.is_file()]
    total_files_count = len(files_to_process)
    print(
        f"[INFO] Original dataset folder `{data_folder}` has {total_files_count} files in total (including subfolders)."
    )
    file_types_count = Counter(file.suffix.lower() for file in files_to_process)
    print("File type counts:")
    for file_type, count in file_types_count.items():
        print(f"{file_type}: {count}")

    # This set will store filenames or paths that appear in the sampled data
    sample_used_file_names = set()

    # Prepare data handler and reducer
    data_handler = GenericDataHandler()
    data_reducer = dr_cls(min_frac=min_frac, min_num=min_num)

    skip_subfolder_data = any(
        f.is_file() and f.suffix in included_extensions
        for f in data_folder.iterdir()
        if f.name.startswith(("train", "test"))
    )
    processed_files = []

    for file_path in tqdm(files_to_process, desc="Processing data", unit="file"):
        sampled_file_path = sample_folder / file_path.relative_to(data_folder)
        if sampled_file_path.exists():
            continue

        if file_path.suffix.lower() not in included_extensions:
            continue

        if skip_subfolder_data and file_path.parent != data_folder:
            continue  # bypass files in subfolders

        sampled_file_path.parent.mkdir(parents=True, exist_ok=True)

        # Load the original data
        df = data_handler.load(file_path)

        # Create a sampled subset
        df_sampled = data_reducer.reduce(df)
        processed_files.append(file_path)
        # Dump the sampled data
        try:
            data_handler.dump(df_sampled, sampled_file_path)
            # Extract possible file references from the sampled data
            if "submission" in file_path.stem:
                continue  # Skip submission files
            for col in df_sampled.columns:
                unique_vals = df_sampled[col].astype(str).unique()
                for val in unique_vals:
                    # Add the entire string to the set;
                    # in real usage, might want to parse or extract basename, etc.
                    sample_used_file_names.add(val)
        except Exception as e:
            print(f"Error processing {file_path}: {e}")
            continue

    # Process non-data files
    subfolder_dict = {}
    global_groups = defaultdict(list)
    for file_path in files_to_process:
        if file_path in processed_files:
            continue  # Already handled above
        rel_dir = file_path.relative_to(data_folder).parts[0]
        subfolder_dict.setdefault(rel_dir, []).append(file_path)
        global_groups[file_path.stem].append(Path(file_path))

    # For each subfolder, decide which files to copy
    selected_groups = []
    for rel_dir, file_list in tqdm(subfolder_dict.items(), desc="Processing files", unit="file"):
        used_files = []
        not_used_files = []
        extra_files = []

        # Check if each file is in the "used" list
        for fp in file_list:
            if str(fp.name) in sample_used_file_names or str(fp.stem) in sample_used_file_names:
                used_files.append(fp)
            else:
                if file_types_count.get(".txt", 1000) < 100 and fp.suffix.lower() == ".txt":
                    extra_files.append(fp)
                not_used_files.append(fp)

        # Directly copy used files
        for uf in used_files:
            copy_file(uf, sample_folder, data_folder)

        # If no files are used, randomly sample files to keep the folder from being empty
        if len(used_files) == 0:
            if len(file_list) <= min_num:
                num_to_keep = len(file_list)
            else:
                num_to_keep = max(int(len(file_list) * min_frac), min_num)

            # Use a greedy strategy to select groups so that the total number of files is as close as possible to num_to_keep
            total_files = 0
            np.random.shuffle(not_used_files)
            for nf in not_used_files:
                if total_files > num_to_keep:
                    break
                if nf.stem in selected_groups:
                    total_files += 1
                else:
                    selected_groups.append(nf.stem)
                    total_files += 1

            print(f"Sampling {num_to_keep} files without label from {total_files} files in {rel_dir}")

            # Flatten the selected groups into a single list of files
            sampled_not_used = [
                nf for group, value in global_groups.items() if group in selected_groups for nf in value
            ]

            # Copy the selected files to the target directory (all files with the same base name will be copied)
            for nf in sampled_not_used:
                # Construct the target path based on the relative path of nf from data_folder
                sampled_file_path = sample_folder / nf.relative_to(data_folder)
                if sampled_file_path.exists():
                    continue
                sampled_file_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy(nf, sampled_file_path)

        # Copy extra files
        print(f"Copying {len(extra_files)} extra files")
        for uf in extra_files:
            copy_file(uf, sample_folder, data_folder)

    final_files_count = count_files_in_folder(sample_folder)
    print(f"[INFO] After sampling, the sample folder `{sample_folder}` contains {final_files_count} files in total.")



================================================
File: rdagent/scenarios/data_science/dev/coder.py
================================================



================================================
File: rdagent/scenarios/data_science/dev/feedback.py
================================================
import json
from typing import Dict

import pandas as pd

from rdagent.core.proposal import (
    Experiment2Feedback,
    ExperimentFeedback,
    HypothesisFeedback,
)
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.data_science.experiment.experiment import DSExperiment
from rdagent.scenarios.data_science.proposal.exp_gen import DSTrace
from rdagent.utils import convert2bool
from rdagent.utils.agent.tpl import T
from rdagent.utils.repo.diff import generate_diff_from_dict


class DSExperiment2Feedback(Experiment2Feedback):
    def generate_feedback(self, exp: DSExperiment, trace: DSTrace) -> ExperimentFeedback:
        # 用哪些信息来生成feedback
        # 1. pending_tasks_list[0][0] 任务的描述
        # 2. hypothesis 任务的假设
        # 3. 相对sota_exp的改动
        # 4. result 任务的结果
        # 5. sota_exp.result 之前最好的结果
        sota_exp = trace.sota_experiment()
        sota_desc = T("scenarios.data_science.share:describe.exp").r(
            exp=sota_exp, heading="SOTA of previous exploration of the scenario"
        )

        # Get feedback description using shared template
        feedback_desc = T("scenarios.data_science.share:describe.feedback").r(
            exp_and_feedback=(trace.hist[-1] if trace.hist else None), heading="Previous Trial Feedback"
        )

        # TODO:
        # -  Should we choose between the diff from last experiment or last sota ?

        # Retrieve the last experiment from the history
        last_exp = trace.hist[-1][0] if trace.hist else None
        if last_exp and last_exp.experiment_workspace and exp.experiment_workspace:
            # Generate a diff between the two workspaces
            last_exp_files = last_exp.experiment_workspace.file_dict
            current_exp_files = exp.experiment_workspace.file_dict
            diff_edition = generate_diff_from_dict(last_exp_files, current_exp_files)
        else:
            diff_edition = []

        # assumption:
        # The feedback should focus on experiment **improving**.
        # Assume that all the the sota exp is based on the previous sota experiment
        cur_vs_sota_score = None
        if sota_exp:
            cur_score = pd.DataFrame(exp.result).loc["ensemble"].iloc[0]
            sota_score = pd.DataFrame(sota_exp.result).loc["ensemble"].iloc[0]
            cur_vs_sota_score = (
                f"The current score is {cur_score}, while the SOTA score is {sota_score}. "
                f"{'In this competition, higher is better.' if self.scen.metric_direction else 'In this competition, lower is better.'}"
            )

        system_prompt = T(".prompts:exp_feedback.system").r(scenario=self.scen.get_scenario_all_desc())
        user_prompt = T(".prompts:exp_feedback.user").r(
            sota_desc=sota_desc,
            cur_exp=exp,
            diff_edition=diff_edition,
            feedback_desc=feedback_desc,
            cur_vs_sota_score=cur_vs_sota_score,
        )

        resp_dict = json.loads(
            APIBackend().build_messages_and_create_chat_completion(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                json_mode=True,
                json_target_type=Dict[str, str | bool | int],
            )
        )

        return HypothesisFeedback(
            observations=resp_dict.get("Observations", "No observations provided"),
            hypothesis_evaluation=resp_dict.get("Feedback for Hypothesis", "No feedback provided"),
            new_hypothesis=resp_dict.get("New Hypothesis", "No new hypothesis provided"),
            reason=resp_dict.get("Reasoning", "No reasoning provided"),
            decision=convert2bool(resp_dict.get("Replace Best Result", "no")),
        )



================================================
File: rdagent/scenarios/data_science/dev/prompts.yaml
================================================
exp_feedback:
  system: |-
    You are an advanced assistant for analyzing results in data-driven R&D.
    The task is described in the following scenario:
    {{ scenario }}

    You will analyze the current experiment's hypothesis, code, results, and compare them with previous experiments and the best past result.
    
    Step 1: Ensure Submission Format is Correct
    - If the **submission format check fails**, first identify issues in the model, or workflow code.  
    - These issues must be fixed in the following loop.
    - In this case, do not replace SOTA.
    
    Step 2: Analyze Experiment Results (If Submission is Correct)
    Your feedback should:
    1. Confirm if the current result supports or refutes the hypothesis.
    2. Compare with previous best results.
    3. Suggest improvements or new directions. Stay innovative and adaptive.
    4. SOTA results are the best outcomes we have achieved in this scenario. If we do not have complete experiment available (i.e., results that are runnable and can generate evaluation outcomes), **please replace it as the best result/SOTA**.
  
    Please provide detailed and constructive feedback.
    Example JSON Structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }

  user: |-
    We are in a process of finding and validating hypotheses to build powerful codes. Each round aims to confirm or reject hypotheses based on results.

    {{ sota_desc }}

    ## Current solution
    Current solution to be evaluated:

    ### Task of Current solution
    {{ cur_exp.pending_tasks_list[0][0].get_task_information() }}

    {% if cur_exp.hypothesis %}
    the experiment is designed based on hypothesis: {{ cur_exp.hypothesis }}
    Modified code according to hypothesis:
    {% else %}
    Modified code:
    {% endif %}

    {% for de in diff_edition %}
    {{ de }}
    {% endfor %}

    ### Final results of the current solution
    1. Pay close attention to the performance of `ensemble`, as it represents the final score for this iteration.
    2. If any individual model significantly outperforms the ensemble, this may indicate an issue in the ensemble method. But if the final `ensemble` score surpasses the current SOTA, you should update the SOTA record. However, it seems that there are noticeable issues in the ensemble component, be sure to highlight them explicitly.
    Below are the results for this experiment:
    {{ cur_exp.result }}

    {% if cur_vs_sota_score is not none %}
    Below is the comparison of the current `ensemble` performance with the SOTA result:
    {{ cur_vs_sota_score }}
    {% endif %}
    
    {% if cur_exp.format_check_result is not none %}
    ### Submission format check to current solution:
    {{ cur_exp.format_check_result }}
    {% endif %}
    
    ### Complete Code of current solution
    {{ cur_exp.experiment_workspace.all_codes }}

    {{ feedback_desc }}

    Please refer to these hypotheses and feedback to help you recommend new experiment and hypothesis

    Consider Changing Direction for Significant Gaps with the Best Result and the last round:
    - If submission format has issues, fix them before proceeding with analysis.
    - If the new results significantly differ from SOTA, consider a new direction.
    - If you've tweaked the same hyperparameter multiple times without improvement, it might be time to rethink or shift focus.



================================================
File: rdagent/scenarios/data_science/dev/runner/__init__.py
================================================
from pathlib import Path
from typing import Dict

import pandas as pd

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder import CoSTEER
from rdagent.components.coder.CoSTEER import CoSTEER
from rdagent.components.coder.CoSTEER.config import CoSTEER_SETTINGS
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEERMultiEvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.CoSTEER.evolvable_subjects import FBWorkspace
from rdagent.components.coder.CoSTEER.evolving_strategy import (
    CoSTEERQueriedKnowledge,
    MultiProcessEvolvingStrategy,
)
from rdagent.components.coder.CoSTEER.task import CoSTEERTask
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.exception import RunnerError
from rdagent.core.scenario import Scenario
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend, md5_hash
from rdagent.scenarios.data_science.dev.runner.eval import DSCoSTEERCoSTEEREvaluator
from rdagent.utils.agent.ret import BatchEditOut
from rdagent.utils.agent.tpl import T
from rdagent.utils.env import DockerEnv, MLEBDockerConf


class DSRunnerMultiProcessEvolvingStrategy(MultiProcessEvolvingStrategy):
    def implement_one_task(
        self,
        target_task: CoSTEERTask,
        queried_knowledge: CoSTEERQueriedKnowledge | None = None,
        workspace: FBWorkspace | None = None,
        prev_task_feedback: CoSTEERSingleFeedback | None = None,
    ) -> dict[str, str]:
        if prev_task_feedback is None:
            # if no prev_tak_feedback, it is the first loop; we do not make any changes and goto evaluators directly.
            return {}

        task_information_str = target_task.get_task_information()
        # 1. code
        system_prompt = T(".prompts:DSCoSTEER_debugger.system").r(
            task_desc=task_information_str,
            out_spec=BatchEditOut.get_spec(with_del=False),
        )
        user_prompt = T(".prompts:DSCoSTEER_debugger.user").r(
            code=workspace.all_codes,
            feedback=prev_task_feedback,
        )

        batch_edit = BatchEditOut.extract_output(
            APIBackend().build_messages_and_create_chat_completion(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                json_mode=BatchEditOut.json_mode,
                json_target_type=Dict[str, str],
            )
        )

        batch_edit = {k: v for k, v in batch_edit.items() if k in workspace.file_dict.keys()}

        return batch_edit

    def assign_code_list_to_evo(self, code_list: list[dict[str, str]], evo):
        """
        Assign the code list to the evolving item.

        The code list is aligned with the evolving item's sub-tasks.
        If a task is not implemented, put a None in the list.
        """
        for index in range(len(evo.sub_tasks)):
            if code_list[index] is None:
                continue
            if evo.sub_workspace_list[index] is None:
                # evo.sub_workspace_list[index] = FBWorkspace(target_task=evo.sub_tasks[index])
                evo.sub_workspace_list[index] = evo.experiment_workspace
            evo.sub_workspace_list[index].inject_files(**code_list[index])
        return evo


class DSCoSTEERRunner(CoSTEER):
    def __init__(
        self,
        scen: Scenario,
        *args,
        **kwargs,
    ) -> None:
        eva = CoSTEERMultiEvaluator(
            DSCoSTEERCoSTEEREvaluator(scen=scen), scen=scen
        )  # Please specify whether you agree running your eva in parallel or not
        es = DSRunnerMultiProcessEvolvingStrategy(scen=scen, settings=CoSTEER_SETTINGS)

        # In runner, we don't need very big loops, so we set max_loop to 3
        super().__init__(
            *args, settings=CoSTEER_SETTINGS, eva=eva, es=es, evolving_version=2, scen=scen, max_loop=3, **kwargs
        )

    def develop(self, exp):
        bak_sub_tasks = exp.sub_tasks
        exp.sub_tasks = [
            CoSTEERTask(
                name="Debug running solution",
                description=f"The whole workflow of the solution has finished with some execution error, please check the error message and debug the whole code repo.\nCurrent code repo md5: {md5_hash(exp.experiment_workspace.all_codes)}",
            )
        ]
        exp = super().develop(exp)
        exp.sub_tasks = bak_sub_tasks

        score_fp = exp.experiment_workspace.workspace_path / "scores.csv"
        if not score_fp.exists():
            logger.error("Metrics file (scores.csv) is not generated.")
            raise RunnerError(f"Metrics file (scores.csv) is not generated")
        exp.result = pd.read_csv(score_fp, index_col=0)

        # DockerEnv for MLEBench submission validation
        mde = get_ds_env(conf_type="mlebench")
        mde.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/zip_files": "/mle/data"}
        mde.prepare()
        # MLEBench Check
        mle_check_code = (
            (Path(__file__).absolute().resolve().parent / "eval_tests" / "mle_submission_format_test.txt")
            .read_text()
            .replace("<competition_id>", self.scen.competition)
        )
        exp.experiment_workspace.inject_files(**{"test/mle_submission_format_test.py": mle_check_code})
        exp.format_check_result = exp.experiment_workspace.execute(
            env=mde, entry=f"python test/mle_submission_format_test.py"
        )

        return exp



================================================
File: rdagent/scenarios/data_science/dev/runner/eval.py
================================================
import json
import re
from pathlib import Path

import pandas as pd

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.CoSTEER.evaluators import (
    CoSTEEREvaluator,
    CoSTEERSingleFeedback,
)
from rdagent.components.coder.data_science.conf import (
    DSCoderCoSTEERSettings,
    get_ds_env,
)
from rdagent.core.evolving_framework import QueriedKnowledge
from rdagent.core.experiment import FBWorkspace, Task
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.tpl import T
from rdagent.utils.agent.workflow import build_cls_from_json_with_retry
from rdagent.utils.env import DockerEnv, MLEBDockerConf
from rdagent.utils.fmt import shrink_text

DIRNAME = Path(__file__).absolute().resolve().parent

DSCoSTEEREvalFeedback = CoSTEERSingleFeedback


class DSCoSTEERCoSTEEREvaluator(CoSTEEREvaluator):

    def evaluate(
        self,
        target_task: Task,
        implementation: FBWorkspace,
        gt_implementation: FBWorkspace,
        queried_knowledge: QueriedKnowledge = None,
        **kwargs,
    ) -> DSCoSTEEREvalFeedback:

        env = get_ds_env()
        env.conf.extra_volumes = {f"{DS_RD_SETTING.local_data_path}/{self.scen.competition}": "/kaggle/input"}
        env.conf.running_timeout_period = DS_RD_SETTING.full_timeout

        stdout = implementation.execute(
            env=env, entry=f"rm submission.csv scores.csv"
        )  # Remove previous submission and scores files generated by worklfow.

        # execute workflow
        stdout = implementation.execute(env=env, entry="coverage run main.py")
        stdout = re.sub(r"=== Start of EDA part ===(.*)=== End of EDA part ===", "", stdout)

        # Check score file
        score_fp = implementation.workspace_path / "scores.csv"
        score_ret_code = 0
        score_check_text = ""
        if not score_fp.exists():
            logger.warning("Metrics file (scores.csv) is not generated!")
            score_check_text = "[Error] Metrics file (scores.csv) is not generated!"
            score_ret_code = 1
        else:
            try:
                score_df = pd.read_csv(score_fp, index_col=0)
                model_set_in_scores = set(score_df.index)
                model_set_in_folder = set(
                    f[:-3] for f in implementation.file_dict.keys() if re.match(r"^model_(?!test)\w+\.py$", f)
                )
                if model_set_in_scores != model_set_in_folder.union({"ensemble"}):
                    score_check_text += f"\n[Error] The scores dataframe does not contain the correct model names as index.\ncorrect model names are: {model_set_in_folder.union({'ensemble'})}\nscore_df is:\n{score_df}"
                    score_ret_code = 1
            except Exception as e:
                logger.error(f"Error in checking the scores.csv file: {e}")
                score_check_text += f"\n[Error] in checking the scores.csv file: {e}\nscores.csv's content:\n-----\n{score_fp.read_text()}\n-----"
                score_ret_code = 1

        # DockerEnv for MLEBench submission validation
        mde = get_ds_env("mlebench")
        mde.conf.extra_volumes = {
            f"{DS_RD_SETTING.local_data_path}/zip_files": "/mle/data",
        }
        mde.prepare()
        # MLEBench Check
        mle_check_code = (
            (Path(__file__).absolute().resolve().parent / "eval_tests" / "mle_submission_format_test.txt")
            .read_text()
            .replace("<competition_id>", self.scen.competition)
        )
        implementation.inject_files(**{"test/mle_submission_format_test.py": mle_check_code})
        submission_check_out, submission_ret_code = implementation.execute_ret_code(
            env=mde, entry="python test/mle_submission_format_test.py"
        )
        stdout += f"\nMLEBench submission check:\n{submission_check_out}"

        system_prompt = T(".prompts:DSCoSTEER_eval.system").r(
            scenario=self.scen.get_scenario_all_desc(),
            task_desc=target_task.get_task_information(),
        )
        user_prompt = T(".prompts:DSCoSTEER_eval.user").r(
            code=implementation.all_codes,
            stdout=shrink_text(stdout),
        )

        feedback = build_cls_from_json_with_retry(
            DSCoSTEEREvalFeedback,
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            init_kwargs_update_func=DSCoSTEEREvalFeedback.val_and_update_init_dict,
        )

        if feedback:
            # remove unused files
            implementation.execute(env=env, entry="coverage json -o coverage.json")
            coverage_report_path = implementation.workspace_path / "coverage.json"
            if coverage_report_path.exists():
                used_files = set(json.loads(coverage_report_path.read_text())["files"].keys())
                coverage_report_path.unlink()
                logger.info(f"All used scripts: {used_files}")

                use_one_model = False
                for f in used_files:
                    if f.startswith("model_") and "test" not in f:
                        use_one_model = True
                        break

                if not use_one_model:
                    feedback.final_decision = False
                    logger.warning("No model script is used in `main.py`.")
                    feedback.code += "\n[Error] No model script is used in `main.py`."

                all_python_files = set(Path(implementation.workspace_path).rglob("*.py"))
                must_have_files = ["load_data.py", "feature.py", "ensemble.py"]

                unused_files = [
                    py_file.name
                    for py_file in all_python_files
                    if not (py_file.name in used_files or py_file.name.endswith("test.py"))
                ]
                if unused_files:
                    logger.warning(f"Unused scripts: {unused_files}")
                    error_files = set(unused_files).intersection(set(must_have_files))
                    if error_files:
                        feedback.final_decision = False
                        logger.warning(f"{error_files} must be used in `main.py`.")
                        feedback.code += f"\n[Error] {error_files} must be used in `main.py`."
                    elif use_one_model:
                        logger.info("Remove unused scripts.")
                        implementation.inject_files(**{file: implementation.DEL_KEY for file in unused_files})

        if score_ret_code != 0:
            feedback.final_decision = False
            feedback.return_checking += "\n" + score_check_text
        if submission_ret_code != 0:
            feedback.final_decision = False
            feedback.return_checking += "\nSubmission file check failed."
        return feedback



================================================
File: rdagent/scenarios/data_science/dev/runner/prompts.yaml
================================================
DSCoSTEER_eval:
  system: |-
    You are a data scientist responsible for evaluating all the code.

    ## Task Description
    The user is trying to build a data science solution in the following scenario:
    {{ scenario }}

    The task is as follows:
    {{ task_desc }}

    The whole workflow includes multiple stages, such as:
    - Data loading
    - Feature engineering
    - Model training
    - Ensembling

    The user will provide you the whole code base, some logs generated during the execution of the whole workflow. Your evaluation scope includes whether the workflow code:
    1. Executes successfully, correctly organizing components and generating a final submission.
    2. Generates predictions in the correct format, ensuring they align with the **sample submission** structure!

    
    Please respond with your feedback in the following JSON format and order
    ```json
    {
        "execution": "Describe whether the whole code base executed successfully and generating the final submission. Include any errors or issues encountered, and retain all error messages and traceback details.",
        "return_checking": "Verify the generated files, particularly the submission file. Ensure that its format matches the sample submission",
        "code": "Provide feedback on code quality, readability, and adherence to the given specifications.",
        "final_decision": <true/false>
    }
    ```
  
  user: |-
    --------- code base ---------
    {{ code }}
    --------- test stdout ---------
    {{ stdout }}

DSCoSTEER_debugger:
  system: |-
    You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science.
    You have finished the implementation of the whole workflow which has executed well on a sampled dataset. However, the user has reported that the workflow failed to execute on the full dataset.

    Your current job is to debug the whole code base, try to correct the errors, and ensure that the workflow can execute successfully on the full dataset.
    The user will provide your the whole code base and some feedback generated during the execution of the whole workflow. Please identify the issues and provide the corrected code.

    Task description:
    {{ task_desc }}

    Your modified code should follow the minimal changes principle. You should only modify the code that is necessary to fix the issues but not affect any other parts of the code. Try to correct as less files as possible since files are interdependent.

    ## Output Format
    {% if out_spec %}
    {{ out_spec }}
    {% else %}
    Please response the code in the following json format. Here is an example structure for the JSON output:
    {
        "code": "The Python code as a string."
    }
    {% endif %}
  user: |-
    --------- code base ---------
    {{ code }}
    --------- feedback ---------
    {{ feedback }}


================================================
File: rdagent/scenarios/data_science/dev/runner/eval_tests/mle_submission_format_test.txt
================================================
from pathlib import Path

from mlebench.grade import validate_submission
from mlebench.registry import registry

# Check if our submission file exists
assert Path('submission.csv').exists(), "Error: submission.csv not found"

COMPETITION_ID = "<competition_id>"
new_registry = registry.set_data_dir(Path("/mle/data"))
competition = new_registry.get_competition(COMPETITION_ID)

is_valid, message = validate_submission(Path("submission.csv"), competition)

print(message)


================================================
File: rdagent/scenarios/data_science/experiment/__init__.py
================================================



================================================
File: rdagent/scenarios/data_science/experiment/experiment.py
================================================
import re
from typing import Literal

import pandas as pd

from rdagent.core.experiment import Experiment, FBWorkspace, Task

COMPONENT = Literal["DataLoadSpec", "FeatureEng", "Model", "Ensemble", "Workflow"]


class DSExperiment(Experiment[Task, FBWorkspace, FBWorkspace]):
    def __init__(self, pending_tasks_list: list, *args, **kwargs) -> None:
        super().__init__(sub_tasks=[], *args, **kwargs)
        # Status
        # - Initial: blank;
        # - Injecting from SOTA code;
        # - New version no matter successful or not
        # the initial workspace or the successful new version after coding
        self.experiment_workspace = FBWorkspace()
        self.pending_tasks_list = pending_tasks_list
        self.format_check_result = None

    def is_ready_to_run(self) -> bool:
        """
        ready to run does not indicate the experiment is runnable
        (so it is different from `trace.next_incomplete_component`.)
        """
        return self.experiment_workspace is not None and "main.py" in self.experiment_workspace.file_dict



================================================
File: rdagent/scenarios/data_science/proposal/__init__.py
================================================



================================================
File: rdagent/scenarios/data_science/proposal/exp_gen.py
================================================
import json
from typing import Dict, Literal

import pandas as pd

from rdagent.components.coder.data_science.ensemble.exp import EnsembleTask
from rdagent.components.coder.data_science.feature.exp import FeatureTask
from rdagent.components.coder.data_science.model.exp import ModelTask
from rdagent.components.coder.data_science.raw_data_loader.exp import DataLoaderTask
from rdagent.components.coder.data_science.workflow.exp import WorkflowTask
from rdagent.core.knowledge_base import KnowledgeBase
from rdagent.core.proposal import ExperimentFeedback, ExpGen, Hypothesis, Trace
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.data_science.experiment.experiment import COMPONENT, DSExperiment
from rdagent.scenarios.data_science.scen import DataScienceScen
from rdagent.utils.agent.tpl import T
from rdagent.utils.repo.diff import generate_diff_from_dict
from rdagent.utils.workflow import wait_retry


class DSHypothesis(Hypothesis):
    def __init__(
        self,
        component: COMPONENT,
        hypothesis: str = "",
        reason: str = "",
        concise_reason: str = "",
        concise_observation: str = "",
        concise_justification: str = "",
        concise_knowledge: str = "",
    ) -> None:
        super().__init__(
            hypothesis, reason, concise_reason, concise_observation, concise_justification, concise_knowledge
        )
        self.component = component

    def __str__(self) -> str:
        if self.hypothesis == "":
            return f"No hypothesis available. Trying to construct the first runnable {self.component} component."
        return f"""Chosen Component: {self.component}
Hypothesis: {self.hypothesis}
Reason: {self.reason}
Concise Reason & Knowledge: {self.concise_reason}
Concise Observation: {self.concise_observation}
Concise Justification: {self.concise_justification}
Concise Knowledge: {self.concise_knowledge}
"""


COMPONENT_TASK_MAPPING = {
    "DataLoadSpec": {
        "target_name": "Data loader and specification generation",
        "spec_file": "spec/data_loader.md",
        "task_output_format": T(".prompts:output_format.data_loader").r(),
        "task_class": DataLoaderTask,
    },
    "FeatureEng": {
        "target_name": "Feature engineering",
        "spec_file": "spec/feature.md",
        "task_output_format": T(".prompts:output_format.feature").r(),
        "task_class": FeatureTask,
    },
    "Model": {
        "target_name": "Building model",
        "spec_file": "spec/model.md",
        "task_output_format": T(".prompts:output_format.model").r(),
        "task_class": ModelTask,
        "extra_params": {
            "model_type": "Model type not provided",
            "architecture": "Model architecture not provided",
            "hyperparameters": "Model hyperparameters not provided",
        },
        "extra_requirement": T(".prompts:extra_requirement.model").r(),
    },
    "Ensemble": {
        "target_name": "Ensemble",
        "spec_file": "spec/ensemble.md",
        "task_output_format": T(".prompts:output_format.ensemble").r(),
        "task_class": EnsembleTask,
    },
    "Workflow": {
        "target_name": "Workflow",
        "spec_file": "spec/workflow.md",
        "task_output_format": T(".prompts:output_format.workflow").r(),
        "task_class": WorkflowTask,
    },
}


class DSTrace(Trace[DataScienceScen, KnowledgeBase]):
    def __init__(self, scen: DataScienceScen, knowledge_base: KnowledgeBase | None = None) -> None:
        self.scen: DataScienceScen = scen
        self.hist: list[tuple[DSExperiment, ExperimentFeedback]] = []
        self.knowledge_base = knowledge_base

    COMPLETE_ORDER = ("DataLoadSpec", "FeatureEng", "Model", "Ensemble", "Workflow")

    def next_incomplete_component(self) -> COMPONENT | None:
        """
        NOTE:
        - A component will be complete until get True decision feedback !!!
        """
        for c in self.COMPLETE_ORDER:
            if not self.has_component(c):
                return c
        return None

    def has_component(self, component: COMPONENT) -> bool:
        for exp, fb in self.hist:
            assert isinstance(exp.hypothesis, DSHypothesis), "Hypothesis should be DSHypothesis (and not None)"
            if exp.hypothesis.component == component and fb:
                return True
        return False

    def experiment_and_feedback_list_after_init(
        self, return_type: Literal["sota", "failed", "all"]
    ) -> list[tuple[DSExperiment, ExperimentFeedback]]:
        """
        Retrieve a list of experiments and feedbacks based on the return_type.

        Parameters
        ----------
        return_type : str
            One of "sota", "failed", "all".

        Returns
        -------
        list[tuple[DSExperiment, ExperimentFeedback]]
            List of experiments and feedbacks.
        """

        final_component = self.COMPLETE_ORDER[-1]
        has_final_component = False
        exp_and_feedback_list = []
        for exp, fb in self.hist:
            if has_final_component:
                if return_type == "all":
                    exp_and_feedback_list.append((exp, fb))
                elif return_type == "failed" and not fb.decision:
                    exp_and_feedback_list.append((exp, fb))
                elif return_type == "sota" and fb.decision:
                    exp_and_feedback_list.append((exp, fb))
            if exp.hypothesis.component == final_component and fb:
                has_final_component = True
        return exp_and_feedback_list

    def sota_experiment(self) -> DSExperiment | None:
        """
        Returns
        -------
        Experiment or None
            The experiment result if found, otherwise None.
        """
        if self.next_incomplete_component() is None:
            for exp, ef in self.hist[::-1]:
                # the sota exp should be accepted decision and all required components are completed.
                if ef.decision:
                    return exp
        return None

    def last_successful_exp(self) -> DSExperiment | None:
        """
        Access the last successful experiment even part of the components are not completed.
        """
        for exp, ef in self.hist[::-1]:
            if ef.decision:
                return exp
        return None

    def last_runnable_exp_fb(self) -> tuple[DSExperiment, ExperimentFeedback] | None:
        """
        Access the last runnable experiment (no exception, usually not all task failed) and feedback
        """
        for exp, ef in self.hist[::-1]:
            if ef.exception is None:
                return exp, ef
        return None


class DSExpGen(ExpGen):
    """Data Science Task Generator."""

    def __init__(self, scen: DataScienceScen, max_trace_hist: int = 3) -> None:
        self.max_trace_hist = max_trace_hist  # max number of historical trace to know when propose new experiment
        super().__init__(scen)

    def _init_task_gen(
        self,
        targets: str,
        scenario_desc: str,
        task_output_format: str,
        workspace_code: str | None = None,
        spec: str = None,
        hypothesis: Hypothesis | None = None,
        exp_and_feedback_desc: str | None = None,
        former_task: str | None = None,
    ) -> dict:
        system_prompt = T(".prompts:task_gen.system").r(
            targets=targets,
            scenario=scenario_desc,
            task_specification=spec,
            hypothesis=hypothesis,
            task_output_format=task_output_format,
        )
        user_prompt = T(".prompts:task_gen.user").r(
            targets=targets,
            hypothesis=hypothesis,
            workspace_code=workspace_code,
            exp_and_feedback_desc=exp_and_feedback_desc,
            former_task_desc=former_task,
        )

        resp_dict = json.loads(
            APIBackend().build_messages_and_create_chat_completion(
                user_prompt=user_prompt, system_prompt=system_prompt, json_mode=True, json_target_type=dict
            )
        )

        return resp_dict

    def _handle_missing_component(
        self,
        component: COMPONENT,
        task_cls: type,
        scenario_desc: str,
        trace: Trace,
        last_successful_exp: DSExperiment | None,
        spec_file: str | None = None,
        component_prompt_key: str | None = None,
    ) -> DSExperiment:
        """Handle any component using a unified approach.

        Args:
            component: Name of the component (e.g. "DataLoadSpec")
            task_cls: The task class to instantiate (e.g. DataLoaderTask)
            scenario_desc: Description of the current scenario
            last_successful_exp: Last successful experiment or None
            spec_file: Path to specification file if needed
        """

        former_tasks_desc = ""
        if len(trace.hist) > 0:
            for exp, fb in reversed(trace.hist):
                if exp is not last_successful_exp:
                    former_task_desc = exp.pending_tasks_list[0][0].get_task_information()
                    former_task_desc += f"\n\nYou have tried to implement the same component and got the following exception: \n{fb.exception}\n Please try different methods to avoid the same errors and results in an infinite loop"
                    former_tasks_desc += former_task_desc
                else:
                    break

        resp_dict = self._init_task_gen(
            targets=component,
            scenario_desc=scenario_desc,
            spec=last_successful_exp.experiment_workspace.file_dict[spec_file] if spec_file else None,
            task_output_format=T(f".prompts:output_format.{component_prompt_key or component.lower()}").r(),
            former_task=former_tasks_desc if former_tasks_desc else None,
        )

        task = task_cls(
            name=component if component != "Model" else resp_dict.pop("model_name"),
            description=resp_dict.get("description", f"{component} description not provided"),
            **{
                k: resp_dict.get("extra_params", {}).get(k, v)
                for k, v in COMPONENT_TASK_MAPPING[component].get("extra_params", {}).items()
            },
        )

        exp = DSExperiment(pending_tasks_list=[[task]], hypothesis=DSHypothesis(component))
        if last_successful_exp:
            # exp.experiment_workspace.inject_code_from_folder(last_successful_exp.experiment_workspace.workspace_path)
            exp.experiment_workspace.inject_code_from_file_dict(last_successful_exp.experiment_workspace)
        return exp

    def gen(self, trace: DSTrace) -> DSExperiment:
        scenario_desc = trace.scen.get_scenario_all_desc()
        last_successful_exp = trace.last_successful_exp()

        next_missing_component = trace.next_incomplete_component()

        init_component_config = {
            "DataLoadSpec": {"task_cls": DataLoaderTask, "spec_file": None, "component_prompt_key": "data_loader"},
            "FeatureEng": {"task_cls": FeatureTask, "spec_file": "spec/feature.md", "component_prompt_key": "feature"},
            "Model": {"task_cls": ModelTask, "spec_file": "spec/model.md", "component_prompt_key": "model"},
            "Ensemble": {"task_cls": EnsembleTask, "spec_file": "spec/ensemble.md", "component_prompt_key": "ensemble"},
            "Workflow": {"task_cls": WorkflowTask, "spec_file": "spec/workflow.md", "component_prompt_key": "workflow"},
        }

        if next_missing_component in init_component_config:
            # TODO: we may merge the if else logic in the future.
            # the current
            config = init_component_config[next_missing_component]
            return self._handle_missing_component(
                component=next_missing_component,
                task_cls=config["task_cls"],
                scenario_desc=scenario_desc,
                last_successful_exp=last_successful_exp,
                spec_file=config.get("spec_file"),
                trace=trace,
                component_prompt_key=config.get("component_prompt_key"),
            )
        else:  # propose new component by LLM
            # Guidelines:
            # System prompts: Shared condition you are facing
            # - scenario description: `scenario_desc`
            # - expected output format
            # User prompts: Task Specific information
            # - Previous Feedback
            # - Current sota implementation (encourage change based on it)
            # - Extra RAG
            sota_exp = trace.sota_experiment()
            assert sota_exp is not None, "SOTA experiment is not provided."
            exp_and_feedback = trace.hist[-1]
            last_exp = exp_and_feedback[0]

            # Step 1: Generate component
            # Describe current best solution using shared template
            sota_exp_desc = T("scenarios.data_science.share:describe.exp").r(
                exp=sota_exp, heading="Best of previous exploration of the scenario"
            )
            last_exp_diff = "\n".join(
                generate_diff_from_dict(
                    sota_exp.experiment_workspace.file_dict, last_exp.experiment_workspace.file_dict
                )
            )  # we use file_dict for hitting the cache when replicate the experiment in another machine.

            sota_exp_feedback_list = trace.experiment_and_feedback_list_after_init(return_type="sota")
            failed_exp_feedback_list = trace.experiment_and_feedback_list_after_init(return_type="failed")[
                -self.max_trace_hist :
            ]
            all_exp_feedback_list = trace.experiment_and_feedback_list_after_init(return_type="all")
            trace_component_to_feedback_df = pd.DataFrame(columns=["component", "hypothesis", "decision"])
            for index, (exp, fb) in enumerate(all_exp_feedback_list):
                trace_component_to_feedback_df.loc[f"trial {index + 1}"] = [
                    exp.hypothesis.component,
                    exp.hypothesis.hypothesis,
                    fb.decision,
                ]

            sota_exp_feedback_list_desc = T("scenarios.data_science.share:describe.trace").r(
                exp_and_feedback_list=sota_exp_feedback_list,
                success=True,
            )
            failed_exp_feedback_list_desc = T("scenarios.data_science.share:describe.trace").r(
                exp_and_feedback_list=failed_exp_feedback_list,
                success=False,
            )

            # Generate component using template with proper context
            component_sys_prompt = T(".prompts:component_gen.system").r(
                scenario=scenario_desc,
                sota_exp_desc=sota_exp_desc,
                last_exp_diff=last_exp_diff,
                component_output_format=T(".prompts:output_format.component").r(),
            )

            component_user_prompt = T(".prompts:component_gen.user").r(
                sota_exp_and_feedback_list_desc=sota_exp_feedback_list_desc,
                failed_exp_and_feedback_list_desc=failed_exp_feedback_list_desc,
                component_and_feedback_df=(
                    trace_component_to_feedback_df.to_string()
                    if len(trace_component_to_feedback_df) > 0
                    else "No experiment and feedback provided"
                ),
            )

            resp_dict_component: dict = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    component_user_prompt, component_sys_prompt, json_mode=True, json_target_type=Dict[str, str]
                )
            )

            component = resp_dict_component.get("component", "Component not provided")
            component_reason = resp_dict_component.get("reason", "Reason not provided")
            sota_exp_model_file_count = len(
                [
                    k
                    for k in sota_exp.experiment_workspace.file_dict.keys()
                    if k.endswith(".py") and "test" not in k and k.startswith("model")
                ]
            )
            if sota_exp_model_file_count <= 1 and component == "Ensemble":
                component = "Model"

            # Why we should split component selection and steps after?
            # - after we know the selected component, we can use RAG.

            # Step 2: Generate the rest of the hypothesis & task
            component_info = COMPONENT_TASK_MAPPING.get(component)

            if component_info:
                system_prompt = T(".prompts:direct_exp_gen.system").r(
                    targets=component_info["target_name"],
                    component=component,
                    scenario=scenario_desc,
                    hypothesis_specification=T(".prompts:hypothesis_specification").r(),
                    hypothesis_output_format=T(".prompts:output_format.hypothesis").r(),
                    task_specification=sota_exp.experiment_workspace.file_dict[component_info["spec_file"]],
                    task_output_format=component_info["task_output_format"],
                    extra_requirement=component_info.get("extra_requirement"),
                    workflow_check=(not component == "Workflow"),
                )

                user_prompt = T(".prompts:direct_exp_gen.user").r(
                    targets=component_info["target_name"],
                    sota_exp_and_feedback_list_desc=sota_exp_feedback_list_desc,
                    failed_exp_and_feedback_list_desc=failed_exp_feedback_list_desc,
                    last_exp_diff=last_exp_diff,
                )

                def _append_retry(args: tuple, kwargs: dict) -> tuple[tuple, dict]:
                    # Only modify the user_prompt on retries (i > 0)
                    user_prompt = args[0]
                    user_prompt += "\n\nretrying..."
                    return (user_prompt,), kwargs

                @wait_retry(retry_n=5, transform_args_fn=_append_retry)
                def _f(user_prompt):
                    resp_dict = json.loads(
                        APIBackend().build_messages_and_create_chat_completion(
                            user_prompt=user_prompt,
                            system_prompt=system_prompt,
                            json_mode=True,
                            # NOTE: corner cases.
                            # workflow_update may be a string
                            # model could have 2 level nested dict.
                            json_target_type=dict[str, dict[str, str | dict] | str],
                        )
                    )
                    assert "hypothesis_proposal" in resp_dict, "Hypothesis proposal not provided."
                    assert "task_design" in resp_dict, "Task design not provided."
                    task_class = component_info["task_class"]
                    hypothesis_proposal = resp_dict.get("hypothesis_proposal", {})
                    hypothesis = DSHypothesis(
                        component=component,
                        hypothesis=hypothesis_proposal.get("hypothesis", ""),
                        reason=component_reason + "\n" + hypothesis_proposal.get("reason", ""),
                        concise_reason=hypothesis_proposal.get("concise_reason", ""),
                        concise_observation=hypothesis_proposal.get("concise_observation", ""),
                        concise_justification=hypothesis_proposal.get("concise_justification", ""),
                        concise_knowledge=hypothesis_proposal.get("concise_knowledge", ""),
                    )

                    task_design = resp_dict.get("task_design", {})
                    task_name = task_design["model_name"] if component == "Model" else component
                    description = task_design.get(
                        "description", f"{component_info['target_name']} description not provided"
                    )
                    task = task_class(
                        name=task_name,
                        description=description,
                        **{k: task_design.get(k, v) for k, v in component_info.get("extra_params", {}).items()},
                    )
                    new_workflow_desc = resp_dict.get("workflow_update", "No update needed")
                    return hypothesis, task, new_workflow_desc

                hypothesis, task, new_workflow_desc = _f(user_prompt)

                exp = DSExperiment(pending_tasks_list=[[task]], hypothesis=hypothesis)
                # exp.experiment_workspace.inject_code_from_folder(sota_exp.experiment_workspace.workspace_path)
                exp.experiment_workspace.inject_code_from_file_dict(sota_exp.experiment_workspace)

                if new_workflow_desc != "No update needed":
                    workflow_task = WorkflowTask(
                        name="Workflow",
                        description=new_workflow_desc,
                    )
                    exp.pending_tasks_list.append([workflow_task])
                return exp
            else:
                raise ValueError(f"Unknown component: {component}")



================================================
File: rdagent/scenarios/data_science/proposal/prompts.yaml
================================================
hypothesis_gen: # It is deprecated now, please refer to direct_exp_gen
  system: |-
    The user is working on generating new hypotheses for the {{ targets }} in a data-driven research and development process. 
    The {{ targets }} are used in the following scenario:
    {{ scenario }}
    
    The user has already proposed several hypotheses and conducted evaluations. This information will be provided to you. Your task is to:
    1. Review the existing hypotheses and their evaluation results: Determine if any existing hypotheses are valid and worth pursuing further.
    2. Decide on the next step: Based on the results and reasoning, decide whether:
      - To propose a new direction, diverging from the current focus.
      - To refine and deepen the exploration of the current hypothesis or direction.
    3. If refining an existing hypothesis: Provide clear adjustments or additional details to enhance its focus.
    4. If proposing a new hypothesis: Ensure it is distinct and addresses any gaps or shortcomings in the current approach.

    The current component to focus on is: {{ component }}.
    {% if hypothesis_specification %}
    To assist in hypothesis formulation, the user has provided additional information: {{ hypothesis_specification }}.
    Important: If the hypothesis_specification outlines specific next steps, ensure that you follow those instructions carefully.
    {% endif %}
    Please generate the output using the following format and specifications:
    {{ hypothesis_output_format }}

  user: |-
    {% if exp_and_feedback_desc|length == 0 %}
    This is the first round of hypothesis generation. The user has not yet proposed any hypotheses for this scenario.
    {% else %}
    This is not the first round. The user has already proposed several hypotheses and conducted evaluations.
    
    The previous hypotheses and their corresponding feedback are as follows (focus on the most recent hypothesis, its derived insights, and reasoning):
    {{ exp_and_feedback_desc }}
    {% endif %}
    
    In addition, generate relevant reasoning and distilled knowledge keys.
    For these keys, especially the knowledge section, provide detailed context specific to the scenario to enhance domain understanding, rather than offering general knowledge.

hypothesis_model: # It is deprecated now, please refer to direct_exp_gen
  system: |-
    The user is working on generating new hypotheses for the {{ targets }} in a data-driven research and development process. 
    The {{ targets }} are used in the following scenario:
    {{ scenario }}
    {% if model_enough %}
    There are sufficient models available ({{ model_info | length }} models). Your task is to choose one of the existing models for further tuning or optimization. Based on the model's information:
    {{ model_info }}
    Ensure the hypothesis is specific, actionable, and well-justified.
    {% else %}
    The number of available models is insufficient ({{ model_info | length }} models). Your task is to first decide whether to:
    - Tune an existing model: Select one of the current models for further tuning and improvement.
    - Add a new model: Introduce a new model to expand the hypothesis space.
    Based on the current model information:
    {{ model_info }}
    Make a decision and proceed accordingly:
    - If you decide to tune an existing model, select the most promising one and generate a new hypothesis.
    - If you decide to add a new model, specify the type of model you would add and generate a new hypothesis related to the new model.
    {% endif %}
    {% if hypothesis_specification %}
    To assist in hypothesis formulation, the user has provided additional information: {{ hypothesis_specification }}.
    Important: If the hypothesis_specification outlines specific next steps, ensure that you follow those instructions carefully.
    {% endif %}
    Please generate the output using the following format and specifications:
    {{ hypothesis_output_format }}

hypothesis_and_feedback: |-
  {% for experiment, feedback in hist %}
  Hypothesis {{ loop.index }}
  The experiment is design driven by hypothesis : {{ experiment.hypothesis }}
  Observation on the result with the hypothesis: {{ feedback.observations }}
  Feedback on the original hypothesis:  {{ feedback.hypothesis_evaluation }}
  Did changing to this hypothesis work? (focus on the change):  {{ feedback.decision }}
  {% endfor %}

task_gen:
  system: |-
    {% if hypothesis is not none %}
    The user is trying to generate new {{ targets }} based on the hypothesis generated in the previous step. 
    {% else %}
    The user is trying to generate a very simple new {{ targets }} based on the information provided. 
    {% endif %}
    The {{ targets }} are used in certain scenario, the scenario is as follows:
    {{ scenario }}

    {% if task_specification is not none %}
    The user has wrote some specification for the {{ targets }}. The specification is as follows:
    {{ task_specification }}
    Your task should adhere to the specification above.
    {% endif %}

    {% if hypothesis is none %}
    Since we are at the very beginning stage, we plan to start from a very simple task. To each component, please only generate the task to implement the most simple and basic function of the component. For example, the feature engineering should only implement the function which output the raw data without any transformation. The model component only uses the most basic and easy to implement model without any tuning. The ensemble component only uses the simplest ensemble method. The main focus at this stage is to build the first runnable version of the solution.
    {% else %}
    The user will use the {{ targets }} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{ targets }} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{ targets }} on similar hypothesis.
    4. Some additional information to help you generate new {{ targets }}.
    {% endif %}

    Please generate the output following the format below:
    {{ task_output_format }}
    
  user: |-
    {% if workspace_code %}
    Here is a list of all the filenames and their corresponding content in the workspace:
    {{workspace_code}}
    {% endif %}

    {% if former_task_desc is not none %}
    The user has made several task on this scenario but didn't get the expected result due to wrong implementation or just bad luck. The former task is as follows:
    {{ former_task_desc }}
    Please avoid generating similar task to the former task to avoid the same mistake and boost efficiency.
    
    {% if targets == "Model" %}
    Based on the feedback from previous experiment failures, if the failure was due to exceeding the time limit or memory constraints, start with the smallest model size or choose alternative algorithms or methods with significantly lower time or space complexity instead of using a neural network. You can then iteratively refine and optimize the model in later stages.
    {% endif %}
    
    {% endif %}

    {% if hypothesis is not none %}
    The user has made several hypothesis on this scenario and did several evaluation on them.
    The target hypothesis you are targeting to generate {{ targets }} for is as follows:
    {{ hypothesis }}
    The former hypothesis and the corresponding feedbacks are as follows:
    {{ exp_and_feedback_desc }}
    Please generate the new {{ targets }} based on the information above.
    {% else %}
    Please generate the new {{ targets }} task.
    {% endif %}

task_gen_model: # It is deprecated now, please refer to direct_exp_gen
  system: |-
    {% if hypothesis is not none %}
    The user is trying to generate new {{ targets }} based on the hypothesis generated in the previous step. 
    {% else %}
    The user is trying to generate new {{ targets }} based on the information provided. 
    {% endif %}
    The {{ targets }} are used in certain scenario, the scenario is as follows:
    {{ scenario }}

    {% if hypothesis is not none %}
    The user will use the {{ targets }} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{ targets }} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{ targets }} on similar hypothesis.
    4. Some additional information to help you generate new {{ targets }}.
    {% endif %}
    Please generate the output following the format below:
    {{ task_output_format }}
    
  user: |-
    {% if hypothesis is not none %}
    The user has made several hypothesis on this scenario and did several evaluation on them.
    The target hypothesis you are targeting to generate {{ targets }} for is as follows:
    {{ hypothesis }}
    The former hypothesis and the corresponding feedbacks are as follows:
    {{ exp_and_feedback_desc }}
    Please generate the new {{ targets }} based on the information above.
    {% else %}
    Please generate the new {{ targets }} task.
    {% endif %}

direct_exp_gen:
  system: |-
    You are a data scientist and a top Kaggle competitor. The user is working on creating a solution for a Kaggle competition. Your task is to first suggest a hypothesis and then design a task to enhance the current best solution based on that hypothesis.

    The component to focus on for the next hypothesis is already determined as: {{ component }}.
    It will be used in the following scenario:
    {{ scenario }}

    # Hypothesis Proposal

    The user has already proposed several hypotheses and conducted evaluations on them. This information will be provided to you later. Your task is to check if a similar hypothesis has already been generated. If one exists and you agree with it, you can use it. If you disagree, please create an improved version.

    ## Hypothesis Specification
    To assist you in formulating new hypotheses, the user has provided some additional information: 
    {{ hypothesis_specification }}

    ## Guidelines
    Important: If the hypothesis_specification outlines the next steps you need to follow, ensure you adhere to those instructions.

    [Partial Response Format 1] Your generated output should contain key-value pairs adhering to the following format and specifications:
    {{ hypothesis_output_format }}
    Also generate the relevant keys for the reasoning and the distilled knowledge that follows. For those keys, in particular for knowledge, explain in the context of the specific scenario to build up domain knowledge in the specific field rather than general knowledge.

    # Task Design

    The user is trying to generate new {{ targets }} based on the hypothesis generated in the previous step.

    ## Task Specification
    The scope of the {{ targets }} can be described by a interface specification as follows:
    ```Python
    {{ task_specification }}
    ```

    ## Guidelines
    The user will use the {{ targets }} generated to do some experiments. The user will provide this information to you:
    1. The target hypothesis you are targeting to generate {{ targets }} for.
    2. The hypothesis generated in the previous steps and their corresponding feedbacks.
    3. Former proposed {{ targets }} on similar hypothesis.
    4. Some additional information to help you generate new {{ targets }}.

    [Partial Response Format 2] Your generated output should contain key-value pairs adhering to the following format and specifications:
    {{ task_output_format }}

    {% if workflow_check %}
    # Workflow update
    Since components have dependencies, the workflow should be updated to reflect the changes made to the target component. Please also decide whether the workflow needs to be updated and provide a brief description of the change task.
    [Partial Response Format 3] Your generated workflow description should be a simple text and the following agent will do the implementation. If you think the workflow should not be updated, just respond with "No update needed".
    {% endif %}

    Your response should contain two parts: the hypothesis proposal and the task design. Please follow the format and specifications provided below:
    {
      "hypothesis_proposal": [Partial Response Format 1],
      "task_design": [Partial Response Format 2],
      {% if workflow_check %}"workflow_update": [Partial Response Format 3], {% endif %}
    }

    {% if extra_requirement %}
    {{ extra_requirement }}
    {% endif %}

  user: |-
    # All former successful experiments and their feedbacks, the current SOTA solution is the combination of the best solutions of these trials:
    {{ sota_exp_and_feedback_list_desc }}

    {% if failed_exp_and_feedback_list_desc %}
    # Several trials after the best experiments
    The user has made several hypothesis on this scenario and did several evaluation on them.
    {{ failed_exp_and_feedback_list_desc }}
    
    {% if targets == "Building model" %}
    Based on the feedback from previous experiment failures, if the failure was due to exceeding the time limit or memory constraints, start with the smallest model size or choose alternative algorithms or methods with significantly lower time or space complexity instead of using a neural network. You can then iteratively refine and optimize the model in later stages.
    {% endif %}
    
    {% endif %}
    
    {% if last_exp_diff %}
    # Here are the differences between the latest version of implementation and the current best version of implementation
    It is presented in diff format, highlighting changes from the best version to the latest version.
    {{ last_exp_diff }}
    {% endif %}

extra_requirement:
  model: |-
    If there are sufficient models available. Your task is to choose one of the existing models for further tuning or optimization. Based on the model's information:

    If the number of available models is insufficient. Your task is to first decide whether to:
    - Tune an existing model: Select one of the current models for further tuning and improvement.
    - Add a new model: Introduce a new model to expand the hypothesis space.

    The information of the model is described by the code of workspace.

    Make a decision and proceed accordingly:
    - If you decide to tune an existing model, select the existing model file and generate a new hypothesis.
    - If you decide to add a new model, specify the type of model you would add and generate a new hypothesis related to the new model.

    When building the model, if the runtime permits, consider incorporating hyperparameter search methods to improve performance.

component_gen:
  system: |-
    You are a Kaggle Grander Master. You are going to provide a solution for a kaggle competition.

    Here is the description of the competition scenario:
    ```
    {{ scenario }}
    ```

    # Here is the current best version of implementation:
    {{ sota_exp_desc }}

    {% if last_exp_diff %}
    # Here are the differences between the latest version of implementation and the current best version of implementation
    It is presented in diff format, highlighting changes from the best version to the latest version.
    {{ last_exp_diff }}
    {% endif %}

    You will be provided the feedback for the latest implementation.

    Please select the component you are going to improve the latest implementation or sota implementation. 

    Please generate the output in JSON format following the format below:
    {{ component_output_format }}

  user: |-
    Here's the former SOTA experiments and their feedbacks:
    {{ sota_exp_and_feedback_list_desc }}

    Also, here's the former failed experiments and their feedbacks:
    {{ failed_exp_and_feedback_list_desc }}

    All former trials and their feedbacks are provided in pandas DataFrame format. The user has already made several hypothesis on this scenario and did several evaluation on them:
    {{ component_and_feedback_df }}
    
    Please choose the most proper component to focus on based on the information above. Please balance the exploration and exploitation.
    Avoid selecting the same component more than 5 times in a row to ensure that the chosen component is not overly repetitive.


exp_and_feedback: |-
  {% for experiment, feedback in trace.hist[-10:] %}
  ## Experiment {{ loop.index }}
  Experiment are focusing on task: {{ experiment.pending_tasks_list[0][0] }}
  {% if experiment.hypothesis %}
  The experiment is design driven by hypothesis : {{ experiment.hypothesis }}
  Observation on the result with the hypothesis: {{ feedback.observations }}
  {% endif %}
  Feedback on the original hypothesis:  {{ feedback.hypothesis_evaluation }}
  Did changing to this hypothesis work? (focus on the change):  {{ feedback.decision }}
  {% endfor %}

hypothesis_specification: |-
  1. The hypothesis should be precise, testable, and directly actionable. Avoid general or vague statements. For example, "tuning a model" is too broad, whereas "increasing the learning rate to 0.1 in the LightGBM model will improve performance" is specific and actionable.
  2. Each hypothesis should focus on a single direction per experiment. Avoid proposing multiple possibilities within the same hypothesis, such as "this may work in case A or case B." Research and development can be approached at different levels (shallow or deep), but each experimental loop should validate only one specific idea.
  3. The hypothesis should based on current SOTA solution. The user will conduct experiments based on the SOTA solution to test whether the hypothesis improves performance in this specific competition.

output_format:
  component: |-
    {
      "reason": "The reason why you choose this component. Based on the current status and former trials, why this component is the most promising one to focus on.",
      "component": "The component you suggest to focus on. It must be one of ['DataLoadSpec', 'FeatureEng', 'Model', 'Ensemble', 'Workflow']."
    }
  hypothesis: |-
    The output should follow JSON format. The schema is as follows:
    {
      "component": "If "hypothesis_specification" provides the component you need to take, please follow "hypothesis_specification" to choose the component. Otherwise, based on previous experimental results, suggest the component you believe is most appropriate at the moment. It should be one of ["DataLoadSpec", "FeatureEng", "Model", "Ensemble", "Workflow"]",
      "hypothesis": "The new hypothesis generated based on the information provided.",
      "reason": "The reason why you generate this hypothesis. It should be comprehensive and logical. It should cover the other keys below and extend them.",
      "concise_reason": "Two-line summary. First line focuses on a concise justification for the change. Second line generalizes a knowledge statement.",
      "concise_observation": "One line summary. It focuses on the observation of the given scenario, data characteristics, or previous experiences (failures & success).",
      "concise_justification": "One line summary. Justify the hypothesis based on theoretical principles or initial assumptions.",
      "concise_knowledge": "One line summary. Transferable knowledge based on theoretical principles. Use conditional grammar. eg. "If...., ..; When..., .; and etc" Make sure that you state things clearly without ambiguity. Eg. avoid saying "previous hypothesis", because one wouldn't know what that is."
    }
  data_loader: |-
    Design a specific and detailed data loader task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of the overall data loader for the data science workflow",
        # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
    }
  feature: |-
    Design a specific and detailed feature engineering task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of feature engineering task",
        # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
    }
  model: |-
    Design a specific and detailed model task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows: 
    {
        "model_name": "model name, must start with 'model_' and only contain letters, numbers, and underscores",
        "description": "A precise and comprehensive description of the model",
        "extra_params":
        {
          "model_type": "The type of the model, e.g., neural network, tree-based model, etc.",
          "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
          "hyperparameters": {
            "hyperparameter_name_1": "value of hyperparameter 1",
            "hyperparameter_name_2": "value of hyperparameter 2",
            "hyperparameter_name_3": "value of hyperparameter 3"
          },
        },
    }
  ensemble: |-
    Design a specific and detailed ensemble task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of the ensemble",
    }
  workflow: |-
    Design a specific and detailed workflow task based on the given hypothesis. The output should be detailed enough to directly implement the corresponding code.
    The output should follow JSON format. The schema is as follows:
    {
        "description": "A precise and comprehensive description of the workflow",
    }



================================================
File: rdagent/scenarios/data_science/scen/__init__.py
================================================
import json
import os
from pathlib import Path
from typing import Dict

import pandas as pd
from PIL import Image, TiffTags

from rdagent.app.data_science.conf import DS_RD_SETTING
from rdagent.components.coder.data_science.conf import get_ds_env
from rdagent.core.experiment import FBWorkspace
from rdagent.core.scenario import Scenario
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.kaggle_crawler import (
    crawl_descriptions,
    leaderboard_scores,
)
from rdagent.utils.agent.tpl import T


def read_csv_head(file_path, indent=0, lines=5, max_col_width=100):
    """
    Reads the first few rows of a CSV file and formats them with indentation and optional truncation.

    Parameters:
        file_path (str): Path to the CSV file.
        indent (int): Number of spaces to prepend to each line for indentation.
        lines (int): Number of rows to read from the CSV file.
        max_col_width (int): Maximum width of each column's content.

    Returns:
        str: A formatted string of the first few rows of the CSV file.
    """
    try:
        # Read the CSV file with specified rows
        df = pd.read_csv(file_path, nrows=lines)

        if df.empty:
            return " " * indent + "(No data in the file)"

        # Truncate column contents to a maximum width
        truncated_df = df.copy()
        for col in truncated_df.columns:
            truncated_df[col] = (
                truncated_df[col]
                .astype(str)
                .apply(lambda x: (x[:max_col_width] + "...") if len(x) > max_col_width else x)
            )

        # Convert DataFrame to a string representation
        df_string_lines = truncated_df.to_string(index=False).split("\n")

        # Add indentation to each line
        indented_lines = [" " * indent + line for line in df_string_lines]

        return "\n".join(indented_lines)
    except FileNotFoundError:
        return f"Error: File not found at path '{file_path}'."
    except pd.errors.EmptyDataError:
        return f"Error: The file at '{file_path}' is empty."
    except Exception as e:
        return f"Error reading CSV: {e}"


def get_dir_snapshot(folder_path):
    """
    [note]
        - Returns a set of file extensions within the subfolder (excluding subfolder names)
        - Compares only the types of files contained, not specific file names or quantities
    """
    exts = set()
    try:
        with os.scandir(folder_path) as it:
            for entry in it:
                if entry.is_file():
                    file_ext = os.path.splitext(entry.name)[1]
                    exts.add(file_ext)
    except Exception as e:
        logger.error(f"Error scanning directory: {e}")

    return frozenset(exts)


def describe_data_folder(folder_path, indent=0, max_files=2, partial_expand_subfolders=2, is_top_level=True):
    """
    folder_path              : Current directory path
    indent                   : Current indentation
    max_files                : Maximum number of files of the same type to display
    partial_expand_subfolders: When all subfolders have the same internal file types, only expand this many subfolders, the rest are omitted
    is_top_level             : Indicates if the current folder is the top-level folder
    """
    result = []
    files_count = {}
    files_details = {}

    for root, dirs, files in os.walk(folder_path):
        dirs.sort()
        files.sort()
        if not dirs:
            for file in files:
                file_path = os.path.join(root, file)
                file_type = os.path.splitext(file)[1][1:]
                file_size = os.path.getsize(file_path)

                if file_type not in files_count:
                    files_count[file_type] = 0
                    files_details[file_type] = []
                files_count[file_type] += 1

                # At top level, collect all CSV and Markdown files without restrictions
                # In deeper levels, follow the max_files restriction
                if is_top_level and file_type in ["csv", "md"]:
                    files_details[file_type].append((file, file_size, file_path))
                elif len(files_details[file_type]) < max_files:
                    files_details[file_type].append((file, file_size, file_path))
            break

        # Collect "type snapshots" of subfolders
        snapshots = []
        for d in dirs:
            subfolder_path = os.path.join(root, d)
            snapshot = get_dir_snapshot(subfolder_path)
            snapshots.append(snapshot)

        # Determine if all subfolders have the same file type distribution
        first_snapshot = snapshots[0]
        all_same_structure = all(s == first_snapshot for s in snapshots)

        if all_same_structure:
            for i, d in enumerate(dirs):
                if i < partial_expand_subfolders:
                    result.append(" " * indent + f"- Folder: {d}")
                    subfolder_path = os.path.join(root, d)
                    result.append(
                        describe_data_folder(
                            folder_path=subfolder_path,
                            indent=indent + 2,
                            max_files=max_files,
                            partial_expand_subfolders=partial_expand_subfolders,
                            is_top_level=False,
                        )
                    )
                else:
                    remaining = len(dirs) - i
                    result.append(" " * indent + f"... ({remaining} more subfolders)")
                    break
        else:
            for d in dirs:
                result.append(" " * indent + f"- Folder: {d}")
                subfolder_path = os.path.join(root, d)
                result.append(
                    describe_data_folder(
                        folder_path=subfolder_path,
                        indent=indent + 2,
                        max_files=max_files,
                        partial_expand_subfolders=partial_expand_subfolders,
                        is_top_level=False,
                    )
                )

        for file in files:
            file_path = os.path.join(root, file)
            file_type = os.path.splitext(file)[1][1:]
            file_size = os.path.getsize(file_path)

            if file_type not in files_count:
                files_count[file_type] = 0
                files_details[file_type] = []
            files_count[file_type] += 1

            # At top level, collect all CSV and Markdown files without restrictions
            # In deeper levels, follow the max_files restriction
            if is_top_level and file_type in ["csv", "md"]:
                files_details[file_type].append((file, file_size, file_path))
            elif not is_top_level and len(files_details[file_type]) <= max_files:
                files_details[file_type].append((file, file_size, file_path))

        break

    # Print the folder and its contents
    for file_type, count in files_count.items():
        if count > max_files and file_type not in ["csv", "md", "txt"]:
            result.append(" " * indent + f"{count} {file_type}s:")
            for file, size, path in files_details[file_type]:
                result.append(" " * (indent + 2) + f"- {file} ({size} bytes)")
            result.append(" " * (indent + 2) + "... (file limit reached)")
        else:
            for file, size, path in files_details[file_type]:
                if file_type == "csv":
                    df = pd.read_csv(path)
                    result.append(
                        " " * indent + f"- {file} ({size} bytes, with {df.shape[0]} rows and {df.shape[1]} columns)"
                    )
                    result.append(" " * (indent + 2) + f"- Head of {file}:")
                    csv_head = read_csv_head(path, indent + 4)
                    result.append(csv_head)
                    continue
                result.append(" " * indent + f"- {file} ({size} bytes)")
                if file_type == "md":
                    result.append(" " * (indent + 2) + f"- Content of {file}:")
                    if file == "description.md":
                        result.append(" " * (indent + 4) + f"Please refer to the background of the scenario context.")
                        continue
                    with open(path, "r", encoding="utf-8") as f:
                        result.append(" " * (indent + 4) + f.read())
                if file_type == "tif":
                    result.append(" " * (indent + 2) + f"- Metadata of {file}:")
                    with Image.open(path) as img:
                        for tag, value in img.tag_v2.items():
                            tag_name = TiffTags.TAGS_V2.get(tag, f"Unknown Tag {tag}")
                            result.append(" " * (indent + 4) + f"{tag_name}: {value}")
                if file_type in ["json", "txt"]:
                    result.append(" " * (indent + 2) + f"- Content of {file}:")
                    with open(path, "r", encoding="utf-8") as f:
                        for i, line in enumerate(f):
                            if i < 2:
                                result.append(
                                    " " * (indent + 4) + line.strip()[:100] + ("..." if len(line.strip()) > 100 else "")
                                )
                            else:
                                break

    return "\n".join(result) + "\n"


class DataScienceScen(Scenario):
    """Data Science Scenario"""

    def __init__(self, competition: str) -> None:
        self.competition = competition
        self.raw_description = self._get_description()
        self.processed_data_folder_description = self._get_data_folder_description()
        self._analysis_competition_description()
        self.metric_direction = self._get_direction()
        self.eda_output = None

    def _get_description(self):
        if (fp := Path(f"{DS_RD_SETTING.local_data_path}/{self.competition}.json")).exists():
            logger.info(f"Found {self.competition}.json, loading from local file.")
            with fp.open("r") as f:
                return json.load(f)
        else:
            logger.error(
                f"Cannot find {self.competition}.json in {DS_RD_SETTING.local_data_path}, please check the file."
            )

    def _get_direction(self):
        return self.metric_direction_guess if hasattr(self, "metric_direction_guess") else True

    def _analysis_competition_description(self):
        sys_prompt = T(".prompts:competition_description_template.system").r()
        user_prompt = T(".prompts:competition_description_template.user").r(
            competition_raw_description=self.raw_description,
            competition_processed_data_folder_description=self.processed_data_folder_description,
        )

        response_analysis = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=sys_prompt,
            json_mode=True,
            json_target_type=Dict[str, str | int | bool],
        )

        response_json_analysis = json.loads(response_analysis)
        self.task_type = response_json_analysis.get("Task Type", "No type provided")
        self.data_type = response_json_analysis.get("Data Type", "No data type provided")
        self.brief_description = response_json_analysis.get("Brief Description", "No brief description provided")
        self.dataset_description = response_json_analysis.get("Dataset Description", "No dataset description provided")
        self.target_description = response_json_analysis.get("Evaluation Description", "No target description provided")
        self.submission_specifications = response_json_analysis.get(
            "Submission Specifications", "No submission requirements provided"
        )
        self.model_output_channel = response_json_analysis.get("Submission channel number to each sample", 1)
        self.metric_direction_guess = response_json_analysis.get("Metric Direction", True)

    def get_competition_full_desc(self) -> str:
        return f"""Task Type: {self.task_type}
    Data Type: {self.data_type}
    Brief Description: {self.brief_description}
    Dataset Description: {self.dataset_description}
    Target Description: {self.target_description}
    Submission Specifications: {self.submission_specifications}
    Model Output Channel: {self.model_output_channel}
    """

    @property
    def background(self) -> str:
        background_template = T(".prompts:competition_background")
        background_prompt = background_template.r(
            task_type=self.task_type,
            data_type=self.data_type,
            brief_description=self.brief_description,
            dataset_description=self.dataset_description,
            target_description=self.target_description,
        )
        return background_prompt

    @property
    def rich_style_description(self) -> str:
        return T(".prompts:rich_style_description").r(
            name="Data Science",
            competition=self.competition,
        )

    def get_scenario_all_desc(self) -> str:
        return T(".prompts:scenario_description").r(
            background=self.background,
            submission_specifications=self.submission_specifications,
            evaluation=self.target_description,
            metric_direction=self.metric_direction,
            eda_output=self.eda_output,
        )

    def get_runtime_environment(self) -> str:
        # TODO:  add it into base class.  Environment should(i.e. `DSDockerConf`) should be part of the scenario class.
        env = get_ds_env()
        implementation = FBWorkspace()
        fname = "temp.py"
        implementation.inject_files(
            **{fname: (Path(__file__).absolute().resolve().parent / "runtime_info.py").read_text()}
        )
        stdout = implementation.execute(env=env, entry=f"python {fname}")
        return stdout

    def _get_data_folder_description(self) -> str:
        return describe_data_folder(Path(DS_RD_SETTING.local_data_path) / self.competition)


class KaggleScen(DataScienceScen):
    """Kaggle Scenario
    It is based on kaggle now.
        - But it is not use the same interface with previous kaggle version.
        - Ideally, we should reuse previous kaggle scenario.
          But we found that too much scenario unrelated code in kaggle scenario and hard to reuse.
          So we start from a simple one....
    """

    def _get_description(self):
        return crawl_descriptions(self.competition, DS_RD_SETTING.local_data_path)

    def _get_direction(self):
        if DS_RD_SETTING.if_using_mle_data:
            return super()._get_direction()
        leaderboard = leaderboard_scores(self.competition)
        return "maximize" if float(leaderboard[0]) > float(leaderboard[-1]) else "minimize"

    @property
    def rich_style_description(self) -> str:
        return T(".prompts:rich_style_description").r(
            name="Kaggle",
            competition=f"[{self.competition}](https://www.kaggle.com/competitions/{self.competition})",
        )


if __name__ == "__main__":
    print(describe_data_folder(Path("/data/userdata/share/mle_kaggle") / "stanford-covid-vaccine"))



================================================
File: rdagent/scenarios/data_science/scen/prompts.yaml
================================================
scenario_description: |-
  ------Background of the scenario------
  {{background}}

  ------ Guidelines for participating in the competition ----
  Before submitting your results, we have numerous tests ready to check your code. Please ensure your submission is genuine and do not manipulate data or return values just to pass the tests, as this will not lead to successful final results.

  ------The expected output & submission format specifications------
  {{submission_specifications}}

  {% if evaluation is not none %}
  ------Evaluation------
  {{evaluation}}
  {% endif %}

  The evaluation metrics used is directed as:
  {% if metric_direction %}The metric is better when it is bigger. 
  {% else %}The metric is better when it is smaller.
  {% endif %}

  {% if eda_output is not none %}
  {{ eda_output }}
  {% endif %}

competition_description_template:
  system: |-
    You are a data science assistant that extracts structured information from unstructured text.
    The user will provide you a Kaggle competition description, and you need to extract specific details from it.
    For the dataset, the competition may not include detailed information about the dataset. The user has read the dataset and provide you the relevant information. Please include it in your response.
    Please answer in Json format with the following schema:
    {
      "Task Type": "The type of competition task, e.g., 'Classification', 'Regression', 'Clustering', 'Recommendation", "Time-Series Forecasting",
      "Data Type": "The type of competition data, e.g., 'Tabular', 'Time Series', 'Text (Natural Language Processing)', 'Image (Computer Vision)', 'Audio', 'Video'", 
      "Brief Description": "A brief description of the competition",
      "Dataset Description": "The dataset utilized in the competition is described based on two sources: the Competition Description, which provides contextual details about the original files, and the Processed Data folder description, which outlines the structure of the dataset after processing. While there may be differences—for instance, original files mentioned in the Competition Description (e.g., .zip files) may have been extracted or restructured—your task is to interpret the new file structure accurately (do not contain any file or folder that is not in Processed Data folder description) and reconcile it with the contextual information from the Competition Description to provide a clear and updated explanation.",
      "Evaluation Description": "A description of the evaluation used in the competition.",
      "Submission Specifications": "The submission specification & sample submission file descriptions for the model to output."
      "Submission channel number to each sample": "The number of channels in the output for each sample, e.g., 1 for regression, N for N class classification with probabilities, etc. A Integer. If not specified, it is 1."
      "Metric direction": True or False as True means bigger metric number is better, False means smaller is better.
    }
  user: |-
    Competition Description: 
    {{ competition_raw_description }}

    Processed Data folder description:
    {{ competition_processed_data_folder_description }}
    
    [Note] There may be some discrepancies between the competition description and the processed data folder description. Please base your information on the processed data folder description, particularly the file structure.


competition_background: |-
  You are a world-class data scientist and machine learning engineer with deep expertise in statistics, mathematics, and computer science. 
  Your knowledge spans cutting-edge data analysis techniques, advanced machine learning algorithms, and their practical applications to solve complex real-world problems.
  You are dedicated to producing accurate, efficient, and innovative solutions.

  The task type for this competition is {{ task_type }}.
  The data type used in this competition is {{ data_type }}.
  Briefly, the competition involves: {{ brief_description }}.
  The dataset used in this competition is: {{ dataset_description }}.
  Your goal in this competition is to: {{target_description }}.

rich_style_description: |-
  ### {{ name }} Agent: Automated Feature Engineering & Model Tuning Evolution

  #### [Overview](#_summary)

  In this scenario, our automated system proposes hypothesis, choose action, implements code, conducts validation, and utilizes feedback in a continuous, iterative process.

  #### {{ name }} Competition info

  Current Competition: {{ competition }}

  #### [Automated R&D](#_rdloops)

  - **[R (Research)](#_research)**
  - Iteration of ideas and hypotheses.
  - Continuous learning and knowledge construction.

  - **[D (Development)](#_development)**
  - Evolving code generation, model refinement, and features generation.
  - Automated implementation and testing of models/features.

  #### [Objective](#_summary)

  To automatically optimize performance metrics within the validation set, ultimately discovering the most efficient features and models through autonomous research and development.


================================================
File: rdagent/scenarios/data_science/scen/runtime_info.py
================================================
import platform
import sys
from importlib.metadata import distributions


def print_runtime_info():
    print(f"Python {sys.version} on {platform.system()} {platform.release()}")


def get_installed_packages():
    return {dist.metadata["Name"].lower(): dist.version for dist in distributions()}


def print_filtered_packages(installed_packages, filtered_packages):
    for package_name in filtered_packages:
        version = installed_packages.get(package_name.lower())
        if version:
            print(f"{package_name}=={version}")


if __name__ == "__main__":
    print_runtime_info()
    filtered_packages = [
        "transformers",
        "accelerate",
        "torch",
        "tensorflow",
        "pandas",
        "numpy",
        "scikit-learn",
        "scipy",
        "lightgbm",
        "vtk",
        "opencv-python",
        "keras",
        "matplotlib",
        "pydicom",
    ]
    installed_packages = get_installed_packages()
    print_filtered_packages(installed_packages, filtered_packages)



================================================
File: rdagent/scenarios/data_science/sing_docker/Dockerfile
================================================
# Use the official PyTorch image as the base image  
FROM pytorch/pytorch:latest
# FROM pytorch/pytorch:2.4.1-cuda12.1-cudnn9-runtime
# torch.__version__ == '2.4.1+cu121' in "gcr.io/kaggle-gpu-images/python"
  
# Install additional tools  
RUN apt-get update && apt-get install -y \  
    curl \  
    vim \  
    git \  
    build-essential \  
    git-lfs \  
    unzip && \  
    rm -rf /var/lib/apt/lists/*  
  
# Default command to keep the container running  
ENTRYPOINT ["/workspace/run/entrypoint.sh"]

RUN conda init bash

# MLE-Bench
RUN conda create -n mlebench python==3.11 pip -y
RUN cd /workspace && git clone https://github.com/openai/mle-bench.git
RUN cd /workspace/mle-bench && git lfs fetch --all
RUN cd /workspace/mle-bench && git lfs pull
RUN cd /workspace/mle-bench && conda run -n mlebench pip install -e .

# Kaggle Environment
COPY ./kaggle_environment.yaml /workspace
RUN cd /workspace && conda env create -f /workspace/kaggle_environment.yaml

# RD-Agent
RUN cd /workspace && git clone https://github.com/microsoft/RD-Agent
RUN cd RD-Agent && git fetch && make dev


# litellm
RUN cd /workspace && mkdir -p litellm-srv
RUN cd /workspace/litellm-srv && curl https://raw.githubusercontent.com/you-n-g/deploy/refs/heads/master/configs/python/litellm.trapi.yaml -o litellm.trapi.yaml
RUN pip install 'litellm[proxy]'
RUN pip install git+https://github.com/you-n-g/litellm@add_mi_cred_pr

run cd /workspace && mkdir -p run
COPY ./entrypoint.sh /workspace/run


WORKDIR /workspace/RD-Agent/



================================================
File: rdagent/scenarios/data_science/sing_docker/entrypoint.sh
================================================
#!/bin/sh
set -x

DIR="$( cd "$(dirname "$(readlink -f "$0")")" || exit ; pwd -P )"

sudo mkdir -p /mle/ /kaggle/

CURRENT_USER=$(id -un)
sudo chown -R $CURRENT_USER:$CURRENT_USER /workspace/ /mle/ /kaggle/

ls -lat /

cd $DIR/../RD-Agent
mkdir -p log/
git fetch
git checkout ${RD_COMMIT:-ee8d97c52062607cac778b8aeb10769b075a8d11}
make dev
pip install 'litellm[proxy]'
pip install git+https://github.com/you-n-g/litellm@add_mi_cred_pr


cd $DIR/../litellm-srv/
export AZURE_CLIENT_ID
export AZURE_SCOPE=api://trapi/.default
export AZURE_CREDENTIAL=ManagedIdentityCredential
sed -i '/proxy_handler_instance/d' litellm.trapi.yaml  # remove useless handler in production
nohup litellm --config litellm.trapi.yaml &

sleep 10  # wait for litellm to start


cd $DIR/../RD-Agent
script -c "timeout ${RD_TIMEOUT:-24h} python rdagent/app/data_science/loop.py --competition $DS_COMPETITION" log/stdout.${DS_COMPETITION}.log

unset LOG_TRACE_PATH  # avoid make the original log dirty.
python rdagent/log/mle_summary.py grade_summary --log_folder=./log/

tar cf log.tar log

# NOTE: when we have $AMLT_OUTPUT_DIR, maybe we don't have to copy file actively to azure blob now.
# RD_OUTPUT_DIR=${RD_OUTPUT_DIR:-/data/rdagent}/
# mkdir -p $RD_OUTPUT_DIR
# cp -r log.tar $RD_OUTPUT_DIR/${RD_RES_NAME:-log.tar}

cp -r log.tar $AMLT_OUTPUT_DIR/${RD_RES_NAME:-log.tar}

set > $AMLT_OUTPUT_DIR/env



================================================
File: rdagent/scenarios/data_science/sing_docker/kaggle_environment.yaml
================================================
name: kaggle
channels:
  - defaults
  - https://repo.anaconda.com/pkgs/main
  - https://repo.anaconda.com/pkgs/r
dependencies:
  - _libgcc_mutex=0.1=main
  - _openmp_mutex=5.1=1_gnu
  - bzip2=1.0.8=h5eee18b_6
  - ca-certificates=2025.2.25=h06a4308_0
  - ld_impl_linux-64=2.40=h12ee557_0
  - libffi=3.4.4=h6a678d5_1
  - libgcc-ng=11.2.0=h1234567_1
  - libgomp=11.2.0=h1234567_1
  - libstdcxx-ng=11.2.0=h1234567_1
  - libuuid=1.41.5=h5eee18b_0
  - ncurses=6.4=h6a678d5_0
  - openssl=3.0.15=h5eee18b_0
  - pip=25.0=py311h06a4308_0
  - python=3.11.11=he870216_0
  - readline=8.2=h5eee18b_0
  - setuptools=75.8.0=py311h06a4308_0
  - sqlite=3.45.3=h5eee18b_0
  - tk=8.6.14=h39e8969_0
  - wheel=0.45.1=py311h06a4308_0
  - xz=5.6.4=h5eee18b_1
  - zlib=1.2.13=h5eee18b_1
  - pip:
      - absl-py==2.1.0
      - accelerate==0.33.0
      - aideml==0.1.4
      - aiohappyeyeballs==2.4.6
      - aiohttp==3.11.13
      - aiosignal==1.3.2
      - albucore==0.0.23
      - albumentations==1.4.14
      - alembic==1.14.1
      - annotated-types==0.7.0
      - anthropic==0.34.1
      - antlr4-python3-runtime==4.9.3
      - anyio==4.8.0
      - arrow==1.3.0
      - asttokens==3.0.0
      - astunparse==1.6.3
      - attrs==25.1.0
      - audioread==3.0.1
      - azure-ai-formrecognizer==3.3.3
      - azure-common==1.1.28
      - azure-core==1.32.0
      - azure-identity==1.20.0
      - azure-storage-blob==12.24.1
      - backoff==2.2.1
      - bayesian-optimization==1.5.1
      - bayespy==0.5.1
      - biopython==1.84
      - black==24.3.0
      - bleach==6.2.0
      - blis==0.7.11
      - brotli==1.1.0
      - bson==0.5.10
      - cachetools==5.5.2
      - catalogue==2.0.10
      - catboost==1.2.5
      - certifi==2025.1.31
      - cffi==1.17.1
      - charset-normalizer==3.4.1
      - click==8.1.8
      - cloudpathlib==0.20.0
      - cloudpickle==3.1.1
      - colorama==0.4.6
      - colorlog==6.9.0
      - comm==0.2.2
      - confection==0.1.5
      - contourpy==1.3.1
      - coolname==2.2.0
      - cryptography==44.0.2
      - cycler==0.12.1
      - cymem==2.0.11
      - cython==3.0.11
      - dacite==1.8.1
      - dataclasses-json==0.6.7
      - datasets==2.1.0
      - debugpy==1.8.12
      - decorator==5.2.1
      - defusedxml==0.7.1
      - dill==0.3.9
      - distro==1.9.0
      - efficientnet-pytorch==0.7.1
      - eval-type-backport==0.2.2
      - evaluate==0.4.2
      - executing==2.2.0
      - fastai==2.7.17
      - fastcore==1.7.29
      - fastdownload==0.0.7
      - fastdtw==0.3.4
      - fastjsonschema==2.21.1
      - fastprogress==1.0.3
      - faust-cchardet==2.1.19
      - filelock==3.17.0
      - flatbuffers==25.2.10
      - fonttools==4.56.0
      - frozenlist==1.5.0
      - fsspec==2025.2.0
      - funcy==2.0
      - future==1.0.0
      - gast==0.6.0
      - gdcm==1.1
      - gensim==4.3.3
      - genson==1.3.0
      - geographiclib==2.0
      - geopy==2.4.1
      - graphviz==0.20.3
      - greenlet==3.1.1
      - grpcio==1.71.0rc2
      - gym==0.26.2
      - gym-notices==0.0.8
      - h11==0.14.0
      - h5py==3.11.0
      - hmmlearn==0.3.2
      - httpcore==1.0.7
      - httplib2==0.22.0
      - httpx==0.27.2
      - huggingface-hub==0.29.1
      - humanize==4.8.0
      - hyperopt==0.2.7
      - idna==3.10
      - igraph==0.11.6
      - imagecodecs==2024.6.1
      - imageio==2.37.0
      - imbalanced-learn==0.12.3
      - imgaug==0.4.0
      - implicit==0.7.2
      - inflate64==1.0.1
      - iniconfig==2.0.0
      - ipykernel==6.29.5
      - ipython==8.27.0
      - isodate==0.7.2
      - jedi==0.19.2
      - jinja2==3.1.5
      - jiter==0.8.2
      - joblib==1.4.2
      - jsonlines==4.0.0
      - jsonpatch==1.33
      - jsonpointer==3.0.0
      - jsonschema==4.19.2
      - jsonschema-specifications==2024.10.1
      - jupyter-client==8.6.3
      - jupyter-core==5.7.2
      - kaggle==1.6.17
      - keras==3.5.0
      - kiwisolver==1.4.8
      - kornia==0.6.10
      - kornia-rs==0.1.8
      - langchain==0.2.15
      - langchain-anthropic==0.1.23
      - langchain-core==0.2.43
      - langchain-text-splitters==0.2.4
      - langcodes==3.5.0
      - langsmith==0.1.147
      - language-data==1.3.0
      - lazy-loader==0.4
      - levenshtein==0.25.1
      - libclang==18.1.1
      - librosa==0.10.2.post1
      - lightgbm==4.5.0
      - lightning-utilities==0.12.0
      - littleutils==0.2.4
      - llvmlite==0.43.0
      - loguru==0.7.2
      - lxml==5.3.1
      - mako==1.3.9
      - marisa-trie==1.2.1
      - markdown==3.7
      - markdown-it-py==3.0.0
      - markovify==0.9.4
      - markupsafe==3.0.2
      - marshmallow==3.26.1
      - matplotlib==3.9.2
      - matplotlib-inline==0.1.7
      - mdurl==0.1.2
      - ml-dtypes==0.4.1
      - mpmath==1.3.0
      - msal==1.31.1
      - msal-extensions==1.2.0
      - msgpack==1.1.0
      - msgpack-numpy==0.4.8
      - msrest==0.7.1
      - multidict==6.1.0
      - multiprocess==0.70.17
      - multivolumefile==0.2.3
      - munch==4.0.0
      - murmurhash==1.0.12
      - mypy-extensions==1.0.0
      - namex==0.0.8
      - nbformat==5.10.4
      - nest-asyncio==1.6.0
      - networkx==3.3
      - nltk==3.9.1
      - numba==0.60.0
      - numpy==1.26.2
      - nvidia-cublas-cu12==12.1.3.1
      - nvidia-cuda-cupti-cu12==12.1.105
      - nvidia-cuda-nvcc-cu12==12.3.107
      - nvidia-cuda-nvrtc-cu12==12.1.105
      - nvidia-cuda-runtime-cu12==12.1.105
      - nvidia-cudnn-cu12==8.9.2.26
      - nvidia-cufft-cu12==11.0.2.54
      - nvidia-curand-cu12==10.3.2.106
      - nvidia-cusolver-cu12==11.4.5.107
      - nvidia-cusparse-cu12==12.1.0.106
      - nvidia-nccl-cu12==2.19.3
      - nvidia-nvjitlink-cu12==12.3.101
      - nvidia-nvtx-cu12==12.1.105
      - oauthlib==3.2.2
      - ogb==1.3.6
      - omegaconf==2.3.0
      - openai==1.48.0
      - opencv-python==4.10.0.84
      - opencv-python-headless==4.11.0.86
      - opt-einsum==3.4.0
      - optree==0.14.1
      - optuna==4.0.0
      - orjson==3.10.15
      - outdated==0.2.2
      - packaging==24.2
      - pandas==2.1.4
      - parso==0.8.4
      - pathspec==0.12.1
      - pdf2image==1.17.0
      - peft==0.12.0
      - pexpect==4.9.0
      - pillow==10.4.0
      - platformdirs==4.3.6
      - plotly==5.24.0
      - pluggy==1.5.0
      - pooch==1.8.2
      - portalocker==2.10.1
      - preshed==3.0.9
      - pretrainedmodels==0.7.4
      - prompt-toolkit==3.0.50
      - propcache==0.3.0
      - proto-plus==1.26.0
      - psutil==7.0.0
      - ptyprocess==0.7.0
      - pure-eval==0.2.3
      - py4j==0.10.9.9
      - py7zr==0.22.0
      - pyaml==25.1.0
      - pyarrow==17.0.0
      - pyasn1==0.6.1
      - pyasn1-modules==0.4.1
      - pybcj==1.0.3
      - pycparser==2.22
      - pycryptodomex==3.21.0
      - pydantic==2.9.2
      - pydantic-core==2.23.4
      - pydantic-settings==2.6.1
      - pydicom==2.4.4
      - pygments==2.19.1
      - pyjwt==2.10.1
      - pylibjpeg==2.0.1
      - pyocr==0.8.5
      - pyparsing==3.1.4
      - pypdf==4.3.1
      - pyppmd==1.1.1
      - pytest==7.4.3
      - python-dateutil==2.9.0.post0
      - python-dotenv==1.0.1
      - python-slugify==8.0.4
      - pytorch-lightning==2.4.0
      - pytz==2024.1
      - pyyaml==6.0.2
      - pyzmq==26.2.1
      - pyzstd==0.16.2
      - ranger21==0.1.0
      - rapidfuzz==3.12.2
      - referencing==0.36.2
      - regex==2024.11.6
      - requests==2.31.0
      - requests-oauthlib==2.0.0
      - requests-toolbelt==1.0.0
      - resampy==0.4.3
      - responses==0.18.0
      - rich==13.7.0
      - rouge-score==0.1.2
      - rpds-py==0.23.1
      - rsa==4.9
      - sacrebleu==2.4.3
      - safetensors==0.5.3
      - scikit-image==0.24.0
      - scikit-learn==1.2.2
      - scikit-optimize==0.10.2
      - scikit-surprise==1.1.4
      - scipy==1.11.4
      - seaborn==0.13.2
      - segmentation-models-pytorch==0.3.4
      - sentence-transformers==3.0.1
      - sentencepiece==0.2.0
      - shapely==2.0.7
      - shellingham==1.5.4
      - shutup==0.2.0
      - simsimd==6.2.1
      - six==1.17.0
      - sklearn-pandas==2.2.0
      - smart-open==7.1.0
      - sniffio==1.3.1
      - soundfile==0.13.1
      - soxr==0.5.0.post1
      - spacy==3.7.6
      - spacy-legacy==3.0.12
      - spacy-loggers==1.0.5
      - sqlalchemy==2.0.38
      - srsly==2.5.1
      - stack-data==0.6.3
      - stringzilla==3.12.2
      - sympy==1.13.2
      - tabulate==0.9.0
      - tenacity==8.5.0
      - tensorboard==2.17.1
      - tensorboard-data-server==0.7.2
      - tensorflow==2.17.0
      - tensorflow-hub==0.16.1
      - tensorflow-io-gcs-filesystem==0.37.1
      - tensorpack==0.11
      - termcolor==2.5.0
      - text-unidecode==1.3
      - textblob==0.18.0.post0
      - texttable==1.7.0
      - tf-keras==2.17.0
      - thinc==8.2.5
      - threadpoolctl==3.5.0
      - tifffile==2025.2.18
      - tiktoken==0.7.0
      - timm==0.9.7
      - tokenizers==0.19.1
      - torch==2.2.0
      - torch-geometric==2.3.1
      - torchaudio==2.2.0
      - torchdata==0.7.1
      - torchinfo==1.8.0
      - torchmetrics==1.3.1
      - torchtext==0.17.0
      - torchvision==0.17.0
      - tornado==6.4.2
      - tqdm==4.66.2
      - traitlets==5.14.3
      - transformers==4.44.2
      - triton==2.2.0
      - typer==0.15.2
      - types-python-dateutil==2.9.0.20241206
      - typing-extensions==4.12.2
      - typing-inspect==0.9.0
      - tzdata==2025.1
      - unidecode==1.3.8
      - uritemplate==4.1.1
      - urllib3==2.3.0
      - wasabi==1.1.3
      - wcwidth==0.2.13
      - weasel==0.4.1
      - webencodings==0.5.1
      - werkzeug==3.1.3
      - wrapt==1.17.2
      - xgboost==2.1.1
      - xlrd==2.0.1
      - xxhash==3.5.0
      - yarl==1.18.3
prefix: /opt/conda/envs/kaggle



================================================
File: rdagent/scenarios/general_model/prompts.yaml
================================================
general_model_background: |-
  The general model is a flexible and comprehensive framework designed to integrate factor-based, model-based, and graph-based approaches in quantitative investment. It allows users to define custom models that leverage various financial factors to predict the returns and risks of portfolios or single assets. These models are central to many advanced quantitative investment strategies and can be adapted to a wide range of use cases, from factor-based alpha generation to complex deep learning predictions.

  Each general model incorporates the following components:
  1. Name: The name of the model.
  2. Description: A detailed description of the model.
  3. Factors: The financial factors used as inputs, including their definitions and formulations.
  4. Architecture: The structure of the machine learning, deep learning, or graph-based model.
  5. Hyperparameters: The hyperparameters used in the model, such as learning rate, number of epochs, etc.
  6. ModelType: The type of the model, "Tabular" for tabular data, "TimeSeries" for time series data, or "Graph" for graph data.
  The general model should provide clear and detailed documentation of its factors, architecture, and hyperparameters. Each model should have a fixed architecture and hyperparameters to ensure reproducibility and consistency.

general_model_interface: |-
  Your python code should follow the interface to better interact with the user's system. It should be a pytorch model. 
  Your code should contain several parts:
  1. The import part: import the necessary libraries.
  2. A class which is a sub-class of pytorch.nn.Module. This class should have an init function and a forward function which inputs a tensor and outputs a tensor.
  3. Set a variable called "model_cls" to the class you defined.

  The user will save your code into a python file called "model.py". Then the user imports model_cls in file "model.py" after setting the cwd into the directory:
  ```python
  from model import model_cls

  So your python code should follow the pattern:

  class XXXModel(torch.nn.Module):
    ...
  model_cls = XXXModel

  The model has three types, "Tabular" for tabular data, "TimeSeries" for time series data, and "Graph" for graph data.

  The input shape to a tabular model is (batch_size, num_features).
  The input shape to a time series model is (batch_size, num_features, num_timesteps).
  The input to a graph model are two tensors. 
  node_feature: a tensor of shape (batch_size, num_features)
  edge_index: a tensor of shape (2, num_edges) 
  
  The batch_size is a dynamic value which is determined by the input of the forward function.
  
  The output shape of the model should be (batch_size, 1).

  The "num_features", "num_timesteps" are static and will be provided to the model through the init function.

  User will initialize the tabular model with the following code:

  model = model_cls(num_features=num_features)

  User will initialize the time series model with the following code:

  model = model_cls(num_features=num_features, num_timesteps=num_timesteps)

  User will initialize the graph model with the following code:

  model = model_cls(num_features=num_features)


  No other parameters will be passed to the model, so give other parameters a default value or make them static.

  When dealing with a time series model, remember to permute the input tensor since the input tensor is in the shape of (batch_size, num_features, num_timesteps) and a normal time series model is expecting the input tensor in the shape of (batch_size, num_timesteps, num_features).

  Don't write any try-except block in your python code. The user will catch the exception message and provide the feedback to you. Also, don't write a main function in your python code. The user will call the forward method in the model_cls to get the output tensor.

  Please note that your model should only use current features as input. The user will provide the input tensor to the model's forward function.

general_model_output_format: |-
  Your output should be a tensor with shape (batch_size, 1).
  The output tensor should be saved in a file named "output.pth" in the same directory as your python file.
  The user will evaluate the shape of the output tensor, so the tensor read from "output.pth" should be 8 numbers.

general_model_simulator: |-
  The models are not loaded and backtested. That said, pay attention to its architecture.

general_model_rich_style_description: |-
  ### [Model Research & Development Co-Pilot](#_scenario)

  #### [Overview](#_summary)

  This demo automates the extraction and development of PyTorch models from academic papers. It supports various model types through two main components: Reader and Coder.
  
  #### [Workflow Components](#_rdloops)
  
  1. **[Reader](#_research)**
      - Extracts model information from papers, including architectures and parameters.
      - Converts content into a structured format using Large Language Models.
  
  2. **[Evolving Coder](#_development)**
      - Translates structured information into executable PyTorch code.
      - Ensures correct tensor shapes with an evolving coding mechanism.
      - Refines the code to match source specifications.



================================================
File: rdagent/scenarios/general_model/scenario.py
================================================
from copy import deepcopy
from pathlib import Path

from rdagent.core.experiment import Task
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class GeneralModelScenario(Scenario):
    def __init__(self) -> None:
        super().__init__()
        self._background = deepcopy(prompt_dict["general_model_background"])
        self._output_format = deepcopy(prompt_dict["general_model_output_format"])
        self._interface = deepcopy(prompt_dict["general_model_interface"])
        self._simulator = deepcopy(prompt_dict["general_model_simulator"])
        self._rich_style_description = deepcopy(prompt_dict["general_model_rich_style_description"])

    @property
    def background(self) -> str:
        return self._background

    @property
    def source_data(self) -> str:
        raise NotImplementedError("source_data of GeneralModelScenario is not implemented")

    @property
    def output_format(self) -> str:
        return self._output_format

    @property
    def interface(self) -> str:
        return self._interface

    @property
    def simulator(self) -> str:
        return self._simulator

    @property
    def rich_style_description(self) -> str:
        return self._rich_style_description

    def get_scenario_all_desc(
        self, task: Task | None = None, filtered_tag: str | None = None, simple_background: bool | None = None
    ) -> str:
        return f"""Background of the scenario:
{self.background}
The interface you should follow to write the runnable code:
{self.interface}
The output of your code should be in the format:
{self.output_format}
The simulator user can use to test your model:
{self.simulator}
"""



================================================
File: rdagent/scenarios/kaggle/README.md
================================================
# Kaggle Crawler

## Install chrome & chromedriver for Linux

In one folder
```shell
# install chrome
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install ./google-chrome-stable_current_amd64.deb
google-chrome --version

# install chromedriver
wget https://storage.googleapis.com/chrome-for-testing-public/<chrome-version>/linux64/chromedriver-linux64.zip
unzip chromedriver-linux64.zip
cd chromedriver-linux64
sudo mv chromedriver /usr/local/bin
sudo chmod +x /usr/local/bin/chromedriver

chromedriver --version
```

## config

1. authentication: `~/.kaggle/kaggle.json`
2. Accept Rules in competition website. (Join Competition)

## notebook crawler

1. `download_notebooks()`
2. `convert_notebooks_to_text()`


================================================
File: rdagent/scenarios/kaggle/kaggle_crawler.py
================================================
# %%
import bisect
import json
import shutil
import subprocess
import time
import zipfile
from itertools import chain
from pathlib import Path

import nbformat
from jinja2 import Environment, StrictUndefined
from rich import print
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.core.conf import ExtendedBaseSettings
from rdagent.core.exception import KaggleError
from rdagent.core.utils import cache_with_pickle
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.data_science.debug.data import create_debug_data
from rdagent.utils.agent.tpl import T
from rdagent.utils.env import MLEBDockerEnv

# %%
options = webdriver.ChromeOptions()
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")
options.add_argument("--headless")

service = Service("/usr/local/bin/chromedriver")


def crawl_descriptions(
    competition: str, local_data_path: str, wait: float = 3.0, force: bool = False
) -> dict[str, str] | str:
    if (fp := Path(f"{local_data_path}/{competition}/description.md")).exists() and not force:
        logger.info(f"Found {competition}/description.md, loading from it.")
        return fp.read_text()

    if (fp := Path(f"{local_data_path}/{competition}.json")).exists() and not force:
        logger.info(f"Found {competition}.json, loading from local file.")
        with fp.open("r") as f:
            return json.load(f)

    driver = webdriver.Chrome(options=options, service=service)
    overview_url = f"https://www.kaggle.com/competitions/{competition}/overview"
    driver.get(overview_url)
    time.sleep(wait)
    site_body = driver.find_element(By.ID, "site-content")
    descriptions = {}

    # Get the subtitles
    elements = site_body.find_elements(By.CSS_SELECTOR, f"a[href^='/competitions/{competition}/overview/']")
    subtitles = []
    for e in elements:
        inner_text = ""
        for child in e.find_elements(By.XPATH, ".//*"):
            inner_text += child.get_attribute("innerHTML").strip()
        subtitles.append(inner_text)

    def kaggle_description_css_selectors() -> tuple[str, str]:
        # Get the class name of the main contents
        ab_elm = site_body.find_element(By.ID, "abstract")
        others_elm = ab_elm.find_element(By.XPATH, "../*[2]")
        first_elm = others_elm.find_element(By.XPATH, "./*[1]")
        first_content_elm = first_elm.find_element(By.XPATH, "./*[1]/*[2]")
        selector_elm = first_content_elm.find_element(By.XPATH, "./*[1]/*[1]")
        main_class = selector_elm.get_attribute("class").split()[-1]

        # Get the class name of the citation
        citation_elm = site_body.find_element(By.ID, "citation")
        citation_content_elm = citation_elm.find_element(By.XPATH, "./*[1]/*[2]/*[1]/*[1]")
        citation_class = citation_content_elm.get_attribute("class").split()[-1]

        return main_class, citation_class

    main_class, citation_class = kaggle_description_css_selectors()

    # Get main contents
    contents = []
    elements = site_body.find_elements(By.CSS_SELECTOR, f".{main_class}")
    for e in elements:
        content = e.get_attribute("innerHTML")
        contents.append(content)

    assert len(subtitles) == len(contents) + 1 and subtitles[-1] == "Citation"
    for i in range(len(subtitles) - 1):
        descriptions[subtitles[i]] = contents[i]

    # Get the citation
    element = site_body.find_element(By.CSS_SELECTOR, f".{citation_class}")
    citation = element.get_attribute("innerHTML")
    descriptions[subtitles[-1]] = citation

    data_url = f"https://www.kaggle.com/competitions/{competition}/data"
    driver.get(data_url)
    time.sleep(wait)
    data_element = driver.find_element(By.CSS_SELECTOR, f".{main_class}")
    descriptions["Data Description"] = data_element.get_attribute("innerHTML")

    driver.quit()
    with open(f"{local_data_path}/{competition}.json", "w") as f:
        json.dump(descriptions, f)
    return descriptions


def download_data(competition: str, settings: ExtendedBaseSettings = KAGGLE_IMPLEMENT_SETTING) -> None:
    local_path = settings.local_data_path
    if settings.if_using_mle_data:
        zipfile_path = f"{local_path}/zip_files"
        zip_competition_path = Path(zipfile_path) / competition

        if not zip_competition_path.exists():
            mleb_env = MLEBDockerEnv()
            mleb_env.prepare()
            (Path(zipfile_path)).mkdir(parents=True, exist_ok=True)
            mleb_env.run(
                f"mlebench prepare -c {competition} --data-dir ./zip_files",
                local_path=local_path,
                running_extra_volume={str(Path("~/.kaggle").expanduser().absolute()): "/root/.kaggle"},
            )

        if not (Path(local_path) / competition).exists() or list((Path(local_path) / competition).iterdir()) == []:
            (Path(local_path) / competition).mkdir(parents=True, exist_ok=True)

            mleb_env = MLEBDockerEnv()
            mleb_env.prepare()
            mleb_env.run(f"cp -r ./zip_files/{competition}/prepared/public/* ./{competition}", local_path=local_path)

            for zip_path in (Path(local_path) / competition).rglob("*.zip"):
                with zipfile.ZipFile(zip_path, "r") as zip_ref:
                    if len(zip_ref.namelist()) == 1:
                        mleb_env.run(
                            f"unzip -o ./{zip_path.relative_to(local_path)} -d {zip_path.parent.relative_to(local_path)}",
                            local_path=local_path,
                        )
                    else:
                        mleb_env.run(
                            f"mkdir -p ./{zip_path.parent.relative_to(local_path)}/{zip_path.stem}; unzip -o ./{zip_path.relative_to(local_path)} -d ./{zip_path.parent.relative_to(local_path)}/{zip_path.stem}",
                            local_path=local_path,
                        )
            # NOTE:
            # Patching:  due to mle has special renaming mechanism for different competition;
            # We have to switch the schema back to a uniform one;
            if competition in {"new-york-city-taxi-fare-prediction"}:
                cpath = Path(local_path) / f"{competition}"
                labels_path = cpath / "labels.csv"
                train_path = cpath / "train.csv"
                if labels_path.exists():
                    shutil.copy(labels_path, train_path)
                else:
                    logger.error(f"labels.csv not found in {cpath}")
                    raise FileNotFoundError(f"{labels_path} does not exist")
    else:
        zipfile_path = f"{local_path}/zip_files"
        if not Path(f"{zipfile_path}/{competition}.zip").exists():
            try:
                subprocess.run(
                    ["kaggle", "competitions", "download", "-c", competition, "-p", zipfile_path],
                    check=True,
                    stderr=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                )
            except subprocess.CalledProcessError as e:
                logger.error(f"Download failed: {e}, stderr: {e.stderr}, stdout: {e.stdout}")
                raise KaggleError(f"Download failed: {e}, stderr: {e.stderr}, stdout: {e.stdout}")

            # unzip data
            unzip_path = f"{local_path}/{competition}"
            if not Path(unzip_path).exists():
                unzip_data(unzip_file_path=f"{zipfile_path}/{competition}.zip", unzip_target_path=unzip_path)
                for sub_zip_file in Path(unzip_path).rglob("*.zip"):
                    unzip_data(sub_zip_file, unzip_target_path=unzip_path)

    # sample data
    if not Path(f"{local_path}/sample/{competition}").exists():
        create_debug_data(competition, dataset_path=local_path)


def unzip_data(unzip_file_path: str, unzip_target_path: str) -> None:
    with zipfile.ZipFile(unzip_file_path, "r") as zip_ref:
        zip_ref.extractall(unzip_target_path)


@cache_with_pickle(hash_func=lambda x: x, force=True)
def leaderboard_scores(competition: str) -> list[float]:
    from kaggle.api.kaggle_api_extended import KaggleApi

    api = KaggleApi()
    api.authenticate()
    ll = api.competition_leaderboard_view(competition)
    return [float(x.score) for x in ll]


def score_rank(competition: str, score: float) -> tuple[int, float]:
    """
    Return
    ------
    rank: int
    rank_percent: float
    """
    scores = leaderboard_scores(competition)
    if scores[0] < scores[-1]:  # Ascending order
        rank = bisect.bisect_right(scores, score)
    else:  # Descending order
        scores = scores[::-1]  # Reverse the list to use bisect
        rank = len(scores) - bisect.bisect_right(scores, score)

    rank = rank + 1
    rank_percent = rank / len(scores) * 100

    return rank, rank_percent


def download_notebooks(
    competition: str, local_path: str = f"{KAGGLE_IMPLEMENT_SETTING.local_data_path}/notebooks", num: int = 15
) -> None:
    data_path = Path(f"{local_path}/{competition}")
    from kaggle.api.kaggle_api_extended import KaggleApi

    api = KaggleApi()
    api.authenticate()

    # judge the sort_by
    ll = api.competition_leaderboard_view(competition)
    score_diff = float(ll[0].score) - float(ll[-1].score)
    if score_diff > 0:
        sort_by = "scoreDescending"
    else:
        sort_by = "scoreAscending"

    # download notebooks
    nl = api.kernels_list(competition=competition, sort_by=sort_by, page=1, page_size=num)
    for nb in nl:
        author = nb.ref.split("/")[0]
        api.kernels_pull(nb.ref, path=data_path / author)
    print(f"Downloaded {len(nl)} notebooks for {competition}. ([red]{sort_by}[/red])")


def notebook_to_knowledge(notebook_text: str) -> str:
    sys_prompt = T(".prompts:gen_knowledge_from_code_mini_case.system").r()
    user_prompt = T(".prompts:gen_knowledge_from_code_mini_case.user").r(notebook=notebook_text)

    response = APIBackend().build_messages_and_create_chat_completion(
        user_prompt=user_prompt,
        system_prompt=sys_prompt,
        json_mode=False,
    )
    return response


def convert_notebooks_to_text(
    competition: str, local_path: str = f"{KAGGLE_IMPLEMENT_SETTING.local_data_path}/notebooks"
) -> None:
    data_path = Path(f"{local_path}/{competition}")
    converted_num = 0

    # convert ipynb and irnb files
    for nb_path in chain(data_path.glob("**/*.ipynb"), data_path.glob("**/*.irnb")):
        with nb_path.open("r", encoding="utf-8") as f:
            nb = nbformat.read(f, as_version=4)
        text = []
        for cell in nb.cells:
            if cell.cell_type == "markdown":
                text.append(f"```markdown\n{cell.source}```")
            elif cell.cell_type == "code":
                text.append(f"```code\n{cell.source}```")
        text = "\n\n".join(text)

        text = notebook_to_knowledge(text)

        text_path = nb_path.with_suffix(".txt")
        text_path.write_text(text, encoding="utf-8")
        converted_num += 1

    # convert py files
    for py_path in data_path.glob("**/*.py"):
        with py_path.open("r", encoding="utf-8") as f:
            text = f"```code\n{f.read()}```"

        text = notebook_to_knowledge(text)

        text_path = py_path.with_suffix(".txt")
        text_path.write_text(text, encoding="utf-8")
        converted_num += 1

    print(f"Converted {converted_num} notebooks to text files.")


def collect_knowledge_texts(local_path: str = KAGGLE_IMPLEMENT_SETTING.local_data_path) -> dict[str, list[str]]:
    """
    {
        "competition1": [
            "knowledge_text1",
            "knowledge_text2",
            ...
        ],
        “competition2”: [
            "knowledge_text1",
            "knowledge_text2",
            ...
        ],
        ...
    }
    """
    notebooks_dir = Path(local_path) / "notebooks"

    competition_knowledge_texts_dict = {}
    for competition_dir in notebooks_dir.iterdir():
        knowledge_texts = []
        for text_path in competition_dir.glob("**/*.txt"):
            text = text_path.read_text(encoding="utf-8")
            knowledge_texts.append(text)

        competition_knowledge_texts_dict[competition_dir.name] = knowledge_texts

    return competition_knowledge_texts_dict


# %%
if __name__ == "__main__":
    mini_case_cs = [
        "feedback-prize-english-language-learning",
        "playground-series-s3e11",
        "playground-series-s3e14",
        "spaceship-titanic",
        "playground-series-s3e18",
        "playground-series-s3e16",
        "playground-series-s3e9",
        "playground-series-s3e25",
        "playground-series-s3e26",
        "playground-series-s3e24",
        "playground-series-s3e23",
    ]

    other_cs = [
        "amp-parkinsons-disease-progression-prediction",
        "arc-prize-2024",
        "ariel-data-challenge-2024",
        "child-mind-institute-detect-sleep-states",
        "connectx",
        "contradictory-my-dear-watson",
        "digit-recognizer",
        "fathomnet-out-of-sample-detection",
        "forest-cover-type-prediction",
        "gan-getting-started",
        "google-research-identify-contrails-reduce-global-warming",
        "house-prices-advanced-regression-techniques",
        "isic-2024-challenge",
        "leash-BELKA",
        "llm-20-questions",
        "nlp-getting-started",
        "playground-series-s4e1",
        "playground-series-s4e2",
        "playground-series-s4e3",
        "playground-series-s4e4",
        "playground-series-s4e5",
        "playground-series-s4e6",
        "playground-series-s4e7",
        "playground-series-s4e8",
        "rsna-2024-lumbar-spine-degenerative-classification",
        "sf-crime",
        "store-sales-time-series-forecasting",
        "titanic",
        "tpu-getting-started",
        # scenario competition
        "covid19-global-forecasting-week-1",
        "statoil-iceberg-classifier-challenge",
        "optiver-realized-volatility-prediction",
        "facebook-v-predicting-check-ins",
    ]

    # all_cs = mini_case_cs + other_cs
    # for c in all_cs:
    #     convert_notebooks_to_text(c)
    # exit()
    # from kaggle.api.kaggle_api_extended import KaggleApi

    # api = KaggleApi()
    # api.authenticate()
    # cs = api.competitions_list()
    # for c in cs:
    #     name = c.ref.split("/")[-1]
    #     crawl_descriptions(name)
    res = leaderboard_scores(competition="playground-series-s4e8")
    rank, rank_percent = score_rank(competition="playground-series-s4e8", score=0.9832)
    print(rank, rank_percent)
# %%



================================================
File: rdagent/scenarios/kaggle/prompts.yaml
================================================
KG_hypothesis_gen_RAG: |-
  The user has proposed several hypothesis and conducted experiments to validate them. 
  The hypothesis can divided into two categories:
  1. Insights: These are the observations user did to other similar problems. You can either apply the same hypothesis or modify them to fit the current problem.
  2. Experience: These are former hypothesis and experiments user did to the current problem. You can either continue to improve the hypothesis or change to a new one.
  
  {% if insights %}
  The insights are as follows:
  {% for insight in insights %}
  Insight: {{ loop.index }}
  - hypothesis: {{ insight.hypothesis }}
  - experiments: {{ insight.experiments }}
  - conclusion: {{ insight.conclusion }}
  {% endfor %}
  {% endif %}

  {% if experiences %}
  The experiences are as follows:
  {% for experience in experiences %}
  Experience: {{ loop.index }}
  - hypothesis: {{ experience.hypothesis }}
  - experiments: {{ experience.experiments }}
  - conclusion: {{ experience.conclusion }}
  {% endfor %}
  {% endif %}

hypothesis_and_feedback: |-
  {% for experiment, feedback in trace.hist[-10:] %}
  Hypothesis {{ loop.index }}: {{ experiment.hypothesis }}
  Observation on the result with the hypothesis: {{ feedback.observations }}
  Feedback on the original hypothesis:  {{ feedback.hypothesis_evaluation }}
  Did changing to this hypothesis work? (focus on the change):  {{ feedback.decision }}
  {% endfor %}

hypothesis_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
    "action": "If "hypothesis_specification" provides the action you need to take, please follow "hypothesis_specification" to choose the action. Otherwise, based on previous experimental results, suggest the action you believe is most appropriate at the moment. It should be one of ["Feature engineering", "Feature processing", "Model feature selection", "Model tuning"]"
    "hypothesis": "The new hypothesis generated based on the information provided.",
    "reason": "The reason why you generate this hypothesis. It should be comprehensive and logical. It should cover the other keys below and extend them.",
    "concise_reason": "Two-line summary. First line focuses on a concise justification for the change. Second line generalizes a knowledge statement.",
    "concise_observation": "One line summary. It focuses on the observation of the given scenario, data characteristics, or previous experiences (failures & succeses).",
    "concise_justification": "One line summary. Justify the hypothesis based on theoretical principles or initial assumptions.",
    "concise_knowledge": "One line summary. Transferable knowledge based on theoretical principles. Use conditional grammar. eg. "If...., ..; When..., .; and etc" Make sure that you state things clearly without ambiguity. Eg. avoid saying "previous hypothesis", because one wouldn't know what that is."
  }

hypothesis_specification:
  Feature engineering: |-
    Action: Feature engineering
    
    Description: We engineer the features for the sake of best model performance on the basis of engineering the most influential features.
    
    1. Type of Feature and Data Characteristics:
      - Clearly define the type of feature being introduced.
      - Explain what data characteristics or patterns this feature captures.
      - Keep descriptions focused, avoiding redundant details to ensure clarity.

    2. Simple and Effective Features First:
      - Start by introducing features that are simple yet likely to be effective.
      - Provide a concise explanation of why these features are expected to perform well.
      - Avoid complex or combined features during the initial stages.
    
    3. Gradual Complexity Increase:
      - After initial feature testing, introduce more complex features.
      - Discuss both the potential benefits and any additional complexities of these features.
      - Begin combining features only after simpler ones have been tested and validated.

    4. New Directions and Optimizations:
      - If results suggest a need for a new approach, explain why, using data analysis, domain knowledge, or observed patterns.
      - Propose one new direction per iteration for clarity and focus.
      - If a previous hypothesis did not surpass the previous best but shows promise, continue in the same direction with optimizations.
      - Emphasize that features that outperform previous best results are added to the feature library, avoiding redundant work.
      
    5. 1-3 Feature Tasks per Generation:
      - Each generation should produce 1-3 feature tasks.
      - Maintain a balance between simplicity and complexity to develop a diverse and robust feature library.

  Feature processing: |-
    Action: Feature processing
    
    1. Feature Transformation and Normalization:
      - Clearly define any transformations applied to features (e.g., scaling, normalization, log transforms).
      - Explain how these transformations improve the data's suitability for the model.
      - Ensure transformations do not introduce unnecessary complexity early on.
    
    2. Handling Missing Values and Outliers:
      - Define any imputation methods used for missing data (e.g., mean, median, or more complex methods).
      - Explain how outliers are handled (e.g., clipping, removal, or transformation).
      - Ensure these processes are straightforward, enhancing data quality without overcomplicating early feature processing.
    
    3. Feature Interactions and Combinations:
      - After testing individual features, introduce combinations or interactions.
      - Discuss the potential advantages of feature interaction terms (e.g., polynomial or multiplicative features).
      - Ensure interactions are only applied after simpler, individual features have been processed.

    4. 1-3 Feature Tasks per Generation:
      - Each generation should produce 1-3 feature tasks.
      - Maintain a balance between simplicity and complexity to develop a diverse and robust feature library.

  Model feature selection: |-
    Action: Model feature selection

    1. Selection based on model_type:
      - Specify which features are being selected and explain why, considering the model type (e.g., NN, Random Forest, LightGBM, XGBoost).
      - Ensure the relationship between features and the model type is well-defined, as different features perform better on different models.
    
    2. Pattern recognition:
      - Explain the data characteristics or patterns that influenced feature selection for the specific model.
      - Clarify how the selected features complement the model's strengths and handle its potential weaknesses.

  Model tuning: |-
    Action: Model tuning
      
    1. Overview:
    - Clearly explain your hypothesis.
      - Which model are you tuning (one of the four types)?
      - How are you revising it, and why?
      - What are the innovations?
    - Base your hypothesis on previous structures and your understanding of the model code.
    - "Tuning" includes changing the model architecture or hyperparameters.

    2. Focus on Architecture and/or Hyperparameter Tuning:
      - Concentrate on designing new model architectures one at a time, hyperparameter tuning, or both.
      - Each hypothesis should introduce a novel architecture or a significant modification to an existing one.
      - Leverage prior experiences and hypothesis history.
      - If necessary, write source code manually to implement innovations beyond existing packages.

    3. Specific to Model Type:
      - Tuning must be specific to the model types available in our workspace (e.g., Neural Networks, XGBoost, Random Forest, LightGBM).
      - Clearly define the model type and the architecture or tuning being introduced.
      - Ensure the changes align with data characteristics and the model's strengths or limitations.

    4. Rationale Behind Architecture and Tuning:
      - Explain the reasoning behind your architectural design or tuning approach.
      - Justify how the new structure or parameter changes more effectively capture data patterns and improve learning efficiency.

feature_experiment_output_format: |-
  According to the hypothesis, please help user design one or more feature engineering tasks.
  The output should follow JSON format. The schema is as follows:
  {
      "factor or group name 1": {
          "description": "description of factor or group name 1",
          "formulation": "latex formulation of factor or group name 1",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      },
      "factor or group name 2": {
          "description": "description of factor or group name 2",
          "formulation": "latex formulation of factor or group name 2",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      }
      # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
  }

model_experiment_output_format: |-
  According to the hypothesis, please help user design one model task.
  We only build one model from four main model types: ["XGBoost", "RandomForest", "LightGBM", "NN"].
  The output should follow JSON format. The schema is as follows: 
  {
      "model_name": "model_name",
      "description": "A detailed description of the model",
      "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
      "hyperparameters": {
          "hyperparameter_name_1": "value of hyperparameter 1",
          "hyperparameter_name_2": "value of hyperparameter 2",
          "hyperparameter_name_3": "value of hyperparameter 3"
      },
      "model_type": "Please select only **one** model type from the following four options: XGBoost, RandomForest, LightGBM, or NN. The selected model must be unique and used as the **primary model**. You may choose an auxiliary model for support or optimization on specific tasks if necessary, but the primary model must come from the provided options."

  }

kg_feedback_generation_user: |-
  We are in a process of finding and validating hypotheses to build a powerful model. Each round aims to confirm or reject hypotheses based on results.

  The SOTA solution for the task is as follows:
  Features and its corresponding channel: {{ sota_features }}
  Models and its corresponding code: {{ sota_models }}
  Final result of the SOTA solution (we select the best-performing model's metric as the final result): {{ sota_result }}
  {% if sota_sub_results %}
  Sub-results of all sub-models: {{ sota_sub_results }}
  {% endif %}

  Current solution to be evaluated:
  Hypothesis: {{ current_hypothesis }}
  Reasoning: {{ current_hypothesis_reason }}
  Current target action: {{ current_target_action }}
  Experiments conducted and their code: {{ current_sub_exps_to_code }}
  Final result of the current solution (we select the best-performing model's metric as the final result): {{ current_result }}
  {% if current_sub_results %}
  Sub-results of all sub-models: {{ current_sub_results }}
  {% endif %}

  A more detailed comparison between the current solution and the SOTA solution:
  {{ combined_result }}

  Some information about comparing the current solution with the SOTA solution:
  {{ evaluation_description }}

  {% if last_hypothesis_and_feedback %}
  The user has made some hypothesis and conducted experiments to validate them, and the results are as follows:
  hypothesis: {{ last_hypothesis_and_feedback[0].hypothesis }}
  feedback decision: {{ last_hypothesis_and_feedback[1].decision }} 
  reason: {{ last_hypothesis_and_feedback[1].reason }}
  {% endif %}
  Please refer to these hypothesis and feedback to help you recommend new hypothesis

  Consider Changing Direction for Significant Gaps with the Best Result and the last round:
    - If the new results significantly differ from SOTA, consider a new direction.
    - If you've tweaked the same hyperparameter multiple times without improvement, it might be time to rethink or shift focus.
    - If it is model tuning, focus on comparing the SOTA's Sub-results of all sub-models: {{ sota_sub_results }} with the current experiment's Sub-results of all sub-models: {{ current_sub_results }}. For example, identify which model is currently the best, which model was adjusted in this experiment, and whether the adjustment was effective. Determine if there is potential to continue with this model or if another model shows more promise.

model_tuning_feedback_generation:
  system: |-
    You are an advanced assistant for analyzing results in data-driven R&D, in the context of designing machine learning models.
    The task is described in the following scenario:
    {{ scenario }}

    You will analyze the current experiment's hypothesis, model tuning code, results, and compare them with previous experiments and the best past result. 
    Your feedback should:
    1. Confirm if the current result supports or refutes the hypothesis.
    2. Compare with previous best results.
    3. Suggest improvements or new directions. Stay innovative and adaptive.

    Please provide detailed and constructive feedback. Note that as hypothesis evolve, a general trend should be that the model grows larger. 
    Example JSON Structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }

    Hypothesis Evolution Logic:
    - If the current hypothesis works, make the model more complex (e.g., add layers, neurons, etc.).
    - If a hypothesis works, build on it. If not, adjust at the same level before growing deeper. Think step by step and make changes. Act innovatively. 
    - If it doesn't, modify elements at the current level (e.g., adjust regularization, change features).

    Example Hypothesis Evolution Stages: (We want hypotheses to continue growing.) Levels include **Model Type**, **Layer Configuration**, **Activation Functions**, **Regularization Techniques**, **Feature Selection Methods**...
      - Initial Hypothesis: Use CNN with no feature selection.
      - Next Level (if successful): Add 5 convolutional layers, use all features.
      - Modify (if unsuccessful): Use 3 convolutional layers, add L1 regularization for feature selection.
      - Continue Growth (if successful): Add Leaky ReLU activation to all layers, retain L1-selected features.
      - Further Growth (if successful): Add dropout regularization (0.5 rate), retain L1 features.
      - Adjust (if unsuccessful): Use 5 layers, Leaky ReLU, dropout 0.3 rate.

factor_feedback_generation:
  system: |-
    You are a professional data feature engineering assistant in data-driven R&D. 
    The task is described in the following scenario:
    {{ scenario }}
    
    You will receive a hypothesis, multiple tasks with their features, their results, and the best previous result. 
    Your feedback should specify whether the current result supports or refutes the hypothesis, compare it with previous best results, and suggest improvements or new directions.
    
    Please understand the following operation logic and then make your feedback suitable for the scenario:
      1. Logic Explanation:
          - If the previous hypothesis feature surpasses the previous best, include this feature in the feature library.
          - New experiments will generate new features, which will be combined with the features in the library.
          - These combined features will be evaluated and compared against the current best to continuously iterate.
      2. Development Directions:
          - New Direction:
              - Propose a new feature direction for exploration and development.
          - Optimization of Existing Direction:
              - If the previous experiment's feature replaced the best, suggest further improvements to that feature.
              - Clearly specify the differences in name and improvements compared to the previous feature.
          - Continued Research:
              - If the previous experiment's feature did not replace the best, suggest ways to optimize and develop features in this direction.
      3. Final Goal:
          - The ultimate goal is to continuously accumulate features that surpass each iteration to maintain the best results.
    
    Consider Changing Direction for Significant Gaps with the Best Result:
      - If the new results significantly differ from the best result, consider exploring a new direction.
      - Avoid re-implementing previous features as those that surpassed the best are already included in the feature library and will be used in each run.
    Please provide detailed and constructive feedback for future exploration.
    Respond in JSON format. Example JSON structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }

feature_selection_feedback_generation:
  system: |-
    You are a professional feature selection assistant for machine learning models. Your task is to analyze the current feature selection strategy, evaluate its effectiveness, and suggest improvements.
    The task is described in the following scenario:
    {{ scenario }}
    
    In your feedback, consider:
    1. How effective is the current feature selection strategy?
    2. Are there any patterns in the selected or discarded features that might inform future selections?
    3. How might we refine or change the feature selection approach to improve model performance?
    4. Are there any domain-specific considerations that should inform our feature selection?

    Provide detailed and constructive feedback, focusing on actionable insights for feature selection improvement.
    
    Respond in JSON format. Example JSON structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }


model_feature_selection:
  system: |-
    You are an assistant for model feature selection in machine learning. Your task is to understand the current feature groups and choose the most relevant features for the model to get the best performance.

    The user is currently working on a Kaggle competition scenario as follows:
    {{ scenario }}

    The user is now working on the following model type:
    {{ model_type }}

    The user will give you several feature groups and their descriptions. Your task is to select the most relevant features for the model to achieve the best performance. You should consider the following:
    1. How well do the selected features support the scenario?
    2. Are there any features that might be redundant or noisy?

    Please answer the chosen group index in JSON format. Example JSON structure for Result Analysis:
    {
      "Selected Group Index": [1, 3, 5], # List of selected group indices, notice: the index starts from 1
    }

  user: |-
    Current feature groups:
    {% for feature in feature_groups %}
      Group {{ loop.index }}: 
      {{ feature }}
    {% endfor %}

gen_knowledge_from_code_mini_case:
  system: |-
    You were a proficient data scientist.
  user: |-
    The following notebook (contain markdown part and code part) is a high-performing solution for a kaggle competition.
    Please answer the following questions one by one and **as detailed as possible**.
    Make sure that another data scientist can exactly reproduce this copy of code based on your answer.
    Focus on the training process.

    (1) Please give a summary of the overall design.
    (2) What is the overall model architecture? Please use a long article to answer this question as accurately and in detail as possible.
    (3) How are the important hyper-parameters setting in this code?
    (4) What is the optimization objective?
    (5) What advanced machine learning technique does this copy of code use?
    (6) What other important tricks do you think play an important role for high performance?
    
    Note that make sure the answers are directly included from the code or markdown text, rather than based on your assumption.
    
    --------------------
    {{ notebook }}
    --------------------

gen_knowledge_from_code_RDAgent:
  system: |-
    You were a proficient data scientist.
  user: |-
    TODO...



================================================
File: rdagent/scenarios/kaggle/developer/coder.py
================================================
import json
from pathlib import Path
from typing import Dict, List

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.factor_coder import FactorCoSTEER
from rdagent.components.coder.model_coder import ModelCoSTEER
from rdagent.core.developer import Developer
from rdagent.core.prompts import Prompts
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.experiment.kaggle_experiment import (
    KG_SELECT_MAPPING,
    KGModelExperiment,
)

KGModelCoSTEER = ModelCoSTEER
KGFactorCoSTEER = FactorCoSTEER

prompt_dict = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")

DEFAULT_SELECTION_CODE = """
import pandas as pd
def select(X: pd.DataFrame) -> pd.DataFrame:
    \"""
    Select relevant features. To be used in fit & predict function.
    \"""
    if X.columns.nlevels == 1:
        return X
    {% if feature_index_list is not none %}
    X = X.loc[:, X.columns.levels[0][{{feature_index_list}}].tolist()]
    {% endif %}
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X
"""


class KGModelFeatureSelectionCoder(Developer[KGModelExperiment]):
    def develop(self, exp: KGModelExperiment) -> KGModelExperiment:
        target_model_type = exp.sub_tasks[0].model_type
        assert target_model_type in KG_SELECT_MAPPING
        if len(exp.experiment_workspace.data_description) == 1:
            code = (
                Environment(undefined=StrictUndefined)
                .from_string(DEFAULT_SELECTION_CODE)
                .render(feature_index_list=None)
            )
        else:
            system_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["model_feature_selection"]["system"])
                .render(scenario=self.scen.get_scenario_all_desc(), model_type=exp.sub_tasks[0].model_type)
            )
            user_prompt = (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["model_feature_selection"]["user"])
                .render(feature_groups=[desc[0] for desc in exp.experiment_workspace.data_description])
            )

            chosen_index = json.loads(
                APIBackend().build_messages_and_create_chat_completion(
                    user_prompt=user_prompt,
                    system_prompt=system_prompt,
                    json_mode=True,
                    json_target_type=Dict[str, List[int]],
                )
            ).get("Selected Group Index", [i + 1 for i in range(len(exp.experiment_workspace.data_description))])
            chosen_index_to_list_index = [i - 1 for i in chosen_index]

            code = (
                Environment(undefined=StrictUndefined)
                .from_string(DEFAULT_SELECTION_CODE)
                .render(feature_index_list=chosen_index_to_list_index)
            )
        exp.experiment_workspace.inject_files(**{KG_SELECT_MAPPING[target_model_type]: code})
        return exp



================================================
File: rdagent/scenarios/kaggle/developer/feedback.py
================================================
import json
from pathlib import Path
from typing import Dict

import pandas as pd
from jinja2 import Environment, StrictUndefined

from rdagent.components.knowledge_management.graph import UndirectedNode
from rdagent.core.experiment import Experiment
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import (
    Experiment2Feedback,
    Hypothesis,
    HypothesisFeedback,
    Trace,
)
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.experiment.kaggle_experiment import KG_SELECT_MAPPING
from rdagent.utils import convert2bool

prompt_dict = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")
DIRNAME = Path(__file__).absolute().resolve().parent


class KGExperiment2Feedback(Experiment2Feedback):
    def process_results(self, current_result, sota_result):
        # Convert the results to dataframes
        current_df = pd.DataFrame(current_result)
        sota_df = pd.DataFrame(sota_result)

        # Combine the dataframes on the Metric index
        combined_df = pd.concat([current_df, sota_df], axis=1)
        combined_df.columns = ["current_df", "sota_df"]

        # combined_df["the largest"] = combined_df.apply(
        #     lambda row: "sota_df"
        #     if row["sota_df"] > row["current_df"]
        #     else ("Equal" if row["sota_df"] == row["current_df"] else "current_df"),
        #     axis=1,
        # )

        # Add a note about metric direction
        evaluation_direction = "higher" if self.scen.evaluation_metric_direction else "lower"
        evaluation_description = f"Direction of improvement (higher/lower is better) should be judged per metric. Here '{evaluation_direction}' is better for the metrics."
        combined_df["Note"] = evaluation_description

        return combined_df, evaluation_description

    def generate_feedback(self, exp: Experiment, trace: Trace) -> HypothesisFeedback:
        """
        The `ti` should be executed and the results should be included, as well as the comparison between previous results (done by LLM).
        For example: `mlflow` of Qlib will be included.
        """
        """
        Generate feedback for the given experiment and hypothesis.
        Args:
            exp: The experiment to generate feedback for.
            hypothesis: The hypothesis to generate feedback for.
            trace: The trace of the experiment.
        Returns:
            Any: The feedback generated for the given experiment and hypothesis.
        """
        hypothesis = exp.hypothesis
        logger.info("Generating feedback...")
        current_result = exp.result

        evaluation_description = None
        # Check if there are any based experiments
        if exp.based_experiments:
            sota_result = exp.based_experiments[-1].result
            # Process the results to filter important metrics
            combined_result, evaluation_description = self.process_results(current_result, sota_result)
        else:
            # If there are no based experiments, we'll only use the current result
            combined_result, evaluation_description = self.process_results(
                current_result, current_result
            )  # Compare with itself
            print("Warning: No previous experiments to compare against. Using current result as baseline.")

        # Generate the user prompt based on the action type
        if hypothesis.action == "Model tuning":
            prompt_key = "model_tuning_feedback_generation"
        elif hypothesis.action == "Model feature selection":
            prompt_key = "feature_selection_feedback_generation"
        else:
            prompt_key = "factor_feedback_generation"

        # Generate the system prompt
        sys_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict[prompt_key]["system"])
            .render(scenario=self.scen.get_scenario_all_desc(filtered_tag="feedback"))
        )

        sota_exp = exp.based_experiments[-1] if exp.based_experiments else None
        assert sota_exp is not None
        sota_features = str(exp.based_experiments[-1].experiment_workspace.data_description)
        sota_models = json.dumps(exp.based_experiments[-1].experiment_workspace.model_description, indent=2)
        sota_result = exp.based_experiments[-1].result
        sota_sub_results = exp.based_experiments[-1].sub_results

        current_hypothesis = hypothesis.hypothesis
        current_hypothesis_reason = hypothesis.reason
        current_target_action = hypothesis.action
        current_sub_exps_to_code = {}
        if hypothesis.action == "Model tuning":
            current_sub_exps_to_code[exp.sub_tasks[0].get_task_information()] = exp.sub_workspace_list[0].all_codes
        elif hypothesis.action == "Model feature selection":
            current_sub_exps_to_code[exp.sub_tasks[0].get_task_information()] = exp.experiment_workspace.file_dict[
                KG_SELECT_MAPPING[exp.sub_tasks[0].model_type]
            ]
        else:
            current_sub_exps_to_code = {
                sub_ws.target_task.get_task_information(): sub_ws.all_codes for sub_ws in exp.sub_workspace_list
            }
        current_sub_exps_to_code_str = json.dumps(current_sub_exps_to_code, indent=2)
        current_result = exp.result
        current_sub_results = exp.sub_results

        last_hypothesis_and_feedback = None
        if trace.hist and len(trace.hist) > 0:
            last_hypothesis_and_feedback = (trace.hist[-1][0].hypothesis, trace.hist[-1][1])

        # Prepare render dictionary
        render_dict = {
            "sota_features": sota_features,
            "sota_models": sota_models,
            "sota_result": sota_result,
            "sota_sub_results": sota_sub_results,
            "current_hypothesis": current_hypothesis,
            "current_hypothesis_reason": current_hypothesis_reason,
            "current_target_action": current_target_action,
            "current_sub_exps_to_code": current_sub_exps_to_code_str,
            "current_result": current_result,
            "current_sub_results": current_sub_results,
            "combined_result": combined_result,
            "evaluation_description": evaluation_description,
            "last_hypothesis_and_feedback": last_hypothesis_and_feedback,
        }

        usr_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["kg_feedback_generation_user"])
            .render(**render_dict)
        )

        response = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=usr_prompt,
            system_prompt=sys_prompt,
            json_mode=True,
            json_target_type=Dict[str, str | bool | int],
        )

        response_json = json.loads(response)

        observations = response_json.get("Observations", "No observations provided")
        hypothesis_evaluation = response_json.get("Feedback for Hypothesis", "No feedback provided")
        new_hypothesis = response_json.get("New Hypothesis", "No new hypothesis provided")
        reason = response_json.get("Reasoning", "No reasoning provided")
        decision = convert2bool(response_json.get("Replace Best Result", "no"))
        # leaderboard = self.scen.leaderboard
        # current_score = current_result.iloc[0]
        # sorted_scores = sorted(leaderboard, reverse=True)
        # import bisect

        # if self.scen.evaluation_metric_direction:
        #     insert_position = bisect.bisect_right([-score for score in sorted_scores], -current_score)
        # else:
        #     insert_position = bisect.bisect_left(sorted_scores, current_score, lo=0, hi=len(sorted_scores))
        # percentile_ranking = (insert_position) / (len(sorted_scores)) * 100

        experiment_feedback = {
            "hypothesis_text": current_hypothesis,
            "tasks_factors": current_sub_exps_to_code,
            "current_result": current_result,
        }

        if self.scen.if_using_vector_rag:
            raise NotImplementedError("Vector RAG is not implemented yet since there are plenty bugs!")
            self.scen.vector_base.add_experience_to_vector_base(experiment_feedback)
            self.scen.vector_base.dump()
        elif self.scen.if_using_graph_rag:
            competition_node = UndirectedNode(content=self.scen.get_competition_full_desc(), label="competition")
            hypothesis_node = UndirectedNode(content=hypothesis.hypothesis, label=hypothesis.action)
            exp_code_nodes = []
            for exp, code in current_sub_exps_to_code.items():
                exp_code_nodes.append(UndirectedNode(content=exp, label="experiments"))
                if code != "":
                    exp_code_nodes.append(UndirectedNode(content=code, label="code"))
            conclusion_node = UndirectedNode(content=response, label="conclusion")
            all_nodes = [competition_node, hypothesis_node, *exp_code_nodes, conclusion_node]
            all_nodes = trace.knowledge_base.batch_embedding(all_nodes)
            for node in all_nodes:
                if node is not competition_node:
                    trace.knowledge_base.add_node(node, competition_node)

        if self.scen.if_action_choosing_based_on_UCB:
            self.scen.action_counts[hypothesis.action] += 1

        return HypothesisFeedback(
            observations=observations,
            hypothesis_evaluation=hypothesis_evaluation,
            new_hypothesis=new_hypothesis,
            reason=reason,
            decision=decision,
        )



================================================
File: rdagent/scenarios/kaggle/developer/runner.py
================================================
import json
import pickle
import shutil
from pathlib import Path

import pandas as pd

from rdagent.components.runner import CachedRunner
from rdagent.core.exception import CoderError, FactorEmptyError, ModelEmptyError
from rdagent.core.experiment import ASpecificExp, Experiment
from rdagent.core.prompts import Prompts
from rdagent.core.utils import cache_with_pickle
from rdagent.oai.llm_utils import md5_hash
from rdagent.scenarios.kaggle.experiment.kaggle_experiment import (
    KGFactorExperiment,
    KGModelExperiment,
)

prompt_dict = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")


class KGCachedRunner(CachedRunner[ASpecificExp]):
    def get_cache_key(self, exp: ASpecificExp) -> str:
        codes = []
        for f in sorted((exp.experiment_workspace.workspace_path / "feature").glob("*.py"), key=lambda x: x.name):
            codes.append(f.read_text())
        for f in sorted((exp.experiment_workspace.workspace_path / "model").glob("*.py"), key=lambda x: x.name):
            codes.append(f.read_text())
        codes = "\n".join(codes)
        cached_key_from_exp = CachedRunner.get_cache_key(self, exp)
        return md5_hash(codes + cached_key_from_exp)

    def assign_cached_result(self, exp: Experiment, cached_res: Experiment) -> Experiment:
        exp = CachedRunner.assign_cached_result(self, exp, cached_res)
        if cached_res.experiment_workspace.workspace_path.exists():
            for csv_file in cached_res.experiment_workspace.workspace_path.glob("*.csv"):
                shutil.copy(csv_file, exp.experiment_workspace.workspace_path)
            for py_file in (cached_res.experiment_workspace.workspace_path / "feature").glob("*.py"):
                shutil.copy(py_file, exp.experiment_workspace.workspace_path / "feature")
            for py_file in (cached_res.experiment_workspace.workspace_path / "model").glob("*.py"):
                shutil.copy(py_file, exp.experiment_workspace.workspace_path / "model")
        exp.experiment_workspace.data_description = cached_res.experiment_workspace.data_description
        return exp

    @cache_with_pickle(get_cache_key, CachedRunner.assign_cached_result)
    def init_develop(self, exp: KGFactorExperiment | KGModelExperiment) -> KGFactorExperiment | KGModelExperiment:
        """
        For the initial development, the experiment serves as a benchmark for feature engineering.
        """

        env_to_use = {"PYTHONPATH": "./"}

        result = exp.experiment_workspace.execute(run_env=env_to_use)

        exp.result = result

        sub_result_score_path = Path(exp.experiment_workspace.workspace_path) / "sub_submission_score.csv"
        if sub_result_score_path.exists():
            sub_submission_df = pd.read_csv(sub_result_score_path)
            exp.sub_results = sub_submission_df.set_index("Model")["score"].to_dict()

        return exp


class KGModelRunner(KGCachedRunner[KGModelExperiment]):
    @cache_with_pickle(KGCachedRunner.get_cache_key, KGCachedRunner.assign_cached_result)
    def develop(self, exp: KGModelExperiment) -> KGModelExperiment:
        if exp.based_experiments and exp.based_experiments[-1].result is None:
            exp.based_experiments[-1] = self.init_develop(exp.based_experiments[-1])

        sub_ws = exp.sub_workspace_list[0]
        if sub_ws is not None:
            # TODO: There's a possibility of generating a hybrid model (lightgbm + xgboost), which results in having two items in the model_type list.
            model_type = sub_ws.target_task.model_type

            if sub_ws.file_dict == {}:
                raise ModelEmptyError("No model is implemented.")
            else:
                model_file_name = f"model/model_{model_type.lower()}.py"
                exp.experiment_workspace.inject_files(**{model_file_name: sub_ws.file_dict["model.py"]})
        else:
            raise ModelEmptyError("No model is implemented.")
        env_to_use = {"PYTHONPATH": "./"}

        result = exp.experiment_workspace.execute(run_env=env_to_use)

        if result is None:
            raise CoderError("No result is returned from the experiment workspace")

        exp.result = result
        sub_result_score_path = Path(exp.experiment_workspace.workspace_path) / "sub_submission_score.csv"
        if sub_result_score_path.exists():
            sub_submission_df = pd.read_csv(sub_result_score_path)
            exp.sub_results = sub_submission_df.set_index("Model")["score"].to_dict()

        return exp


class KGFactorRunner(KGCachedRunner[KGFactorExperiment]):
    @cache_with_pickle(KGCachedRunner.get_cache_key, KGCachedRunner.assign_cached_result)
    def develop(self, exp: KGFactorExperiment) -> KGFactorExperiment:
        current_feature_file_count = len(list(exp.experiment_workspace.workspace_path.glob("feature/feature*.py")))
        implemented_factor_count = 0
        for sub_ws in exp.sub_workspace_list:
            if sub_ws.file_dict == {}:
                continue
            execued_df = sub_ws.execute()[1]
            if execued_df is None:
                continue
            implemented_factor_count += 1
            target_feature_file_name = f"feature/feature_{current_feature_file_count:05d}.py"
            exp.experiment_workspace.inject_files(**{target_feature_file_name: sub_ws.file_dict["factor.py"]})
            feature_shape = execued_df.shape[-1]
            exp.experiment_workspace.data_description.append((sub_ws.target_task.get_task_information(), feature_shape))
            current_feature_file_count += 1
        if implemented_factor_count == 0:
            raise FactorEmptyError("No factor is implemented")

        # initial template result
        if exp.based_experiments and exp.based_experiments[-1].result is None:
            exp.based_experiments[-1] = self.init_develop(exp.based_experiments[-1])

        env_to_use = {"PYTHONPATH": "./"}

        result = exp.experiment_workspace.execute(run_env=env_to_use)

        if result is None:
            raise CoderError("No result is returned from the experiment workspace")

        exp.result = result
        sub_result_score_path = Path(exp.experiment_workspace.workspace_path) / "sub_submission_score.csv"
        if sub_result_score_path.exists():
            sub_submission_df = pd.read_csv(sub_result_score_path)
            exp.sub_results = sub_submission_df.set_index("Model")["score"].to_dict()

        return exp



================================================
File: rdagent/scenarios/kaggle/docker/kaggle_docker/Dockerfile
================================================
FROM pytorch/pytorch:2.2.1-cuda12.1-cudnn8-runtime
# For GPU support, please choose the proper tag from https://hub.docker.com/r/pytorch/pytorch/tags

RUN apt-get clean && apt-get update && apt-get install -y \  
    curl \  
    vim \  
    git \  
    build-essential \
    && rm -rf /var/lib/apt/lists/* 

WORKDIR /workspace

RUN python -m pip install numpy
# RUN python -m pip install --upgrade cython
# RUN python -m pip install -e .

RUN python -m pip install pandas
# RUN pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.3.0%2Bcu121.html
RUN pip install torch_geometric
RUN pip install pytorch_lightning
RUN pip install ogb
RUN pip install networkx
RUN pip install scikit-learn
RUN pip install catboost
RUN pip install xgboost
RUN pip install sparse
RUN pip install lightgbm==3.3.5
RUN pip install pyarrow
RUN pip install fastparquet
RUN pip install optuna


================================================
File: rdagent/scenarios/kaggle/docker/mle_bench_docker/Dockerfile
================================================
FROM pytorch/pytorch:2.4.0-cuda12.4-cudnn9-runtime
# For GPU support, please choose the proper tag from https://hub.docker.com/r/pytorch/pytorch/tags

RUN apt-get clean && apt-get update && apt-get install -y \  
    curl \  
    vim \  
    git \  
    build-essential \
    git-lfs \
    unzip \
    && rm -rf /var/lib/apt/lists/* 

RUN git clone https://github.com/openai/mle-bench.git
RUN cd mle-bench && git lfs fetch --all
RUN cd mle-bench && git lfs pull
RUN cd mle-bench && python -m pip install -e .

WORKDIR /workspace



================================================
File: rdagent/scenarios/kaggle/experiment/README.md
================================================

# Meta template
It is an example of how we organize the workspace of a competition.
We expect all the competitions to align with it so the knowledge in modules (model, feature) can transfer.

The generation process of the initial template is hoped to be conducted by LLM (however, it is based on human efforts currently).



================================================
File: rdagent/scenarios/kaggle/experiment/kaggle_experiment.py
================================================
from copy import deepcopy
from pathlib import Path

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.components.coder.factor_coder.factor import (
    FactorFBWorkspace,
    FactorTask,
    FeatureExperiment,
)
from rdagent.components.coder.model_coder.model import (
    ModelExperiment,
    ModelFBWorkspace,
    ModelTask,
)
from rdagent.scenarios.kaggle.experiment.workspace import KGFBWorkspace

KG_MODEL_TYPE_XGBOOST = "XGBoost"
KG_MODEL_TYPE_RANDOMFOREST = "RandomForest"
KG_MODEL_TYPE_LIGHTGBM = "LightGBM"
KG_MODEL_TYPE_NN = "NN"

KG_MODEL_MAPPING = {
    KG_MODEL_TYPE_XGBOOST: "model/model_xgboost.py",
    KG_MODEL_TYPE_RANDOMFOREST: "model/model_randomforest.py",
    KG_MODEL_TYPE_LIGHTGBM: "model/model_lightgbm.py",
    KG_MODEL_TYPE_NN: "model/model_nn.py",
}

KG_SELECT_MAPPING = {
    KG_MODEL_TYPE_XGBOOST: "model/select_xgboost.py",
    KG_MODEL_TYPE_RANDOMFOREST: "model/select_randomforest.py",
    KG_MODEL_TYPE_LIGHTGBM: "model/select_lightgbm.py",
    KG_MODEL_TYPE_NN: "model/select_nn.py",
}


class KGModelExperiment(ModelExperiment[ModelTask, KGFBWorkspace, ModelFBWorkspace]):
    def __init__(self, *args, source_feature_size: int = None, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.experiment_workspace = KGFBWorkspace(
            template_folder_path=Path(__file__).resolve()
            / Path(KAGGLE_IMPLEMENT_SETTING.template_path).resolve()
            / KAGGLE_IMPLEMENT_SETTING.competition
        )
        if len(self.based_experiments) > 0:
            self.experiment_workspace.inject_files(**self.based_experiments[-1].experiment_workspace.file_dict)
            self.experiment_workspace.data_description = deepcopy(
                self.based_experiments[-1].experiment_workspace.data_description
            )
        else:
            self.experiment_workspace.data_description = [
                (
                    FactorTask(
                        factor_name="Original features",
                        factor_description="The original features",
                        factor_formulation="",
                    ).get_task_information(),
                    source_feature_size,
                )
            ]


class KGFactorExperiment(FeatureExperiment[FactorTask, KGFBWorkspace, FactorFBWorkspace]):
    def __init__(self, *args, source_feature_size: int = None, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.experiment_workspace = KGFBWorkspace(
            template_folder_path=Path(__file__).resolve()
            / Path(KAGGLE_IMPLEMENT_SETTING.template_path).resolve()
            / KAGGLE_IMPLEMENT_SETTING.competition
        )
        if len(self.based_experiments) > 0:
            self.experiment_workspace.inject_files(**self.based_experiments[-1].experiment_workspace.file_dict)
            self.experiment_workspace.data_description = deepcopy(
                self.based_experiments[-1].experiment_workspace.data_description
            )
        else:
            self.experiment_workspace.data_description = [
                (
                    FactorTask(
                        factor_name="Original features",
                        factor_description="The original features",
                        factor_formulation="",
                    ).get_task_information(),
                    source_feature_size,
                )
            ]



================================================
File: rdagent/scenarios/kaggle/experiment/prompts.yaml
================================================
kg_description_template:
  system: |-
    You are an assistant that extracts structured information from unstructured text.
    The user will provide you a Kaggle competition description, and you need to extract specific details from it.
    For the dataset, the competition may not include detailed information about the dataset. The user has read the dataset and provide you the relevant information. Please include it in your response.
    Please answer in Json format with the following schema:
    {
      "Competition Type": "The type of competition, e.g., 'Classification', 'Regression', 'Clustering', 'Prediction", "Time-Series Forecasting",
      "Competition Description": "A brief description of the competition",
      "Target Description": "A description of the target variable to be predicted",
      "Competition Features": "Two-line description of the overall features involved within the competition as background."
      "Submission Specifications": "The submission specification & sample submission csv descriptions for the model to output."
      "Submission channel number to each sample": "The number of channels in the output for each sample, e.g., 1 for regression, N for N class classification with probabilities, etc. A Integer. If not specified, it is 1."
      "Evaluation Description": "A brief description of the metrics used in the evaluation. Please note that if `evaluation_metric_direction` is True, it indicates that higher values are better; if False, lower values are preferred."
    }
    Since these might be very similar column names in data like one_hot_encoded columns, you can use some regex to group them together.


  user: |-
    Competition Description: 
    {{ competition_descriptions }}
    The raw data information:
    {{ raw_data_information }}
    Evaluation_metric_direction: 
    {{ evaluation_metric_direction }}

kg_background: |-
  You are solving a data science tasks and the type of the competition is {{ competition_type }}.
  The competition description is: {{competition_description}}. 
  
  We provide an overall script in file: train.py. The user will run the train.py script along with several feature and model scripts to train several model to get a good performance on this task.

  The train.py script is as follows:
  ```python
  {{ train_script }}
  ```
  
  The final output of our pipeline is from a ensemble of up to four models. Each model is trained on a different subset of the data.
  The four model types are: XGBoost, RandomForest, LightGBM and Neural Network (A Pytorch model).
  About the Neural Network model, You can try different architectures and hyperparameters to improve the performance. You can even use a pytorch model to ensemble the other three types of models. Try to open your mind on the NN model.
  
  The data is extracted from the competition dataset, focusing on relevant attributes in {{ competition_features }}.

  The user firstly designs and implements a feature book for each model. The feature book is a combination of several features and feature groups.
  The feature book is built from:
  - Raw features: The raw features are the original features from the dataset.
  - generated features: The generated features are the features that are calculated based on the raw features according to some formulations. The calculation should be align with some physical or logical meaning. Don't just simply apply some numeric operations to the raw features.
  - feature groups: The feature groups are preprocessed group of features from the raw features like normalization, one hot encoding, etc.
  The feature or feature group is defined in the following parts:
  - Name: The name of the feature or feature group.
  - Description: A description of the feature or feature group.
  - Formulation: The formulation of the feature or feature group.
  - Variables: The variable list used in the formulation. Notice: The variable should be a specific feature in the dataset. Please make sure the feature name is exactly the same as the feature name in the dataset.
  
  For each model, the user will design and implement the model in a separate script.
  The model is defined in the following parts:
  - Name: The name of the model.
  - Description: A description of the model.
  - Architecture: The detailed architecture of the model, such as neural network layers or tree structures.
  - ModelType: The type of the model, which should be one of ["XGBoost", "RandomForest", "LightGBM", "NN"].
  The model should provide clear and detailed documentation of its architecture and hyperparameters.

  The user tries to optimize the performance iteratively by employing one of the feature related or model related action items:
  - Feature related:
    - "Feature engineering": The user will design several new tasks and implement several new features. The new feature might only affect the model using all the feature book.
    - "Feature processing": The user will design a new task to process the feature book like normalization or one hot encoding to improve the model performance. Any processing with help of a deep model is not included in this task.
  - Model related:
    - "Model feature selection": The user will modify one model to select the part of the features from the feature book to improve the model performance.
    - "Model tuning": The user will tune the hyperparameters of XGBoost, RandomForest or LightGBM or build or improve the NN model to improve the model performance. 
  Notice: You can automatically optimize the hyperparameters of the model using some library when training the model. Since we don't have a lot of time to train the model, please use a small number of trials to optimize the hyperparameters. 
  Our validation set split is not deterministic, so when you are using hyperparameter tuning, you can merge training and validation and use cross validation method to tune the hyperparameters.
  One you have determine the best model parameter, you should retrain the model on all training and validation set to get the final model.

  For each loop, you need to help user decide which action item to choose and provide the corresponding code to implement the action item.

kg_feature_interface: |-
  Your code should contain several parts:
  1. The import part: import the necessary libraries.
  2. A class that contains the feature engineering logic.
    The class should have the following methods:
      - fit: This method should fit the feature engineering model to the training data.
      - transform: This method should transform the input data and return it.
    For some tasks like generating new features, the fit method may not be necessary. Please pass this function as a no-op.
  3. A variable called feature_engineering_cls that contains the class name.
  The input to 'fit' is the training data in pandas dataframe, and the input to 'transform' is the data to be transformed in pandas dataframe.
  The original columns should be excluded from the returned DataFrame.

  Notice: Since we have a very big dataset, the feature engineering should be efficient and fast. Otherwise, please sufficiently exploit the multiprocessing or parallel computing to speed up the feature engineering process!

  Exception handling will be managed externally, so avoid using try-except blocks in your code. The user will handle any exceptions that arise and provide feedback as needed.
  
  The feat_eng function can be one of the following:
  - Feature engineering: This function calculated one new feature based on the existing raw data.
  - Feature processing: This function processes the existing raw data like normalization or one hot encoding and return the processed data in the form of a pandas DataFrame.

  Here is an example of how your Python code should be structured:
  ```python
  import pandas as pd

  class FeatureEngineeringName:
      def fit(self, train_df: pd.DataFrame):
          """
          Fit the feature engineering model to the training data. 
          For example, for one hot encoding, this would involve fitting the encoder to the training data.
          For feature scaling, this would involve fitting the scaler to the training data.
          """
          return self

      def transform(self, X: pd.DataFrame):
          """
          Transform the input data.
          """
          return X
          return X.mean(axis=1).to_frame("mean_feature") # Example feature engineering
          return X.fillna(0) # Example feature processing

  feature_engineering_cls = FeatureEngineeringName
  ```

  To Note:
  Top 0. I have already completed the encoded labeling process, so please avoid any one-hot encoding or similar operations in the future. Focus instead on targeted and efficient feature engineering techniques, such as normalizing float-type features, filtering based on specific categories, or other concise transformations that can be quickly implemented and tested without unnecessary complexity. Also, ensure that the index of the output DataFrame matches the original DataFrame's index, and that the number of columns remains consistent across train, validation, and test sets.
  1. Ensure that your code meets these requirements and produces a feature-engineered DataFrame that contains only the newly engineered columns, aligning with the user's data and objectives.
  2. Ensure that the index of the output DataFrame matches the index of the original DataFrame. For example:
    Incorrect: `normalized_df = pd.DataFrame(normalized_features, columns=X.columns)`
    Correct: `normalized_df = pd.DataFrame(normalized_features, columns=X.columns, index=X.index)`
  3. Ensure consistency in column count across train, validation, and test sets post-feature engineering. For example, fit PCA on the training set and apply the same transformation to validation and test sets to keep the number of columns aligned, and use OneHotEncoder may also cause different number of columns.
  4. Ensure that the generation of new features does not drastically increase the number of columns, which can slow down data processing. For example, avoid creating pairwise interactions for all features, as this would lead to a quadratic increase in the number of columns.
  5. Avoids raising a `ValueError` or any other exceptions that could interrupt the main program's flow. The code should not include checks that could potentially lead to a `ValueError`. Instead, focus on writing robust and fault-tolerant feature engineering functions that handle edge cases and missing data gracefully, without stopping the program.
  6. Specific categories of features can be filtered, and processing can be applied to those categories. For example, normalization can be applied to float-type features, but such processing should not be done on one-hot encoded features.
  7. You are participating in a Kaggle competition and need data engineering ideas that are small, efficient, and quick to execute. Your suggestions should avoid unnecessary complexity or excessive processing time. Focus on delivering concise, impactful transformations or preprocessing steps that improve model performance with minimal resource usage. Please suggest clear, targeted approaches that can be implemented and tested rapidly.

kg_model_interface: |-
  Your code should contain several parts:
  1. The import part: import the necessary libraries.
  2. A function called fit() that trains the model and returns the trained model.
    The function should take the following arguments:
      - X_train: The training features as a pandas DataFrame.
      - y_train: The training labels as a pandas Series.
      - X_valid: The validation features as a pandas DataFrame.
      - y_valid: The validation labels as a pandas Series.
    The function should return the trained model.
  3. A function called predict() that makes predictions using the trained model. 
    The function should take the following arguments:
      - model: The trained model.
      - X: The features as a pandas DataFrame.
    The function should return the predicted probabilities or boolean predictions in numpy.ndarray format.
    Please refer to the train.py script to verify whether the output should be a class label or a probability!

  Here are some examples of how your Python code should be structured:

  {% if tag == "XGBoost" or tag is none %}
  For XGBoost:
  ```python
  import pandas as pd
  import numpy as np
  import xgboost
  from xgboost import DMatrix

  def fit(
      X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series
  ) -> xgboost.Booster:
      dtrain = DMatrix(X_train, label=y_train)
      dvalid = DMatrix(X_valid, label=y_valid)
      params = ...  # Set parameters to XGBoost model
      model = xgboost.train(params, dtrain, num_boost_round=100)
      y_pred = model.predict(dvalid)

      accuracy = ...  # Calculate accuracy
      return model


  def predict(model: xgboost.Booster, X: pd.DataFrame) -> np.ndarray:
      dtest = DMatrix(X)
      y_pred = model.predict(dtest)

      return y_pred
  ```
  {% endif %}
  {% if tag == "RandomForest" or tag is none %}
  For RandomForest:
  ```python
  import pandas as pd
  import numpy as np
  from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
  from sklearn.metrics import accuracy_score

  def fit(
      X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series
  ) -> RandomForestClassifier | RandomForestRegressor:
      model = RandomForestClassifier(...)  # fir classification tasks
      model = RandomForestRegressor(...)  # for regression tasks
      model.fit(X_train, y_train, ...) # Train the model

      return model


  def predict(model: RandomForestClassifier | RandomForestRegressor, X: pd.DataFrame) -> np.ndarray:
      y_pred = model.predict(X)

      return y_pred
  ```
  {% endif %}
  {% if tag == "LightGBM" or tag is none %}
  For LightGBM:
  ```python
  import pandas as pd
  import numpy as np
  from lightgbm import LGBMClassifier, LGBMRegressor

  def fit(
      X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series
  ) -> LGBMClassifier | LGBMRegressor:
      model = LGBMClassifier(...)  # for classification tasks, please add parameters here
      model = LGBMRegressor(...)  # for regression tasks, please add parameters here

      model.fit(X=X_train, y=y_train, eval_set=[(X_valid, y_valid)])
      return model


  def predict(model: LGBMClassifier | LGBMRegressor, X: pd.DataFrame) -> np.ndarray:
      y_pred = model.predict(X)

      return y_pred
  ```
  {% endif %}
  {% if tag == "NN" or tag is none %}
  For Neural Network:
  ```python
  import pandas as pd
  import numpy as np
  import torch
  from torch.utils.data import DataLoader, TensorDataset


  class NNModel(torch.nn.Module):
      def __init__(self):
          super(Model, self).__init__()
          # Define your model here

      def forward(self, x):
          # Define the forward pass
          return x

  def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame) -> torch.nn.Module:
      model = NNModel()  # Initialize the model, You can write your own model class

      optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Example optimizer, you can use any optimizer
      criterion = torch.nn.CrossEntropyLoss()  # Example loss function, you can use any loss function

      train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)
      valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=64, shuffle=False)

      # Example training loop, you can customize this loop as per your requirement
      for epoch in range(10):
          model.train()
          for X_batch, y_batch in train_loader:
              optimizer.zero_grad()
              outputs = model(X_batch)
              loss = criterion(outputs, y_batch)
              loss.backward()
              optimizer.step()

          model.eval()
          y_pred = []
          with torch.no_grad():
              for X_batch, _ in valid_loader:
                  outputs = model(X_batch)
                  y_pred.extend(outputs.squeeze().tolist())

          y_pred = torch.tensor(y_pred)
          accuracy = (y_pred == y_valid).float().mean()
          # You can early stop based on the validation, please customize this as per your requirement
      return model


  def predict(model: torch.nn.Module, X: pd.DataFrame) -> np.ndarray:
      X = torch.tensor(X.values).float()
      model.eval()
      with torch.no_grad():
          y_pred = model(X).squeeze().numpy()

      return y_pred
  ```
  {% endif %}

kg_feature_simulator: |-
  The data preprocessing method you provide will be used to prepare data by processing it, concatenating the results with other features, and removing unnecessary features before training the model. 
  The processed data will then be used for model training and prediction.
  
  User will use your data preprocessing method to do the following steps:
  1. Execute your Python files to process the data. (what you need to do)
  2. Concatenate the processed features with other features and the original data.
  3. Remove any unnecessary features before training the model.
  4. Train a model such as LightGBM, CatBoost, LSTM, or a simple PyTorch model using the processed data.
  5. Evaluate the performance of your preprocessing method and provide feedback.

kg_feature_output_format: |-
  The output should be a pandas DataFrame with the new features. The columns should be the new features, and the rows should correspond to the number of samples in the input DataFrame.
  Sample output dataframe info:
  <class 'pandas.core.frame.DataFrame'>
  Index: {Same to the input DataFrame}
  Data columns (total N columns):
  #   Column      Dtype  
  ---  ------      -----  
  0   feature_name_0   float64
  1   feature_name_1  float64
  dtypes: float64(N)
  memory usage: {Memory usage of the output DataFrame}

kg_model_output_format: |-
  For model related tasks, the output should be an np.ndarray with the appropriate number of predictions. 
  Please refer to the train.py script to verify whether the output should be a class label or a probability!
  {% if channel == 1 %}
  For each sample, the output should be a single value (e.g., (8, 1) if there are 8 samples).
  {% else %}
  For each sample, the output should be multiple values with {{ channel }} numbers (e.g., (8, {{ channel }}) if there are 8 samples).
  {% endif %}
  
kg_model_simulator: |-
  The models will be trained on the competition dataset and evaluated on their ability to predict the target. Metrics like accuracy and AUC-ROC is used to evaluate the model performance. 
  Model performance will be iteratively improved based on feedback from evaluation results.
  Your output should follow some requirements to submit to the competition:
  {{ submission_specifications }}


================================================
File: rdagent/scenarios/kaggle/experiment/scenario.py
================================================
import io
import json
import pickle
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict

import pandas as pd
from jinja2 import Environment, StrictUndefined

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.core.experiment import Task
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.experiment.kaggle_experiment import KGFactorExperiment
from rdagent.scenarios.kaggle.kaggle_crawler import (
    crawl_descriptions,
    leaderboard_scores,
)
from rdagent.scenarios.kaggle.knowledge_management.vector_base import (
    KaggleExperienceBase,
)

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")

KG_ACTION_FEATURE_PROCESSING = "Feature processing"
KG_ACTION_FEATURE_ENGINEERING = "Feature engineering"
KG_ACTION_MODEL_FEATURE_SELECTION = "Model feature selection"
KG_ACTION_MODEL_TUNING = "Model tuning"
KG_ACTION_LIST = [
    KG_ACTION_FEATURE_PROCESSING,
    KG_ACTION_FEATURE_ENGINEERING,
    KG_ACTION_MODEL_FEATURE_SELECTION,
    KG_ACTION_MODEL_TUNING,
]


class KGScenario(Scenario):
    def __init__(self, competition: str) -> None:
        super().__init__()
        self.competition = competition
        self.competition_descriptions = crawl_descriptions(competition, KAGGLE_IMPLEMENT_SETTING.local_data_path)
        self.input_shape = None

        self.competition_type = None
        self.competition_description = None
        self.target_description = None
        self.competition_features = None
        self.submission_specifications = None
        self.model_output_channel = None
        self.evaluation_desc = None
        self.leaderboard = leaderboard_scores(competition)
        self.evaluation_metric_direction = float(self.leaderboard[0]) > float(self.leaderboard[-1])
        self.vector_base = None
        self.mini_case = KAGGLE_IMPLEMENT_SETTING.mini_case
        self._analysis_competition_description()
        self.if_action_choosing_based_on_UCB = KAGGLE_IMPLEMENT_SETTING.if_action_choosing_based_on_UCB
        self.if_using_graph_rag = KAGGLE_IMPLEMENT_SETTING.if_using_graph_rag
        self.if_using_vector_rag = KAGGLE_IMPLEMENT_SETTING.if_using_vector_rag

        if self.if_using_vector_rag and KAGGLE_IMPLEMENT_SETTING.rag_path:
            self.vector_base = KaggleExperienceBase(KAGGLE_IMPLEMENT_SETTING.rag_path)
            self.vector_base.path = Path(datetime.now(timezone.utc).strftime("%Y-%m-%d-%H-%M-%S") + "_kaggle_kb.pkl")
            self.vector_base.dump()

        self.action_counts = dict.fromkeys(KG_ACTION_LIST, 0)
        self.reward_estimates = {action: 0.0 for action in KG_ACTION_LIST}
        # self.reward_estimates["Model feature selection"] = 0.2
        # self.reward_estimates["Model tuning"] = 1.0
        self.reward_estimates["Feature processing"] = 0.2
        self.reward_estimates["Feature engineering"] = 1.0
        self.confidence_parameter = 1.0
        self.initial_performance = 0.0

    def _analysis_competition_description(self):
        sys_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["kg_description_template"]["system"])
            .render()
        )

        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["kg_description_template"]["user"])
            .render(
                competition_descriptions=self.competition_descriptions,
                raw_data_information=self.source_data,
                evaluation_metric_direction=self.evaluation_metric_direction,
            )
        )

        response_analysis = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=sys_prompt,
            json_mode=True,
            json_target_type=Dict[str, str | bool | int],
        )

        response_json_analysis = json.loads(response_analysis)
        self.competition_type = response_json_analysis.get("Competition Type", "No type provided")
        self.competition_description = response_json_analysis.get("Competition Description", "No description provided")
        self.target_description = response_json_analysis.get("Target Description", "No target provided")
        self.competition_features = response_json_analysis.get("Competition Features", "No features provided")
        self.submission_specifications = response_json_analysis.get(
            "Submission Specifications", "No submission requirements provided"
        )
        self.model_output_channel = response_json_analysis.get("Submission channel number to each sample", 1)
        self.evaluation_desc = response_json_analysis.get(
            "Evaluation Description", "No evaluation specification provided."
        )

    def get_competition_full_desc(self) -> str:
        evaluation_direction = "higher the better" if self.evaluation_metric_direction else "lower the better"
        return f"""Competition Type: {self.competition_type}
    Competition Description: {self.competition_description}
    Target Description: {self.target_description}
    Competition Features: {self.competition_features}
    Submission Specifications: {self.submission_specifications}
    Model Output Channel: {self.model_output_channel}
    Evaluation Descriptions: {self.evaluation_desc}
    Is the evaluation metric the higher the better: {evaluation_direction}
    """

    @property
    def background(self) -> str:
        background_template = prompt_dict["kg_background"]

        train_script = (
            Path(__file__).parent / "templates" / KAGGLE_IMPLEMENT_SETTING.competition / "train.py"
        ).read_text()

        background_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(background_template)
            .render(
                train_script=train_script,
                competition_type=self.competition_type,
                competition_description=self.competition_description,
                target_description=self.target_description,
                competition_features=self.competition_features,
                submission_specifications=self.submission_specifications,
                evaluation_desc=self.evaluation_desc,
                evaluate_bool=self.evaluation_metric_direction,
            )
        )
        return background_prompt

    @property
    def source_data(self) -> str:
        data_folder = Path(KAGGLE_IMPLEMENT_SETTING.local_data_path) / self.competition

        if not (data_folder / "X_valid.pkl").exists():
            preprocess_experiment = KGFactorExperiment([])
            (
                X_train,
                X_valid,
                y_train,
                y_valid,
                X_test,
                *others,
            ) = preprocess_experiment.experiment_workspace.generate_preprocess_data()

            data_folder.mkdir(exist_ok=True, parents=True)
            pickle.dump(X_train, open(data_folder / "X_train.pkl", "wb"))
            pickle.dump(X_valid, open(data_folder / "X_valid.pkl", "wb"))
            pickle.dump(y_train, open(data_folder / "y_train.pkl", "wb"))
            pickle.dump(y_valid, open(data_folder / "y_valid.pkl", "wb"))
            pickle.dump(X_test, open(data_folder / "X_test.pkl", "wb"))
            pickle.dump(others, open(data_folder / "others.pkl", "wb"))

        X_valid = pd.read_pickle(data_folder / "X_valid.pkl")
        # TODO: Hardcoded for now, need to be fixed
        if self.competition == "feedback-prize-english-language-learning":
            return "This is a sparse matrix of descriptive text."

        buffer = io.StringIO()
        X_valid.info(verbose=True, buf=buffer, show_counts=False)
        data_info = buffer.getvalue()
        self.input_shape = X_valid.shape
        return data_info

    def output_format(self, tag=None) -> str:
        assert tag in [None, "feature", "model"]
        feature_output_format = f"""The feature code should output following the format:
{prompt_dict['kg_feature_output_format']}"""
        model_output_format = f"""The model code should output following the format:\n""" + (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["kg_model_output_format"])
            .render(channel=self.model_output_channel)
        )
        if tag is None:
            return feature_output_format + "\n" + model_output_format
        elif tag == "feature":
            return feature_output_format
        elif tag == "model":
            return model_output_format

    def interface(self, tag=None) -> str:
        assert tag in [None, "feature", "XGBoost", "RandomForest", "LightGBM", "NN"]
        feature_interface = f"""The feature code should follow the interface:
{prompt_dict['kg_feature_interface']}"""
        if tag == "feature":
            return feature_interface

        model_interface = "The model code should follow the interface:\n" + (
            Environment(undefined=StrictUndefined).from_string(prompt_dict["kg_model_interface"]).render(tag=tag)
        )
        if tag is None:
            return feature_interface + "\n" + model_interface
        else:
            return model_interface

    def simulator(self, tag=None) -> str:
        assert tag in [None, "feature", "model"]
        kg_feature_simulator = "The feature code will be sent to the simulator:\n" + prompt_dict["kg_feature_simulator"]

        kg_model_simulator = "The model code will be sent to the simulator:\n" + (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["kg_model_simulator"])
            .render(submission_specifications=self.submission_specifications)
        )
        if tag is None:
            return kg_feature_simulator + "\n" + kg_model_simulator
        elif tag == "feature":
            return kg_feature_simulator
        elif tag == "model":
            return kg_model_simulator

    @property
    def rich_style_description(self) -> str:
        return f"""
### Kaggle Agent: Automated Feature Engineering & Model Tuning Evolution

#### [Overview](#_summary)

In this scenario, our automated system proposes hypothesis, choose action, implements code, conducts validation, and utilizes feedback in a continuous, iterative process.

#### Kaggle Competition info

Current Competition: [{self.competition}](https://www.kaggle.com/competitions/{self.competition})

#### [Automated R&D](#_rdloops)

- **[R (Research)](#_research)**
- Iteration of ideas and hypotheses.
- Continuous learning and knowledge construction.

- **[D (Development)](#_development)**
- Evolving code generation, model refinement, and features generation.
- Automated implementation and testing of models/features.

#### [Objective](#_summary)

To automatically optimize performance metrics within the validation set or Kaggle Leaderboard, ultimately discovering the most efficient features and models through autonomous research and development.
"""

    def get_scenario_all_desc(
        self, task: Task | None = None, filtered_tag: str | None = None, simple_background: bool | None = None
    ) -> str:
        def common_description() -> str:
            return f"""\n------Background of the scenario------
{self.background}

------The source dataset you can use to generate the features------
{self.source_data}

------The expected output & submission format specifications------
{self.submission_specifications}
"""

        def interface(tag: str | None) -> str:
            return f"""
------The interface you should follow to write the runnable code------
{self.interface(tag)}
"""

        def output(tag: str | None) -> str:
            return f"""
------The output of your code should be in the format------
{self.output_format(tag)}
"""

        def simulator(tag: str | None) -> str:
            return f"""
------The simulator user can use to test your solution------
{self.simulator(tag)}
"""

        if filtered_tag is None:
            return common_description() + interface(None) + output(None) + simulator(None)
        elif filtered_tag == "hypothesis_and_experiment" or filtered_tag == "feedback":
            return common_description() + simulator(None)
        elif filtered_tag == "feature":
            return common_description() + interface("feature") + output("feature") + simulator("feature")
        else:
            return common_description() + interface(filtered_tag) + output("model") + simulator("model")



================================================
File: rdagent/scenarios/kaggle/experiment/utils.py
================================================
from pathlib import Path

import nbformat as nbf


def python_files_to_notebook(competition: str, py_dir: str):
    py_dir: Path = Path(py_dir)
    save_path: Path = py_dir / "merged.ipynb"

    pre_file = py_dir / "fea_share_preprocess.py"
    pre_py = pre_file.read_text()

    pre_py = pre_py.replace("/kaggle/input", f"/kaggle/input/{competition}")

    fea_files = list(py_dir.glob("feature/*.py"))
    fea_pys = {
        f"{fea_file.stem}_cls": fea_file.read_text().replace("feature_engineering_cls", f"{fea_file.stem}_cls").strip()
        + "()\n"
        for fea_file in fea_files
    }

    model_files = list(py_dir.glob("model/model*.py"))
    model_pys = {f"{model_file.stem}": model_file.read_text().strip() for model_file in model_files}
    for k, v in model_pys.items():
        model_pys[k] = v.replace("def fit(", "def fit(self, ").replace("def predict(", "def predict(self, ")

        lines = model_pys[k].split("\n")
        indent = False
        first_line = -1
        for i, line in enumerate(lines):
            if "def " in line:
                indent = True
                if first_line == -1:
                    first_line = i
            if indent:
                lines[i] = "    " + line
        lines.insert(first_line, f"class {k}:\n")
        model_pys[k] = "\n".join(lines)

    select_files = list(py_dir.glob("model/select*.py"))
    select_pys = {
        f"{select_file.stem}": select_file.read_text().replace("def select(", f"def {select_file.stem}(")
        for select_file in select_files
    }

    train_file = py_dir / "train.py"
    train_py = train_file.read_text()

    train_py = train_py.replace("from fea_share_preprocess import preprocess_script", "")
    train_py = train_py.replace("DIRNAME = Path(__file__).absolute().resolve().parent", "")

    fea_cls_list_str = "[" + ", ".join(list(fea_pys.keys())) + "]"
    train_py = train_py.replace(
        'for f in DIRNAME.glob("feature/feat*.py"):', f"for cls in {fea_cls_list_str}:"
    ).replace("cls = import_module_from_path(f.stem, f).feature_engineering_cls()", "")

    model_cls_list_str = "[" + ", ".join(list(model_pys.keys())) + "]"
    train_py = (
        train_py.replace('for f in DIRNAME.glob("model/model*.py"):', f"for mc in {model_cls_list_str}:")
        .replace("m = import_module_from_path(f.stem, f)", "m = mc()")
        .replace('select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)', "")
        .replace(
            "select_m = import_module_from_path(select_python_path.stem, select_python_path)",
            'select_m = eval(mc.__name__.replace("model", "select"))',
        )
        .replace("select_m.select", "select_m")
        .replace("[2].select", "[2]")
    )

    nb = nbf.v4.new_notebook()
    all_py = ""

    nb.cells.append(nbf.v4.new_code_cell(pre_py))
    all_py += pre_py + "\n\n"

    for v in fea_pys.values():
        nb.cells.append(nbf.v4.new_code_cell(v))
        all_py += v + "\n\n"

    for v in model_pys.values():
        nb.cells.append(nbf.v4.new_code_cell(v))
        all_py += v + "\n\n"

    for v in select_pys.values():
        nb.cells.append(nbf.v4.new_code_cell(v))
        all_py += v + "\n\n"

    nb.cells.append(nbf.v4.new_code_cell(train_py))
    all_py += train_py + "\n"

    with save_path.open("w", encoding="utf-8") as f:
        nbf.write(nb, f)

    with save_path.with_suffix(".py").open("w", encoding="utf-8") as f:
        f.write(all_py)



================================================
File: rdagent/scenarios/kaggle/experiment/workspace.py
================================================
import subprocess
import zipfile
from pathlib import Path
from typing import Any, List, Tuple

import pandas as pd

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.core.experiment import FBWorkspace
from rdagent.log import rdagent_logger as logger
from rdagent.utils.env import KGDockerEnv

KG_FEATURE_PREPROCESS_SCRIPT = """import pickle

from fea_share_preprocess import preprocess_script

X_train, X_valid, y_train, y_valid, X_test, *others = preprocess_script()

pickle.dump(X_train, open("X_train.pkl", "wb"))
pickle.dump(X_valid, open("X_valid.pkl", "wb"))
pickle.dump(y_train, open("y_train.pkl", "wb"))
pickle.dump(y_valid, open("y_valid.pkl", "wb"))
pickle.dump(X_test, open("X_test.pkl", "wb"))
pickle.dump(others, open("others.pkl", "wb"))
"""


class KGFBWorkspace(FBWorkspace):
    def __init__(self, template_folder_path: Path, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.inject_code_from_folder(template_folder_path)
        self.data_description: List[Tuple[str, int]] = []

    @property
    def model_description(self) -> dict[str, str]:
        model_description = {}
        for k, v in self.file_dict.items():
            if k.startswith("model/"):
                model_description[k] = v
        return model_description

    def generate_preprocess_data(
        self,
    ) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.DataFrame, Any]:
        kgde = KGDockerEnv(KAGGLE_IMPLEMENT_SETTING.competition)
        kgde.prepare()

        execute_log, results = kgde.dump_python_code_run_and_get_results(
            code=KG_FEATURE_PREPROCESS_SCRIPT,
            local_path=str(self.workspace_path),
            dump_file_names=[
                "X_train.pkl",
                "X_valid.pkl",
                "y_train.pkl",
                "y_valid.pkl",
                "X_test.pkl",
                "others.pkl",
            ],
            running_extra_volume=(
                {KAGGLE_IMPLEMENT_SETTING.local_data_path + "/" + KAGGLE_IMPLEMENT_SETTING.competition: "/kaggle/input"}
                if KAGGLE_IMPLEMENT_SETTING.competition
                else None
            ),
        )
        if len(results) == 0:
            logger.error("Feature preprocess failed.")
            raise Exception("Feature preprocess failed.")
        else:
            X_train, X_valid, y_train, y_valid, X_test, others = results
            return X_train, X_valid, y_train, y_valid, X_test, *others

    def execute(self, run_env: dict = {}, *args, **kwargs) -> str:
        logger.info(f"Running the experiment in {self.workspace_path}")

        kgde = KGDockerEnv(KAGGLE_IMPLEMENT_SETTING.competition)
        kgde.prepare()

        running_extra_volume = {}
        if KAGGLE_IMPLEMENT_SETTING.competition:
            running_extra_volume = {
                KAGGLE_IMPLEMENT_SETTING.local_data_path + "/" + KAGGLE_IMPLEMENT_SETTING.competition: "/kaggle/input"
            }
        else:
            running_extra_volume = {}

        execute_log = kgde.run(
            local_path=str(self.workspace_path),
            env=run_env,
            running_extra_volume=running_extra_volume,
        )

        csv_path = self.workspace_path / "submission_score.csv"

        if not csv_path.exists():
            logger.error(f"File {csv_path} does not exist.")
            return None
        return pd.read_csv(csv_path, index_col=0).iloc[:, 0]



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["PassengerId"], axis=1)

    X = data_df.drop(["Transported"], axis=1)
    y = data_df["Transported"]

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)  # Convert class labels to numeric

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    """
    Fits the preprocessor on the training data and returns the fitted preprocessor.
    """
    # Identify numerical and categorical features
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]
    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == "object"]

    # Define preprocessors for numerical and categorical features
    label_encoders = {col: LabelEncoder().fit(X_train[col]) for col in categorical_cols}

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numerical_transformer, numerical_cols),
        ],
        remainder="passthrough",
    )

    # Fit the preprocessor on the training data
    preprocessor.fit(X_train)

    return preprocessor, label_encoders


def preprocess_transform(X: pd.DataFrame, preprocessor, label_encoders):
    """
    Transforms the given DataFrame using the fitted preprocessor.
    Ensures the processed data has consistent features across train, validation, and test sets.
    """
    # Encode categorical features
    for col, le in label_encoders.items():
        # Handle unseen labels by setting them to a default value (e.g., -1)
        X[col] = X[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

    # Transform the data using the fitted preprocessor
    X_array = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_array, columns=X.columns, index=X.index)

    return X_transformed


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        y_train = pd.Series(y_train).reset_index(drop=True)
        y_valid = pd.Series(y_valid).reset_index(drop=True)

        return X_train, X_valid, y_train, y_valid, X_test, *others
    X_train, X_valid, y_train, y_valid = prepreprocess()
    y_train = pd.Series(y_train).reset_index(drop=True)
    y_valid = pd.Series(y_valid).reset_index(drop=True)

    # Fit the preprocessor on the training data
    preprocessor, label_encoders = preprocess_fit(X_train)

    # Preprocess the train, validation, and test data
    X_train = preprocess_transform(X_train, preprocessor, label_encoders)
    X_valid = preprocess_transform(X_valid, preprocessor, label_encoders)

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    passenger_ids = submission_df["PassengerId"]
    submission_df = submission_df.drop(["PassengerId"], axis=1)
    X_test = preprocess_transform(submission_df, preprocessor, label_encoders)

    return X_train, X_valid, y_train, y_valid, X_test, passenger_ids



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import accuracy_score

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute accuracy metric for classification."""
    accuracy = accuracy_score(y_true, y_pred)
    return accuracy


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, passenger_ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]


# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
# metrics_all = []
# for model, predict_func, select_m in model_l:
#     X_valid_selected = select_m.select(X_valid.copy())
#     y_valid_pred = predict_func(model, X_valid_selected)
#     y_valid_pred = (y_valid_pred > 0.5).astype(int)
#     metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
#     print(f"Accuracy on valid set: {metrics}")
#     metrics_all.append(metrics)

# 4) Use grid search to find the best ensemble model
valid_pred_list = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    valid_pred_list.append(y_valid_pred)

metrics_all = []
weight_list = []
searched_set = set()
for i in range(1000):
    weight = np.random.randint(0, high=10, size=(len(valid_pred_list),), dtype="i")
    if str(weight.tolist()) in searched_set or weight.sum() == 0:
        continue
    weight = weight / weight.sum()
    searched_set.add(str(weight.tolist()))
    y_valid_pred = np.zeros_like(valid_pred_list[0])
    for j in range(len(valid_pred_list)):
        y_valid_pred += valid_pred_list[j] * weight[j]
    y_valid_pred = (y_valid_pred > 0.5).astype(int)
    metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
    metrics_all.append(metrics)
    weight_list.append(weight)


# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["MCC"]).to_csv("submission_score.csv")
print(f"Accuracy on valid set: {metrics_all[max_index]}")

# 6) Make predictions on the test set and save them
test_pred_list = []
for model, predict_func, select_m in model_l:
    X_test_selected = select_m.select(X_test.copy())
    y_test_pred = predict_func(model, X_test_selected)
    test_pred_list.append(y_test_pred)
y_test_pred = np.zeros_like(test_pred_list[0])
for j in range(len(test_pred_list)):
    y_test_pred += test_pred_list[j] * weight_list[max_index][j]
y_test_pred = (y_test_pred > 0.5).astype(bool)
y_test_pred = y_test_pred.ravel()

submission_result = pd.DataFrame({"PassengerId": passenger_ids, "Transported": y_test_pred})

# 8) Submit predictions for the test set
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred_prob = model.predict_proba(X)[:, 1]

    # Apply threshold to get boolean predictions
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/spaceship-titanic_template/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/covid19-global-forecasting-week-1/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


def prepreprocess():
    # Load the data
    train = pd.read_csv("/kaggle/input/train.csv")
    test = pd.read_csv("/kaggle/input/test.csv")

    # Combine train and test for preprocessing
    all_data = pd.concat([train, test], sort=False)

    # Convert date to datetime
    all_data["Date"] = pd.to_datetime(all_data["Date"])

    # Create new features
    all_data["Day"] = all_data["Date"].dt.day
    all_data["Month"] = all_data["Date"].dt.month
    all_data["Year"] = all_data["Date"].dt.year

    # Encode categorical variables
    le = LabelEncoder()
    all_data["Country/Region"] = le.fit_transform(all_data["Country/Region"])
    all_data["Province/State"] = le.fit_transform(all_data["Province/State"].fillna("None"))

    # Split back into train and test
    train = all_data[all_data["ForecastId"].isna()]
    test = all_data[all_data["ForecastId"].notna()]

    # Prepare features and targets
    features = ["Country/Region", "Province/State", "Day", "Month", "Year"]
    X = train[features]
    y = train[["ConfirmedCases", "Fatalities"]]

    # Split into train and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

    return X_train, X_valid, y_train, y_valid, test[features], test["ForecastId"]


def preprocess_script():
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        forecast_ids = pd.read_pickle("/kaggle/input/forecast_ids.pkl")
    else:
        X_train, X_valid, y_train, y_valid, X_test, forecast_ids = prepreprocess()

        # Save preprocessed data
        X_train.to_pickle("/kaggle/input/X_train.pkl")
        X_valid.to_pickle("/kaggle/input/X_valid.pkl")
        y_train.to_pickle("/kaggle/input/y_train.pkl")
        y_valid.to_pickle("/kaggle/input/y_valid.pkl")
        X_test.to_pickle("/kaggle/input/X_test.pkl")
        forecast_ids.to_pickle("/kaggle/input/forecast_ids.pkl")

    return X_train, X_valid, y_train, y_valid, X_test, forecast_ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/covid19-global-forecasting-week-1/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import mean_squared_log_error

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_rmsle(y_true, y_pred):
    """Compute Root Mean Squared Logarithmic Error for regression."""
    return np.sqrt(mean_squared_log_error(y_true, y_pred))


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, forecast_ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]

# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))


# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)

    # Add a small positive value to avoid negative or zero values
    epsilon = 1e-8
    y_valid_cases = np.maximum(y_valid["ConfirmedCases"], epsilon)
    y_pred_cases = np.maximum(y_valid_pred["ConfirmedCases"], epsilon)

    rmsle_cases = compute_rmsle(y_valid_cases, y_pred_cases)
    rmsle_fatalities = compute_rmsle(
        np.maximum(y_valid["Fatalities"], epsilon), np.maximum(y_valid_pred["Fatalities"], epsilon)
    )
    rmsle_avg = (rmsle_cases + rmsle_fatalities) / 2
    print(f"Average RMSLE on valid set: {rmsle_avg}")
    metrics_all.append(rmsle_avg)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["RMSLE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)

# 7) Submit predictions for the test set
submission_result = pd.DataFrame(
    {
        "ForecastId": forecast_ids,
        "ConfirmedCases": y_test_pred["ConfirmedCases"],
        "Fatalities": y_test_pred["Fatalities"],
    }
)
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/covid19-global-forecasting-week-1/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/covid19-global-forecasting-week-1/model/model_xgboost.py
================================================
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model for both ConfirmedCases and Fatalities."""
    models = {}
    for target in ["ConfirmedCases", "Fatalities"]:
        dtrain = xgb.DMatrix(X_train, label=y_train[target])
        dvalid = xgb.DMatrix(X_valid, label=y_valid[target])

        params = {
            "objective": "reg:squarederror",
            "eval_metric": "rmse",
            "nthread": -1,
            "tree_method": "gpu_hist",
            "device": "cuda",
        }
        num_round = 1000

        evallist = [(dtrain, "train"), (dvalid, "eval")]
        models[target] = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=50)

    return models


def predict(models, X):
    """Make predictions for both ConfirmedCases and Fatalities."""
    dtest = xgb.DMatrix(X)
    predictions = {}
    for target, model in models.items():
        predictions[target] = model.predict(dtest)
    return pd.DataFrame(predictions)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/covid19-global-forecasting-week-1/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    # data_df = data_df.drop(["ImageId"], axis=1)

    X = data_df.drop(["label"], axis=1)
    y = data_df["label"]

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid = prepreprocess()

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    # ids = submission_df["ImageId"]
    X_test = submission_df

    X_train = X_train / 255
    X_valid = X_valid / 255
    X_test = X_test / 255

    return X_train, X_valid, y_train, y_valid, X_test


def clean_and_impute_data(X_train, X_valid, X_test):
    """
    Handles inf and -inf values by replacing them with NaN,
    then imputes missing values using the mean strategy.
    Also removes duplicate columns.
    """
    # Impute missing values
    imputer = SimpleImputer(strategy="mean")
    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
    X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

    return X_train, X_valid, X_test



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import clean_and_impute_data, preprocess_script
from sklearn.metrics import accuracy_score

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_metrics_for_classification(y_true, y_pred):
    """Compute accuracy for classification."""
    return accuracy_score(y_true, y_pred)


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train, X_valid, X_test = clean_and_impute_data(X_train, X_valid, X_test)


model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"final accuracy on valid set: {accuracy}")
    metrics_all.append(accuracy)

# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["multi-class accuracy"]).to_csv("submission_score.csv")

# 6) Submit predictions for the test
ids = range(1, len(X_test) + 1)

# TODO: fix selection
print(X_valid_selected.columns)
y_test_pred = model_l[max_index][1](model_l[max_index][0], model_l[max_index][2].select(X_test)).flatten()
submission_result = pd.DataFrame({"ImageId": ids, "Label": y_test_pred})
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/model/model_nn.py
================================================
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Define the neural network model with Batch Normalization
class NeuralNetwork(nn.Module):
    def __init__(self, input_channels, num_classes):
        super(NeuralNetwork, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=30, kernel_size=(3, 3), stride=2)
        self.dropout1 = nn.Dropout(0.5)
        self.conv2 = nn.Conv2d(in_channels=30, out_channels=30, kernel_size=(3, 3), stride=2)
        self.dropout2 = nn.Dropout(0.5)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(30 * 6 * 6, 128)  # Adjust based on your input size
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.dropout1(x)
        x = F.relu(self.conv2(x))
        x = self.dropout2(x)
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = F.softmax(self.fc2(x), dim=1)
        return x


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    # Convert data to PyTorch tensors and reshape it for convolutional layers
    X_train_tensor = (
        torch.tensor(X_train.values, dtype=torch.float32).view(-1, 1, 28, 28).to(device)
    )  # Reshape and move to GPU
    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device)
    X_valid_tensor = torch.tensor(X_valid.values, dtype=torch.float32).view(-1, 1, 28, 28).to(device)
    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.long).to(device)

    # Create datasets and dataloaders
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)

    # Initialize the model, loss function and optimizer
    model = NeuralNetwork(input_channels=1, num_classes=len(set(y_train))).to(device)
    criterion = nn.CrossEntropyLoss().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.0005)

    # Train the model
    num_epochs = 400
    for epoch in range(num_epochs):
        model.train()
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()

        # Validate the model
        model.eval()
        valid_loss = 0
        correct = 0
        with torch.no_grad():
            for X_batch, y_batch in valid_loader:
                outputs = model(X_batch)
                valid_loss += criterion(outputs, y_batch).item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == y_batch).sum().item()

        accuracy = correct / len(valid_loader.dataset)
        print(f"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {accuracy:.4f}")

    return model


def predict(model, X):
    X_tensor = torch.tensor(X.values, dtype=torch.float32).view(-1, 1, 28, 28).to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(X_tensor)
        _, predicted = torch.max(outputs, 1)
    return predicted.cpu().numpy().reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train, y_train, X_valid, y_valid):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "objective": "multi:softmax",
        "eval_metric": "mlogloss",
        "num_class": 10,
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    model = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=10)

    return model


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    return model.predict(dtest).astype(int)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/digit-recognizer/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/fea_share_preprocess.py
================================================
import os
import re

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    def data_cleaner(text):
        text = text.strip()
        text = re.sub(r"\n", "", text)
        text = text.lower()
        return text

    # train
    train = pd.read_csv("/kaggle/input/train.csv")
    test = pd.read_csv("/kaggle/input/test.csv")

    train["full_text"] = train["full_text"].apply(data_cleaner)
    test["full_text"] = test["full_text"].apply(data_cleaner)

    y_train = train[["cohesion", "syntax", "vocabulary", "phraseology", "grammar", "conventions"]]

    X_train = train[["full_text"]]
    X_test = test[["full_text"]]

    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    return X_train, X_valid, y_train, y_valid, X_test



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/train.py
================================================
import importlib.util
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script

DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


def MCRMSE(y_true, y_pred):
    return np.mean(np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0)))


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    print(X_train.head())
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    metrics = MCRMSE(y_valid, y_valid_pred)
    print(f"MCRMSE on valid set: {metrics}")
    metrics_all.append(metrics)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["MCRMSE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = select_m.select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)

# 7) Submit predictions for the test set
submission_result = pd.read_csv("/kaggle/input/sample_submission.csv")
submission_result["cohesion"] = y_test_pred[:, 0]
submission_result["syntax"] = y_test_pred[:, 1]
submission_result["vocabulary"] = y_test_pred[:, 2]
submission_result["phraseology"] = y_test_pred[:, 3]
submission_result["grammar"] = y_test_pred[:, 4]
submission_result["conventions"] = y_test_pred[:, 5]

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/feature/feature.py
================================================
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        self.vectorizer = TfidfVectorizer()
        self.vectorizer.fit(train_df["full_text"])

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        X = self.vectorizer.transform(X["full_text"])
        X = pd.DataFrame.sparse.from_spmatrix(X)
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/model/model_randomforest.py
================================================
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb
from sklearn.multioutput import MultiOutputRegressor


def is_sparse_df(df: pd.DataFrame) -> bool:
    # 检查 DataFrame 中的每一列是否为稀疏类型
    return any(isinstance(dtype, pd.SparseDtype) for dtype in df.dtypes)


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    xgb_estimator = xgb.XGBRegressor(
        n_estimators=500, random_state=0, objective="reg:squarederror", tree_method="hist", device="cuda"
    )

    model = MultiOutputRegressor(xgb_estimator, n_jobs=-1)

    if is_sparse_df(X_train):
        X_train = X_train.sparse.to_coo()

    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    if is_sparse_df(X_test):
        X_test = X_test.sparse.to_coo()
    y_pred = model.predict(X_test)
    return y_pred



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/feedback-prize-english-language-learning/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["Id"], axis=1)

    X = data_df.drop(["Cover_Type"], axis=1)
    y = data_df["Cover_Type"] - 1

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid = prepreprocess()

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    ids = submission_df["Id"]
    X_test = submission_df.drop(["Id"], axis=1)

    return X_train, X_valid, y_train, y_valid, X_test, ids


def clean_and_impute_data(X_train, X_valid, X_test):
    """
    Handles inf and -inf values by replacing them with NaN,
    then imputes missing values using the mean strategy.
    Also removes duplicate columns.
    """
    # Replace inf and -inf with NaN
    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Impute missing values
    imputer = SimpleImputer(strategy="mean")
    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
    X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

    # Remove duplicate columns
    X_train = X_train.loc[:, ~X_train.columns.duplicated()]
    X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
    X_test = X_test.loc[:, ~X_test.columns.duplicated()]

    return X_train, X_valid, X_test



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import clean_and_impute_data, preprocess_script
from sklearn.metrics import accuracy_score, matthews_corrcoef

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_metrics_for_classification(y_true, y_pred):
    """Compute MCC for classification."""
    mcc = matthews_corrcoef(y_true, y_pred)
    return mcc


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train, X_valid, X_test = clean_and_impute_data(X_train, X_valid, X_test)


model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"final accuracy on valid set: {accuracy}")
    metrics_all.append(accuracy)

# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["multi-class accuracy"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[max_index][2].select(X_test.copy())
y_test_pred = model_l[max_index][1](model_l[max_index][0], X_test_selected).flatten() + 1


# 7) Submit predictions for the test set
submission_result = pd.DataFrame(y_test_pred, columns=["Cover_Type"])
submission_result.insert(0, "Id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/train_past.py
================================================
import importlib.util
import random
from collections import defaultdict
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
print("-1")
data_df = pd.read_csv("/kaggle/input/train.csv")
data_df = data_df.drop(["Id"], axis=1)
print("0")
X_train = data_df.drop(["Cover_Type"], axis=1)
y_train = data_df["Cover_Type"] - 1
print("81")
submission_df = pd.read_csv("/kaggle/input/test.csv")
ids = submission_df["Id"]
X_test = submission_df.drop(["Id"], axis=1)


# Store results
accuracies = []
y_test_pred_l = []
scaler = StandardScaler()

print("12")
# 3) Train and evaluate using KFold
fold_number = 1
model_count = defaultdict(int)
print("123")
for train_index, valid_index in kf.split(X_train):
    print(f"Starting fold {fold_number}...")

    X_train_l, X_valid_l, X_test_l = [], [], []  # Reset feature lists for each fold
    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]
    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[valid_index]
    X_te = X_test

    # Feature engineering
    for f in DIRNAME.glob("feature/feat*.py"):
        cls = import_module_from_path(f.stem, f).feature_engineering_cls()
        cls.fit(X_tr)
        X_train_f = cls.transform(X_tr)
        X_valid_f = cls.transform(X_val)
        X_test_f = cls.transform(X_te)

        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

    X_tr = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
    X_val = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
    X_te = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

    print("Shape of X_tr: ", X_tr.shape, " Shape of X_val: ", X_val.shape, " Shape of X_te: ", X_te.shape)

    # Replace inf and -inf with NaN
    X_tr.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_val.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_te.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Impute missing values
    imputer = SimpleImputer(strategy="mean")
    X_tr = pd.DataFrame(imputer.fit_transform(X_tr), columns=X_tr.columns)
    X_val = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)
    X_te = pd.DataFrame(imputer.transform(X_te), columns=X_te.columns)

    # Standardize the data
    X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=X_tr.columns)
    X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)
    X_te = pd.DataFrame(scaler.transform(X_te), columns=X_te.columns)

    # Remove duplicate columns
    X_tr = X_tr.loc[:, ~X_tr.columns.duplicated()]
    X_val = X_val.loc[:, ~X_val.columns.duplicated()]
    X_te = X_te.loc[:, ~X_te.columns.duplicated()]

    model_l = []  # list[tuple[model, predict_func]]
    for f in DIRNAME.glob("model/model*.py"):
        select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
        select_m = import_module_from_path(select_python_path.stem, select_python_path)
        X_train_selected = select_m.select(X_tr.copy())
        X_valid_selected = select_m.select(X_val.copy())

        m = import_module_from_path(f.stem, f)
        model_l.append((m.fit(X_train_selected, y_tr, X_valid_selected, y_val), m.predict))

    # 4) Evaluate the models on the validation set and choose the best one
    best_accuracy = -1
    best = None
    for model, predict_func in model_l:
        X_valid_selected = select_m.select(X_val.copy())
        y_valid_pred = predict_func(model, X_valid_selected)
        accuracy = accuracy_score(y_val, y_valid_pred)
        print(f"Accuracy on valid set: {accuracy}")

        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best = (model, predict_func)

    model_count[best] += 1
    fold_number += 1

# 5) Save the validation accuracy
final_model = max(model_count, key=model_count.get)
pd.Series(data=best_accuracy, index=["multi-class accuracy"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = select_m.select(X_te.copy())
y_test_pred = final_model[1](final_model[0], X_test_selected).flatten() + 1

submission_result = pd.DataFrame(y_test_pred, columns=["Cover_Type"])
submission_result.insert(0, "Id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=200, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"Validation Accuracy: {accuracy:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "objective": "multi:softmax",  # Use softmax for multi-class classification
        "num_class": len(set(y_train)),  # Number of classes
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest)
    return y_pred.astype(int).reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/forest-cover-type-prediction/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/meta_tpl_deprecated/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, OneHotEncoder


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["id"], axis=1)

    X = data_df.drop(["class"], axis=1)
    y = data_df[["class"]]

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)  # Convert class labels to numeric

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    """
    Fits the preprocessor on the training data and returns the fitted preprocessor.
    """
    # Identify numerical and categorical features
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]
    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == "object"]

    # Define preprocessors for numerical and categorical features
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", categorical_transformer, categorical_cols),
            ("num", numerical_transformer, numerical_cols),
        ]
    )

    # Fit the preprocessor on the training data
    preprocessor.fit(X_train)

    return preprocessor


def preprocess_transform(X: pd.DataFrame, preprocessor):
    """
    Transforms the given DataFrame using the fitted preprocessor.
    Ensures the processed data has consistent features across train, validation, and test sets.
    """
    # Transform the data using the fitted preprocessor
    X_array = preprocessor.transform(X).toarray()

    # Get feature names for the columns in the transformed data
    categorical_cols = [cname for cname in X.columns if X[cname].dtype == "object"]
    feature_names = preprocessor.named_transformers_["cat"]["onehot"].get_feature_names_out(
        categorical_cols
    ).tolist() + [cname for cname in X.columns if X[cname].dtype in ["int64", "float64"]]

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_array, columns=feature_names, index=X.index)

    return X_transformed


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others
    X_train, X_valid, y_train, y_valid = prepreprocess()

    # Fit the preprocessor on the training data
    preprocessor = preprocess_fit(X_train)

    # Preprocess the train, validation, and test data
    X_train = preprocess_transform(X_train, preprocessor)
    X_valid = preprocess_transform(X_valid, preprocessor)

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    passenger_ids = submission_df["id"]
    submission_df = submission_df.drop(["id"], axis=1)
    X_test = preprocess_transform(submission_df, preprocessor)

    return X_train, X_valid, y_train, y_valid, X_test, passenger_ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/meta_tpl_deprecated/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import accuracy_score, matthews_corrcoef

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute accuracy metric for classification."""
    accuracy = accuracy_score(y_true, y_pred)
    return accuracy


def compute_metrics_for_classification(y_true, y_pred):
    """Compute MCC for classification."""
    mcc = matthews_corrcoef(y_true, y_pred)
    return mcc


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
# TODO 如果已经做过数据预处理了，不需要再做了
X_train, X_valid, y_train, y_valid, X_test, passenger_ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    X_train_l.append(X_train_f)
    X_valid_l.append(X_valid_f)
    X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1)
X_valid = pd.concat(X_valid_l, axis=1)
X_test = pd.concat(X_test_l, axis=1)

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]

# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train, y_train, X_valid, y_valid), m.predict))

# 4) Evaluate the model on the validation set
y_valid_pred_l = []
for model, predict_func in model_l:
    y_valid_pred_l.append(predict_func(model, X_valid))

# 5) Ensemble
# TODO: ensemble method in a script
# Average the predictions and apply a threshold to determine class labels
y_valid_pred = np.mean(y_valid_pred_l, axis=0)
y_valid_pred = (y_valid_pred > 0.5).astype(int)

mcc = compute_metrics_for_classification(y_valid, y_valid_pred)
print("Final on validation set: ", mcc)

# 6) Save the validation accuracy
pd.Series(data=[mcc], index=["MCC"]).to_csv("submission_score.csv")

# 7) Make predictions on the test set and save them
y_test_pred_l = []
for m, m_pred in model_l:
    y_test_pred_l.append(m_pred(m, X_test))  # TODO Make this an ensemble. Currently it uses the last prediction

y_test_pred = np.mean(y_test_pred_l, axis=0)
y_test_pred = (y_test_pred > 0.5).astype(int)

y_test_pred_labels = np.where(y_test_pred == 1, "p", "e")  # 将整数转换回 'e' 或 'p'

submission_result = pd.DataFrame({"id": passenger_ids, "class": y_test_pred_labels})

# 8) Submit predictions for the test set
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/meta_tpl_deprecated/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/meta_tpl_deprecated/model/model_nn.py
================================================
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

# Check if a GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Restored three-layer model structure
class FeatureInteractionModel(nn.Module):
    def __init__(self, num_features):
        super(FeatureInteractionModel, self).__init__()
        self.fc1 = nn.Linear(num_features, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.fc2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.fc3 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = torch.sigmoid(self.fc3(x))
        return x


# Training function
def fit(X_train, y_train, X_valid, y_valid):
    num_features = X_train.shape[1]
    model = FeatureInteractionModel(num_features).to(device)
    criterion = nn.BCELoss()  # Binary classification problem
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # Convert to TensorDataset and create DataLoader
    train_dataset = TensorDataset(
        torch.tensor(X_train.to_numpy(), dtype=torch.float32), torch.tensor(y_train.reshape(-1), dtype=torch.float32)
    )
    valid_dataset = TensorDataset(
        torch.tensor(X_valid.to_numpy(), dtype=torch.float32), torch.tensor(y_valid.reshape(-1), dtype=torch.float32)
    )
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)

    # Train the model
    model.train()
    for epoch in range(5):
        print(f"Epoch {epoch + 1}/5")
        epoch_loss = 0
        for X_batch, y_batch in tqdm(train_loader, desc="Training", leave=False):
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # Move data to the device
            optimizer.zero_grad()
            outputs = model(X_batch).squeeze(1)  # Reshape outputs to [32]
            loss = criterion(outputs, y_batch)  # Adjust target shape
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        print(f"End of epoch {epoch + 1}, Avg Loss: {epoch_loss / len(train_loader):.4f}")

    return model


# Prediction function
def predict(model, X):
    model.eval()
    predictions = []
    with torch.no_grad():
        X_tensor = torch.tensor(X.values, dtype=torch.float32).to(device)  # Move data to the device
        for i in tqdm(range(0, len(X_tensor), 32), desc="Predicting", leave=False):
            batch = X_tensor[i : i + 32]  # Predict in batches
            pred = model(batch).squeeze().cpu().numpy()  # Move results back to CPU
            predictions.extend(pred)
    return np.array(predictions)  # Return boolean predictions



================================================
File: rdagent/scenarios/kaggle/experiment/templates/meta_tpl_deprecated/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    return X


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=100, random_state=32, n_jobs=-1)

    # Select features (if any feature selection is needed)
    X_train_selected = select(X_train)
    X_valid_selected = select(X_valid)

    # Fit the model
    model.fit(X_train_selected, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid_selected)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"Validation Accuracy: {accuracy:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Select features (if any feature selection is needed)
    X_selected = select(X)

    # Predict using the trained model
    y_pred_prob = model.predict_proba(X_selected)[:, 1]

    # Apply threshold to get boolean predictions
    return y_pred_prob



================================================
File: rdagent/scenarios/kaggle/experiment/templates/meta_tpl_deprecated/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def select(X: pd.DataFrame) -> pd.DataFrame:
    # Ignore feature selection logic
    return X


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    X_train = select(X_train)
    X_valid = select(X_valid)
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    # Parameters for regression
    params = {
        "objective": "reg:squarederror",  # Use squared error for regression
        "nthread": -1,
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    X = select(X)
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

index_name = "key"
label_name = "fare_amount"


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop([index_name], axis=1)

    X = data_df.drop([label_name], axis=1)
    y = data_df[label_name]

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid = prepreprocess()

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    ids = submission_df[index_name]
    X_test = submission_df.drop([index_name], axis=1)

    return X_train, X_valid, y_train, y_valid, X_test, ids


def clean_and_impute_data(X_train, X_valid, X_test):
    """
    Handles inf and -inf values by replacing them with NaN,
    then imputes missing values using the mean strategy.
    Also removes duplicate columns.
    """
    # Replace inf and -inf with NaN
    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Impute missing values
    imputer = SimpleImputer(strategy="mean")
    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
    X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

    # Remove duplicate columns
    X_train = X_train.loc[:, ~X_train.columns.duplicated()]
    X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
    X_test = X_test.loc[:, ~X_test.columns.duplicated()]

    return X_train, X_valid, X_test



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import clean_and_impute_data, preprocess_script
from sklearn.metrics import matthews_corrcoef, root_mean_squared_error

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_metrics_for_classification(y_true, y_pred):
    """Compute MCC for classification."""
    mcc = matthews_corrcoef(y_true, y_pred)
    return mcc


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train)
    X_valid_f = cls.transform(X_valid)
    X_test_f = cls.transform(X_test)

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train, X_valid, X_test = clean_and_impute_data(X_train, X_valid, X_test)


model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    rmse = root_mean_squared_error(y_valid, y_valid_pred)
    print(f"final root mean squared error on valid set: {rmse}")
    metrics_all.append(rmse)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["root mean squared error"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected).flatten() + 1


# 7) Submit predictions for the test set
submission_result = pd.DataFrame(y_test_pred, columns=["fare_amount"])
submission_result.insert(0, "key", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class DatetimeFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        X["pickup_datetime"] = pd.to_datetime(X["pickup_datetime"], format="%Y-%m-%d %H:%M:%S UTC")
        X["hour"] = X.pickup_datetime.dt.hour
        X["day"] = X.pickup_datetime.dt.day
        X["month"] = X.pickup_datetime.dt.month
        X["weekday"] = X.pickup_datetime.dt.weekday
        X["year"] = X.pickup_datetime.dt.year
        X.drop(columns=["pickup_datetime"], inplace=True)
        return X


feature_engineering_cls = DatetimeFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/model/model_linear.py
================================================
"""
Motivation of the model:
The Linear Regression model is chosen for its simplicity and interpretability. It is a good starting point for regression tasks
and provides a baseline to compare more complex models against. Linear Regression assumes a linear relationship between the
features and the target variable, which can be a reasonable assumption for many problems.
"""

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Linear Regression model. Merge feature selection into the pipeline.
    """
    # Initialize the Linear Regression model
    model = LinearRegression()

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    mse = mean_squared_error(y_valid, y_valid_pred)
    print(f"Validation Mean Squared Error: {mse:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/model/select_linear.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/new-york-city-taxi-fare-prediction/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OrdinalEncoder


def prepreprocess():
    # Load the training data
    train_df = pd.read_csv("/kaggle/input/train.csv")

    # Load book and trade data
    book_train = pd.read_parquet("/kaggle/input/book_train.parquet")
    trade_train = pd.read_parquet("/kaggle/input/trade_train.parquet")

    # Merge book and trade data with train_df
    merged_df = pd.merge(train_df, book_train, on=["stock_id", "time_id"], how="left")
    merged_df = pd.merge(merged_df, trade_train, on=["stock_id", "time_id"], how="left")

    # Split the data
    X = merged_df.drop(["target"], axis=1)
    y = merged_df["target"]

    print(X.columns.to_list())

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

    print(X_train.columns.to_list())

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]
    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == "object"]

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("ordinal", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)),
        ]
    )

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numerical_transformer, numerical_cols),
            ("cat", categorical_transformer, categorical_cols),
        ]
    )

    preprocessor.fit(X_train)

    return preprocessor, numerical_cols, categorical_cols


def preprocess_transform(X: pd.DataFrame, preprocessor, numerical_cols, categorical_cols):
    X_transformed = preprocessor.transform(X)

    X_transformed = pd.DataFrame(X_transformed, columns=numerical_cols + categorical_cols, index=X.index)

    return X_transformed


def preprocess_script():
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid = prepreprocess()

    submission_df = pd.read_csv("/kaggle/input/test.csv")

    ids = submission_df["row_id"]
    submission_df = submission_df.drop(["row_id"], axis=1)

    # Add missing columns to submission_df
    for col in X_train.columns:
        if col not in submission_df.columns:
            submission_df[col] = 0  # Fill with 0 or another appropriate value

    # Handle missing values
    for df in [X_train, X_valid, submission_df]:
        df.fillna(df.mean(), inplace=True)

    return X_train, X_valid, y_train, y_valid, submission_df, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.impute import SimpleImputer

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_rmspe(y_true, y_pred):
    """Compute Root Mean Squared Percentage Error (RMSPE) for regression."""
    rmspe = np.sqrt(np.mean(((y_true - y_pred) / y_true) ** 2))
    return rmspe


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()


# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]


# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    metrics = compute_rmspe(y_valid, y_valid_pred.ravel())
    print(f"RMSPE on valid set: {metrics}")
    metrics_all.append(metrics)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["RMSPE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected).ravel()

submission_result = pd.DataFrame({"row_id": ids, "target": y_test_pred})
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/model/model_randomforest.py
================================================
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    mse = mean_squared_error(y_valid, y_valid_pred)
    rmse = np.sqrt(mse)
    print(f"Validation RMSE: {rmse:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/model/model_xgboost.py
================================================
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    # Parameters for regression
    params = {
        "objective": "reg:squarederror",  # Use squared error for regression
        "nthread": -1,
        "tree_method": "hist",
        "device": "cuda",
    }
    num_round = 200

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/optiver-realized-volatility-prediction/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/fea_share_preprocess.py
================================================
import os

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        y_train = pd.Series(y_train).reset_index(drop=True)
        y_valid = pd.Series(y_valid).reset_index(drop=True)

        return X_train, X_valid, y_train, y_valid, X_test, *others

    # train
    train = pd.read_csv("/kaggle/input/train.csv")
    train = train.drop(["id"], axis=1)
    train["store_sqft"] = train["store_sqft"].astype("category")
    train["salad"] = (train["salad_bar"] + train["prepared_food"]) / 2
    train["log_cost"] = np.log1p(train["cost"])
    most_important_features = [
        "total_children",
        "num_children_at_home",
        "avg_cars_at home(approx).1",
        "store_sqft",
        "coffee_bar",
        "video_store",
        "salad",
        "florist",
    ]

    X_train, X_valid, y_train, y_valid = train_test_split(
        train[most_important_features], train["log_cost"], test_size=0.2, random_state=2023
    )
    y_train = pd.Series(y_train).reset_index(drop=True)
    y_valid = pd.Series(y_valid).reset_index(drop=True)

    # test
    test = pd.read_csv("/kaggle/input/test.csv")
    test["store_sqft"] = test["store_sqft"].astype("category")
    test["salad"] = (test["salad_bar"] + test["prepared_food"]) / 2

    ids = test["id"]
    X_test = test.drop(["id"], axis=1)
    X_test = X_test[most_important_features]

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/train.py
================================================
import importlib.util
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import mean_squared_error

DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])


# 3) Train the model
model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    metrics = mean_squared_error(y_valid, y_valid_pred, squared=False)
    print(f"RMLSE on valid set: {metrics}")
    metrics_all.append(metrics)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["RMLSE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)

# 7) Submit predictions for the test set
submission_result = pd.DataFrame(np.expm1(y_test_pred), columns=["cost"])
submission_result.insert(0, "id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/model/model_randomforest.py
================================================
import pandas as pd
from sklearn.ensemble import RandomForestRegressor


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the Random Forest model. Merge feature_select"""
    rf_params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 2,
        "min_samples_leaf": 1,
        "max_features": "sqrt",
        "random_state": 2023,
        "n_jobs": -1,
        "verbose": 1,
    }
    model = RandomForestRegressor(**rf_params)
    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    y_pred = model.predict(X_test)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    xgb_params = {
        "n_estimators": 280,
        "learning_rate": 0.05,
        "max_depth": 10,
        "subsample": 1.0,
        "colsample_bytree": 1.0,
        "tree_method": "hist",
        "enable_categorical": True,
        "verbosity": 1,
        "min_child_weight": 3,
        "base_score": 4.6,
        "random_state": 2023,
    }
    model = xgb.XGBRegressor(**xgb_params)
    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    y_pred = model.predict(X_test)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e11/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/fea_share_preprocess.py
================================================
import os

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        y_train = pd.Series(y_train).reset_index(drop=True)
        y_valid = pd.Series(y_valid).reset_index(drop=True)

        return X_train, X_valid, y_train, y_valid, X_test, *others

    # train
    train = pd.read_csv("/kaggle/input/train.csv")
    X_train, X_valid, y_train, y_valid = train_test_split(
        train.drop(["yield", "id"], axis=1), train["yield"], test_size=0.2, random_state=2023
    )
    y_train = pd.Series(y_train).reset_index(drop=True)
    y_valid = pd.Series(y_valid).reset_index(drop=True)

    # test
    test = pd.read_csv("/kaggle/input/test.csv")

    ids = test["id"]
    X_test = test.drop(["id"], axis=1)

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/train.py
================================================
import importlib.util
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import mean_absolute_error

DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

if len(X_train_l) > 1:
    X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
    X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
    X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])


# 3) Train the model
model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    metrics = mean_absolute_error(y_valid, y_valid_pred)
    print(f"MAE on valid set: {metrics}")
    metrics_all.append(metrics)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["MAE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)

# 7) Submit predictions for the test set
submission_result = pd.DataFrame(y_test_pred, columns=["yield"])
submission_result.insert(0, "id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/model/model_randomforest.py
================================================
import pandas as pd
from sklearn.ensemble import RandomForestRegressor


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the Random Forest model. Merge feature_select"""
    rf_params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 2,
        "min_samples_leaf": 1,
        "max_features": "sqrt",
        "random_state": 2023,
        "n_jobs": -1,
        "verbose": 1,
    }
    model = RandomForestRegressor(**rf_params)
    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    y_pred = model.predict(X_test)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    xgb_params = {
        "n_estimators": 280,
        "learning_rate": 0.05,
        "max_depth": 10,
        "subsample": 1.0,
        "colsample_bytree": 1.0,
        "tree_method": "hist",
        "enable_categorical": True,
        "verbosity": 1,
        "min_child_weight": 3,
        "base_score": 4.6,
        "random_state": 2023,
    }
    model = xgb.XGBRegressor(**xgb_params)
    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    y_pred = model.predict(X_test)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e14/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/fea_share_preprocess.py
================================================
import os

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        y_train = pd.Series(y_train).reset_index(drop=True)
        y_valid = pd.Series(y_valid).reset_index(drop=True)

        return X_train, X_valid, y_train, y_valid, X_test, *others

    # train
    train = pd.read_csv("/kaggle/input/train.csv")

    le = LabelEncoder()
    train["Sex"] = le.fit_transform(train["Sex"])

    X_train, X_valid, y_train, y_valid = train_test_split(
        train.drop(["Age", "id"], axis=1), train["Age"], test_size=0.2, random_state=2023
    )
    y_train = pd.Series(y_train).reset_index(drop=True)
    y_valid = pd.Series(y_valid).reset_index(drop=True)

    # test
    test = pd.read_csv("/kaggle/input/test.csv")

    test["Sex"] = le.transform(test["Sex"])
    ids = test["id"]

    X_test = test.drop(["id"], axis=1)

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/train.py
================================================
import importlib.util
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import mean_absolute_error

DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

if len(X_train_l) > 1:
    X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
    X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
    X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])


# 3) Train the model
model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    metrics = mean_absolute_error(y_valid, y_valid_pred)
    print(f"MAE on valid set: {metrics}")
    metrics_all.append(metrics)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["MAE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)

# 7) Submit predictions for the test set
submission_result = pd.DataFrame(np.round(y_test_pred).astype(int), columns=["Age"])
submission_result.insert(0, "id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/model/model_randomforest.py
================================================
import pandas as pd
from sklearn.ensemble import RandomForestRegressor


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the Random Forest model. Merge feature_select"""
    rf_params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 2,
        "min_samples_leaf": 1,
        "max_features": "sqrt",
        "random_state": 2023,
        "n_jobs": -1,
        "verbose": 1,
    }
    model = RandomForestRegressor(**rf_params)
    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    y_pred = model.predict(X_test)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    xgb_params = {
        "n_estimators": 280,
        "learning_rate": 0.05,
        "max_depth": 10,
        "subsample": 1.0,
        "colsample_bytree": 1.0,
        "tree_method": "hist",
        "enable_categorical": True,
        "verbosity": 1,
        "min_child_weight": 3,
        "base_score": 4.6,
        "random_state": 2023,
    }
    model = xgb.XGBRegressor(**xgb_params)
    model.fit(X_train, y_train)
    return model


def predict(model, X_test):
    """
    Keep feature select's consistency.
    """
    y_pred = model.predict(X_test)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e16/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    train = pd.read_csv("/kaggle/input/train.csv")
    # train = train.drop(["Descript", "Resolution", "Address"], axis=1)

    test = pd.read_csv("/kaggle/input/test.csv")
    test_ids = test["id"]
    # test = test.drop(["Address"], axis=1)

    # Encoding 'PdDistrict'
    categorical_cols = ["Drug", "Sex", "Ascites", "Hepatomegaly", "Spiders", "Edema"]
    encoders = {col: LabelEncoder().fit(train[col]) for col in categorical_cols}

    for col, encoder in encoders.items():
        train[col] = encoder.transform(train[col])
        test[col] = encoder.transform(test[col])

    # Encoding 'Stage' in train set
    status_encoder = LabelEncoder()
    train["StatusEncoded"] = status_encoder.fit_transform(train["Status"])

    # Selecting feature columns for modeling
    x_cols = train.columns.drop(["id", "Status", "StatusEncoded"])
    X = train[x_cols]
    y = train["StatusEncoded"]
    X_test = test.drop(["id"], axis=1)

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)
    print(X.shape, y.shape, X_test.shape)

    return X_train, X_valid, y_train, y_valid, X_test, status_encoder, test_ids


def preprocess_fit(X_train: pd.DataFrame):
    """
    Fits the preprocessor on the training data and returns the fitted preprocessor.
    """
    # Identify numerical features
    numerical_cols = X_train.columns  # All columns are numerical

    # Define preprocessor for numerical features
    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    # Combine preprocessing steps
    preprocessor = ColumnTransformer(transformers=[("num", numerical_transformer, numerical_cols)])

    # Fit the preprocessor on the training data
    preprocessor.fit(X_train)

    return preprocessor


def preprocess_transform(X: pd.DataFrame, preprocessor):
    """
    Transforms the given DataFrame using the fitted preprocessor.
    """
    # Transform the data using the fitted preprocessor
    X_array = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_array, columns=X.columns, index=X.index)

    return X_transformed


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("X_train.pkl"):
        X_train = pd.read_pickle("X_train.pkl")
        X_valid = pd.read_pickle("X_valid.pkl")
        y_train = pd.read_pickle("y_train.pkl")
        y_valid = pd.read_pickle("y_valid.pkl")
        X_test = pd.read_pickle("X_test.pkl")
        return X_train, X_valid, y_train, y_valid, X_test

    X_train, X_valid, y_train, y_valid, test, status_encoder, test_ids = prepreprocess()

    # Fit the preprocessor on the training data
    preprocessor = preprocess_fit(X_train)

    # Preprocess the train and validation data
    X_train = preprocess_transform(X_train, preprocessor)
    X_valid = preprocess_transform(X_valid, preprocessor)

    # Preprocess the test data
    X_test = preprocess_transform(test, preprocessor)

    return X_train, X_valid, y_train, y_valid, X_test, status_encoder, test_ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import log_loss

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# Support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute log loss for classification."""
    all_classes = np.unique(y_true)
    logloss = log_loss(y_true, y_pred, labels=all_classes)
    return logloss


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, status_encoder, test_ids = preprocess_script()


# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1)
X_valid = pd.concat(X_valid_l, axis=1)
X_test = pd.concat(X_test_l, axis=1)

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]


# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    logloss = compute_metrics_for_classification(y_valid, y_valid_pred)
    print(f"log_loss on valid set: {logloss}")
    metrics_all.append(logloss)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["log_loss"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)

class_labels = ["Status_" + label for label in status_encoder.classes_]

submission_result = pd.DataFrame(y_test_pred, columns=class_labels)
submission_result.insert(0, "id", test_ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred_prob = model.predict_proba(X)

    # Apply threshold to get boolean predictions
    return y_pred_prob



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import numpy as np
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)
    num_classes = len(np.unique(y_train))

    # TODO: for quick running....
    params = {
        "objective": "multi:softprob",
        "num_class": num_classes,
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s3e26/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline


def prepreprocess():
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["id"], axis=1)

    X = data_df.drop(["FloodProbability"], axis=1)
    y = data_df["FloodProbability"]

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numerical_transformer, numerical_cols),
        ]
    )

    preprocessor.fit(X_train)

    return preprocessor, numerical_cols


def preprocess_transform(X: pd.DataFrame, preprocessor, numerical_cols):
    X_transformed = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_transformed, columns=numerical_cols, index=X.index)

    return X_transformed


def preprocess_script():
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid = prepreprocess()

    preprocessor, numerical_cols = preprocess_fit(X_train)

    X_train = preprocess_transform(X_train, preprocessor, numerical_cols)
    X_valid = preprocess_transform(X_valid, preprocessor, numerical_cols)

    submission_df = pd.read_csv("/kaggle/input/test.csv")
    ids = submission_df["id"]
    submission_df = submission_df.drop(["id"], axis=1)
    X_test = preprocess_transform(submission_df, preprocessor, numerical_cols)

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import r2_score

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_r2(y_true, y_pred):
    """Compute R² score for regression."""
    return r2_score(y_true, y_pred)


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)


# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_name = f.stem
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m, model_name))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m, model_name in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    r2 = compute_r2(y_valid, y_valid_pred)
    print(f"R2 on valid set for {model_name}: {r2}")
    metrics_all.append(r2)

# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["R2"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[max_index][2].select(X_test.copy())
y_test_pred = model_l[max_index][1](model_l[max_index][0], X_test_selected).ravel()

# 7) Submit predictions for the test set
submission_result = pd.DataFrame({"id": ids, "FloodProbability": y_test_pred})
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/model/model_randomforest.py
================================================
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    mse = mean_squared_error(y_valid, y_valid_pred)
    rmse = np.sqrt(mse)
    print(f"Validation RMSE: {rmse:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/model/model_xgboost.py
================================================
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    # Parameters for regression
    params = {
        "objective": "reg:squarederror",  # Use squared error for regression
        "nthread": -1,
        "n_estimators": 8000,
        "tree_method": "gpu_hist",
        "device": "cuda",
        "max_depth": 10,
        "learning_rate": 0.01,
    }
    num_round = 5000

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e5/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["id"], axis=1)

    X = data_df.drop(["class"], axis=1)
    y = data_df[["class"]]

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)  # Convert class labels to numeric

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    """
    Fits the preprocessor on the training data and returns the fitted preprocessor.
    """
    # Identify numerical and categorical features
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]
    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == "object"]

    # Define preprocessors for numerical and categorical features
    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("ordinal", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)),
        ]
    )

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numerical_transformer, numerical_cols),
            ("cat", categorical_transformer, categorical_cols),
        ]
    )

    # Fit the preprocessor on the training data
    preprocessor.fit(X_train)

    return preprocessor, numerical_cols, categorical_cols


def preprocess_transform(X: pd.DataFrame, preprocessor, numerical_cols, categorical_cols):
    X_transformed = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_transformed, columns=numerical_cols + categorical_cols, index=X.index)

    return X_transformed


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        y_train = pd.Series(y_train).reset_index(drop=True)
        y_valid = pd.Series(y_valid).reset_index(drop=True)

        return X_train, X_valid, y_train, y_valid, X_test, *others
    X_train, X_valid, y_train, y_valid = prepreprocess()

    # Fit the preprocessor on the training data
    preprocessor, numerical_cols, categorical_cols = preprocess_fit(X_train)
    y_train = pd.Series(y_train).reset_index(drop=True)
    y_valid = pd.Series(y_valid).reset_index(drop=True)

    # Preprocess the train, validation, and test data
    X_train = preprocess_transform(X_train, preprocessor, numerical_cols, categorical_cols)
    X_valid = preprocess_transform(X_valid, preprocessor, numerical_cols, categorical_cols)

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    ids = submission_df["id"]
    submission_df = submission_df.drop(["id"], axis=1)
    X_test = preprocess_transform(submission_df, preprocessor, numerical_cols, categorical_cols)

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import matthews_corrcoef

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute MCC for classification."""
    mcc = matthews_corrcoef(y_true, y_pred)
    return mcc


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]

# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    y_valid_pred = (y_valid_pred > 0.5).astype(int)
    metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
    print("MCC on validation set: ", metrics)
    metrics_all.append(metrics)

# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["MCC"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[max_index][2].select(X_test.copy())
y_test_pred = model_l[max_index][1](model_l[max_index][0], X_test_selected)
y_test_pred = (y_test_pred > 0.5).astype(int)

y_test_pred_labels = np.where(y_test_pred == 1, "p", "e")  # 将整数转换回 'e' 或 'p'

# 7) Submit predictions for the test set
submission_result = pd.DataFrame({"id": ids, "class": y_test_pred_labels.ravel()})
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=200, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"Validation Accuracy: {accuracy:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred_prob = model.predict_proba(X)[:, 1]

    # Apply threshold to get boolean predictions
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 200

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e8/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OrdinalEncoder


def prepreprocess():
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["id"], axis=1)

    X = data_df.drop(["price"], axis=1)
    y = data_df["price"]

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]
    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == "object"]

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("ordinal", OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)),
        ]
    )

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numerical_transformer, numerical_cols),
            ("cat", categorical_transformer, categorical_cols),
        ]
    )

    preprocessor.fit(X_train)

    return preprocessor, numerical_cols, categorical_cols


def preprocess_transform(X: pd.DataFrame, preprocessor, numerical_cols, categorical_cols):
    X_transformed = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_transformed, columns=numerical_cols + categorical_cols, index=X.index)

    return X_transformed


def preprocess_script():
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid = prepreprocess()

    preprocessor, numerical_cols, categorical_cols = preprocess_fit(X_train)

    X_train = preprocess_transform(X_train, preprocessor, numerical_cols, categorical_cols)
    X_valid = preprocess_transform(X_valid, preprocessor, numerical_cols, categorical_cols)

    submission_df = pd.read_csv("/kaggle/input/test.csv")
    ids = submission_df["id"]
    submission_df = submission_df.drop(["id"], axis=1)
    X_test = preprocess_transform(submission_df, preprocessor, numerical_cols, categorical_cols)

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def compute_rmse(y_true, y_pred):
    """Compute RMSE for regression."""
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    return rmse


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]


# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    rmse = compute_rmse(y_valid, y_valid_pred)
    print(f"RMSE on valid set: {rmse}")
    metrics_all.append(rmse)

# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["RMSE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected).ravel()

# 7) Submit predictions for the test set
submission_result = pd.DataFrame({"id": ids, "price": y_test_pred})
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/model/model_randomforest.py
================================================
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    mse = mean_squared_error(y_valid, y_valid_pred)
    rmse = np.sqrt(mse)
    print(f"Validation RMSE: {rmse:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/model/model_xgboost.py
================================================
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    # Parameters for regression
    params = {
        "objective": "reg:squarederror",  # Use squared error for regression
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 10

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest)
    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/playground-series-s4e9/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    train = pd.read_csv(
        "/kaggle/input/train.csv",
        parse_dates=["Dates"],
        index_col=False,
    )
    train = train.drop(["Descript", "Resolution", "Address"], axis=1)

    test = pd.read_csv(
        "/kaggle/input/test.csv",
        parse_dates=["Dates"],
        index_col=False,
    )
    test_ids = test["Id"]
    test = test.drop(["Address"], axis=1)

    # Feature engineering
    def feature_engineering(data):
        data["Day"] = data["Dates"].dt.day
        data["Month"] = data["Dates"].dt.month
        data["Year"] = data["Dates"].dt.year
        data["Hour"] = data["Dates"].dt.hour
        data["Minute"] = data["Dates"].dt.minute
        data["DayOfWeek"] = data["Dates"].dt.dayofweek
        data["WeekOfYear"] = data["Dates"].dt.isocalendar().week
        return data

    train = feature_engineering(train)
    test = feature_engineering(test)

    # Encoding 'PdDistrict'
    enc = LabelEncoder()
    train["PdDistrict"] = enc.fit_transform(train["PdDistrict"])
    test["PdDistrict"] = enc.transform(test["PdDistrict"])

    # Encoding 'Category' in train set
    category_encoder = LabelEncoder()
    category_encoder.fit(train["Category"])
    train["CategoryEncoded"] = category_encoder.transform(train["Category"])

    # Selecting feature columns for modeling
    x_cols = list(train.columns[2:12].values)
    x_cols.remove("Minute")  # Exclude the 'Minute' column
    X = train[x_cols]
    y = train["CategoryEncoded"]
    X_test = test[x_cols]

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)
    print(X.shape, y.shape, X_test.shape)

    return X_train, X_valid, y_train, y_valid, X_test, category_encoder, test_ids


def preprocess_fit(X_train: pd.DataFrame):
    """
    Fits the preprocessor on the training data and returns the fitted preprocessor.
    """
    # Identify numerical features
    numerical_cols = X_train.columns  # All columns are numerical

    # Define preprocessor for numerical features
    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    # Combine preprocessing steps
    preprocessor = ColumnTransformer(transformers=[("num", numerical_transformer, numerical_cols)])

    # Fit the preprocessor on the training data
    preprocessor.fit(X_train)

    return preprocessor


def preprocess_transform(X: pd.DataFrame, preprocessor):
    """
    Transforms the given DataFrame using the fitted preprocessor.
    """
    # Transform the data using the fitted preprocessor
    X_array = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_array, columns=X.columns, index=X.index)

    return X_transformed


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        return X_train, X_valid, y_train, y_valid, X_test, *others

    X_train, X_valid, y_train, y_valid, test, category_encoder, test_ids = prepreprocess()

    # Fit the preprocessor on the training data
    preprocessor = preprocess_fit(X_train)

    # Preprocess the train and validation data
    X_train = preprocess_transform(X_train, preprocessor)
    X_valid = preprocess_transform(X_valid, preprocessor)

    # Preprocess the test data
    X_test = preprocess_transform(test, preprocessor)

    return X_train, X_valid, y_train, y_valid, X_test, category_encoder, test_ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import log_loss

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# Support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute log loss for classification."""
    all_classes = np.arange(39)
    logloss = log_loss(y_true, y_pred, labels=all_classes)
    return logloss


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, category_encoder, test_ids = preprocess_script()

X_train = X_train.iloc[: X_train.shape[0] // 10]
y_train = y_train.iloc[: y_train.shape[0] // 10]
X_valid = X_valid.iloc[: X_valid.shape[0] // 10]
y_valid = y_valid.iloc[: y_valid.shape[0] // 10]
X_test = X_test.iloc[: X_test.shape[0] // 10]

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]

# 3) Train the model
model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
# metrics_all = []
# for model, predict_func, select_m in model_l:
#     X_valid_selected = select_m.select(X_valid.copy())
#     y_valid_pred = predict_func(model, X_valid_selected)
#     metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
#     print(f"log_loss on valid set: {metrics}")
#     metrics_all.append(metrics)
# 4) Use grid search to find the best ensemble model
valid_pred_list = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    valid_pred_list.append(y_valid_pred)

metrics_all = []
weight_list = []
searched_set = set()
for i in range(100):
    weight = np.random.randint(0, high=10, size=(len(valid_pred_list),), dtype="i")
    if str(weight.tolist()) in searched_set or weight.sum() == 0:
        continue
    weight = weight / weight.sum()
    searched_set.add(str(weight.tolist()))
    y_valid_pred = np.zeros_like(valid_pred_list[0])
    for j in range(len(valid_pred_list)):
        y_valid_pred += valid_pred_list[j] * weight[j]
    # normalize y_valid_pred each row to sum 1
    y_valid_pred = y_valid_pred / y_valid_pred.sum(axis=1)[:, np.newaxis]
    metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
    metrics_all.append(metrics)
    weight_list.append(weight)


# 5) Save the validation accuracy
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["log_loss"]).to_csv("submission_score.csv")
print(f"Accuracy on valid set: {metrics_all[min_index]}")

# 6) Make predictions on the test set and save them
test_pred_list = []
for model, predict_func, select_m in model_l:
    X_test_selected = select_m.select(X_test.copy())
    y_test_pred = predict_func(model, X_test_selected)
    test_pred_list.append(y_test_pred)
y_test_pred = np.zeros_like(test_pred_list[0])
for j in range(len(test_pred_list)):
    y_test_pred += test_pred_list[j] * weight_list[min_index][j]
y_test_pred = y_test_pred / y_test_pred.sum(axis=1)[:, np.newaxis]


# 7) Submit predictions for the test set
class_labels = category_encoder.classes_

submission_result = pd.DataFrame(y_test_pred, columns=class_labels)
submission_result.insert(0, "Id", test_ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=10, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred_prob = model.predict_proba(X)

    # Apply threshold to get boolean predictions
    return y_pred_prob



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import numpy as np
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)
    num_classes = len(np.unique(y_train))

    # TODO: for quick running....
    params = {
        "objective": "multi:softprob",
        "num_class": num_classes,
        "nthread": -1,
        "tree_method": "hist",
        "device": "cuda",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/sf-crime/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["PassengerId"], axis=1)

    X = data_df.drop(["Transported"], axis=1)
    y = data_df["Transported"]

    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)  # Convert class labels to numeric

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_fit(X_train: pd.DataFrame):
    """
    Fits the preprocessor on the training data and returns the fitted preprocessor.
    """
    # Identify numerical and categorical features
    numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ["int64", "float64"]]
    categorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == "object"]

    # Define preprocessors for numerical and categorical features
    label_encoders = {col: LabelEncoder().fit(X_train[col]) for col in categorical_cols}

    numerical_transformer = Pipeline(steps=[("imputer", SimpleImputer(strategy="mean"))])

    # Combine preprocessing steps
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numerical_transformer, numerical_cols),
        ],
        remainder="passthrough",
    )

    # Fit the preprocessor on the training data
    preprocessor.fit(X_train)

    return preprocessor, label_encoders


def preprocess_transform(X: pd.DataFrame, preprocessor, label_encoders):
    """
    Transforms the given DataFrame using the fitted preprocessor.
    Ensures the processed data has consistent features across train, validation, and test sets.
    """
    # Encode categorical features
    for col, le in label_encoders.items():
        # Handle unseen labels by setting them to a default value (e.g., -1)
        X[col] = X[col].apply(lambda x: le.transform([x])[0] if x in le.classes_ else -1)

    # Transform the data using the fitted preprocessor
    X_array = preprocessor.transform(X)

    # Convert arrays back to DataFrames
    X_transformed = pd.DataFrame(X_array, columns=X.columns, index=X.index)

    return X_transformed


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")
        y_train = pd.Series(y_train).reset_index(drop=True)
        y_valid = pd.Series(y_valid).reset_index(drop=True)

        return X_train, X_valid, y_train, y_valid, X_test, *others
    X_train, X_valid, y_train, y_valid = prepreprocess()
    y_train = pd.Series(y_train).reset_index(drop=True)
    y_valid = pd.Series(y_valid).reset_index(drop=True)

    # Fit the preprocessor on the training data
    preprocessor, label_encoders = preprocess_fit(X_train)

    # Preprocess the train, validation, and test data
    X_train = preprocess_transform(X_train, preprocessor, label_encoders)
    X_valid = preprocess_transform(X_valid, preprocessor, label_encoders)

    # Load and preprocess the test data
    submission_df = pd.read_csv("/kaggle/input/test.csv")
    passenger_ids = submission_df["PassengerId"]
    submission_df = submission_df.drop(["PassengerId"], axis=1)
    X_test = preprocess_transform(submission_df, preprocessor, label_encoders)

    return X_train, X_valid, y_train, y_valid, X_test, passenger_ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import accuracy_score

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute accuracy metric for classification."""
    accuracy = accuracy_score(y_true, y_pred)
    return accuracy


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, passenger_ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]


# 3) Train the model
model_l = []  # list[tuple[model, predict_func,]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
# metrics_all = []
# for model, predict_func, select_m in model_l:
#     X_valid_selected = select_m.select(X_valid.copy())
#     y_valid_pred = predict_func(model, X_valid_selected)
#     y_valid_pred = (y_valid_pred > 0.5).astype(int)
#     metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
#     print(f"Accuracy on valid set: {metrics}")
#     metrics_all.append(metrics)

# 4) Use grid search to find the best ensemble model
valid_pred_list = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    valid_pred_list.append(y_valid_pred)

metrics_all = []
weight_list = []
searched_set = set()
for i in range(1000):
    weight = np.random.randint(0, high=10, size=(len(valid_pred_list),), dtype="i")
    if str(weight.tolist()) in searched_set or weight.sum() == 0:
        continue
    weight = weight / weight.sum()
    searched_set.add(str(weight.tolist()))
    y_valid_pred = np.zeros_like(valid_pred_list[0])
    for j in range(len(valid_pred_list)):
        y_valid_pred += valid_pred_list[j] * weight[j]
    y_valid_pred = (y_valid_pred > 0.5).astype(int)
    metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
    metrics_all.append(metrics)
    weight_list.append(weight)


# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["MCC"]).to_csv("submission_score.csv")
print(f"Accuracy on valid set: {metrics_all[max_index]}")

# 6) Make predictions on the test set and save them
test_pred_list = []
for model, predict_func, select_m in model_l:
    X_test_selected = select_m.select(X_test.copy())
    y_test_pred = predict_func(model, X_test_selected)
    test_pred_list.append(y_test_pred)
y_test_pred = np.zeros_like(test_pred_list[0])
for j in range(len(test_pred_list)):
    y_test_pred += test_pred_list[j] * weight_list[max_index][j]
y_test_pred = (y_test_pred > 0.5).astype(bool)
y_test_pred = y_test_pred.ravel()

submission_result = pd.DataFrame({"PassengerId": passenger_ids, "Transported": y_test_pred})

# 8) Submit predictions for the test set
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred_prob = model.predict_proba(X)[:, 1]

    # Apply threshold to get boolean predictions
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "nthread": -1,
        "tree_method": "gpu_hist",
        "device": "cuda",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/spaceship-titanic/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/statoil-iceberg-classifier-challenge/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split


def prepreprocess():
    """
    This method loads the data, processes it, and splits it into train and validation sets.
    """
    # Load the data
    train = pd.read_json("/kaggle/input/train.json")
    train = train.drop(columns=["id"])
    test = pd.read_json("/kaggle/input/test.json")
    test_ids = test["id"]
    test = test.drop(columns=["id"])

    # Process the data
    def process_data(df):
        X = df.copy()
        X["band_1"] = X["band_1"].apply(lambda x: np.array(x).reshape(75, 75))
        X["band_2"] = X["band_2"].apply(lambda x: np.array(x).reshape(75, 75))
        X["band_3"] = (X["band_1"] + X["band_2"]) / 2

        # Extract features
        X["band_1_mean"] = X["band_1"].apply(np.mean)
        X["band_2_mean"] = X["band_2"].apply(np.mean)
        X["band_3_mean"] = X["band_3"].apply(np.mean)
        X["band_1_max"] = X["band_1"].apply(np.max)
        X["band_2_max"] = X["band_2"].apply(np.max)
        X["band_3_max"] = X["band_3"].apply(np.max)

        # Handle missing incidence angles
        X["inc_angle"] = X["inc_angle"].replace("na", np.nan).astype(float)
        X["inc_angle"].fillna(X["inc_angle"].mean(), inplace=True)

        return X

    X_train = process_data(train)
    X_test = process_data(test)

    y_train = X_train["is_iceberg"]
    X_train = X_train.drop(["is_iceberg", "band_1", "band_2", "band_3"], axis=1)
    X_test = X_test.drop(["band_1", "band_2", "band_3"], axis=1)

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)

    return X_train, X_valid, y_train, y_valid, X_test, test_ids


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("X_train.pkl"):
        X_train = pd.read_pickle("X_train.pkl")
        X_valid = pd.read_pickle("X_valid.pkl")
        y_train = pd.read_pickle("y_train.pkl")
        y_valid = pd.read_pickle("y_valid.pkl")
        X_test = pd.read_pickle("X_test.pkl")
        test_ids = pd.read_pickle("test_ids.pkl")
        return X_train, X_valid, y_train, y_valid, X_test, test_ids

    X_train, X_valid, y_train, y_valid, X_test, test_ids = prepreprocess()

    # Save preprocessed data
    X_train.to_pickle("X_train.pkl")
    X_valid.to_pickle("X_valid.pkl")
    y_train.to_pickle("y_train.pkl")
    y_valid.to_pickle("y_valid.pkl")
    X_test.to_pickle("X_test.pkl")
    test_ids.to_pickle("test_ids.pkl")

    return X_train, X_valid, y_train, y_valid, X_test, test_ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/statoil-iceberg-classifier-challenge/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import log_loss

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


# Support various method for metrics calculation
def compute_metrics_for_classification(y_true, y_pred):
    """Compute log loss for classification."""
    return log_loss(y_true, y_pred)


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, test_ids = preprocess_script()


# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train.copy())
    X_valid_f = cls.transform(X_valid.copy())
    X_test_f = cls.transform(X_test.copy())

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)

X_train = pd.concat(X_train_l, axis=1)
X_valid = pd.concat(X_valid_l, axis=1)
X_test = pd.concat(X_test_l, axis=1)


# Handle inf and -inf values
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="mean")

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Remove duplicate columns
X_train = X_train.loc[:, ~X_train.columns.duplicated()]
X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
X_test = X_test.loc[:, ~X_test.columns.duplicated()]

print(X_train.shape, X_valid.shape, X_test.shape)

# 3) Train the model
model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    metrics = compute_metrics_for_classification(y_valid, y_valid_pred)
    print("Metrics: ", metrics)
    metrics_all.append(metrics)

# 5) Save the validation log loss
min_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[min_index]], index=["Log Loss"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[min_index][2].select(X_test.copy())
y_test_pred = model_l[min_index][1](model_l[min_index][0], X_test_selected)


# 7) Submit predictions for the test set
submission_result = pd.DataFrame({"id": test_ids, "is_iceberg": y_test_pred.ravel()})
submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/statoil-iceberg-classifier-challenge/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/statoil-iceberg-classifier-challenge/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import numpy as np
import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "objective": "binary:logistic",
        "eval_metric": "logloss",
        "eta": 0.1,
        "max_depth": 6,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "nthread": -1,
    }
    num_round = 200  # Increase number of rounds

    evallist = [(dtrain, "train"), (dvalid, "eval")]
    bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=50)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred_prob = model.predict(dtest)
    return y_pred_prob.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/statoil-iceberg-classifier-challenge/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/statoil-iceberg-classifier-challenge/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/fea_share_preprocess.py
================================================
import os

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

index_col_name = "key"


def prepreprocess():
    """
    This method loads the data, drops the unnecessary columns, and splits it into train and validation sets.
    """
    # Load and preprocess the data
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["Id"], axis=1)

    X = data_df.drop(["Cover_Type"], axis=1)
    y = data_df["Cover_Type"] - 1

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

    return X_train, X_valid, y_train, y_valid


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    label_encoder = LabelEncoder()
    data_df = pd.read_csv("/kaggle/input/train.csv")
    data_df = data_df.drop(["Id"], axis=1)
    data_df["Cover_Type"] = label_encoder.fit_transform(data_df["Cover_Type"])

    X = data_df.drop(["Cover_Type", "Soil_Type7", "Soil_Type15"], axis=1)
    y = data_df["Cover_Type"]

    # Split the data into training and validation sets
    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

    # Load and preprocess the test data
    test_df = pd.read_csv("/kaggle/input/test.csv")
    ids = test_df["Id"]
    X_test = test_df.drop(["Id", "Soil_Type7", "Soil_Type15"], axis=1)

    return X_train, X_valid, y_train, y_valid, X_test, ids, label_encoder


def clean_and_impute_data(X_train, X_valid, X_test):
    """
    Handles inf and -inf values by replacing them with NaN,
    then imputes missing values using the mean strategy.
    Also removes duplicate columns.
    """
    # Replace inf and -inf with NaN
    X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_valid.replace([np.inf, -np.inf], np.nan, inplace=True)
    X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

    # Impute missing values
    imputer = SimpleImputer(strategy="mean")
    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
    X_valid = pd.DataFrame(imputer.transform(X_valid), columns=X_valid.columns)
    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

    # Remove duplicate columns
    X_train = X_train.loc[:, ~X_train.columns.duplicated()]
    X_valid = X_valid.loc[:, ~X_valid.columns.duplicated()]
    X_test = X_test.loc[:, ~X_test.columns.duplicated()]

    return X_train, X_valid, X_test



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import accuracy_score

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids, label_encoder = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train)
    X_valid_f = cls.transform(X_valid)
    X_test_f = cls.transform(X_test)

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)
        print(f"Feature [{f.stem}] has been added to the feature list")

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])

print(X_train.shape, X_valid.shape, X_test.shape)

# Handle inf and -inf values
# X_train, X_valid, X_test = clean_and_impute_data(X_train, X_valid, X_test)


model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))
    print(f"Model [{f.stem}] has been trained")

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"[{type(model).__name__}] MCC on valid set: {accuracy}")
    metrics_all.append(accuracy)

# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["multi-class accuracy"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[max_index][2].select(X_test.copy())
y_test_pred = label_encoder.inverse_transform(model_l[max_index][1](model_l[max_index][0], X_test_selected))


# 7) Submit predictions for the test set
submission_result = pd.DataFrame(y_test_pred, columns=["Cover_Type"])
submission_result.insert(0, "Id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=200, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    accuracy = accuracy_score(y_valid, y_valid_pred)
    print(f"Validation Accuracy: {accuracy:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame):
    """Define and train the model. Merge feature_select"""
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "objective": "multi:softmax",  # Use softmax for multi-class classification
        "num_class": len(set(y_train)),  # Number of classes
        "nthread": -1,
        "tree_method": "hist",
        "device": "cuda",
        "eval_metric": "merror",
    }
    num_round = 100

    evallist = [(dtrain, "train"), (dvalid, "valid")]
    bst = xgb.train(params, dtrain, num_round, evallist, verbose_eval=10)

    return bst


def predict(model, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest)
    return y_pred.astype(int).reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-dec-2021/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    train_df = pd.read_csv("/kaggle/input/train.csv")
    test_df = pd.read_csv("/kaggle/input/test.csv")

    x = train_df.drop(columns=["target", "id", "f_27"])
    y = train_df["target"]
    scaler = MinMaxScaler()
    x_scaled = pd.DataFrame(scaler.fit_transform(x))

    X_train, X_valid, y_train, y_valid = train_test_split(x_scaled, y, test_size=0.20, random_state=101)

    # Load and preprocess the test data
    ids = test_df["id"]
    X_test = test_df.drop(["id", "f_27"], axis=1)
    X_test = pd.DataFrame(scaler.transform(X_test))

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import roc_auc_score

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train)
    X_valid_f = cls.transform(X_valid)
    X_test_f = cls.transform(X_test)

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)
        print(f"Feature [{f.stem}] has been added to the feature list")

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])


model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m, f.stem))
    print(f"Model [{f.stem}] has been trained")

# 4) Evaluate the model on the validation set
sub_submission = pd.DataFrame(columns=["Model", "score"])
metrics_all = []
for model, predict_func, select_m, model_name in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    auroc = roc_auc_score(y_valid, y_valid_pred)
    print(f"[{type(model).__name__}] AUROC on valid set: {auroc}")
    metrics_all.append(auroc)
    sub_submission = sub_submission._append({"Model": model_name, "score": auroc}, ignore_index=True)
sub_submission.to_csv("sub_submission_score.csv")

# 5) Save the validation accuracy
max_index = np.argmax(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["AUROC"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[max_index][2].select(X_test.copy())
y_test_pred = model_l[max_index][1](model_l[max_index][0], X_test_selected).flatten()


# 7) Submit predictions for the test set
submission_result = pd.DataFrame(y_test_pred, columns=["target"])
submission_result.insert(0, "id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestClassifier(n_estimators=200, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Validate the model
    y_valid_pred = model.predict(X_valid)
    auroc = roc_auc_score(y_valid, y_valid_pred)
    print(f"Validation AUROC: {auroc:.4f}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame) -> xgb.Booster:
    """Define and train the model. Merge feature_select"""
    # 将数据转换为 DMatrix 并指定设备
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "learning_rate": 0.5,
        "max_depth": 10,
        "device": "cuda",
        "tree_method": "hist",
        "objective": "binary:logistic",
        "eval_metric": "auc",
    }
    num_boost_round = 10

    model = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dvalid, "validation")], verbose_eval=100)
    return model


def predict(model: xgb.Booster, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest).reshape(-1, 1)
    return y_pred



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/tabular-playground-series-may-2022/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/fea_share_preprocess.py
================================================
import os

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler


def preprocess_script():
    """
    This method applies the preprocessing steps to the training, validation, and test datasets.
    """
    if os.path.exists("/kaggle/input/X_train.pkl"):
        X_train = pd.read_pickle("/kaggle/input/X_train.pkl")
        X_valid = pd.read_pickle("/kaggle/input/X_valid.pkl")
        y_train = pd.read_pickle("/kaggle/input/y_train.pkl")
        y_valid = pd.read_pickle("/kaggle/input/y_valid.pkl")
        X_test = pd.read_pickle("/kaggle/input/X_test.pkl")
        others = pd.read_pickle("/kaggle/input/others.pkl")

        return X_train, X_valid, y_train, y_valid, X_test, *others

    train_df = pd.read_csv("/kaggle/input/train.csv")
    test_df = pd.read_csv("/kaggle/input/test.csv")

    X = train_df.drop(["pressure", "id"], axis=1)
    y = train_df["pressure"]

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)

    # Load and preprocess the test data
    ids = test_df["id"]
    X_test = test_df.drop(["id"], axis=1)

    return X_train, X_valid, y_train, y_valid, X_test, ids



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/train.py
================================================
import importlib.util
import random
from pathlib import Path

import numpy as np
import pandas as pd
from fea_share_preprocess import preprocess_script
from sklearn.metrics import mean_absolute_error

# Set random seed for reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
DIRNAME = Path(__file__).absolute().resolve().parent


def import_module_from_path(module_name, module_path):
    spec = importlib.util.spec_from_file_location(module_name, module_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module


# 1) Preprocess the data
X_train, X_valid, y_train, y_valid, X_test, ids = preprocess_script()
mask = X_valid["u_out"] == 0

# 2) Auto feature engineering
X_train_l, X_valid_l = [], []
X_test_l = []

for f in DIRNAME.glob("feature/feat*.py"):
    cls = import_module_from_path(f.stem, f).feature_engineering_cls()
    cls.fit(X_train)
    X_train_f = cls.transform(X_train)
    X_valid_f = cls.transform(X_valid)
    X_test_f = cls.transform(X_test)

    if X_train_f.shape[-1] == X_valid_f.shape[-1] and X_train_f.shape[-1] == X_test_f.shape[-1]:
        X_train_l.append(X_train_f)
        X_valid_l.append(X_valid_f)
        X_test_l.append(X_test_f)
        print(f"Feature [{f.stem}] has been added to the feature list")

X_train = pd.concat(X_train_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_train_l))])
X_valid = pd.concat(X_valid_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_valid_l))])
X_test = pd.concat(X_test_l, axis=1, keys=[f"feature_{i}" for i in range(len(X_test_l))])


model_l = []  # list[tuple[model, predict_func]]
for f in DIRNAME.glob("model/model*.py"):
    select_python_path = f.with_name(f.stem.replace("model", "select") + f.suffix)
    select_m = import_module_from_path(select_python_path.stem, select_python_path)
    X_train_selected = select_m.select(X_train.copy())
    X_valid_selected = select_m.select(X_valid.copy())

    m = import_module_from_path(f.stem, f)
    model_l.append((m.fit(X_train_selected, y_train, X_valid_selected, y_valid), m.predict, select_m))
    print(f"Model [{f.stem}] has been trained")

# 4) Evaluate the model on the validation set
metrics_all = []
for model, predict_func, select_m in model_l:
    X_valid_selected = select_m.select(X_valid.copy())
    y_valid_pred = predict_func(model, X_valid_selected)
    y_valid_filtered = y_valid[mask]
    y_valid_pred_filtered = y_valid_pred[mask]
    mae = mean_absolute_error(y_valid_filtered, y_valid_pred_filtered)
    print(f"[{type(model).__name__}] MAE on valid set: {mae}")
    metrics_all.append(mae)

# 5) Save the validation accuracy
max_index = np.argmin(metrics_all)
pd.Series(data=[metrics_all[max_index]], index=["MAE"]).to_csv("submission_score.csv")

# 6) Make predictions on the test set and save them
X_test_selected = model_l[max_index][2].select(X_test.copy())
y_test_pred = model_l[max_index][1](model_l[max_index][0], X_test_selected).flatten() + 1


# 7) Submit predictions for the test set
submission_result = pd.DataFrame(y_test_pred, columns=["pressure"])
submission_result.insert(0, "id", ids)

submission_result.to_csv("submission.csv", index=False)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/feature/feature.py
================================================
import pandas as pd

"""
Here is the feature engineering code for each task, with a class that has a fit and transform method.
Remember
"""


class IdentityFeature:
    def fit(self, train_df: pd.DataFrame):
        """
        Fit the feature engineering model to the training data.
        """
        pass

    def transform(self, X: pd.DataFrame):
        """
        Transform the input data.
        """
        return X


feature_engineering_cls = IdentityFeature



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/model/model_randomforest.py
================================================
"""
Motivation of the model:
The Random Forest model is chosen for its robustness and ability to handle large datasets with higher dimensionality.
It reduces overfitting by averaging multiple decision trees and typically performs well out of the box, making it a good
baseline model for many classification tasks.
"""

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error


def fit(X_train: pd.DataFrame, y_train: pd.Series, X_valid: pd.DataFrame, y_valid: pd.Series):
    """
    Define and train the Random Forest model. Merge feature selection into the pipeline.
    """
    # Initialize the Random Forest model
    model = RandomForestRegressor(n_estimators=100, random_state=32, n_jobs=-1)

    # Fit the model
    model.fit(X_train, y_train)

    # Predict on the validation set
    y_valid_pred = model.predict(X_valid)

    # Calculate the mean absolute error on the validation set
    mae = mean_absolute_error(y_valid, y_valid_pred)
    print(f"Validation MAE of RandomForestRegressor: {mae}")

    return model


def predict(model, X):
    """
    Keep feature selection's consistency and make predictions.
    """
    # Predict using the trained model
    y_pred = model.predict(X)

    return y_pred.reshape(-1, 1)



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/model/model_xgboost.py
================================================
"""
motivation  of the model
"""

import pandas as pd
import xgboost as xgb


def fit(X_train: pd.DataFrame, y_train: pd.DataFrame, X_valid: pd.DataFrame, y_valid: pd.DataFrame) -> xgb.Booster:
    """Define and train the model. Merge feature_select"""
    # 将数据转换为 DMatrix 并指定设备
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dvalid = xgb.DMatrix(X_valid, label=y_valid)

    params = {
        "learning_rate": 0.1,
        "subsample": 0.95,
        "colsample_bytree": 0.11,
        "max_depth": 2,
        "booster": "gbtree",
        "reg_lambda": 66.1,
        "reg_alpha": 15.9,
        "random_state": 42,
        "tree_method": "hist",
        "device": "cuda",
        "eval_metric": "mae",
    }
    num_boost_round = 1000

    model = xgb.train(params, dtrain, num_boost_round=num_boost_round, evals=[(dvalid, "validation")], verbose_eval=100)
    return model


def predict(model: xgb.Booster, X):
    """
    Keep feature select's consistency.
    """
    dtest = xgb.DMatrix(X)
    y_pred = model.predict(dtest)
    return y_pred



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/model/select_lightgbm.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/model/select_nn.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/model/select_randomforest.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/experiment/templates/ventilator-pressure-prediction/model/select_xgboost.py
================================================
import pandas as pd


def select(X: pd.DataFrame) -> pd.DataFrame:
    """
    Select relevant features. To be used in fit & predict function.
    """
    # For now, we assume all features are relevant. This can be expanded to feature selection logic.
    if X.columns.nlevels == 1:
        return X
    X.columns = ["_".join(str(i) for i in col).strip() for col in X.columns.values]
    return X



================================================
File: rdagent/scenarios/kaggle/knowledge_management/README.md
================================================
## Usage

This folder implements a knowledge base using RAG based on Kaggle competitions. 
It allows you to store Kaggle competition experiences into the knowledge base, as well as store experimental experiences from RD-Agent.

1. First, generate a knowledge base (in JSON format) by running the `main` function in `extract_knowledge.py`.
2. Then, create a vector base in `vector_base.py` and save it.
3. Finally, add the field `KG_RAG_PATH="xxx.pkl"` (the path to the saved vector base) in your `.env` file.


================================================
File: rdagent/scenarios/kaggle/knowledge_management/extract_knowledge.py
================================================
import json
import os
from pathlib import Path

from jinja2 import Environment, StrictUndefined

from rdagent.core.prompts import Prompts
from rdagent.oai.llm_utils import APIBackend

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


def extract_knowledge_from_high_score_answers(content: str):
    sys_prompt = (
        Environment(undefined=StrictUndefined)
        .from_string(prompt_dict["extract_kaggle_knowledge_prompts"]["system"])
        .render()
    )

    user_prompt = (
        Environment(undefined=StrictUndefined)
        .from_string(prompt_dict["extract_kaggle_knowledge_prompts"]["user"])
        .render(file_content=content)
    )

    response_analysis = APIBackend().build_messages_and_create_chat_completion(
        user_prompt=user_prompt,
        system_prompt=sys_prompt,
        json_mode=True,
    )

    try:
        response_json_analysis = json.loads(response_analysis)
    except json.JSONDecodeError:
        response_json_analysis = {"error": "Failed to parse LLM's response as JSON"}

    return response_json_analysis


def extract_knowledge_from_feedback(feedback_response: dict) -> dict:
    """
    Extracts knowledge from LLM-generated feedback and structures it.
    """
    sys_prompt = (
        Environment(undefined=StrictUndefined)
        .from_string(prompt_dict["extract_kaggle_knowledge_from_feedback_prompts"]["system"])
        .render()
    )

    user_prompt = (
        Environment(undefined=StrictUndefined)
        .from_string(prompt_dict["extract_kaggle_knowledge_from_feedback_prompts"]["user"])
        .render(experiment_strategy=feedback_response)
    )

    response_analysis = APIBackend().build_messages_and_create_chat_completion(
        user_prompt=user_prompt,
        system_prompt=sys_prompt,
        json_mode=True,
    )

    try:
        response_json_analysis = json.loads(response_analysis)
    except json.JSONDecodeError:
        response_json_analysis = {"error": "Failed to parse LLM's response as JSON"}

    return response_json_analysis


def process_all_case_files(directory_path: str):
    output_file = Path(directory_path) / "kaggle_experience_results.json"
    json_output = []

    for file_path in Path(directory_path).rglob("*.case"):
        with open(file_path, "r", encoding="utf-8") as file:
            content = file.read()
            knowledge = extract_knowledge_from_high_score_answers(content)
            json_output.append(knowledge)

    with open(output_file, "w", encoding="utf-8") as json_file:
        json.dump(json_output, json_file, ensure_ascii=False)


if __name__ == "__main__":
    process_all_case_files(directory_path="git_ignore_folder/data/kaggle")



================================================
File: rdagent/scenarios/kaggle/knowledge_management/graph.py
================================================
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import List

from jinja2 import Environment, StrictUndefined
from tqdm import tqdm

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.components.knowledge_management.graph import (
    UndirectedGraph,
    UndirectedNode,
)
from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.prompts import Prompts
from rdagent.core.utils import multiprocessing_wrapper
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.experiment.scenario import KGScenario

PROMPT_DICT = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class KGKnowledgeGraph(UndirectedGraph):
    def __init__(self, path: str | Path | None, scenario: KGScenario | None) -> None:
        super().__init__(path)
        if path is not None and Path(path).exists():
            self.load()
            self.path = Path(path).parent / (
                datetime.now(timezone.utc).strftime("%Y-%m-%d-%H-%M-%S") + "_kaggle_kb.pkl"
            )
        else:
            documents = []
            print(Path(KAGGLE_IMPLEMENT_SETTING.domain_knowledge_path))
            for file_path in (Path(KAGGLE_IMPLEMENT_SETTING.domain_knowledge_path)).rglob("*.case"):
                with open(file_path, "r") as f:
                    documents.append(f.read())
            self.load_from_documents(documents=documents, scenario=scenario)
            self.dump()

    def add_document(self, document_content: str, scenario: KGScenario | None) -> None:
        self.load_from_documents([document_content], scenario)
        self.dump()  # Each valid experiment will overwrite this file once again.

    def analyze_one_document(self, document_content: str, scenario: KGScenario | None) -> list:
        session_system_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(PROMPT_DICT["extract_knowledge_graph_from_document"]["system"])
            .render(scenario=scenario.get_scenario_all_desc() if scenario is not None else "")
        )

        session = APIBackend().build_chat_session(
            session_system_prompt=session_system_prompt,
        )
        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(PROMPT_DICT["extract_knowledge_graph_from_document"]["user"])
            .render(document_content=document_content)
        )
        knowledge_list = []
        for _ in range(10):
            response = session.build_chat_completion(user_prompt=user_prompt, json_mode=True)
            knowledge = json.loads(response)
            knowledge_list.append(knowledge)
            user_prompt = "Continue from the last step please. Don't extract the same knowledge again."
        return knowledge_list

    def load_from_documents(self, documents: List[str], scenario: KGScenario | None) -> None:
        knowledge_list_list = multiprocessing_wrapper(
            [
                (
                    self.analyze_one_document,
                    (
                        document_content,
                        scenario,
                    ),
                )
                for document_content in documents
            ],
            n=RD_AGENT_SETTINGS.multi_proc_n,
        )
        node_pairs = []
        node_list = []
        for knowledge_list in tqdm(knowledge_list_list):
            for knowledge in knowledge_list:
                if knowledge == {}:
                    break
                competition = knowledge.get("competition", "")

                competition_node = UndirectedNode(
                    content=(
                        "General knowledge not related to any competition"
                        if (competition == "" or competition == "N/A")
                        else competition
                    ),
                    label="competition",
                )
                node_list.append(competition_node)

                for action in ["hypothesis", "experiments", "code", "conclusion"]:
                    if action == "hypothesis":
                        if isinstance(knowledge.get("hypothesis", ""), str) and knowledge.get("hypothesis", "") in [
                            "N/A",
                            "",
                        ]:
                            break
                        label = knowledge[action]["type"]
                    else:
                        label = action
                    content = str(knowledge.get(action, ""))
                    if content == "" or content == "N/A":
                        continue
                    node = UndirectedNode(content=content, label=label)
                    node_list.append(node)
                    node_pairs.append((node, competition_node))

        node_list = self.batch_embedding(node_list)
        for node_pair in node_pairs:
            self.add_node(node_pair[0], node_pair[1])


if __name__ == "__main__":
    graph = KGKnowledgeGraph(path="git_ignore_folder/kg_graph.pkl", scenario=None)



================================================
File: rdagent/scenarios/kaggle/knowledge_management/prompts.yaml
================================================
extract_kaggle_knowledge_prompts:
  system: |-
    You are a Kaggle competition expert with extensive experience in analyzing high-ranking Kaggle notebooks and competition strategies. 
    Your task is to summarize or infer key information such as the competition name, task type, and specific techniques employed in the notebook or strategy.
    For each provided content, you are expected to extract valuable insights and organize the analysis in the structured format outlined below.
    
    Please provide the analysis in the following JSON format:
    {
      "content": "Put the provided content here",
      "title": "extracted title, if available",
      "competition_name": "extracted competition name",
      "task_category": "extracted task type, e.g., Classification, Regression",
      "field": "field of focus, e.g., Feature Engineering, Modeling",
      "ranking": "extracted ranking, if available",
      "score": "extracted score or metric, if available"
    }
  
  user: |-
    High-ranking Kaggle notebooks or competition strategies: {{ file_content }}

extract_kaggle_knowledge_from_feedback_prompts:
  system: |-
    You are a Kaggle competition expert with extensive experience in analyzing Kaggle notebooks and competition strategies. 
    Your task is to summarize or infer key information such as the competition name, task type, and specific techniques employed in the notebook or strategy.
    For each provided content, you are expected to extract valuable insights and organize the analysis in the structured format outlined below.
    
    Please provide the analysis in the following JSON format:
    {
      "content": "all provided content",
      "title": "extracted title, if available",
      "competition_name": "extracted competition name",
      "task_category": "extracted task type, e.g., Classification, Regression",
      "field": "field of focus, e.g., Feature Engineering, Modeling",
      "ranking": "extracted ranking, if available",
      "score": "extracted score or metric, if available"
    }
  
  user: |-
    Experiment strategy: {{ experiment_strategy }}


extract_knowledge_graph_from_document:
  system: |-
    You are helping the user extract knowledge from a document.
    {% if scenario %}
      The user is working on data science competitions in Kaggle, with the following scenario: {{ scenario }}
    {% else %}
      The user is working on general data science competitions on Kaggle.
    {% endif %}

    The user has identified valuable documents from other experts and requires your help to extract meaningful insights from them.

    Considering each document might contain several valuable insights, you need to extract them one by one and organize them in a structured format.

    You should return a dict containing a single knowledge which includes several fields:
    1. The competition the document is related to.
    2. The hypothesis the document is trying to prove. Containing a type to the hypothesis and very detailed explanation to the hypothesis. The type should be one from ["Feature engineering", "Feature processing", "Model feature selection", "Model tuning"].
    3. Detailed experiments the document has conducted. 
    4. Any related code snippets related to the hypothesis if available.
    5. The conclusion to this knowledge. A bool value indicating whether the hypothesis is proved or not is required. More explainable conclusion is also needed.

    Please provide the analysis in the following JSON format:
    {
      "competition": "(Plain text) extracted competition information, including the competition name, type, description, target, and features (If no specific competition name or other fields are found, leave them blank).", 
      "hypothesis":
        {
          "type": "one of the hypothesis types from ['Feature engineering', 'Feature processing', 'Model feature selection', 'Model tuning']",
          "explanation": "(Plain text) extracted detailed explanation to the hypothesis"
        },
      "experiments": "(Plain text) Detailed descriptions of the experiments conducted in the document, which can be listed in bullet points.",
      "code": "extracted code snippets if available",
      "conclusion": 
        {
          "proved": "bool value indicating whether the hypothesis is proved or not",
          "explanation": "(Plain text) extracted detailed explanation to the conclusion"
        }
    }
    All fields are required so don't miss any key in the schema. The document might not contain all the fields, so you should extract as much information as possible. If a field is not available, please put "N/A" in the field.

    If you find no valuable insights in the document, please return an empty dict.
  
  user: |-
    Document content: {{ document_content }}

refine_with_LLM:
  system: |-
    You are an experienced data science expert and an assistant, helping the user evaluate and improve content.
  
  user: |-
    Here is the target: {{ target }}. 
    Please evaluate whether the following RAG query result aligns with the target. 
    If it does not, simply respond with "There are no relevant RAG results to support."
    RAG query result: {{ text }}.


================================================
File: rdagent/scenarios/kaggle/knowledge_management/vector_base.py
================================================
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Union

import pandas as pd
from jinja2 import Environment, StrictUndefined

from rdagent.components.knowledge_management.vector_base import Document, PDVectorBase
from rdagent.core.prompts import Prompts
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.knowledge_management.extract_knowledge import (
    extract_knowledge_from_feedback,
)


class KGKnowledgeDocument(Document):
    """
    Class for handling Kaggle competition specific metadata
    """

    def __init__(
        self,
        content: str = "",
        label: str = None,
        embedding=None,
        identity=None,
        competition_name=None,
        task_category=None,
        field=None,
        ranking=None,
        score=None,
        entities=None,
        relations=None,
    ):
        """
        Initialize KGKnowledgeMetaData for Kaggle competition posts

        Parameters:
        ----------
        competition_name: str, optional
            The name of the Kaggle competition.
        task_category: str, required
            The type of task (e.g., classification, regression).
        field: str, optional
            The specific field of knowledge (e.g., feature engineering, modeling).
        ranking: str or int, optional
            The ranking achieved in the competition.
        score: float, optional
            The score or metric achieved in the competition.
        entities: list, optional
            Entities related to the content (for knowledge graph integration).
        relations: list, optional
            Relations between entities (for knowledge graph integration).
        """
        super().__init__(content, label, embedding, identity)
        self.competition_name = competition_name
        self.task_category = task_category  # Task type is required
        self.field = field  # Knowledge field, optional (model/data/others/overall)
        self.ranking = ranking  # Ranking
        # TODO ranking and score might be unified
        self.score = score  # Competition score
        # TODO Perhaps this shouldn't be here?
        self.entities = entities or []  # Entities in the knowledge graph
        self.relations = relations or []  # Relations in the knowledge graph

    def split_into_trunk(self, size: int = 1000, overlap: int = 0):
        """
        Split content into trunks and create embeddings by trunk
        #TODO let GPT do the split based on the field of knowledge(data/model/others)
        """

        def split_string_into_chunks(string: str, chunk_size: int):
            chunks = []
            for i in range(0, len(string), chunk_size):
                chunk = string[i : i + chunk_size]
                chunks.append(chunk)
            return chunks

        self.trunks = split_string_into_chunks(self.content, chunk_size=size)
        self.trunks_embedding = APIBackend().create_embedding(input_content=self.trunks)

    def from_dict(self, data: dict):
        """
        Load Kaggle post data from a dictionary
        """
        super().from_dict(data)
        self.competition_name = data.get("competition_name", None)
        self.task_category = data.get("task_category", None)
        self.field = data.get("field", None)
        self.ranking = data.get("ranking", None)
        self.score = data.get("score", None)
        self.entities = data.get("entities", [])
        self.relations = data.get("relations", [])
        return self

    def __repr__(self):
        return (
            f"KGKnowledgeMetaData(id={self.id}, label={self.label}, competition={self.competition_name}, "
            f"task_category={self.task_category}, field={self.field}, ranking={self.ranking}, score={self.score})"
        )


KGDocument = KGKnowledgeDocument


class KaggleExperienceBase(PDVectorBase):
    """
    Class for handling Kaggle competition experience posts and organizing them for reference
    """

    def __init__(self, vector_df_path: Union[str, Path] = None, kaggle_experience_path: Union[str, Path] = None):
        """
        Initialize the KaggleExperienceBase class

        Parameters:
        ----------
        vector_df_path: str or Path, optional
            Path to the vector DataFrame for embedding management.
        kaggle_experience_path: str or Path, optional
            Path to the Kaggle experience post data.
        """
        super().__init__(vector_df_path)
        self.kaggle_experience_path = kaggle_experience_path
        self.kaggle_experience_data = []
        if kaggle_experience_path:
            self.load_kaggle_experience(kaggle_experience_path)

    def add(self, document: Union[KGDocument, List[KGDocument]]):
        document.split_into_trunk()
        docs = [
            {
                "id": document.id,
                "label": document.label,
                "content": document.content,
                "competition_name": document.competition_name,
                "task_category": document.task_category,
                "field": document.field,
                "ranking": document.ranking,
                "score": document.score,
                "embedding": document.embedding,
            }
        ]
        if len(document.trunks) > 1:
            docs.extend(
                [
                    {
                        "id": document.id,
                        "label": document.label,
                        "content": document.content,
                        "competition_name": document.competition_name,
                        "task_category": document.task_category,
                        "field": document.field,
                        "ranking": document.ranking,
                        "score": document.score,
                        "embedding": trunk_embedding,
                    }
                    for trunk, trunk_embedding in zip(document.trunks, document.trunks_embedding)
                ]
            )
        self.vector_df = pd.concat([self.vector_df, pd.DataFrame(docs)], ignore_index=True)

    def load_kaggle_experience(self, kaggle_experience_path: Union[str, Path]):
        """
        Load Kaggle experience posts from a JSON or text file

        Parameters:
        ----------
        kaggle_experience_path: str or Path
            Path to the Kaggle experience post data.
        """
        try:
            with open(kaggle_experience_path, "r", encoding="utf-8") as file:
                self.kaggle_experience_data = json.load(file)
            logger.info(f"Kaggle experience data loaded from {kaggle_experience_path}")
        except FileNotFoundError:
            logger.error(f"Kaggle experience data not found at {kaggle_experience_path}")
            self.kaggle_experience_data = []

    def add_experience_to_vector_base(self, experiment_feedback=None):
        """
        Process Kaggle experience data or experiment feedback and add relevant information to the vector base.

        Args:
            experiment_feedback (dict, optional): A dictionary containing experiment feedback.
                                                If provided, this feedback will be processed and added to the vector base.
        """
        # If experiment feedback is provided, extract relevant knowledge and add it to the vector base
        if experiment_feedback:
            extracted_knowledge = extract_knowledge_from_feedback(experiment_feedback)

            document = KGKnowledgeDocument(
                content=experiment_feedback.get("hypothesis_text", ""),
                label="Experiment Feedback",
                competition_name="Experiment Result",
                task_category=experiment_feedback.get("tasks_factors", "General Task"),
                field="Research Feedback",
                ranking=None,
                score=experiment_feedback.get("current_result", None),
            )
            document.create_embedding()
            self.add(document)
            return

        # Process Kaggle experience data
        logger.info(f"Processing {len(self.kaggle_experience_data)} Kaggle experience posts")
        for experience in self.kaggle_experience_data:
            logger.info(f"Processing experience index: {self.kaggle_experience_data.index(experience)}")
            content = experience.get("content", "")
            label = experience.get("title", "Kaggle Experience")
            competition_name = experience.get("competition_name", "Unknown Competition")
            task_category = experience.get("task_category", "General Task")
            field = experience.get("field", None)
            ranking = experience.get("ranking", None)
            score = experience.get("score", None)

            document = KGKnowledgeDocument(
                content=content,
                label=label,
                competition_name=competition_name,
                task_category=task_category,
                field=field,
                ranking=ranking,
                score=score,
            )
            document.create_embedding()
            self.add(document)

    def search_experience(self, target: str, query: str, topk_k: int = 5, similarity_threshold: float = 0.1):
        """
        Search for Kaggle experience posts related to the query, initially filtered by the target.

        Parameters:
        ----------
        target: str
            The target context to refine the search query.
        query: str
            The search query to find relevant experience posts.
        topk_k: int, optional
            Number of top similar results to return (default is 5).
        similarity_threshold: float, optional
            The similarity threshold for filtering results (default is 0.1).

        Returns:
        -------
        List[KGKnowledgeMetaData], List[float]:
            A list of the most relevant documents and their similarities.
        """

        # Modify the query to include the target
        modified_query = f"The target is {target}. And I need you to query {query} based on the {target}."

        # First, search based on the modified query
        search_results, similarities = super().search(
            modified_query, topk_k=topk_k, similarity_threshold=similarity_threshold
        )

        # If the results do not match the target well, refine the search using LLM or further adjustment
        kaggle_docs = []
        for result in search_results:
            kg_doc = KGKnowledgeDocument().from_dict(result.__dict__)

            gpt_feedback = self.refine_with_LLM(target, kg_doc)
            if gpt_feedback:
                kg_doc.content = gpt_feedback

            kaggle_docs.append(kg_doc)

        return kaggle_docs, similarities

    def refine_with_LLM(self, target: str, text: str) -> str:
        prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")

        sys_prompt = (
            Environment(undefined=StrictUndefined).from_string(prompt_dict["refine_with_LLM"]["system"]).render()
        )

        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(prompt_dict["refine_with_LLM"]["user"])
            .render(target=target, text=text)
        )

        response = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=sys_prompt,
            json_mode=False,
        )

        return response

    def save(self, vector_df_path: Union[str, Path]):
        """
        Save the vector DataFrame to a file

        Parameters:
        ----------
        vector_df_path: str or Path
            Path to save the vector DataFrame.
        """
        self.vector_df.to_pickle(vector_df_path)
        logger.info(f"Vector DataFrame saved to {vector_df_path}")


if __name__ == "__main__":
    kaggle_base = KaggleExperienceBase(
        kaggle_experience_path="git_ignore_folder/data_minicase/kaggle_experience_results.json"
    )

    kaggle_base.add_experience_to_vector_base()

    kaggle_base.save("git_ignore_folder/vector_base/kaggle_vector_base.pkl")

    print(f"There are {kaggle_base.shape()[0]} records in the vector base.")

    search_results, similarities = kaggle_base.search_experience(query="image classification", topk_k=3)

    for result, similarity in zip(search_results, similarities):
        print(
            f"Competition name: {result.competition_name}, task_category: {result.task_category}, score: {result.score}, similarity: {similarity}"
        )



================================================
File: rdagent/scenarios/kaggle/proposal/proposal.py
================================================
import json
import math
from pathlib import Path
from typing import List, Tuple

from jinja2 import Environment, StrictUndefined

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.components.coder.factor_coder.factor import FactorTask
from rdagent.components.coder.model_coder.model import ModelExperiment, ModelTask
from rdagent.components.knowledge_management.vector_base import VectorBase
from rdagent.components.proposal import (
    FactorAndModelHypothesis2Experiment,
    FactorAndModelHypothesisGen,
)
from rdagent.core.exception import ModelEmptyError
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import Hypothesis, Scenario, Trace
from rdagent.scenarios.kaggle.experiment.kaggle_experiment import (
    KG_MODEL_MAPPING,
    KG_SELECT_MAPPING,
    KGFactorExperiment,
    KGModelExperiment,
)
from rdagent.scenarios.kaggle.experiment.scenario import KGScenario
from rdagent.scenarios.kaggle.knowledge_management.graph import KGKnowledgeGraph
from rdagent.scenarios.kaggle.knowledge_management.vector_base import (
    KaggleExperienceBase,
)

prompt_dict = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")


from rdagent.scenarios.kaggle.experiment.scenario import (
    KG_ACTION_FEATURE_ENGINEERING,
    KG_ACTION_FEATURE_PROCESSING,
    KG_ACTION_LIST,
    KG_ACTION_MODEL_FEATURE_SELECTION,
    KG_ACTION_MODEL_TUNING,
)


class KGHypothesis(Hypothesis):
    def __init__(
        self,
        hypothesis: str,
        reason: str,
        concise_reason: str,
        concise_observation: str,
        concise_justification: str,
        concise_knowledge: str,
        action: str,
    ) -> None:
        super().__init__(
            hypothesis, reason, concise_reason, concise_observation, concise_justification, concise_knowledge
        )
        self.action = action

    def __str__(self) -> str:
        return f"""Chosen Action: {self.action}
Hypothesis: {self.hypothesis}
Reason: {self.reason}
Concise Reason & Knowledge: {self.concise_reason}
Concise Observation: {self.concise_observation}
Concise Justification: {self.concise_justification}
Concise Knowledge: {self.concise_knowledge}
"""


def generate_RAG_content(
    scen: KGScenario,
    trace: Trace,
    hypothesis_and_feedback: str,
    target: str = None,
    chosen_hypothesis: str = None,
    chosen_hypothesis_type: str = None,
) -> str:
    if scen.if_using_vector_rag:
        if scen.mini_case:
            rag_results, _ = scen.vector_base.search_experience(target, hypothesis_and_feedback, topk_k=1)
        else:
            rag_results, _ = scen.vector_base.search_experience(target, hypothesis_and_feedback, topk_k=5)
        return "\n".join([doc.content for doc in rag_results])
    if scen.if_using_graph_rag is False or trace.knowledge_base is None:
        return None
    same_competition_node = trace.knowledge_base.get_node_by_content(trace.scen.get_competition_full_desc())
    if same_competition_node is not None:
        related_hypothesis_nodes = []
        for action in KG_ACTION_LIST:
            related_hypothesis_nodes.extend(
                trace.knowledge_base.get_nodes_within_steps(
                    start_node=same_competition_node,
                    steps=1,
                    constraint_labels=[action],
                )[:1]
            )
    else:
        related_hypothesis_nodes = []
    experiences = []
    for hypothesis_node in related_hypothesis_nodes:
        experience = {"hypothesis": hypothesis_node.content}
        experiment_node_list = trace.knowledge_base.get_nodes_within_steps(
            start_node=hypothesis_node, steps=1, constraint_labels=["experiments"]
        )
        if len(experiment_node_list) > 0:
            experience["experiments"] = experiment_node_list[0].content
        else:
            experience["experiments"] = "No experiment information available."
        conclusion_node_list = trace.knowledge_base.get_nodes_within_steps(
            start_node=hypothesis_node, steps=1, constraint_labels=["conclusion"]
        )
        if len(conclusion_node_list) > 0:
            experience["conclusion"] = conclusion_node_list[0].content
        else:
            experience["conclusion"] = "No conclusion information available."
        experiences.append(experience)

    found_nodes = []
    insights = []
    if chosen_hypothesis is not None:
        similar_nodes = trace.knowledge_base.semantic_search(
            node=chosen_hypothesis,
            topk_k=2,
        )

        for similar_node in similar_nodes:
            hypothesis_nodes = trace.knowledge_base.get_nodes_within_steps(
                start_node=similar_node,
                steps=3,
                constraint_labels=[chosen_hypothesis_type],
            )
            found_nodes.extend(hypothesis_nodes[:5])

        found_nodes = sorted(list(set(found_nodes)), key=lambda x: len(x.content))

        for exp_node in found_nodes[:5]:
            insight = {"experiments": exp_node.content}
            hypothesis_node_list = trace.knowledge_base.get_nodes_within_steps(
                start_node=exp_node, steps=2, constraint_labels=KG_ACTION_LIST
            )
            if len(hypothesis_node_list) > 0:
                insight["hypothesis"] = hypothesis_node_list[0].content
            else:
                insight["hypothesis"] = "No hypothesis information available."
            conclusion_node_list = trace.knowledge_base.get_nodes_within_steps(
                start_node=exp_node, steps=2, constraint_labels=["conclusion"]
            )
            if len(conclusion_node_list) > 0:
                insight["conclusion"] = conclusion_node_list[0].content
            else:
                insight["conclusion"] = "No conclusion information available."
            insights.append(insight)
    else:
        similar_nodes = trace.knowledge_base.semantic_search(
            node=trace.scen.get_competition_full_desc(),
            topk_k=2,
        )

        for similar_node in similar_nodes:
            for hypothesis_type in KG_ACTION_LIST:
                hypothesis_nodes = trace.knowledge_base.get_nodes_within_steps(
                    start_node=similar_node,
                    steps=3,
                    constraint_labels=[hypothesis_type],
                )
                found_nodes.extend(hypothesis_nodes[:2])

        found_nodes = sorted(list(set(found_nodes)), key=lambda x: len(x.content))

        for hypothesis_node in found_nodes[:5]:
            if hypothesis_node in related_hypothesis_nodes:
                continue
            insight = {"hypothesis": hypothesis_node.content}
            experiment_node_list = trace.knowledge_base.get_nodes_within_steps(
                start_node=hypothesis_node, steps=2, constraint_labels=["experiments"]
            )
            if len(experiment_node_list) > 0:
                insight["experiments"] = experiment_node_list[0].content
            else:
                insight["experiments"] = "No experiment information available."
            conclusion_node_list = trace.knowledge_base.get_nodes_within_steps(
                start_node=hypothesis_node, steps=2, constraint_labels=["conclusion"]
            )
            if len(conclusion_node_list) > 0:
                insight["conclusion"] = conclusion_node_list[0].content
            else:
                insight["conclusion"] = "No conclusion information available."
            insights.append(insight)

    RAG_content = (
        Environment(undefined=StrictUndefined)
        .from_string(prompt_dict["KG_hypothesis_gen_RAG"])
        .render(insights=insights, experiences=experiences)
    )
    return RAG_content


class KGHypothesisGen(FactorAndModelHypothesisGen):
    """
    # NOTE: we can share this class across different data mining scenarios
    # It may better to move the class into components folder like `rdagent/components/proposal/model_proposal.py`
    # Here is the use case:

    .. code-block:: python

        class KGHypothesisGen(ModelHypothesisGen):
            prompts: Prompts = a_specific_prompt_dict
    """

    def __init__(self, scen: Scenario) -> Tuple[dict, bool]:
        super().__init__(scen)

    def update_reward_estimates(self, trace: Trace) -> None:
        if len(trace.hist) > 0:
            last_entry = trace.hist[-1]
            last_action = last_entry[0].action
            last_result = last_entry[1].result
            # Extract performance_t
            performance_t = last_result.get("performance", 0.0)
            # Get performance_{t-1}
            if len(trace.hist) > 1:
                prev_entry = trace.hist[-2]
                prev_result = prev_entry[1].result
                performance_t_minus_1 = prev_result.get("performance", 0.0)
            else:
                performance_t_minus_1 = self.scen.initial_performance

            if self.scen.evaluation_metric_direction:
                reward = (performance_t - performance_t_minus_1) / max(performance_t_minus_1, 1e-8)
            else:
                reward = (performance_t_minus_1 - performance_t) / max(performance_t_minus_1, 1e-8)

            reward = (performance_t - performance_t_minus_1) / performance_t_minus_1
            n_o = self.scen.action_counts[last_action]
            mu_o = self.scen.reward_estimates[last_action]
            self.scen.reward_estimates[last_action] += (reward - mu_o) / n_o
        else:
            # First iteration, nothing to update
            pass

    def execute_next_action(self, trace: Trace) -> str:
        actions = list(self.scen.action_counts.keys())
        t = sum(self.scen.action_counts.values()) + 1

        # If any action has not been tried yet, select it
        for action in actions:
            if self.scen.action_counts[action] == 0:
                selected_action = action
                return selected_action

        c = self.scen.confidence_parameter
        ucb_values = {}
        for action in actions:
            mu_o = self.scen.reward_estimates[action]
            n_o = self.scen.action_counts[action]
            ucb = mu_o + c * math.sqrt(math.log(t) / n_o)
            ucb_values[action] = ucb
        # Select action with highest UCB
        selected_action = max(ucb_values, key=ucb_values.get)

        return selected_action

    def prepare_context(self, trace: Trace) -> Tuple[dict, bool]:
        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )

        if self.scen.if_action_choosing_based_on_UCB:
            action = self.execute_next_action(trace)

        hypothesis_specification = f"Hypothesis should avoid being too general and vague, and should be specific and actionable. For example, hypothesis like 'tune a model' is too general, while hypothesis like 'increase the learning rate to 0.1 of the lightgbm model will improve the performance' is specific and actionable."
        if len(trace.hist) > 0:
            sota_features = str(trace.hist[-1][0].based_experiments[-1].experiment_workspace.data_description)
            sota_models = json.dumps(
                trace.hist[-1][0].based_experiments[-1].experiment_workspace.model_description, indent=2
            )
            sota_result = trace.hist[-1][0].based_experiments[-1].result
            hypothesis_specification += f"\nYour hypothesis should based on current SOTA solution. The user will conduct experiments based on the SOTA solution to test whether your hypothesis is right on this specific ecompetition. \n\nSOTA Features: {sota_features}\n\nSOTA Models: {sota_models}\n\nSOTA Result: {sota_result}"
        if self.scen.if_action_choosing_based_on_UCB:
            hypothesis_specification += (
                "\n\nNext experiment action is "
                + action
                + "\nspecification: "
                + prompt_dict["hypothesis_specification"][action]
            )

        context_dict = {
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "RAG": generate_RAG_content(
                scen=self.scen,
                trace=trace,
                hypothesis_and_feedback=hypothesis_and_feedback,
                target=action if self.scen.if_action_choosing_based_on_UCB else None,
            ),
            "hypothesis_output_format": prompt_dict["hypothesis_output_format"],
            "hypothesis_specification": hypothesis_specification,
        }
        return context_dict, True

    def convert_response(self, response: str) -> Hypothesis:
        response_dict = json.loads(response)

        hypothesis = KGHypothesis(
            hypothesis=response_dict.get("hypothesis", "Hypothesis not provided"),
            reason=response_dict.get("reason", "Reason not provided"),
            concise_reason=response_dict.get("concise_reason", "Concise reason not provided"),
            concise_observation=response_dict.get("concise_observation", "Concise observation not provided"),
            concise_justification=response_dict.get("concise_justification", "Concise justification not provided"),
            concise_knowledge=response_dict.get("concise_knowledge", "Concise knowledge not provided"),
            action=response_dict.get("action", "Action not provided"),
        )

        return hypothesis


class KGHypothesis2Experiment(FactorAndModelHypothesis2Experiment):
    def prepare_context(self, hypothesis: Hypothesis, trace: Trace) -> Tuple[dict, bool]:
        scenario = trace.scen.get_scenario_all_desc(filtered_tag="hypothesis_and_experiment")
        assert isinstance(hypothesis, KGHypothesis)
        experiment_output_format = (
            prompt_dict["feature_experiment_output_format"]
            if hypothesis.action in [KG_ACTION_FEATURE_ENGINEERING, KG_ACTION_FEATURE_PROCESSING]
            else prompt_dict["model_experiment_output_format"]
        )
        self.current_action = hypothesis.action

        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )

        experiment_list: List[ModelExperiment] = [t[0] for t in trace.hist]

        model_list = []
        for experiment in experiment_list:
            for sub_task in experiment.sub_tasks:
                model_list.extend(sub_task.get_task_information())

        return {
            "target_hypothesis": str(hypothesis),
            "scenario": scenario,
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "experiment_output_format": experiment_output_format,
            "target_list": model_list,
            "RAG": generate_RAG_content(
                trace.scen,
                trace,
                hypothesis_and_feedback,
                chosen_hypothesis=hypothesis.hypothesis,
                chosen_hypothesis_type=hypothesis.action,
            ),
        }, True

    def convert_feature_experiment(self, response: str, hypothesis: Hypothesis, trace: Trace) -> KGFactorExperiment:
        response_dict = json.loads(response)
        tasks = []

        for factor_name in response_dict:
            description = (response_dict[factor_name].get("description", "Factor description not provided"),)
            formulation = (response_dict[factor_name].get("formulation", "Factor formulation not provided"),)
            variables = (response_dict[factor_name].get("variables", "Variables not provided"),)
            tasks.append(
                FactorTask(
                    factor_name=factor_name,
                    factor_description=description,
                    factor_formulation=formulation,
                    variables=variables,
                    version=2,
                )
            )

        exp = KGFactorExperiment(
            sub_tasks=tasks,
            based_experiments=(
                [KGFactorExperiment(sub_tasks=[], source_feature_size=trace.scen.input_shape[-1])]
                + [t[0] for t in trace.hist if t[1]]
            ),
            hypothesis=hypothesis,
        )
        return exp

    def convert_model_experiment(self, response: str, hypothesis: Hypothesis, trace: Trace) -> KGModelExperiment:
        response_dict = json.loads(response)
        tasks = []
        model_type = response_dict.get("model_type", "Model type not provided")
        if not isinstance(model_type, str) or model_type not in KG_SELECT_MAPPING:
            raise ModelEmptyError(
                f"Invalid model type '{model_type}'. Allowed model types are: {', '.join(KG_SELECT_MAPPING)}."
            )

        based_experiments = [KGModelExperiment(sub_tasks=[], source_feature_size=trace.scen.input_shape[-1])] + [
            t[0] for t in trace.hist if t[1]
        ]
        model_type = response_dict.get("model_type", "Model type not provided")
        if model_type in KG_MODEL_MAPPING:
            base_code = based_experiments[-1].experiment_workspace.file_dict.get(KG_MODEL_MAPPING[model_type], None)
        else:
            base_code = None

        tasks.append(
            ModelTask(
                name=response_dict.get("model_name", "Model name not provided"),
                description=response_dict.get("description", "Description not provided"),
                architecture=response_dict.get("architecture", "Architecture not provided"),
                hyperparameters=response_dict.get("hyperparameters", "Hyperparameters not provided"),
                model_type=model_type,
                version=2,
                base_code=base_code,
            )
        )
        exp = KGModelExperiment(
            sub_tasks=tasks,
            based_experiments=based_experiments,
            hypothesis=hypothesis,
        )
        return exp

    def convert_response(self, response: str, hypothesis: Hypothesis, trace: Trace) -> ModelExperiment:
        if self.current_action in [KG_ACTION_FEATURE_ENGINEERING, KG_ACTION_FEATURE_PROCESSING]:
            return self.convert_feature_experiment(response, hypothesis, trace)
        elif self.current_action in [KG_ACTION_MODEL_FEATURE_SELECTION, KG_ACTION_MODEL_TUNING]:
            return self.convert_model_experiment(response, hypothesis, trace)


class KGTrace(Trace[KGScenario, KGKnowledgeGraph]):
    pass



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/README.md
================================================
# Motivation of the example
We use a runnable concrete example to demonstrate what the project should be like after being generated by a large language model.


# Content example and the workflow

> NOTE: the `README.md` itself is note generated by LLM. the content remains are generated by LLM.
>


## Extra input information beyond the competition information

[[../meta/spec.md]]
- [ ] TODO

## Step0: Specification generation

- Generate specification
  [[spec.md]]
  - [ ] TODO: perfect
- Generate loading data
  [[load_data.py]]

- Why do we merge this step together.
  - Successfully run `load_data.py` is a kind of verification of `spec.md`


## Step1: write the feature engineering code
- We can generate some file like [[feature.py]] that match the pattern `feat.*\.py`

## Step2: Model training


## Step3: ensemble and decision
- generate `ens_and_decsion`
  - why we generate score on ensemble phase
  - ensemble has following tasks which has great overlap 
    - ensemble usually check the performance before ensemble
    - A additional step to record performance is easier.

## Step4: Build workflow

[[main.py]]



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/ensemble.py
================================================
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score


def ensemble_workflow(test_pred_l: list[np.ndarray], val_pred_l: list[np.ndarray], val_label: np.ndarray) -> np.ndarray:
    """
    Handle the following:
    1) Ensemble predictions using a simple average.
    2) Make final decision after ensemble (convert the predictions to final binary form).

    Parameters
    ----------
    test_pred_l : list[np.ndarray]
        List of predictions on the test data.
    val_pred_l : list[np.ndarray]
        List of predictions on the validation data.
    val_label : np.ndarray
        True labels of the validation data.

    Returns
    -------
    np.ndarray
        Binary predictions on the test data.
    """

    scores = []
    for id, val_pred in enumerate(val_pred_l):
        scores.append(roc_auc_score(val_label, val_pred))

    # Normalize the scores to get weights
    total_score = sum(scores)
    weights = [score / total_score for score in scores]

    # Weighted average of test predictions
    weighted_test_pred = np.zeros_like(test_pred_l[0])
    for weight, test_pred in zip(weights, test_pred_l):
        weighted_test_pred += weight * test_pred

    weighted_valid_pred = np.zeros_like(val_pred_l[0])
    for weight, val_pred in zip(weights, val_pred_l):
        weighted_valid_pred += weight * val_pred

    weighted_valid_pred_score = roc_auc_score(val_label, weighted_valid_pred)

    scores_df = pd.DataFrame(
        {
            "Model": list(range(len(val_pred_l))) + ["weighted_average_ensemble"],
            "AUROC": scores + [weighted_valid_pred_score],
        }
    )
    scores_df.to_csv("scores.csv", index=False)

    pred_binary_l = [0 if value < 0.50 else 1 for value in weighted_test_pred]
    return np.array(pred_binary_l)



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/feature.py
================================================
import numpy as np


def feat_eng(
    X: np.ndarray,
    y: np.ndarray | None = None,
    X_fit: np.ndarray | None = None,
    y_fit: np.ndarray | None = None,
    param: object | None = None,
) -> tuple[np.ndarray, np.ndarray | None, object]:
    """
    Perform feature engineering on the input data.

    Parameters:
    - X: np.ndarray
        The input data to be transformed. A concrete example could be:
        array([[[[207, 194, 203],
                ...,
                [191, 183, 164],
                [176, 168, 149],
                [181, 173, 152]]]], dtype=uint8)
    - y: np.ndarray | None
        The target data. A concrete example could be:
        array([1, 0, 1, 0, 1, 1, ..., ])
    - X_fit: np.ndarray | None
        Data for fitting the transformation parameters.
    - y_fit: np.ndarray | None
        Target data for fitting.
    - param: object | None
        Pre-fitted parameters for transformation.

    Returns:
    - transformed_data: np.ndarray
        Transformed data.
    - transformed_target: np.ndarray | None
        Transformed target data.
    - fitted_param: object
        Fitted parameters.

    Notes:
    - Some preprocessing (e.g., data selection) is based on y.

    Typical usage:
    .. code-block:: python

        X_transformed, y_transformed, fitted_param = feat_eng(X, y, X, y)
        X_test_transformed, _, _ = feat_eng(X_test, fitted_param)
    """
    # This is an example of identity feature transformation.
    # We'll not change the content of the data, but we'll demonstrate the typical workflow of feature engineering.
    if param is None:
        # Get parameters from the X_fit and y_fit
        pass
    # Use the fitted parameters to transform the data X, y
    return X, y, param



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/load_data.py
================================================
"""
Load competition data to uniform format
"""

import os

import numpy as np
import pandas as pd
from PIL import Image


def load_test_images(folder):
    images = []
    filenames = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            images.append(np.array(img))
            filenames.append(filename)
    return np.array(images), filenames


def load_images_and_labels(csv_file, image_folder):
    images = []
    labels = []
    df = pd.read_csv(csv_file)
    for idx, row in df.iterrows():
        img = Image.open(os.path.join(image_folder, row["id"]))
        if img is not None:
            images.append(np.array(img))
            labels.append(row["has_cactus"])
    return np.array(images), np.array(labels)


def load_data() -> tuple[np.ndarray, np.ndarray, np.ndarray, list[str]]:
    """
    load raw data from disk to get data in uniform data

    Return:
        X: np.array

            a concrete example could be:

            .. code-block:: text

                array([[[[207, 194, 203],
                        ...,
                        [191, 183, 164],
                        [176, 168, 149],
                        [181, 173, 152]]]], dtype=uint8)

        y: np.array

            a concrete example could be:

            .. code-block:: python

                array([1, 0, 1, 0, 1, 1, ..., ])

        X_test: np.array

            a concrete example is similar to `X`.

        test_ids: the id representing the image. it is used to generate the submission file

            a concrete example could be:

            .. code-block:: python

                ['1398ad045aa57aee5f38e7661e9d49e8.jpg',
                '0051207eb794887c619341090de84b50.jpg',
                'a8202dd82c42e252bef921ada7607b6c.jpg',
                '76c329ff9e3c5036b616f4e88ebba814.jpg',
                ...]
    """
    X, y = load_images_and_labels("/kaggle/input/train.csv", "/kaggle/input/train/")

    test_folder = "/kaggle/input/test/"
    X_test, test_filenames = load_test_images(test_folder)
    # Store filenames separately
    test_ids = [os.path.basename(filename).replace(".tif", "") for filename in test_filenames]
    return X, y, X_test, test_ids



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/main.py
================================================
from load_data import load_data
from sklearn.model_selection import train_test_split

# Load data
train_images, train_labels, test_images, test_ids = load_data()


# feature engineering
from feature import feat_eng

train_images, train_lables, train_param = feat_eng(train_images, train_labels, train_images, train_labels)
test_images, _, _ = feat_eng(test_images, param=train_param)


# (Cross) Validation
train_images, validation_images, train_labels, validation_labels = train_test_split(
    train_images, train_labels, test_size=0.1, random_state=42
)


# Model workflow
from model01 import model_workflow

val_pred, test_pred, _ = model_workflow(train_images, train_labels, validation_images, validation_labels, test_images)


# Ensemble
from ensemble import ensemble_workflow

pred_binary = ensemble_workflow([test_pred], [val_pred], validation_labels)


# Save
with open("submission.csv", "w") as csv_file:
    csv_file.write("id,has_cactus\n")
    for tid, prediction in zip(test_ids, pred_binary):
        csv_file.write(f"{tid},{prediction}\n")



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/model01.py
================================================
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import (
    Activation,
    BatchNormalization,
    Conv2D,
    Dense,
    Dropout,
    Flatten,
    MaxPooling2D,
)
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator

print(tf.__version__)
print(tf.test.is_gpu_available())


def model_workflow(
    X: np.ndarray,
    y: np.ndarray,
    val_X: np.ndarray = None,
    val_y: np.ndarray = None,
    test_X: np.ndarray = None,
    **hyper_params,
) -> tuple[np.ndarray | None, np.ndarray | None, dict]:
    """
    Manages the workflow of a machine learning model, including training, validation, and testing.

    If hyper_params is given, please get important hyperparameters from it. Otherwise, use the default values.
    (the hyper_params only contains important hyperparameters that is worth tunning)

    Parameters
    ----------
    X : np.ndarray
        Training data features.
    y : np.ndarray
        Training data labels.
    val_X : np.ndarray, optional
        Validation data features.
    val_y : np.ndarray, optional
        Validation data labels.
    test_X : np.ndarray, optional
        Test data features.
    **hyper_params
        Additional hyperparameters for the model.

    Returns
    -------
    tuple[np.ndarray | None, np.ndarray | None]
        Predictions on the validation data, predictions on the test data
    """
    train_images, train_labels = X, y
    validation_images, validation_labels = val_X, val_y
    test_images = test_X

    # Data augmentation is crucial for generalization, especially with small datasets.
    batch_size = hyper_params.get("batch_size", 64)

    train_datagen = ImageDataGenerator(rescale=1.0 / 255, horizontal_flip=True, vertical_flip=True)
    train_generator = train_datagen.flow(train_images, train_labels, batch_size=batch_size, shuffle=True)

    # Get input shape from the training data
    input_shape = X.shape[1:]
    num_classes = hyper_params.get("num_classes", 2)

    # Model Creation: Convolutional Neural Network
    dropout_dense_layer = hyper_params.get("dropout_dense_layer", 0.6)

    model = Sequential(
        [
            Conv2D(32, (3, 3), input_shape=input_shape),
            BatchNormalization(),
            Activation("relu"),
            Conv2D(32, (3, 3)),
            BatchNormalization(),
            Activation("relu"),
            Conv2D(32, (3, 3)),
            BatchNormalization(),
            Activation("relu"),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(64, (3, 3)),
            BatchNormalization(),
            Activation("relu"),
            Conv2D(64, (3, 3)),
            BatchNormalization(),
            Activation("relu"),
            Conv2D(64, (3, 3)),
            BatchNormalization(),
            Activation("relu"),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(128, (3, 3)),
            BatchNormalization(),
            Activation("relu"),
            Flatten(),
            Dense(1024),
            Activation("relu"),
            Dropout(dropout_dense_layer),
            Dense(256),
            Activation("relu"),
            Dropout(dropout_dense_layer),
            Dense(1),
            Activation("sigmoid"),
        ]
    )

    model.compile(
        loss=keras.losses.binary_crossentropy,
        optimizer=keras.optimizers.Adam(learning_rate=hyper_params.get("learning_rate", 0.001)),
        metrics=["accuracy"],
    )

    # Extract early_stop_round from hyper_params, default is 25
    early_stop_round = hyper_params.get("early_stop_round", 25)

    callbacks = [
        EarlyStopping(monitor="val_loss", patience=early_stop_round),
        ModelCheckpoint(filepath="best_model.keras", monitor="val_loss", save_best_only=True),
    ]

    # Training
    epochs = hyper_params.get("epochs", 100)
    if val_X is not None and val_y is not None:
        validation_datagen = ImageDataGenerator(rescale=1.0 / 255)
        validation_generator = validation_datagen.flow(validation_images, validation_labels, batch_size=batch_size)
        history = model.fit(
            train_generator,
            validation_data=validation_generator,
            epochs=epochs,
            verbose=1,
            shuffle=True,
            callbacks=callbacks,
        )
        # Dynamic adjustment of early_stop_round
        if "early_stop_round" not in hyper_params:
            val_loss = history.history["val_loss"]
            best_epoch = np.argmin(val_loss)
            dynamic_early_stop = max(5, int((len(val_loss) - best_epoch) * 0.5))  # 50% of remaining epochs

            print(f"Dynamic early_stop_round: {dynamic_early_stop}")
            hyper_params["early_stop_round"] = dynamic_early_stop

        # Predict on validation data
        val_pred = model.predict(validation_datagen.flow(validation_images, batch_size=1, shuffle=False), verbose=1)
    else:
        history = model.fit(
            train_generator,
            epochs=epochs,
            verbose=1,
            shuffle=True,
            callbacks=callbacks,
        )
        val_pred = None

    # Predict on test data
    if test_X is not None:
        test_datagen = ImageDataGenerator(rescale=1.0 / 255)
        test_generator = test_datagen.flow(test_images, batch_size=1, shuffle=False)
        test_pred = model.predict(test_generator, verbose=1)
    else:
        test_pred = None

    return val_pred, test_pred, hyper_params



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/spec/data_loader.md
================================================
## Data Loading

- Implement a function to load data from raw files.
- The function should return training images, training labels, test images, and test IDs.


================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/spec/ensemble.md
================================================
## Ensemble and Decision Making

- Implement a function for ensemble and decision making with the following signature:

```python
def ensemble_workflow(test_pred_l: list[np.ndarray], val_pred_l: list[np.ndarray], val_label: np.ndarray) -> np.ndarray:
    """
    Handle the following:
    1) Ensemble predictions using a simple average.
    2) Make final decision after ensemble (convert the predictions to final form).

    Parameters
    ----------
    test_pred_l : list[np.ndarray]
        List of predictions on the test data.
    val_pred_l : list[np.ndarray]
        List of predictions on the validation data.
    val_label : np.ndarray
        True labels of the validation data.

    Returns
    -------
    np.ndarray
        Predictions on the test data.
    """
```

- The function should combine predictions and convert them to a proper format.



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/spec/feature.md
================================================

## Feature Engineering

- Implement a function for feature engineering with the following signature:

```python
def feat_eng(X: np.ndarray, y: np.ndarray | None = None, X_fit: np.ndarray | None = None, y_fit: np.ndarray | None = None, param: object | None = None) -> tuple[np.ndarray, np.ndarray | None, object]:
    """
    Perform feature engineering on the input data.

    Parameters:
    - X: np.ndarray
        The input data to be transformed.
    - y: np.ndarray | None
        The target data.
    - X_fit: np.ndarray | None
        Data for fitting the transformation parameters.
    - y_fit: np.ndarray | None
        Target data for fitting.
    - param: object | None
        Pre-fitted parameters for transformation.

    Returns:
    - transformed_data: np.ndarray
        Transformed data.
    - transformed_target: np.ndarray | None
        Transformed target data.
    - fitted_param: object
        Fitted parameters.
    """
```

- Ensure that the feature engineering process is consistent and can be applied to both training and test data.



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/spec/model.md
================================================
## Model Workflow

- Implement a function to manage the model workflow with the following signature:

```python
def model_workflow(X: np.ndarray, y: np.ndarray, val_X: np.ndarray = None, val_y: np.ndarray = None, test_X: np.ndarray = None, **hyper_params) -> tuple[np.ndarray | None, np.ndarray | None, dict]:
    """
    Manages the workflow of a machine learning model, including training, validation
    The testing&validation's inference is included, as well

    - If test/valid exist, output inference on them
    - Follow the hyperparameter if exists
        - Hyperparameters at least has <early stop round>. The code must check if it is given and use it.
        - the returned hyperparameter should align with the input(except the newly generated early stop)
    - Return hyperparameters for retrain if not exists. Hyperparameters should have <early stop round>
    - If valid exist, add <early stop round> to update the hyperparameter


    Parameters
    ----------
    X : np.ndarray
        Training data features.
    y : np.ndarray
        Training data labels.
    val_X : np.ndarray, optional
        Validation data features.
    val_y : np.ndarray, optional
        Validation data labels.
    test_X : np.ndarray, optional
        Test data features.
    **hyper_params
        Additional hyperparameters for the model.

    Returns
    -------
    tuple[np.ndarray | None, np.ndarray | None, dict]
        Predictions on the validation data, predictions on the test data
    """
```
- In this task, the shape of input(X of train, valid and test) should be (num_samples, height, width, channels).

- In this task, the shape of output should be (num_samples, num_class), as num_class = 1 here.

- The function should handle data augmentation, model creation, training, and prediction.



================================================
File: rdagent/scenarios/kaggle/tpl_ex/aerial-cactus-identification/spec/workflow.md
================================================
# Specification for Implementing a Kaggle Competition Project

This document outlines the structure and interface protocols for implementing a machine learning project, similar to a Kaggle competition. Follow these guidelines to ensure consistency and maintainability across projects.

## Project Structure

The project should be organized into the following components:

1. **Data Loading** (`load_data.py`): A module responsible for loading and preprocessing raw data.
2. **Feature Engineering**(`feat*.py`): A module for transforming raw data into features suitable for model training.
3. **Model Workflow**(`model*.py`): A module that manages the training, validation, and testing of machine learning models.
4. **Ensemble and Decision Making**(`ensemble.py`): A module for combining predictions from multiple models and making final decisions.
5. **Workflow**(`main.py`): A script to put the above component together to get the final submission(`submission.csv`)

## Submission

- Implement a script to generate the submission file.
- The script should write predictions to a CSV file in the format required by the competition.

## General Guidelines

- Ensure that all modules and functions are well-documented.
- Follow consistent naming conventions and code style.
- Use type annotations for function signatures to improve code readability and maintainability.



================================================
File: rdagent/scenarios/kaggle/tpl_ex/meta/spec.md
================================================


Information to generate spec


```python
def feature_eng(x: {{type of the feature}}) -> {{type of the feature}}:
    """
    
    x: np.ndarray
          {{description}}
    """
```

Standard to generate the qualified specification

| field       | requireemtnnts                                |
| --          | --                                            |
| description | fully describe the data, including dimension (number,meaning,  exmaple)|

Example of generated specification
```python
def feature_eng(x: {{type of the feature}}) -> {{type of the feature}}:
    """

    x: np.ndarray
        3 dimension, the meaning of the dimensions will be:
        - channel
        - high
        - width
    """
```





================================================
File: rdagent/scenarios/qlib/prompts.yaml
================================================
hypothesis_and_feedback: |-
  {% for experiment, feedback in trace.hist[-10:] %}
  Hypothesis {{ loop.index }}: {{ experiment.hypothesis }}
  
  Corresponding Code (that leads to the difference in performance): 
  {% for workspace in experiment.sub_workspace_list %}
  {% if workspace is not none %}
  Workspace {{loop.index}}:
  {{workspace.all_codes}}{% endif %}{% endfor %}

  Observation on the result with the hypothesis: {{ feedback.observations }}
  Feedback on the original hypothesis:  {{ feedback.hypothesis_evaluation }}
  New Feedback for Context (For you to agree or improve upon):  {{ feedback.new_hypothesis }}
  Reasoning for new hypothesis:  {{ feedback.reason }}
  Did changing to this hypothesis work? (focus on the change):  {{ feedback.decision }}
  {% endfor %}

hypothesis_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
  "hypothesis": "The new hypothesis generated based on the information provided.",
  "reason": "The reason why you generate this hypothesis. It should be comprehensive and logical. It should cover the other keys below and extend them.",
  "concise_reason": "Two-line summary. First line focuses on a concise justification for the change. Second line generalizes a knowledge statement.",
  "concise_observation": "One line summary. It focuses on the observation of the given scenario, data characteristics, or previous experiences (failures & succeses).",
  "concise_justification": "One line summary. Justify the hypothesis based on theoretical principles or initial assumptions.",
  "concise_knowledge": "One line summary. Transferable knowledge based on theoretical principles. Use conditional grammar. eg. "If...., ..; When..., .; and etc" Make sure that you state things clearly without ambiguity. Eg. avoid saying "previous hypothesis", because one wouldn't know what that is."
  }



model_hypothesis_specification: |-
  Additional Specifications:
    
    Hypotheses should grow and evolve based on the previous hypothesis. If there is no previous hypothesis, start with something simple. Gradually Build Up Upon previous hypothesis & feedbacks. In each round, hypothesis is different. Pay attention to your previous hypothesis.

    Ensure that the hypothesis focuses on the architecture of a PyTorch model. Each hypothesis should address specific architectural choices such as the type of layers, activation functions, regularization techniques, and the overall structure of the model. Avoid hypotheses related to input features or optimization processes.

  Remember: if there is no hypothesis, start with something simple like MLP.

  Usually, a larger model works better than a smaller one. 

  Logic for generating a new hypothesis: If the previous hypothesis works, try to inherit from it and grow deeper. If the previous hypotheis doesn't work, try to make changes in the current level.

  Sample hypothesis evolution loop: (This is the entire loop, see what stage you are at. We want hypothesis to continue growing.) Levels include **Model Type**, **Layer Configuration**, **Activation Functions**, **Regularization Techniques**

    1st Round Hypothesis: The model should be a CNN. 

    2nd Round Hypothesis (If first round worked: CNN is the model type level, which means that we should extend to the next level, like layer configuration): The model should be a CNN. The CNN should have 5 convolutional layers. (Reasoning: As CNN worked, we now specify the layers specification to grow the hypothesis deeper.)

    3rd Round Hypothesis (If second round didn't work): The model should be a CNN. The CNN should have 3 convolutional layers. (Reasoning: As 5-layer structure didn't work in the 2nd round hypothesis, try something else within the layer configuration level.)

    4th Round Hypothesis (If third round worked): The model should be a CNN. The CNN should have 3 convolutional layers. Use Leaky ReLU activation for all layers. (As last round worked, now proceed to the next level: activation functions)
    
    5th Round Hypothesis (If fourth round worked): The model should be a CNN. The CNN should have 3 convolutional layers. Use Leaky ReLU activation for all layers. Use dropout regularization with a rate of 0.5. (Similar Reasoning & Continuing to Grow to the dropout setup)

    6th Round Hypothesis (If fourth round didn't work):  The model should be a CNN. The CNN should have 5 convolutional layers. Use Leaky ReLU activation for all layers. Use dropout regularization with a rate of 0.3. (Reasoning: As regularisation rate of 0.5 didn't work, we only change a new regularisation and keep the other elements that worked. This means making changes in the current level.)    

factor_hypothesis_specification: |-
  1. **Type of Factor and Financial Trends:**
    - Define the type of factor introduced.
    - Explain the financial trends or market behaviors indicated by this factor.
    - Omit unnecessary or redundant details.

  2. **Simple and Effective Factors First:**
    - Start with factors that are simple and likely effective.
    - Concisely explain why these factors are expected to work.
    - Avoid complex or combined factors initially.

  3. **Gradual Complexity Increase:**
    - Introduce more complex factors as more experimental results are gathered.
    - Discuss potential advantages and complexities.
    - Combine factors only after simpler ones are tested and validated.

  4. **New Directions and Optimizations:**
    - If a new direction is needed, explain why based on financial principles, economic theories, or market behaviors.
    - Suggest only one new direction at a time for clarity.
    - If a previous hypothesis did not surpass SOTA but seems optimizable, you may continue in the same direction.
    - Highlight that factors surpassing SOTA are included in the library to avoid re-implementation.

  5. **1-3 Factors per Generation:**
    - Ensure each generation produces 1-3 factors.
    - Balance simplicity and complexity to build a robust factor library.

factor_experiment_output_format: |-
  The output should follow JSON format. The schema is as follows:
  {
      "factor name 1": {
          "description": "description of factor 1",
          "formulation": "latex formulation of factor 1",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      },
      "factor name 2": {
          "description": "description of factor 1",
          "formulation": "latex formulation of factor 2",
          "variables": {
              "variable or function name 1": "description of variable or function 1",
              "variable or function name 2": "description of variable or function 2"
          }
      }
      # Don't add ellipsis (...) or any filler text that might cause JSON parsing errors here!
  }

model_experiment_output_format: |-
  So far please only design one model to test the hypothesis! 
  The output should follow JSON format. The schema is as follows: 
  {
    "model_name 1 (The name of the model)": {
        "description": "A detailed description of the model",
        "formulation": "A LaTeX formula representing the model's formulation",
        "architecture": "A detailed description of the model's architecture, e.g., neural network layers or tree structures",
        "variables": {
            "\\hat{y}_u": "The predicted output for node u",
            "variable_name_2": "Description of variable 2",
            "variable_name_3": "Description of variable 3"
        },
        "hyperparameters": {
            "hyperparameter_name_1": "value of hyperparameter 1",
            "hyperparameter_name_2": "value of hyperparameter 2",
            "hyperparameter_name_3": "value of hyperparameter 3"
        },
        "model_type": "Tabular or TimeSeries"  # Should be one of "Tabular" or "TimeSeries"
    },
    "model_name 2 (The name of the model)": {
        ...
    }
  }
  Usually a larger model works better than a smaller one. Hence, the parameters should be larger.

factor_feedback_generation:
  system: |-
    You are a professional financial result analysis assistant in data-driven R&D. 
    The task is described in the following scenario:

    {{ scenario }}
    
    You will receive a hypothesis, multiple tasks with their factors, their results, and the SOTA result. 
    Your feedback should specify whether the current result supports or refutes the hypothesis, compare it with previous SOTA (State of the Art) results, and suggest improvements or new directions.
    
    Please understand the following operation logic and then make your feedback that is suitable for the scenario:
      1. Logic Explanation:
          - If the previous hypothesis factor surpasses the SOTA, include this factor in the SOTA factor library.
          - New experiments will generate new factors, which will be combined with the factors in the SOTA library.
          - These combined factors will be backtested and compared against the current SOTA to continuously iterate.
      2. Development Directions:
          - New Direction:
              - Propose a new factor direction for exploration and development.
          - Optimization of Existing Direction:
              - If the previous experiment's factor replaced the SOTA, suggest further improvements to that factor.
              - Clearly specify the differences in name and improvements compared to the previous factor.
          - Continued Research:
              - If the previous experiment's factor did not replace the SOTA, suggest ways to optimize and develop factors in this direction.
      3. Final Goal:
          - The ultimate goal is to continuously accumulate factors that surpass each iteration to maintain the best SOTA.
    
      When judging the results:
      1. **Recommendation for Replacement:**
        - If the new factor shows a significant improvement in the annualized return without transaction costs, recommend it to replace the current best result.
        - If the annualized return and any other single metric are better than SOTA, recommend the replacement.
        - Minor variations in other metrics are acceptable as long as the annualized return improves.

    Consider Changing Direction for Significant Gaps with SOTA:
      - If the new results significantly differ from the SOTA, consider exploring a new direction.
      - Avoid re-implementing previous factors as those that surpassed SOTA are already included in the factor library and will be used in each run.

    Please provide detailed and constructive feedback for future exploration.
    Respond in JSON format. Example JSON structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Your new hypothesis here",
      "Reasoning": "Reasoning for the new hypothesis",
      "Replace Best Result": "yes or no"
    }
  user: |-
    Target hypothesis: 
    {{ hypothesis_text }}
    Tasks and Factors:
    {% for task in task_details %}
      - {{ task.factor_name }}: {{ task.factor_description }}
        - Factor Formulation: {{ task.factor_formulation }}
        - Variables: {{ task.variables }}
        - Factor Implementation: {{ task.factor_implementation }}
        {% if task.factor_implementation == "False" %}
        **Note: This factor was not implemented in the current experiment. Only the hypothesis for implemented factors can be verified.**
        {% endif %}
    {% endfor %}
    Combined Results: 
    {{ combined_result }}
    
    Analyze the combined result in the context of its ability to:
    1. Support or refute the hypothesis.
    2. Show improvement or deterioration compared to the SOTA experiment.

    Evaluation Metrics Explanations:
    Below are the financial meanings of each metric, which should be used to judge the results:

    - 1day.excess_return_without_cost.max_drawdown: Measures the maximum loss from a peak to a trough without considering transaction costs. (the smaller the better)
    - 1day.excess_return_without_cost.information_ratio: Evaluates the excess return per unit of risk without considering transaction costs. (the bigger the better)
    - 1day.excess_return_without_cost.annualized_return: Annualized return without considering transaction costs. (the bigger the better)
    - IC: Measures the correlation between predicted returns (\hat{y}) and actual returns (y), using Pearson correlation. (the bigger the better)

    When judging the results:
      1. **Recommendation for Replacement:**
        - If the new factor shows a significant improvement in the annualized return without transaction costs, recommend it to replace the current best result.
        - If the annualized return and any other single metric are better than SOTA, recommend the replacement.
        - Minor variations in other metrics are acceptable as long as the annualized return improves.

    Consider Changing Direction for Significant Gaps with SOTA:
      - If the new results significantly differ from the SOTA, consider exploring a new direction.
      - Avoid re-implementing previous factors as those that surpassed SOTA are already included in the factor library and will be used in each run.

    Note: Only factors with 'Factor Implementation' as True are implemented and tested in this experiment. If 'Factor Implementation' is False, the hypothesis for that factor cannot be verified in this run.

model_feedback_generation:
  system: |-
    You are a professional result analysis assistant. You will receive a result and a hypothesis.
    Your task is to provide feedback on how well the result supports or refutes the hypothesis by judging from the observation of performance increase or decrease.
    Please provide detailed and constructive feedback. Note that as hypothesis evolve, a general trend should be that the model grows larger. 
    Example JSON Structure for Result Analysis:
    {
      "Observations": "Your overall observations here",
      "Feedback for Hypothesis": "Observations related to the hypothesis",
      "New Hypothesis": "Put your new hypothesis here.",
      "Reasoning": "Provide reasoning for the hypothesis here.",
      "Decision": <true or false>,
    }

    Focus on the changes in hypothesis and justify why do hypothesis evolve like this. Also, increase complexity as the hypothesis evolves  (give more layers, more neurons, and etc)
    
    Logic for generating a new hypothesis: If the previous hypothesis works, try to inherit from it and grow deeper. If the previous hypotheis doesn't work, try to make changes in the current level.

    Sample hypothesis evolution loop: (This is the entire loop, see what stage you are at. We want hypothesis to continue growing.) Levels include **Model Type**, **Layer Configuration**, **Activation Functions**, **Regularization Techniques**

      1st Round Hypothesis: The model should be a CNN. 

      2nd Round Hypothesis (If first round worked: CNN is the model type level, which means that we should extend to the next level, like layer configuration): The model should be a CNN. The CNN should have 5 convolutional layers. (Reasoning: As CNN worked, we now specify the layers specification to grow the hypothesis deeper.)

      3rd Round Hypothesis (If second round didn't work): The model should be a CNN. The CNN should have 3 convolutional layers. (Reasoning: As 5-layer structure didn't work in the 2nd round hypothesis, try something else within the layer configuration level.)

      4th Round Hypothesis (If third round worked): The model should be a CNN. The CNN should have 3 convolutional layers. Use Leaky ReLU activation for all layers. (As last round worked, now proceed to the next level: activation functions)
      
      5th Round Hypothesis (If fourth round worked): The model should be a CNN. The CNN should have 3 convolutional layers. Use Leaky ReLU activation for all layers. Use dropout regularization with a rate of 0.5. (Similar Reasoning & Continuing to Grow to the dropout setup)

      6th Round Hypothesis (If fourth round didn't work):  The model should be a CNN. The CNN should have 5 convolutional layers. Use Leaky ReLU activation for all layers. Use dropout regularization with a rate of 0.3. (Reasoning: As regularisation rate of 0.5 didn't work, we only change a new regularisation and keep the other elements that worked. This means making changes in the current level.)    

    
  user: |-
    We are in an experiment of finding hypothesis and validating or rejecting them so that in the end we have a powerful model generated.
    Here are the context: {{context}}. 

    {% if last_hypothesis %} 
    Last Round Information:
    Hypothesis: {{last_hypothesis.hypothesis}}
    Task: {{last_task}}
    Code Implemented: {{last_code}}
    Result: {{last_result}}
    {% else %}
    This is the first round. No previous information available. As long as the performance is not too negative (eg.ICIR is greater than 0), treat it as successful. Do not set the threshold too high.  
    {% endif %} 
    
    Now let's come to this round. You will receive the result and you will evaluate if the performance increases or decreases. 
    Hypothesis: {{hypothesis.hypothesis}}
    Experiment Setup: {{exp.sub_tasks[0]}}
    Code Implemented: {{exp.sub_workspace_list[0].file_dict.get("model.py")}}
    Relevant Reasoning: {{hypothesis.reason}}
    Result: {{exp.result}}

    Compare and observe. Which result has a better return and lower risk? If the performance increases, the hypothesis should be considered positive (working). 
    Hence, with the hypotheses, relevant reasoning, and results in mind (comparison), provide detailed and constructive feedback and suggest a new hypothesis. 



================================================
File: rdagent/scenarios/qlib/developer/factor_coder.py
================================================
from rdagent.components.coder.factor_coder import FactorCoSTEER

QlibFactorCoSTEER = FactorCoSTEER



================================================
File: rdagent/scenarios/qlib/developer/factor_runner.py
================================================
import pickle
from pathlib import Path
from typing import List

import pandas as pd
from pandarallel import pandarallel

from rdagent.components.coder.CoSTEER.evaluators import CoSTEERMultiFeedback
from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.utils import cache_with_pickle, multiprocessing_wrapper

pandarallel.initialize(verbose=1)

from rdagent.components.runner import CachedRunner
from rdagent.core.exception import FactorEmptyError
from rdagent.log import rdagent_logger as logger
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorExperiment

DIRNAME = Path(__file__).absolute().resolve().parent
DIRNAME_local = Path.cwd()

# class QlibFactorExpWorkspace:

#     def prepare():
#         # create a folder;
#         # copy template
#         # place data inside the folder `combined_factors`
#         #
#     def execute():
#         de = DockerEnv()
#         de.run(local_path=self.ws_path, entry="qrun conf.yaml")

# TODO: supporting multiprocessing and keep previous results


class QlibFactorRunner(CachedRunner[QlibFactorExperiment]):
    """
    Docker run
    Everything in a folder
    - config.yaml
    - price-volume data dumper
    - `data.py` + Adaptor to Factor implementation
    - results in `mlflow`
    """

    def calculate_information_coefficient(
        self, concat_feature: pd.DataFrame, SOTA_feature_column_size: int, new_feature_columns_size: int
    ) -> pd.DataFrame:
        res = pd.Series(index=range(SOTA_feature_column_size * new_feature_columns_size))
        for col1 in range(SOTA_feature_column_size):
            for col2 in range(SOTA_feature_column_size, SOTA_feature_column_size + new_feature_columns_size):
                res.loc[col1 * new_feature_columns_size + col2 - SOTA_feature_column_size] = concat_feature.iloc[
                    :, col1
                ].corr(concat_feature.iloc[:, col2])
        return res

    def deduplicate_new_factors(self, SOTA_feature: pd.DataFrame, new_feature: pd.DataFrame) -> pd.DataFrame:
        # calculate the IC between each column of SOTA_feature and new_feature
        # if the IC is larger than a threshold, remove the new_feature column
        # return the new_feature

        concat_feature = pd.concat([SOTA_feature, new_feature], axis=1)
        IC_max = (
            concat_feature.groupby("datetime")
            .parallel_apply(
                lambda x: self.calculate_information_coefficient(x, SOTA_feature.shape[1], new_feature.shape[1])
            )
            .mean()
        )
        IC_max.index = pd.MultiIndex.from_product([range(SOTA_feature.shape[1]), range(new_feature.shape[1])])
        IC_max = IC_max.unstack().max(axis=0)
        return new_feature.iloc[:, IC_max[IC_max < 0.99].index]

    @cache_with_pickle(CachedRunner.get_cache_key, CachedRunner.assign_cached_result)
    def develop(self, exp: QlibFactorExperiment) -> QlibFactorExperiment:
        """
        Generate the experiment by processing and combining factor data,
        then passing the combined data to Docker for backtest results.
        """
        if exp.based_experiments and exp.based_experiments[-1].result is None:
            exp.based_experiments[-1] = self.develop(exp.based_experiments[-1])

        if exp.based_experiments:
            SOTA_factor = None
            if len(exp.based_experiments) > 1:
                SOTA_factor = self.process_factor_data(exp.based_experiments)

            # Process the new factors data
            new_factors = self.process_factor_data(exp)

            if new_factors.empty:
                raise FactorEmptyError("No valid factor data found to merge.")

            # Combine the SOTA factor and new factors if SOTA factor exists
            if SOTA_factor is not None and not SOTA_factor.empty:
                new_factors = self.deduplicate_new_factors(SOTA_factor, new_factors)
                if new_factors.empty:
                    raise FactorEmptyError("No valid factor data found to merge.")
                combined_factors = pd.concat([SOTA_factor, new_factors], axis=1).dropna()
            else:
                combined_factors = new_factors

            # Sort and nest the combined factors under 'feature'
            combined_factors = combined_factors.sort_index()
            combined_factors = combined_factors.loc[:, ~combined_factors.columns.duplicated(keep="last")]
            new_columns = pd.MultiIndex.from_product([["feature"], combined_factors.columns])
            combined_factors.columns = new_columns

            # Save the combined factors to the workspace
            with open(exp.experiment_workspace.workspace_path / "combined_factors_df.pkl", "wb") as f:
                pickle.dump(combined_factors, f)

        result = exp.experiment_workspace.execute(
            qlib_config_name=f"conf.yaml" if len(exp.based_experiments) == 0 else "conf_combined.yaml"
        )

        exp.result = result

        return exp

    def process_factor_data(self, exp_or_list: List[QlibFactorExperiment] | QlibFactorExperiment) -> pd.DataFrame:
        """
        Process and combine factor data from experiment implementations.

        Args:
            exp (ASpecificExp): The experiment containing factor data.

        Returns:
            pd.DataFrame: Combined factor data without NaN values.
        """
        if isinstance(exp_or_list, QlibFactorExperiment):
            exp_or_list = [exp_or_list]
        factor_dfs = []

        # Collect all exp's dataframes
        for exp in exp_or_list:
            if len(exp.sub_tasks) > 0:
                # if it has no sub_tasks, the experiment is results from template project.
                # otherwise, it is developed with designed task. So it should have feedback.
                assert isinstance(exp.prop_dev_feedback, CoSTEERMultiFeedback)
                # Iterate over sub-implementations and execute them to get each factor data
                message_and_df_list = multiprocessing_wrapper(
                    [
                        (implementation.execute, ("All",))
                        for implementation, fb in zip(exp.sub_workspace_list, exp.prop_dev_feedback)
                        if implementation and fb
                    ],  # only execute successfully feedback
                    n=RD_AGENT_SETTINGS.multi_proc_n,
                )
                for message, df in message_and_df_list:
                    # Check if factor generation was successful
                    if df is not None and "datetime" in df.index.names:
                        time_diff = df.index.get_level_values("datetime").to_series().diff().dropna().unique()
                        if pd.Timedelta(minutes=1) not in time_diff:
                            factor_dfs.append(df)

        # Combine all successful factor data
        if factor_dfs:
            return pd.concat(factor_dfs, axis=1)
        else:
            raise FactorEmptyError("No valid factor data found to merge.")



================================================
File: rdagent/scenarios/qlib/developer/feedback.py
================================================
import json
from pathlib import Path
from typing import Dict

import pandas as pd
from jinja2 import Environment, StrictUndefined

from rdagent.core.experiment import Experiment
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import (
    Experiment2Feedback,
    Hypothesis,
    HypothesisFeedback,
    Trace,
)
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import APIBackend
from rdagent.utils import convert2bool

feedback_prompts = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")
DIRNAME = Path(__file__).absolute().resolve().parent


def process_results(current_result, sota_result):
    # Convert the results to dataframes
    current_df = pd.DataFrame(current_result)
    sota_df = pd.DataFrame(sota_result)

    # Set the metric as the index
    current_df.index.name = "metric"
    sota_df.index.name = "metric"

    # Rename the value column to reflect the result type
    current_df.rename(columns={"0": "Current Result"}, inplace=True)
    sota_df.rename(columns={"0": "SOTA Result"}, inplace=True)

    # Combine the dataframes on the Metric index
    combined_df = pd.concat([current_df, sota_df], axis=1)

    # Select important metrics for comparison
    important_metrics = [
        "1day.excess_return_without_cost.max_drawdown",
        "1day.excess_return_without_cost.information_ratio",
        "1day.excess_return_without_cost.annualized_return",
        "IC",
    ]

    # Filter the combined DataFrame to retain only the important metrics
    filtered_combined_df = combined_df.loc[important_metrics]

    filtered_combined_df[
        "Bigger columns name (Didn't consider the direction of the metric, you should judge it by yourself that bigger is better or smaller is better)"
    ] = filtered_combined_df.apply(
        lambda row: "Current Result" if row["Current Result"] > row["SOTA Result"] else "SOTA Result", axis=1
    )

    return filtered_combined_df.to_string()


class QlibFactorExperiment2Feedback(Experiment2Feedback):
    def generate_feedback(self, exp: Experiment, trace: Trace) -> HypothesisFeedback:
        """
        Generate feedback for the given experiment and hypothesis.

        Args:
            exp (QlibFactorExperiment): The experiment to generate feedback for.
            hypothesis (QlibFactorHypothesis): The hypothesis to generate feedback for.
            trace (Trace): The trace of the experiment.

        Returns:
            Any: The feedback generated for the given experiment and hypothesis.
        """
        hypothesis = exp.hypothesis
        logger.info("Generating feedback...")
        hypothesis_text = hypothesis.hypothesis
        current_result = exp.result
        tasks_factors = [task.get_task_information_and_implementation_result() for task in exp.sub_tasks]
        sota_result = exp.based_experiments[-1].result

        # Process the results to filter important metrics
        combined_result = process_results(current_result, sota_result)

        # Generate the system prompt
        sys_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(feedback_prompts["factor_feedback_generation"]["system"])
            .render(scenario=self.scen.get_scenario_all_desc())
        )

        # Generate the user prompt
        usr_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(feedback_prompts["factor_feedback_generation"]["user"])
            .render(
                hypothesis_text=hypothesis_text,
                task_details=tasks_factors,
                combined_result=combined_result,
            )
        )

        # Call the APIBackend to generate the response for hypothesis feedback
        response = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=usr_prompt,
            system_prompt=sys_prompt,
            json_mode=True,
            json_target_type=Dict[str, str | bool | int],
        )

        # Parse the JSON response to extract the feedback
        response_json = json.loads(response)

        # Extract fields from JSON response
        observations = response_json.get("Observations", "No observations provided")
        hypothesis_evaluation = response_json.get("Feedback for Hypothesis", "No feedback provided")
        new_hypothesis = response_json.get("New Hypothesis", "No new hypothesis provided")
        reason = response_json.get("Reasoning", "No reasoning provided")
        decision = convert2bool(response_json.get("Replace Best Result", "no"))

        return HypothesisFeedback(
            observations=observations,
            hypothesis_evaluation=hypothesis_evaluation,
            new_hypothesis=new_hypothesis,
            reason=reason,
            decision=decision,
        )


class QlibModelExperiment2Feedback(Experiment2Feedback):
    """Generated feedbacks on the hypothesis from **Executed** Implementations of different tasks & their comparisons with previous performances"""

    def generate_feedback(self, exp: Experiment, trace: Trace) -> HypothesisFeedback:
        """
        The `ti` should be executed and the results should be included, as well as the comparison between previous results (done by LLM).
        For example: `mlflow` of Qlib will be included.
        """
        hypothesis = exp.hypothesis
        logger.info("Generating feedback...")
        # Define the system prompt for hypothesis feedback
        system_prompt = feedback_prompts["model_feedback_generation"]["system"]

        # Define the user prompt for hypothesis feedback
        context = trace.scen
        SOTA_hypothesis, SOTA_experiment = trace.get_sota_hypothesis_and_experiment()

        user_prompt = (
            Environment(undefined=StrictUndefined)
            .from_string(feedback_prompts["model_feedback_generation"]["user"])
            .render(
                context=context,
                last_hypothesis=SOTA_hypothesis,
                last_task=SOTA_experiment.sub_tasks[0].get_task_information() if SOTA_hypothesis else None,
                last_code=SOTA_experiment.sub_workspace_list[0].file_dict.get("model.py") if SOTA_hypothesis else None,
                last_result=SOTA_experiment.result if SOTA_hypothesis else None,
                hypothesis=hypothesis,
                exp=exp,
            )
        )

        # Call the APIBackend to generate the response for hypothesis feedback
        response_hypothesis = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt,
            system_prompt=system_prompt,
            json_mode=True,
            json_target_type=Dict[str, str | bool | int],
        )

        # Parse the JSON response to extract the feedback
        response_json_hypothesis = json.loads(response_hypothesis)
        return HypothesisFeedback(
            observations=response_json_hypothesis.get("Observations", "No observations provided"),
            hypothesis_evaluation=response_json_hypothesis.get("Feedback for Hypothesis", "No feedback provided"),
            new_hypothesis=response_json_hypothesis.get("New Hypothesis", "No new hypothesis provided"),
            reason=response_json_hypothesis.get("Reasoning", "No reasoning provided"),
            decision=convert2bool(response_json_hypothesis.get("Decision", "false")),
        )



================================================
File: rdagent/scenarios/qlib/developer/model_coder.py
================================================
from rdagent.components.coder.model_coder import ModelCoSTEER

QlibModelCoSTEER = ModelCoSTEER



================================================
File: rdagent/scenarios/qlib/developer/model_runner.py
================================================
from rdagent.components.runner import CachedRunner
from rdagent.core.exception import ModelEmptyError
from rdagent.core.utils import cache_with_pickle
from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelExperiment


class QlibModelRunner(CachedRunner[QlibModelExperiment]):
    """
    Docker run
    Everything in a folder
    - config.yaml
    - Pytorch `model.py`
    - results in `mlflow`

    https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_nn.py
    - pt_model_uri:  hard-code `model.py:Net` in the config
    - let LLM modify model.py
    """

    @cache_with_pickle(CachedRunner.get_cache_key, CachedRunner.assign_cached_result)
    def develop(self, exp: QlibModelExperiment) -> QlibModelExperiment:
        if exp.sub_workspace_list[0].file_dict.get("model.py") is None:
            raise ModelEmptyError("model.py is empty")
        # to replace & inject code
        exp.experiment_workspace.inject_files(**{"model.py": exp.sub_workspace_list[0].file_dict["model.py"]})

        env_to_use = {"PYTHONPATH": "./"}

        if exp.sub_tasks[0].model_type == "TimeSeries":
            env_to_use.update({"dataset_cls": "TSDatasetH", "step_len": 20, "num_timesteps": 20})
        elif exp.sub_tasks[0].model_type == "Tabular":
            env_to_use.update({"dataset_cls": "DatasetH"})

        result = exp.experiment_workspace.execute(qlib_config_name="conf.yaml", run_env=env_to_use)

        exp.result = result

        return exp



================================================
File: rdagent/scenarios/qlib/docker/Dockerfile
================================================
FROM pytorch/pytorch:2.2.1-cuda12.1-cudnn8-runtime

# For GPU support, please choose the proper tag from https://hub.docker.com/r/pytorch/pytorch/tags

RUN apt-get clean && apt-get update && apt-get install -y \  
    curl \  
    vim \  
    git \  
    build-essential \
    && rm -rf /var/lib/apt/lists/* 

RUN git clone https://github.com/microsoft/qlib.git

WORKDIR /workspace/qlib

RUN git reset c9ed050ef034fe6519c14b59f3d207abcb693282 --hard

RUN python -m pip install --upgrade cython
RUN python -m pip install -e .

RUN pip install catboost
RUN pip install xgboost
RUN pip install scipy==1.11.4



================================================
File: rdagent/scenarios/qlib/experiment/factor_experiment.py
================================================
from copy import deepcopy
from pathlib import Path

from rdagent.components.coder.factor_coder.factor import (
    FactorExperiment,
    FactorFBWorkspace,
    FactorTask,
)
from rdagent.core.experiment import Task
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario
from rdagent.scenarios.qlib.experiment.utils import get_data_folder_intro
from rdagent.scenarios.qlib.experiment.workspace import QlibFBWorkspace

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class QlibFactorExperiment(FactorExperiment[FactorTask, QlibFBWorkspace, FactorFBWorkspace]):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.experiment_workspace = QlibFBWorkspace(template_folder_path=Path(__file__).parent / "factor_template")


class QlibFactorScenario(Scenario):
    def __init__(self) -> None:
        super().__init__()
        self._background = deepcopy(prompt_dict["qlib_factor_background"])
        self._source_data = deepcopy(get_data_folder_intro())
        self._output_format = deepcopy(prompt_dict["qlib_factor_output_format"])
        self._interface = deepcopy(prompt_dict["qlib_factor_interface"])
        self._strategy = deepcopy(prompt_dict["qlib_factor_strategy"])
        self._simulator = deepcopy(prompt_dict["qlib_factor_simulator"])
        self._rich_style_description = deepcopy(prompt_dict["qlib_factor_rich_style_description"])
        self._experiment_setting = deepcopy(prompt_dict["qlib_factor_experiment_setting"])

    @property
    def background(self) -> str:
        return self._background

    def get_source_data_desc(self, task: Task | None = None) -> str:
        return self._source_data

    @property
    def output_format(self) -> str:
        return self._output_format

    @property
    def interface(self) -> str:
        return self._interface

    @property
    def simulator(self) -> str:
        return self._simulator

    @property
    def rich_style_description(self) -> str:
        return self._rich_style_description

    @property
    def experiment_setting(self) -> str:
        return self._experiment_setting

    def get_scenario_all_desc(
        self, task: Task | None = None, filtered_tag: str | None = None, simple_background: bool | None = None
    ) -> str:
        """A static scenario describer"""
        if simple_background:
            return f"""Background of the scenario:
{self.background}"""
        return f"""Background of the scenario:
{self.background}
The source data you can use:
{self.get_source_data_desc(task)}
The interface you should follow to write the runnable code:
{self.interface}
The output of your code should be in the format:
{self.output_format}
The simulator user can use to test your factor:
{self.simulator}
"""



================================================
File: rdagent/scenarios/qlib/experiment/factor_from_report_experiment.py
================================================
from copy import deepcopy
from pathlib import Path

from rdagent.components.coder.factor_coder.factor import (
    FactorExperiment,
    FactorFBWorkspace,
    FactorTask,
)
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorScenario
from rdagent.scenarios.qlib.experiment.workspace import QlibFBWorkspace

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class QlibFactorFromReportScenario(QlibFactorScenario):
    def __init__(self) -> None:
        super().__init__()
        self._rich_style_description = deepcopy(prompt_dict["qlib_factor_from_report_rich_style_description"])

    @property
    def rich_style_description(self) -> str:
        return self._rich_style_description



================================================
File: rdagent/scenarios/qlib/experiment/model_experiment.py
================================================
from copy import deepcopy
from pathlib import Path

from rdagent.components.coder.model_coder.model import (
    ModelExperiment,
    ModelFBWorkspace,
    ModelTask,
)
from rdagent.core.experiment import Task
from rdagent.core.prompts import Prompts
from rdagent.core.scenario import Scenario
from rdagent.scenarios.qlib.experiment.workspace import QlibFBWorkspace

prompt_dict = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


class QlibModelExperiment(ModelExperiment[ModelTask, QlibFBWorkspace, ModelFBWorkspace]):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.experiment_workspace = QlibFBWorkspace(template_folder_path=Path(__file__).parent / "model_template")


class QlibModelScenario(Scenario):
    def __init__(self) -> None:
        super().__init__()
        self._background = deepcopy(prompt_dict["qlib_model_background"])
        self._output_format = deepcopy(prompt_dict["qlib_model_output_format"])
        self._interface = deepcopy(prompt_dict["qlib_model_interface"])
        self._simulator = deepcopy(prompt_dict["qlib_model_simulator"])
        self._rich_style_description = deepcopy(prompt_dict["qlib_model_rich_style_description"])
        self._experiment_setting = deepcopy(prompt_dict["qlib_model_experiment_setting"])

    @property
    def background(self) -> str:
        return self._background

    @property
    def source_data(self) -> str:
        raise NotImplementedError("source_data of QlibModelScenario is not implemented")

    @property
    def output_format(self) -> str:
        return self._output_format

    @property
    def interface(self) -> str:
        return self._interface

    @property
    def simulator(self) -> str:
        return self._simulator

    @property
    def rich_style_description(self) -> str:
        return self._rich_style_description

    @property
    def experiment_setting(self) -> str:
        return self._experiment_setting

    def get_scenario_all_desc(
        self, task: Task | None = None, filtered_tag: str | None = None, simple_background: bool | None = None
    ) -> str:
        return f"""Background of the scenario:
{self.background}
The interface you should follow to write the runnable code:
{self.interface}
The output of your code should be in the format:
{self.output_format}
The simulator user can use to test your model:
{self.simulator}
"""



================================================
File: rdagent/scenarios/qlib/experiment/prompts.yaml
================================================
qlib_factor_background: |-
  The factor is a characteristic or variable used in quant investment that can help explain the returns and risks of a portfolio or a single asset. Factors are used by investors to identify and exploit sources of excess returns, and they are central to many quantitative investment strategies.
  Each number in the factor represents a physics value to an instrument on a day.
  User will train a model to predict the next several days return based on the factor values of the previous days.
  The factor is defined in the following parts:
  1. Name: The name of the factor.
  2. Description: The description of the factor.
  3. Formulation: The formulation of the factor.
  4. Variables: The variables or functions used in the formulation of the factor.
  The factor might not provide all the parts of the information above since some might not be applicable.
  Please specifically give all the hyperparameter in the factors like the window size, look back period, and so on. One factor should statically defines one output with a static source data. For example, last 10 days momentum and last 20 days momentum should be two different factors.

qlib_factor_interface: |-
  Your python code should follow the interface to better interact with the user's system.
  Your python code should contain the following part: the import part, the function part, and the main part. You should write a main function name: "calculate_{function_name}" and call this function in "if __name__ == __main__" part. Don't write any try-except block in your python code. The user will catch the exception message and provide the feedback to you.
  User will write your python code into a python file and execute the file directly with "python {your_file_name}.py". You should calculate the factor values and save the result into a HDF5(H5) file named "result.h5" in the same directory as your python file. The result file is a HDF5(H5) file containing a pandas dataframe. The index of the dataframe is the "datetime" and "instrument", and the single column name is the factor name,and the value is the factor value. The result file should be saved in the same directory as your python file.

qlib_factor_strategy: |-
  Ensure that for every step of data processing, the data format (including indexes) is clearly explained through comments.
  Each transformation or calculation should be accompanied by a detailed description of how the data is structured, especially focusing on key aspects like whether the data has multi-level indexing, how to access specific columns or index levels, and any operations that affect the data shape (e.g., `reset_index()`, `groupby()`, `merge()`).
  This step-by-step explanation will ensure clarity and accuracy in data handling. For example:
  1. **Start with multi-level index**:  
    ```python
    # The initial DataFrame has a multi-level index with 'datetime' and 'instrument'.
    # To access the 'datetime' index, use df.index.get_level_values('datetime').
    datetime_values = df.index.get_level_values('datetime')
    ```
 
  2. **Reset the index if necessary**:  
    ```python
    # Resetting the index to move 'datetime' and 'instrument' from the index to columns.
    # This operation flattens the multi-index structure.
    df = df.reset_index()
    ```
 
  3. **Perform groupby operations**:  
    ```python
    # Grouping by 'datetime' and 'instrument' to aggregate the data.
    # After groupby, the result will maintain 'datetime' and 'instrument' as a multi-level index.
    df_grouped = df.groupby(['datetime', 'instrument']).sum()
    ```
 
  4. **Ensure consistent datetime formats**:  
    ```python
    # Before merging, ensure that the 'datetime' column in both DataFrames is of the same format.
    # Convert to datetime format if necessary.
    df['datetime'] = pd.to_datetime(df['datetime'])
    other_df['datetime'] = pd.to_datetime(other_df['datetime'])
    ```
 
  5. **Merge operations**:  
    ```python
    # When merging DataFrames, ensure you are merging on both 'datetime' and 'instrument'.
    # If these are part of the index, reset the index before merging.
    merged_df = pd.merge(df, other_df, on=['datetime', 'instrument'], how='inner')
    ```

qlib_factor_output_format: |-
  Your output should be a pandas dataframe similar to the following example information:
  <class 'pandas.core.frame.DataFrame'>
  MultiIndex: 40914 entries, (Timestamp('2020-01-02 00:00:00'), 'SH600000') to (Timestamp('2021-12-31 00:00:00'), 'SZ300059')
  Data columns (total 1 columns):
  #   Column            Non-Null Count  Dtype  
  ---  ------            --------------  -----  
  0   your factor name  40914 non-null  float64
  dtypes: float64(1)
  memory usage: <ignore>
  Notice: The non-null count is OK to be different to the total number of entries since some instruments may not have the factor value on some days.
  One possible format of `result.h5` may be like following:
  datetime    instrument
  2020-01-02  SZ000001     -0.001796
              SZ000166      0.005780
              SZ000686      0.004228
              SZ000712      0.001298
              SZ000728      0.005330
                              ...
  2021-12-31  SZ000750      0.000000
              SZ000776      0.002459

qlib_factor_simulator: |-
  The factors will be sent into Qlib to train a model to predict the next several days return based on the factor values of the previous days. 
  Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.
  User will use Qlib to automatically do the following things:
  1. generate a new factor table based on the factor values.
  2. train a model like LightGBM, CatBoost, LSTM or simple PyTorch model to predict the next several days return based on the factor values.
  3. build a portfolio based on the predicted return based on a strategy.
  4. evaluate the portfolio's performance including the return, sharpe ratio, max drawdown, and so on.

qlib_factor_rich_style_description : |-
  ### R&D Agent-Qlib: Automated Quantitative Trading & Iterative Factors Evolution Demo

  #### [Overview](#_summary)

  The demo showcases the iterative process of hypothesis generation, knowledge construction, and decision-making. It highlights how financial factors evolve through continuous feedback and refinement.

  #### [Automated R&D](#_rdloops)

  - **[R (Research)](#_research)**
    - Iterative development of ideas and hypotheses.
    - Continuous learning and knowledge construction.

  - **[D (Development)](#_development)**
    - Progressive implementation and code generation of factors.
    - Automated testing and validation of financial factors.

  #### [Objective](#_summary)

  To demonstrate the dynamic evolution of financial factors through the Qlib platform, emphasizing how each iteration enhances the accuracy and reliability of the resulting financial factors.

qlib_factor_from_report_rich_style_description : |-
  ### R&D Agent-Qlib: Automated Quantitative Trading & Factor Extraction from Financial Reports Demo

  #### [Overview](#_summary)

  This demo showcases the process of extracting factors from financial research reports, implementing these factors, and analyzing their performance through Qlib backtest, continually expanding and refining the factor library.

  #### [Automated R&D](#_rdloops)

  - **[R (Research)](#_research)**
    - Iterative development of ideas and hypotheses from financial reports.
    - Continuous learning and knowledge construction.

  - **[D (Development)](#_development)**
    - Progressive factor extraction and code generation.
    - Automated implementation and testing of financial factors.

  #### [Objective](#_summary)

  <table border="1" style="width:100%; border-collapse: collapse;">
    <tr>
      <td>💡 <strong>Innovation </strong></td>
      <td>Tool to quickly extract and test factors from research reports.</td>
    </tr>
    <tr>
      <td>⚡ <strong>Efficiency </strong></td>
      <td>Rapid identification of valuable factors from numerous reports.</td>
    </tr>
    <tr>
      <td>🗃️ <strong>Outputs </strong></td>
      <td>Expand and refine the factor library to support further research.</td>
    </tr>
  </table>

qlib_factor_experiment_setting: |-
  | Dataset 📊 | Model 🤖    | Factors 🌟       | Data Split  🧮                                   |
  |---------|----------|---------------|-------------------------------------------------|
  | CSI300  | LGBModel | Alpha158 Plus | Train: 2008-01-01 to 2014-12-31 <br> Valid: 2015-01-01 to 2016-12-31 <br> Test &nbsp;: 2017-01-01 to 2020-08-01 |


qlib_model_background: |-
  The model is a machine learning or deep learning structure used in quantitative investment to predict the returns and risks of a portfolio or a single asset. Models are employed by investors to generate forecasts based on historical data and identified factors, which are central to many quantitative investment strategies.
  Each model takes the factors as input and predicts the future returns. Usually, the bigger the model is, the better the performance would be.
  The model is defined in the following parts:
  1. Name: The name of the model.
  2. Description: The description of the model.
  3. Architecture: The detailed architecture of the model, such as neural network layers or tree structures.
  4. Hyperparameters: The hyperparameters used in the model, such as learning rate, number of epochs, etc.
  5. ModelType: The type of the model, "Tabular" for tabular model and "TimeSeries" for time series model.
  The model should provide clear and detailed documentation of its architecture and hyperparameters. One model should statically define one output with a fixed architecture and hyperparameters. For example, a model with an two GRU layer and a model with three GRU layer should be considered two different models.

qlib_model_interface: |-
  Your python code should follow the interface to better interact with the user's system.
  You code should contain several parts:
  1. The import part: import the necessary libraries.
  2. A class which is a sub-class of pytorch.nn.Module. This class should should have a init function and a forward function which inputs a tensor and outputs a tensor.
  3. Set a variable called "model_cls" to the class you defined.

  The user will save your code into a python file called "model.py". Then the user imports model_cls in file "model.py" after setting the cwd into the directory:
  ```python
  from model import model_cls
  ```
  So your python code should follow the pattern:
  ```python
  class XXXModel(torch.nn.Module):
      ...
  model_cls = XXXModel
  ```

  The model has two types, "Tabular" for tabular model and "TimeSeries" for time series model. The input shape to a tabular model is (batch_size, num_features) and the input shape to a time series model is (batch_size, num_features, num_timesteps). The output shape of the model should be (batch_size, 1).
  The "batch_size" is a dynamic value which is determined by the input of forward function.
  The "num_features" and "num_timesteps" are static which will be provided to the model through init function.
  User will initialize the tabular model with the following code:
  ```python
  model = model_cls(num_features=num_features)
  ```
  User will initialize the time series model with the following code:
  ```python
  model = model_cls(num_features=num_features, num_timesteps=num_timesteps)
  ```
  No other parameters will be passed to the model so give other parameters a default value or just make them static.

  When dealing with TimeSeries model, remember to permute the input tensor since the input tensor is in the shape of (batch_size, num_features, num_timesteps) and a normal time series model is expecting the input tensor in the shape of (batch_size, num_timesteps, num_features).

  Don't write any try-except block in your python code. The user will catch the exception message and provide the feedback to you. Also, don't write main function in your python code. The user will call the forward method in the model_cls to get the output tensor.

  Please notice that your model should only use current features as input. The user will provide the input tensor to the model's forward function.


qlib_model_output_format: |-
  Your output should be a tensor with shape (batch_size, 1). 
  The output tensor should be saved in a file named "output.pth" in the same directory as your python file.
  The user will evaluate the shape of the output tensor so the tensor read from "output.pth" should be 8 numbers.

qlib_model_simulator: |-
  The models will be sent into Qlib to train and evaluate their performance in predicting future returns. Hypothesis is improved upon checking the feedback on the results. 
  Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning (RL).
  User will use Qlib to automatically perform the following tasks:
  1. Generate a baseline factor table.
  2. Train the model defined in your class Net to predict the next several days' returns based on the factor values.
  3. Build a portfolio based on the predicted returns using a specific strategy.
  4. Evaluate the portfolio's performance, including metrics such as return, Sharpe ratio, max drawdown, and others.
  5. Iterate on growing the hypothesis to enable model improvements based on performance evaluations and feedback.

qlib_model_rich_style_description: |-
  ### Qlib Model Evolving Automatic R&D Demo
  
  #### [Overview](#_summary)
  
  The demo showcases the iterative process of hypothesis generation, knowledge construction, and decision-making in model construction in quantitative finance. It highlights how models evolve through continuous feedback and refinement.
  
  #### [Automated R&D](#_rdloops)
  
  - **[R (Research)](#_research)**
    - Iteration of ideas and hypotheses.
    - Continuous learning and knowledge construction.
  
  - **[D (Development)](#_development)**
    - Evolving code generation and model refinement.
    - Automated implementation and testing of models.
  
  #### [Objective](#_summary)
  
  To demonstrate the dynamic evolution of models through the Qlib platform, emphasizing how each iteration enhances the accuracy and reliability of the resulting models. 

qlib_model_experiment_setting: |-
  | Dataset 📊 | Model 🤖    | Factors 🌟       | Data Split  🧮                                   |
  |---------|----------|---------------|-------------------------------------------------|
  | CSI300  | RDAgent-dev | 20 factors (Alpha158)  | Train: 2008-01-01 to 2014-12-31 <br> Valid: 2015-01-01 to 2016-12-31 <br> Test &nbsp;: 2017-01-01 to 2020-08-01 |


================================================
File: rdagent/scenarios/qlib/experiment/utils.py
================================================
import io
import re
import shutil
from pathlib import Path

import pandas as pd

# render it with jinja
from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.factor_coder.config import FACTOR_COSTEER_SETTINGS
from rdagent.utils.env import QTDockerEnv


def generate_data_folder_from_qlib():
    template_path = Path(__file__).parent / "factor_data_template"
    qtde = QTDockerEnv()
    qtde.prepare()

    # Run the Qlib backtest
    execute_log = qtde.run(
        local_path=str(template_path),
        entry=f"python generate.py",
    )

    assert (
        Path(__file__).parent / "factor_data_template" / "daily_pv_all.h5"
    ).exists(), "daily_pv_all.h5 is not generated."
    assert (
        Path(__file__).parent / "factor_data_template" / "daily_pv_debug.h5"
    ).exists(), "daily_pv_debug.h5 is not generated."

    Path(FACTOR_COSTEER_SETTINGS.data_folder).mkdir(parents=True, exist_ok=True)
    shutil.copy(
        Path(__file__).parent / "factor_data_template" / "daily_pv_all.h5",
        Path(FACTOR_COSTEER_SETTINGS.data_folder) / "daily_pv.h5",
    )
    shutil.copy(
        Path(__file__).parent / "factor_data_template" / "README.md",
        Path(FACTOR_COSTEER_SETTINGS.data_folder) / "README.md",
    )

    Path(FACTOR_COSTEER_SETTINGS.data_folder_debug).mkdir(parents=True, exist_ok=True)
    shutil.copy(
        Path(__file__).parent / "factor_data_template" / "daily_pv_debug.h5",
        Path(FACTOR_COSTEER_SETTINGS.data_folder_debug) / "daily_pv.h5",
    )
    shutil.copy(
        Path(__file__).parent / "factor_data_template" / "README.md",
        Path(FACTOR_COSTEER_SETTINGS.data_folder_debug) / "README.md",
    )


def get_file_desc(p: Path, variable_list=[]) -> str:
    """
    Get the description of a file based on its type.

    Parameters
    ----------
    p : Path
        The path of the file.

    Returns
    -------
    str
        The description of the file.
    """
    p = Path(p)

    JJ_TPL = Environment(undefined=StrictUndefined).from_string(
        """
{{file_name}}
```{{type_desc}}
{{content}}
```
"""
    )

    if p.name.endswith(".h5"):
        df = pd.read_hdf(p)
        # get df.head() as string with full width
        pd.set_option("display.max_columns", None)  # or 1000
        pd.set_option("display.max_rows", None)  # or 1000
        pd.set_option("display.max_colwidth", None)  # or 199

        if isinstance(df.index, pd.MultiIndex):
            df_info = f"MultiIndex names:, {df.index.names})\n"
        else:
            df_info = f"Index name: {df.index.name}\n"
        columns = df.dtypes.to_dict()
        filtered_columns = [f"{i, j}" for i, j in columns.items() if i in variable_list]
        if filtered_columns:
            df_info += "Related Data columns: \n"
            df_info += ",".join(filtered_columns)
        else:
            df_info += "Data columns: \n"
            df_info += ",".join(columns)
        df_info += "\n"
        if "REPORT_PERIOD" in df.columns:
            one_instrument = df.index.get_level_values("instrument")[0]
            df_on_one_instrument = df.loc[pd.IndexSlice[:, one_instrument], ["REPORT_PERIOD"]]
            df_info += f"""
A snapshot of one instrument, from which you can tell the distribution of the data:
{df_on_one_instrument.head(5)}
"""
        return JJ_TPL.render(
            file_name=p.name,
            type_desc="h5 info",
            content=df_info,
        )
    elif p.name.endswith(".md"):
        with open(p) as f:
            content = f.read()
            return JJ_TPL.render(
                file_name=p.name,
                type_desc="markdown",
                content=content,
            )
    else:
        raise NotImplementedError(
            f"file type {p.name} is not supported. Please implement its description function.",
        )


def get_data_folder_intro(fname_reg: str = ".*", flags=0, variable_mapping=None) -> str:
    """
    Directly get the info of the data folder.
    It is for preparing prompting message.

    Parameters
    ----------
    fname_reg : str
        a regular expression to filter the file name.

    flags: str
        flags for re.match

    Returns
    -------
        str
            The description of the data folder.
    """

    if (
        not Path(FACTOR_COSTEER_SETTINGS.data_folder).exists()
        or not Path(FACTOR_COSTEER_SETTINGS.data_folder_debug).exists()
    ):
        # FIXME: (xiao) I think this is writing in a hard-coded way.
        # get data folder intro does not imply that we are generating the data folder.
        generate_data_folder_from_qlib()
    content_l = []
    for p in Path(FACTOR_COSTEER_SETTINGS.data_folder_debug).iterdir():
        if re.match(fname_reg, p.name, flags) is not None:
            if variable_mapping:
                content_l.append(get_file_desc(p, variable_mapping.get(p.stem, [])))
            else:
                content_l.append(get_file_desc(p))
    return "\n----------------- file splitter -------------\n".join(content_l)



================================================
File: rdagent/scenarios/qlib/experiment/workspace.py
================================================
from pathlib import Path
from typing import Any

import pandas as pd

from rdagent.core.experiment import FBWorkspace
from rdagent.log import rdagent_logger as logger
from rdagent.utils.env import QTDockerEnv


class QlibFBWorkspace(FBWorkspace):
    def __init__(self, template_folder_path: Path, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.inject_code_from_folder(template_folder_path)

    def execute(self, qlib_config_name: str = "conf.yaml", run_env: dict = {}, *args, **kwargs) -> str:
        qtde = QTDockerEnv()
        qtde.prepare()

        # Run the Qlib backtest
        execute_log = qtde.run(
            local_path=str(self.workspace_path),
            entry=f"qrun {qlib_config_name}",
            env=run_env,
        )

        execute_log = qtde.run(
            local_path=str(self.workspace_path),
            entry="python read_exp_res.py",
            env=run_env,
        )

        ret_df = pd.read_pickle(self.workspace_path / "ret.pkl")
        logger.log_object(ret_df, tag="Quantitative Backtesting Chart")

        csv_path = self.workspace_path / "qlib_res.csv"

        if not csv_path.exists():
            logger.error(f"File {csv_path} does not exist.")
            return None

        return pd.read_csv(csv_path, index_col=0).iloc[:, 0]



================================================
File: rdagent/scenarios/qlib/experiment/factor_data_template/README.md
================================================
# How to read files.
For example, if you want to read `filename.h5`
```Python
import pandas as pd
df = pd.read_hdf("filename.h5", key="data")
```
NOTE: **key is always "data" for all hdf5 files **.

# Here is a short description about the data

| Filename       | Description                                                      |
| -------------- | -----------------------------------------------------------------|
| "daily_pv.h5"  | Adjusted daily price and volume data.                            |


# For different data, We have some basic knowledge for them

## Daily price and volume data
$open: open price of the stock on that day.
$close: close price of the stock on that day.
$high: high price of the stock on that day.
$low: low price of the stock on that day.
$volume: volume of the stock on that day.
$factor: factor value of the stock on that day.


================================================
File: rdagent/scenarios/qlib/experiment/factor_data_template/generate.py
================================================
import qlib

qlib.init(provider_uri="~/.qlib/qlib_data/cn_data")

from qlib.data import D

instruments = D.instruments()
fields = ["$open", "$close", "$high", "$low", "$volume", "$factor"]
data = D.features(instruments, fields, freq="day").swaplevel().sort_index().loc["2008-12-29":].sort_index()

data.to_hdf("./daily_pv_all.h5", key="data")


fields = ["$open", "$close", "$high", "$low", "$volume", "$factor"]
data = (
    (
        D.features(instruments, fields, start_time="2018-01-01", end_time="2019-12-31", freq="day")
        .swaplevel()
        .sort_index()
    )
    .swaplevel()
    .loc[data.reset_index()["instrument"].unique()[:100]]
    .swaplevel()
    .sort_index()
)

data.to_hdf("./daily_pv_debug.h5", key="data")



================================================
File: rdagent/scenarios/qlib/experiment/factor_template/conf.yaml
================================================
qlib_init:
    provider_uri: "~/.qlib/qlib_data/cn_data"
    region: cn

market: &market csi300
benchmark: &benchmark SH000300

data_handler_config: &data_handler_config
    start_time: 2008-01-01
    end_time: 2020-08-01
    fit_start_time: 2008-01-01
    fit_end_time: 2014-12-31
    instruments: *market
port_analysis_config: &port_analysis_config
    strategy:
        class: TopkDropoutStrategy
        module_path: qlib.contrib.strategy
        kwargs:
            signal: <PRED>
            topk: 50
            n_drop: 5
    backtest:
        start_time: 2017-01-01
        end_time: 2020-08-01
        account: 100000000
        benchmark: *benchmark
        exchange_kwargs:
            limit_threshold: 0.095
            deal_price: close
            open_cost: 0.0005
            close_cost: 0.0015
            min_cost: 5
task:
    model:
        class: LGBModel
        module_path: qlib.contrib.model.gbdt
        kwargs:
            loss: mse
            colsample_bytree: 0.8879
            learning_rate: 0.2
            subsample: 0.8789
            lambda_l1: 205.6999
            lambda_l2: 580.9768
            max_depth: 8
            num_leaves: 210
            num_threads: 20
    dataset:
        class: DatasetH
        module_path: qlib.data.dataset
        kwargs:
            handler:
                class: Alpha158
                module_path: qlib.contrib.data.handler
                kwargs: *data_handler_config
            segments:
                train: [2008-01-01, 2014-12-31]
                valid: [2015-01-01, 2016-12-31]
                test: [2017-01-01, 2020-08-01]
    record: 
        - class: SignalRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            model: <MODEL>
            dataset: <DATASET>
        - class: SigAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            ana_long_short: False
            ann_scaler: 252
        - class: PortAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            config: *port_analysis_config



================================================
File: rdagent/scenarios/qlib/experiment/factor_template/conf_combined.yaml
================================================
qlib_init:
    provider_uri: "~/.qlib/qlib_data/cn_data"
    region: cn

market: &market csi300
benchmark: &benchmark SH000300

data_handler_config: &data_handler_config
    start_time: 2008-01-01
    end_time: 2022-08-01
    instruments: *market
    data_loader:
        class: NestedDataLoader
        kwargs:
            dataloader_l:
                - class: qlib.contrib.data.loader.Alpha158DL
                  kwargs:
                    config:
                        label: 
                            - ["Ref($close, -2)/Ref($close, -1) - 1"]
                            - ["LABEL0"]
                - class: qlib.data.dataset.loader.StaticDataLoader
                  kwargs:
                    config: "combined_factors_df.pkl"

    learn_processors:
        - class: DropnaLabel
        - class: CSZScoreNorm
          kwargs:
              fields_group: label

port_analysis_config: &port_analysis_config
    strategy:
        class: TopkDropoutStrategy
        module_path: qlib.contrib.strategy
        kwargs:
            signal: <PRED>
            topk: 50
            n_drop: 5
    backtest:
        start_time: 2017-01-01
        end_time: 2020-08-01
        account: 100000000
        benchmark: *benchmark
        exchange_kwargs:
            limit_threshold: 0.095
            deal_price: close
            open_cost: 0.0005
            close_cost: 0.0015
            min_cost: 5

task:
    model:
        class: LGBModel
        module_path: qlib.contrib.model.gbdt
        kwargs:
            loss: mse
            colsample_bytree: 0.8879
            learning_rate: 0.2
            subsample: 0.8789
            lambda_l1: 205.6999
            lambda_l2: 580.9768
            max_depth: 8
            num_leaves: 210
            num_threads: 20
    dataset:
        class: DatasetH
        module_path: qlib.data.dataset
        kwargs:
            handler:
                class: DataHandlerLP
                module_path: qlib.contrib.data.handler
                kwargs: *data_handler_config
            segments:
                train: [2008-01-01, 2014-12-31]
                valid: [2015-01-01, 2016-12-31]
                test: [2017-01-01, 2020-08-01]
    record: 
        - class: SignalRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            model: <MODEL>
            dataset: <DATASET>
        - class: SigAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            ana_long_short: False
            ann_scaler: 252
        - class: PortAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            config: *port_analysis_config



================================================
File: rdagent/scenarios/qlib/experiment/factor_template/read_exp_res.py
================================================
import pickle
from pathlib import Path

import pandas as pd
import qlib
from mlflow.entities import ViewType
from mlflow.tracking import MlflowClient

qlib.init()

from qlib.workflow import R

# here is the documents of the https://qlib.readthedocs.io/en/latest/component/recorder.html

# TODO: list all the recorder and metrics

# Assuming you have already listed the experiments
experiments = R.list_experiments()

# Iterate through each experiment to find the latest recorder
experiment_name = None
latest_recorder = None
for experiment in experiments:
    recorders = R.list_recorders(experiment_name=experiment)
    for recorder_id in recorders:
        if recorder_id is not None:
            experiment_name = experiment
            recorder = R.get_recorder(recorder_id=recorder_id, experiment_name=experiment)
            end_time = recorder.info["end_time"]
            if latest_recorder is None or end_time > latest_recorder.info["end_time"]:
                latest_recorder = recorder

# Check if the latest recorder is found
if latest_recorder is None:
    print("No recorders found")
else:
    print(f"Latest recorder: {latest_recorder}")

    # Load the specified file from the latest recorder
    metrics = pd.Series(latest_recorder.list_metrics())

    output_path = Path(__file__).resolve().parent / "qlib_res.csv"
    metrics.to_csv(output_path)

    print(f"Output has been saved to {output_path}")

    ret_data_frame = latest_recorder.load_object("portfolio_analysis/report_normal_1day.pkl")
    ret_data_frame.to_pickle("ret.pkl")



================================================
File: rdagent/scenarios/qlib/experiment/model_template/README.md
================================================
## This folder is a template to be copied from for each model implementation & running process. 

Components: Dummy model.py, versatile conf.yaml, and a result reader. 



================================================
File: rdagent/scenarios/qlib/experiment/model_template/conf.yaml
================================================
qlib_init:
    provider_uri: "~/.qlib/qlib_data/cn_data"
    region: cn
market: &market csi300
benchmark: &benchmark SH000300
data_handler_config: &data_handler_config
    start_time: 2008-01-01
    end_time: 2020-08-01
    fit_start_time: 2008-01-01
    fit_end_time: 2014-12-31
    instruments: *market
    infer_processors:
        - class: FilterCol
          kwargs:
              fields_group: feature
              col_list: ["RESI5", "WVMA5", "RSQR5", "KLEN", "RSQR10", "CORR5", "CORD5", "CORR10", 
                            "ROC60", "RESI10", "VSTD5", "RSQR60", "CORR60", "WVMA60", "STD5", 
                            "RSQR20", "CORD60", "CORD10", "CORR20", "KLOW"
                        ]
        - class: RobustZScoreNorm
          kwargs:
              fields_group: feature
              clip_outlier: true
        - class: Fillna
          kwargs:
              fields_group: feature
    learn_processors:
        - class: DropnaLabel
        - class: CSRankNorm
          kwargs:
              fields_group: label
    label: ["Ref($close, -2) / Ref($close, -1) - 1"] 

port_analysis_config: &port_analysis_config
    strategy:
        class: TopkDropoutStrategy
        module_path: qlib.contrib.strategy
        kwargs:
            signal: <PRED>
            topk: 50
            n_drop: 5
    backtest:
        start_time: 2017-01-01
        end_time: 2020-08-01
        account: 100000000
        benchmark: *benchmark
        exchange_kwargs:
            limit_threshold: 0.095
            deal_price: close
            open_cost: 0.0005
            close_cost: 0.0015
            min_cost: 5
task:
    model:
        class: GeneralPTNN
        module_path: qlib.contrib.model.pytorch_general_nn
        kwargs:
            n_epochs: 100
            lr: 1e-3
            early_stop: 10
            batch_size: 2000
            metric: loss
            loss: mse
            n_jobs: 20
            GPU: 0
            # loss: mse
            # lr: 0.002
            # optimizer: adam
            # batch_size: 8192
            # GPU: 0
            weight_decay: 0.0001
            # pt_model_uri: "qlib.contrib.model.pytorch_nn.Net"
            # pt_model_uri: "env_tpl.model.Net"
            # pt_model_uri: "./model.py:Net"
            pt_model_uri: "model.model_cls"
            pt_model_kwargs: {
                "num_features": 20,
                {% if num_timesteps %}num_timesteps: {{ num_timesteps }}{% endif %}
            }            
                # input_dim: 20
            #   How should I use jinja to put step len here conditionally          
    dataset:
        class: {{ dataset_cls | default("DatasetH") }}
        module_path: qlib.data.dataset
        kwargs:
            handler:
                class: Alpha158
                module_path: qlib.contrib.data.handler
                kwargs: *data_handler_config
            segments:
                train: [2008-01-01, 2014-12-31]
                valid: [2015-01-01, 2016-12-31]
                test: [2017-01-01, 2020-08-01]
            {% if step_len %}step_len: {{ step_len }}{% endif %}
    record: 
        - class: SignalRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            model: <MODEL>
            dataset: <DATASET>
        - class: SigAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            ana_long_short: False
            ann_scaler: 252
        - class: PortAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            config: *port_analysis_config



================================================
File: rdagent/scenarios/qlib/experiment/model_template/read_exp_res.py
================================================
import pickle
from pathlib import Path

import pandas as pd
import qlib
from mlflow.entities import ViewType
from mlflow.tracking import MlflowClient

qlib.init()

from qlib.workflow import R

# here is the documents of the https://qlib.readthedocs.io/en/latest/component/recorder.html

# TODO: list all the recorder and metrics

# Assuming you have already listed the experiments
experiments = R.list_experiments()

# Iterate through each experiment to find the latest recorder
experiment_name = None
latest_recorder = None
for experiment in experiments:
    recorders = R.list_recorders(experiment_name=experiment)
    for recorder_id in recorders:
        if recorder_id is not None:
            experiment_name = experiment
            recorder = R.get_recorder(recorder_id=recorder_id, experiment_name=experiment)
            end_time = recorder.info["end_time"]
            if latest_recorder is None or end_time > latest_recorder.info["end_time"]:
                latest_recorder = recorder

# Check if the latest recorder is found
if latest_recorder is None:
    print("No recorders found")
else:
    print(f"Latest recorder: {latest_recorder}")

    # Load the specified file from the latest recorder
    metrics = pd.Series(latest_recorder.list_metrics())

    output_path = Path(__file__).resolve().parent / "qlib_res.csv"
    metrics.to_csv(output_path)

    print(f"Output has been saved to {output_path}")

    ret_data_frame = latest_recorder.load_object("portfolio_analysis/report_normal_1day.pkl")
    ret_data_frame.to_pickle("ret.pkl")



================================================
File: rdagent/scenarios/qlib/factor_experiment_loader/json_loader.py
================================================
import json
from pathlib import Path

from rdagent.components.benchmark.eval_method import TestCase, TestCases
from rdagent.components.coder.factor_coder.factor import (
    FactorExperiment,
    FactorFBWorkspace,
    FactorTask,
)
from rdagent.components.loader.experiment_loader import FactorExperimentLoader
from rdagent.core.experiment import Experiment, Loader
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorExperiment


class FactorExperimentLoaderFromDict(FactorExperimentLoader):
    def load(self, factor_dict: dict) -> list:
        """Load data from a dict."""
        task_l = []
        for factor_name, factor_data in factor_dict.items():
            task = FactorTask(
                factor_name=factor_name,
                factor_description=factor_data["description"],
                factor_formulation=factor_data["formulation"],
                variables=factor_data["variables"],
            )
            task_l.append(task)
        exp = QlibFactorExperiment(sub_tasks=task_l)
        return exp


class FactorExperimentLoaderFromJsonFile(FactorExperimentLoader):
    def load(self, json_file_path: Path) -> list:
        with open(json_file_path, "r") as file:
            factor_dict = json.load(file)
        return FactorExperimentLoaderFromDict().load(factor_dict)


class FactorExperimentLoaderFromJsonString(FactorExperimentLoader):
    def load(self, json_string: str) -> list:
        factor_dict = json.loads(json_string)
        return FactorExperimentLoaderFromDict().load(factor_dict)


# TODO loader only supports generic of task or experiment, testcase might cause CI error here
# class FactorTestCaseLoaderFromJsonFile(Loader[TestCases]):
class FactorTestCaseLoaderFromJsonFile:
    def load(self, json_file_path: Path) -> TestCases:
        with open(json_file_path, "r") as file:
            factor_dict = json.load(file)
        test_cases = TestCases()
        for factor_name, factor_data in factor_dict.items():
            task = FactorTask(
                factor_name=factor_name,
                factor_description=factor_data["description"],
                factor_formulation=factor_data["formulation"],
                variables=factor_data["variables"],
            )
            gt = FactorFBWorkspace(task, raise_exception=False)
            code = {"factor.py": factor_data["gt_code"]}
            gt.inject_files(**code)
            test_cases.test_case_l.append(TestCase(task, gt))

        return test_cases



================================================
File: rdagent/scenarios/qlib/factor_experiment_loader/pdf_loader.py
================================================
from __future__ import annotations

import json
import multiprocessing as mp
import re
from pathlib import Path
from typing import Mapping

import numpy as np
import pandas as pd
from jinja2 import Environment, StrictUndefined
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize
from tqdm.auto import tqdm

from rdagent.components.document_reader.document_reader import (
    load_and_process_pdfs_by_langchain,
)
from rdagent.components.loader.experiment_loader import FactorExperimentLoader
from rdagent.core.conf import RD_AGENT_SETTINGS
from rdagent.core.prompts import Prompts
from rdagent.core.utils import multiprocessing_wrapper
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.qlib.factor_experiment_loader.json_loader import (
    FactorExperimentLoaderFromDict,
)

document_process_prompts = Prompts(file_path=Path(__file__).parent / "prompts.yaml")


def classify_report_from_dict(
    report_dict: Mapping[str, str],
    vote_time: int = 1,
    substrings: tuple[str] = (),
) -> dict[str, dict[str, str]]:
    """
    Parameters:
    - report_dict (Dict[str, str]):
      A dictionary where the key is the path of the report (ending with .pdf),
      and the value is either the report content as a string.
    - input_max_token (int): Specifying the maximum number of input tokens.
    - vote_time (int): An integer specifying how many times to vote.
    - substrings (list(str)): List of hardcode substrings.

    Returns:
    - Dict[str, Dict[str, str]]: A dictionary where each key is the path of the report,
      with a single key 'class' and its value being the classification result (0 or 1).

    """
    # if len(substrings) == 0:
    #     substrings = (
    #         "金融工程",
    #         "金工",
    #         "回测",
    #         "因子",
    #         "机器学习",
    #         "深度学习",
    #         "量化",
    #     )

    res_dict = {}
    classify_prompt = document_process_prompts["classify_system"]

    for key, value in tqdm(report_dict.items()):
        if not key.endswith(".pdf"):
            continue
        file_name = key

        if isinstance(value, str):
            content = value
        else:
            logger.warning(f"Input format does not meet the requirements: {file_name}")
            res_dict[file_name] = {"class": 0}
            continue

        # pre-filter document with key words is not necessary, skip this check for now
        # if (
        #     not any(substring in content for substring in substrings) and False
        # ):
        #     res_dict[file_name] = {"class": 0}
        # else:
        while (
            APIBackend().build_messages_and_calculate_token(
                user_prompt=content,
                system_prompt=classify_prompt,
            )
            > LLM_SETTINGS.chat_token_limit
        ):
            content = content[: -(LLM_SETTINGS.chat_token_limit // 100)]

        vote_list = []
        for _ in range(vote_time):
            user_prompt = content
            system_prompt = classify_prompt
            res = APIBackend().build_messages_and_create_chat_completion(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                json_mode=True,
            )
            try:
                res = json.loads(res)
                vote_list.append(int(res["class"]))
            except json.JSONDecodeError:
                logger.warning(f"Return value could not be parsed: {file_name}")
                res_dict[file_name] = {"class": 0}
            count_0 = vote_list.count(0)
            count_1 = vote_list.count(1)
            if max(count_0, count_1) > int(vote_time / 2):
                break

        result = 1 if count_1 > count_0 else 0
        res_dict[file_name] = {"class": result}

    return res_dict


def __extract_factors_name_and_desc_from_content(
    content: str,
) -> dict[str, dict[str, str]]:
    session = APIBackend().build_chat_session(
        session_system_prompt=document_process_prompts["extract_factors_system"],
    )

    extracted_factor_dict = {}
    current_user_prompt = content

    for _ in range(10):
        extract_result_resp = session.build_chat_completion(
            user_prompt=current_user_prompt,
            json_mode=True,
        )
        ret_dict = json.loads(extract_result_resp)
        factors = ret_dict["factors"]
        if len(factors) == 0:
            break
        for factor_name, factor_description in factors.items():
            extracted_factor_dict[factor_name] = factor_description
        current_user_prompt = document_process_prompts["extract_factors_follow_user"]

    return extracted_factor_dict


def __extract_factors_formulation_from_content(
    content: str,
    factor_dict: dict[str, str],
) -> dict[str, dict[str, str]]:
    factor_dict_df = pd.DataFrame(
        factor_dict.items(),
        columns=["factor_name", "factor_description"],
    )

    system_prompt = document_process_prompts["extract_factor_formulation_system"]
    current_user_prompt = (
        Environment(undefined=StrictUndefined)
        .from_string(
            document_process_prompts["extract_factor_formulation_user"],
        )
        .render(report_content=content, factor_dict=factor_dict_df.to_string())
    )

    session = APIBackend().build_chat_session(session_system_prompt=system_prompt)
    factor_to_formulation = {}

    for _ in range(10):
        extract_result_resp = session.build_chat_completion(
            user_prompt=current_user_prompt,
            json_mode=True,
        )
        ret_dict = json.loads(extract_result_resp)
        for name, formulation_and_description in ret_dict.items():
            if name in factor_dict:
                factor_to_formulation[name] = formulation_and_description
        if len(factor_to_formulation) != len(factor_dict):
            remain_df = factor_dict_df[~factor_dict_df["factor_name"].isin(factor_to_formulation)]
            current_user_prompt = (
                "Some factors are missing. Please check the following"
                " factors and their descriptions and continue extraction.\n"
                "==========================Remaining factors"
                "==========================\n" + remain_df.to_string()
            )
        else:
            break

    return factor_to_formulation


def __extract_factor_and_formulation_from_one_report(
    content: str,
) -> dict[str, dict[str, str]]:
    final_factor_dict_to_one_report = {}
    factor_dict = __extract_factors_name_and_desc_from_content(content)
    if len(factor_dict) != 0:
        factor_to_formulation = __extract_factors_formulation_from_content(
            content,
            factor_dict,
        )
    for factor_name in factor_dict:
        if (
            factor_name not in factor_to_formulation
            or "formulation" not in factor_to_formulation[factor_name]
            or "variables" not in factor_to_formulation[factor_name]
        ):
            continue

        final_factor_dict_to_one_report.setdefault(factor_name, {})
        final_factor_dict_to_one_report[factor_name]["description"] = factor_dict[factor_name]

        # use code to correct _ in formulation
        formulation = factor_to_formulation[factor_name]["formulation"]
        if factor_name in formulation:
            target_factor_name = factor_name.replace("_", r"\_")
            formulation = formulation.replace(factor_name, target_factor_name)
        for variable in factor_to_formulation[factor_name]["variables"]:
            if variable in formulation:
                target_variable = variable.replace("_", r"\_")
                formulation = formulation.replace(variable, target_variable)

        final_factor_dict_to_one_report[factor_name]["formulation"] = formulation
        final_factor_dict_to_one_report[factor_name]["variables"] = factor_to_formulation[factor_name]["variables"]

    return final_factor_dict_to_one_report


def extract_factors_from_report_dict(
    report_dict: dict[str, str],
    useful_no_dict: dict[str, dict[str, str]],
    n_proc: int = 11,
) -> dict[str, dict[str, dict[str, str]]]:
    useful_report_dict = {}
    for key, value in useful_no_dict.items():
        if isinstance(value, dict):
            if int(value.get("class")) == 1:
                useful_report_dict[key] = report_dict[key]
        else:
            logger.warning(f"Invalid input format: {key}")

    file_name_list = list(useful_report_dict.keys())

    final_report_factor_dict = {}
    factor_dict_list = multiprocessing_wrapper(
        [
            (__extract_factor_and_formulation_from_one_report, (useful_report_dict[file_name],))
            for file_name in file_name_list
        ],
        n=RD_AGENT_SETTINGS.multi_proc_n,
    )
    for index, file_name in enumerate(file_name_list):
        final_report_factor_dict[file_name] = factor_dict_list[index]
    logger.info(f"Factor extraction completed for {len(final_report_factor_dict)} reports")

    return final_report_factor_dict


def merge_file_to_factor_dict_to_factor_dict(
    file_to_factor_dict: dict[str, dict],
) -> dict:
    factor_dict = {}
    for file_name in file_to_factor_dict:
        for factor_name in file_to_factor_dict[file_name]:
            factor_dict.setdefault(factor_name, [])
            factor_dict[factor_name].append(file_to_factor_dict[file_name][factor_name])

    factor_dict_simple_deduplication = {}
    for factor_name in factor_dict:
        if len(factor_dict[factor_name]) > 1:
            factor_dict_simple_deduplication[factor_name] = max(
                factor_dict[factor_name],
                key=lambda x: len(x["formulation"]),
            )
        else:
            factor_dict_simple_deduplication[factor_name] = factor_dict[factor_name][0]
    return factor_dict_simple_deduplication


def __check_factor_dict_relevance(
    factor_df_string: str,
) -> dict[str, dict[str, str]]:
    extract_result_resp = APIBackend().build_messages_and_create_chat_completion(
        system_prompt=document_process_prompts["factor_relevance_system"],
        user_prompt=factor_df_string,
        json_mode=True,
    )
    return json.loads(extract_result_resp)


def check_factor_relevance(
    factor_dict: dict[str, dict[str, str]],
) -> tuple[dict[str, dict[str, str]], dict[str, dict[str, str]]]:
    factor_relevance_dict = {}

    factor_df = pd.DataFrame(factor_dict).T
    factor_df.index.names = ["factor_name"]

    while factor_df.shape[0] > 0:
        result_list = multiprocessing_wrapper(
            [
                (__check_factor_dict_relevance, (factor_df.iloc[i : i + 50, :].to_string(),))
                for i in range(0, factor_df.shape[0], 50)
            ],
            n=RD_AGENT_SETTINGS.multi_proc_n,
        )

        for result in result_list:
            for factor_name, relevance in result.items():
                factor_relevance_dict[factor_name] = relevance

        factor_df = factor_df[~factor_df.index.isin(factor_relevance_dict)]

    filtered_factor_dict = {
        factor_name: factor_dict[factor_name]
        for factor_name in factor_dict
        if factor_relevance_dict[factor_name]["relevance"]
    }

    return factor_relevance_dict, filtered_factor_dict


def __check_factor_dict_viability_simulate_json_mode(
    factor_df_string: str,
) -> dict[str, dict[str, str]]:
    extract_result_resp = APIBackend().build_messages_and_create_chat_completion(
        system_prompt=document_process_prompts["factor_viability_system"],
        user_prompt=factor_df_string,
        json_mode=True,
    )
    return json.loads(extract_result_resp)


def check_factor_viability(
    factor_dict: dict[str, dict[str, str]],
) -> tuple[dict[str, dict[str, str]], dict[str, dict[str, str]]]:
    factor_viability_dict = {}

    factor_df = pd.DataFrame(factor_dict).T
    factor_df.index.names = ["factor_name"]

    while factor_df.shape[0] > 0:
        result_list = multiprocessing_wrapper(
            [
                (__check_factor_dict_viability_simulate_json_mode, (factor_df.iloc[i : i + 50, :].to_string(),))
                for i in range(0, factor_df.shape[0], 50)
            ],
            n=RD_AGENT_SETTINGS.multi_proc_n,
        )

        for result in result_list:
            for factor_name, viability in result.items():
                factor_viability_dict[factor_name] = viability

        factor_df = factor_df[~factor_df.index.isin(factor_viability_dict)]

    filtered_factor_dict = {
        factor_name: factor_dict[factor_name]
        for factor_name in factor_dict
        if factor_viability_dict[factor_name]["viability"]
    }

    return factor_viability_dict, filtered_factor_dict


def __check_factor_duplication_simulate_json_mode(
    factor_df: pd.DataFrame,
) -> list[list[str]]:
    current_user_prompt = factor_df.to_string()

    working_list = [factor_df]
    final_list = []

    while len(working_list) > 0:
        current_df = working_list.pop(0)
        if (
            APIBackend().build_messages_and_calculate_token(
                user_prompt=current_df.to_string(), system_prompt=document_process_prompts["factor_duplicate_system"]
            )
            > LLM_SETTINGS.chat_token_limit
        ):
            working_list.append(current_df.iloc[: current_df.shape[0] // 2, :])
            working_list.append(current_df.iloc[current_df.shape[0] // 2 :, :])
        else:
            final_list.append(current_df)

    generated_duplicated_groups = []
    for current_df in final_list:
        current_factor_to_string = current_df.to_string()
        session = APIBackend().build_chat_session(
            session_system_prompt=document_process_prompts["factor_duplicate_system"],
        )
        for _ in range(10):
            extract_result_resp = session.build_chat_completion(
                user_prompt=current_factor_to_string,
                json_mode=True,
            )
            ret_dict = json.loads(extract_result_resp)
            if len(ret_dict) == 0:
                return generated_duplicated_groups
            else:
                generated_duplicated_groups.extend(ret_dict)
                current_factor_to_string = """Continue to extract duplicated groups. If no more duplicated group found please respond empty dict."""
    return generated_duplicated_groups


def __kmeans_embeddings(embeddings: np.ndarray, k: int = 20) -> list[list[str]]:
    x_normalized = normalize(embeddings)

    np.random.seed(42)

    kmeans = KMeans(
        n_clusters=k,
        init="random",
        max_iter=100,
        n_init=10,
        random_state=42,
    )

    # KMeans algorithm uses Euclidean distance, and we need to customize a function to find the most similar cluster center
    def find_closest_cluster_cosine_similarity(
        data: np.ndarray,
        centroids: np.ndarray,
    ) -> np.ndarray:
        similarity = cosine_similarity(data, centroids)
        return np.argmax(similarity, axis=1)

    # Initializes the cluster center
    rng = np.random.default_rng(seed=42)
    centroids = rng.choice(x_normalized, size=k, replace=False)

    # Iterate until convergence or the maximum number of iterations is reached
    for _ in range(kmeans.max_iter):
        # Assign the sample to the nearest cluster center
        closest_clusters = find_closest_cluster_cosine_similarity(
            x_normalized,
            centroids,
        )

        # update the cluster center
        new_centroids = np.array(
            [x_normalized[closest_clusters == i].mean(axis=0) for i in range(k)],
        )
        new_centroids = normalize(new_centroids)  # 归一化新的簇中心

        # Check whether the cluster center has changed
        if np.allclose(centroids, new_centroids):
            break

        centroids = new_centroids

    clusters = find_closest_cluster_cosine_similarity(x_normalized, centroids)
    cluster_to_index = {}
    for index, cluster in enumerate(clusters):
        cluster_to_index.setdefault(cluster, []).append(index)
    return sorted(
        cluster_to_index.values(),
        key=lambda x: len(x),
        reverse=True,
    )


def __deduplicate_factor_dict(factor_dict: dict[str, dict[str, str]]) -> list[list[str]]:
    if len(factor_dict) == 0:
        return []
    factor_df = pd.DataFrame(factor_dict).T
    factor_df.index.names = ["factor_name"]

    factor_names = sorted(factor_dict)

    factor_name_to_full_str = {}
    for factor_name in factor_dict:
        description = factor_dict[factor_name]["description"]
        formulation = factor_dict[factor_name]["formulation"]
        variables = factor_dict[factor_name]["variables"]
        factor_name_to_full_str[
            factor_name
        ] = f"""Factor name: {factor_name}
Factor description: {description}
Factor formulation: {formulation}
Factor variables: {variables}
"""

    full_str_list = [factor_name_to_full_str[factor_name] for factor_name in factor_names]
    embeddings = APIBackend.create_embedding(full_str_list)

    target_k = None
    if len(full_str_list) < RD_AGENT_SETTINGS.max_input_duplicate_factor_group:
        kmeans_index_group = [list(range(len(full_str_list)))]
        target_k = 1
    else:
        for k in range(
            len(full_str_list) // RD_AGENT_SETTINGS.max_input_duplicate_factor_group,
            RD_AGENT_SETTINGS.max_kmeans_group_number,
        ):
            kmeans_index_group = __kmeans_embeddings(embeddings=embeddings, k=k)
            if len(kmeans_index_group[0]) < RD_AGENT_SETTINGS.max_input_duplicate_factor_group:
                target_k = k
                logger.info(f"K-means group number: {k}")
                break
    factor_name_groups = [[factor_names[index] for index in index_group] for index_group in kmeans_index_group]

    duplication_names_list = []

    result_list = multiprocessing_wrapper(
        [
            (__check_factor_duplication_simulate_json_mode, (factor_df.loc[factor_name_group, :],))
            for factor_name_group in factor_name_groups
        ],
        n=RD_AGENT_SETTINGS.multi_proc_n,
    )

    duplication_names_list = []

    for deduplication_factor_names_list in result_list:
        filter_factor_names = [
            factor_name for factor_name in set(deduplication_factor_names_list) if factor_name in factor_dict
        ]
        if len(filter_factor_names) > 1:
            duplication_names_list.append(filter_factor_names)

    return duplication_names_list


def deduplicate_factors_by_llm(  # noqa: C901, PLR0912
    factor_dict: dict[str, dict[str, str]],
    factor_viability_dict: dict[str, dict[str, str]] | None = None,
) -> list[list[str]]:
    final_duplication_names_list = []
    current_round_factor_dict = factor_dict

    # handle multi-round deduplication
    for _ in range(10):
        duplication_names_list = __deduplicate_factor_dict(current_round_factor_dict)

        new_round_names = []
        for duplication_names in duplication_names_list:
            if len(duplication_names) < RD_AGENT_SETTINGS.max_output_duplicate_factor_group:
                final_duplication_names_list.append(duplication_names)
            else:
                new_round_names.extend(duplication_names)

        if len(new_round_names) != 0:
            current_round_factor_dict = {factor_name: factor_dict[factor_name] for factor_name in new_round_names}
        else:
            break

    # sort the final list of duplicates by their length, largest first
    final_duplication_names_list = sorted(final_duplication_names_list, key=lambda x: len(x), reverse=True)

    to_replace_dict = {}  # to map duplicates to the target factor names
    for duplication_names in duplication_names_list:
        if factor_viability_dict is not None:
            # check viability of each factor in the duplicates group
            viability_list = [factor_viability_dict[name]["viability"] for name in duplication_names]
            if True not in viability_list:
                continue
            target_factor_name = duplication_names[viability_list.index(True)]
        else:
            target_factor_name = duplication_names[0]
        for duplication_factor_name in duplication_names:
            if duplication_factor_name == target_factor_name:
                continue
            to_replace_dict[duplication_factor_name] = target_factor_name

    llm_deduplicated_factor_dict = {}
    added_lower_name_set = set()
    for factor_name in factor_dict:
        # only add factors that haven't been replaced and are not duplicates
        if factor_name not in to_replace_dict and factor_name.lower() not in added_lower_name_set:
            if factor_viability_dict is not None and not factor_viability_dict[factor_name]["viability"]:
                continue
            added_lower_name_set.add(factor_name.lower())
            llm_deduplicated_factor_dict[factor_name] = factor_dict[factor_name]

    return llm_deduplicated_factor_dict, final_duplication_names_list


class FactorExperimentLoaderFromPDFfiles(FactorExperimentLoader):
    def load(self, file_or_folder_path: str) -> dict:
        with logger.tag("docs"):
            docs_dict = load_and_process_pdfs_by_langchain(file_or_folder_path)
            logger.log_object(docs_dict)

        selected_report_dict = classify_report_from_dict(report_dict=docs_dict, vote_time=1)

        with logger.tag("file_to_factor_result"):
            file_to_factor_result = extract_factors_from_report_dict(docs_dict, selected_report_dict)
            logger.log_object(file_to_factor_result)

        with logger.tag("factor_dict"):
            factor_dict = merge_file_to_factor_dict_to_factor_dict(file_to_factor_result)
            logger.log_object(factor_dict)

        with logger.tag("filtered_factor_dict"):
            factor_viability, filtered_factor_dict = check_factor_viability(factor_dict)
            logger.log_object(filtered_factor_dict)

        # factor_dict, duplication_names_list = deduplicate_factors_by_llm(factor_dict, factor_viability)

        return FactorExperimentLoaderFromDict().load(filtered_factor_dict)



================================================
File: rdagent/scenarios/qlib/factor_experiment_loader/prompts.yaml
================================================
extract_factors_system: |-
    用户会提供一篇金融工程研报，其中包括了量化因子和模型研究，请按照要求抽取以下信息:
    1. 概述这篇研报的主要研究思路;
    2. 抽取出所有的因子，并概述因子的计算过程，请注意有些因子可能存在于表格中，请不要遗漏，因子的名称请使用英文，不能包含空格，可用下划线连接，研报中可能不含有因子，若没有请返回空字典;
    3. 抽取研报里面的所有模型，并概述模型的计算过程，可以分步骤描述模型搭建或计算的过程，研报中可能不含有模型，若没有请返回空字典;

    user will treat your factor name as key to store the factor, don't put any interaction message in the content. Just response the output without any interaction and explanation.
    All names should be in English.
    Respond with your analysis in JSON format. The JSON schema should include:
    {
        "summary": "The summary of this report",
        "factors": {
            "Name of factor 1": "Description to factor 1",
            "Name of factor 2": "Description to factor 2"
        },
        "models": {
            "Name of model 1": "Description to model 1",
            "Name of model 2": "Description to model 2"
        }
    }

extract_factors_follow_user: |-
    Please continue extracting the factors. Please ignore factors appeared in former messages. If no factor is found, please return an empty dict.
    Notice: You should not miss any factor in the report! Some factors might appear several times in the report. You can repeat them to avoid missing other factors.
    Respond with your analysis in JSON format. The JSON schema should include:
    {
        "factors": {
            "Name of factor 1": "Description to factor 1",
            "Name of factor 2": "Description to factor 2"
        }
    }

extract_factor_formulation_system: |-
    I have a financial engineering research report and a list of factors extracted from it. I need assistance in extracting specific information based on the report and the provided list of factors. The tasks are as follows:

    1. For each factor, I need its calculation formula in LaTeX format. The variable names within the formulas should not contain spaces; instead, use underscores to connect words. Ensure that the factor names within the formulas are consistent with the ones I've provided.
    2. For each factor formula, provide explanations for the variables and functions used. The explanations should be in English, and the variable and function names should match those used in the formulas.

    Here are the sources of data I have:

    1. Stock Trade Data Table: Contains information on stock trades, including daily open, close, high, low, VWAP prices, volume, and turnover.
    2. Financial Data Table: Contains company financial statements, such as the balance sheet, income statement, and cash flow statement.
    3. Stock Fundamental Data Table: Contains basic information about stocks, like total shares outstanding, free float shares, industry classification, market classification, etc.
    4. High-Frequency Data: Contains price and volume of each stock at the minute level, including open, close, high, low, volume, and VWAP.

    Please expand the formulation to use the source data I have provided. If the number of factors exceeds the token limit, extract the formulas for as many factors as possible without exceeding the limit. Ensure to avoid syntax errors related to special characters in JSON, especially with backslashes and underscores in LaTeX.

    Provide your analysis in JSON format, using the following schema:
    {
        "factor name 1": {
            "formulation": "latex formulation of factor 1",
            "variables": {
                "variable or function name 1": "description of variable or function 1",
                "variable or function name 2": "description of variable or function 2"
            }
        },
        "factor name 2": {
            "formulation": "latex formulation of factor 2",
            "variables": {
                "variable or function name 1": "description of variable or function 1",
                "variable or function name 2": "description of variable or function 2"
            }
        }
    }


extract_factor_formulation_user: |-
    ===========================Report content:=============================
    {{ report_content }}
    ===========================Factor list in dataframe=============================
    {{ factor_dict }}

classify_system_chinese: |-
    你是一个研报分类助手。用户会输入一篇金融研报。请按照要求回答：
    因子指能够解释资产收益率或价格等的变量；而模型则指机器学习或深度学习模型，利用因子等变量来预测价格或收益率变化。

    请你对研报进行分类，考虑两个条件：
        1. 是金工量化领域中选股（需与择时，选基等严格区分开）方面的研报;
        2. 涉及了因子或模型的构成，或者是测试了它们的表现。
    如果研报同时满足上述两个条件，请输出1；若没有，请输出0。

    请使用json进行回答。json key为：class

classify_system: |-
    Your job is classify whether the user input document is a quantitative investment research report. The user will input a document and you should classify it based on the following conditions:
    1. The document is about finance other than other fields like biology, physics, chemistry, etc.
    2. The document is a research report on stock selection (which needs to be strictly separated from time selection and base selection) in the field of metalworking quantification.
    3. The document involves the composition of factors or models, or tests their performance.

    If the document meets all the above conditions, please return 1; otherwise, please return 0.

    Please respond with your decision in JSON format. Just respond the output json string without any interaction and explanation.
    The JSON schema should include:
    {
        "class": 1
    }

factor_viability_system: |-
    User has designed several factors in quant investment. Please help the user to check the viability of these factors.
    These factors are used to build a daily frequency strategy in China A-share market.

    User will provide a pandas dataframe like table containing following information:
    1. The name of the factor;
    2. The simple description of the factor;
    3. The formulation of the factor in latex format;
    4. The description to the variables and functions in the formulation of the factor.

    User has several source data:
    1. The Stock Trade Data Table containing information about stock trades, such as daily open, close, high, low, vwap prices, volume, and turnover;
    2. The Financial Data Table containing company financial statements such as the balance sheet, income statement, and cash flow statement;
    3. The Stock Fundamental Data Table containing basic information about stocks, like total shares outstanding, free float shares, industry classification, market classification, etc;
    4. The high frequency data containing price and volume of each stock containing open close high low volume vwap in each minute;
    5. The Consensus Expectations Factor containing the consensus expectations of the analysts about the future performance of the company.


    A viable factor should satisfy the following conditions:
    1. The factor should be able to be calculated in daily frequency;
    2. The factor should be able to be calculated based on each stock;
    3. The factor should be able to be calculated based on the source data provided by user.

    You should give decision to each factor provided by the user. You should reject the factor based on very solid reason.
    Please return true to the viable factor and false to the non-viable factor.

    Notice, you can just return part of the factors due to token limit. Your factor name should be the same as the user's factor name.

    Please respond with your decision in JSON format. Just respond the output json string without any interaction and explanation.
    The JSON schema should include:
    {
        "Name to factor 1":
        {
            "viability": true,
            "reason": "The reason to the viability of this factor"
        },
        "Name to factor 2":
        {
            "viability": false,
            "reason": "The reason to the non-viability of this factor"
        }
        "Name to factor 3":
        {
            "viability": true,
            "reason": "The reason to the viability of this factor"
        }
    }

factor_relevance_system: |-
    User has designed several factors in quant investment. Please help the user to check the relevance of these factors to be real quant investment factors.
    These factors are used to build a daily frequency strategy in China A-share market.

    User will provide a pandas dataframe like table containing following information:
    1. The name of the factor;
    2. The simple description of the factor;
    3. The formulation of the factor in latex format;
    4. The description to the variables and functions in the formulation of the factor.

    A relevant factor should satisfy the following conditions:
    1. The factor should be able to be calculated in daily frequency;
    2. The factor should be able to be calculated based on each stock;
    3. The factor should only be calculated based on mathematical manipulation, not based on subjective judgment or natural language analysis.

    You should give decision to each factor provided by the user. You should reject the factor based on very solid reason.
    Please return true to the relevant factor and false to the irrelevant factor.

    Notice, you can just return part of the factors due to token limit. Your factor name should be the same as the user's factor name.

    Please respond with your decision in JSON format. Just respond the output json string without any interaction and explanation.
    The JSON schema should include:
    {
        "Name to factor 1":
        {
            "relevance": true,
            "reason": "The reason to the relevance of this factor"
        },
        "Name to factor 2":
        {
            "relevance": false,
            "reason": "The reason to the non-relevance of this factor"
        }
        "Name to factor 3":
        {
            "relevance": true,
            "reason": "The reason to the relevance of this factor"
        }
    }


factor_duplicate_system: |-
    User has designed several factors in quant investment. Please help the user to duplicate these factors.
    These factors are used to build a daily frequency strategy in China A-share market.

    User will provide a pandas dataframe like table containing following information:
    1. The name of the factor;
    2. The simple description of the factor;
    3. The formulation of the factor in latex format;
    4. The description to the variables and functions in the formulation of the factor.

    User wants to find whether there are duplicated groups. The factors in a duplicate group should satisfy the following conditions:
    1. They might differ in the name, description, formulation, or the description to the variables and functions in the formulation, some upper or lower case difference is included;
    2. They should be talking about exactly the same factor;
    3. If horizon information like 1 day, 5 days, 10 days, etc is provided, the horizon information should be the same.

    To make your response valid, we have some very important constraint for you to follow! Listed here:
    1. You should be very confident to put duplicated factors into a group;
    2. A group should contain at least two factors;
    3. To a factor which has no duplication, don't put them into your response;
    4. To avoid merging too many similar factor, don't put more than ten factors into a group!
    You should always follow the above constraints to make your response valid. 

    Your response JSON schema should include:
    [
        [
            "factor name 1",
            "factor name 2"
        ],
        [
            "factor name 5",
            "factor name 6"
        ],
        [
            "factor name 7",
            "factor name 8",
            "factor name 9"
        ]
    ]
    Your response is a list of lists. Each list represents a duplicate group containing all the factor names in this group. 
    The factor names in the list should be unique and the factor names should be the same as the user's factor name.
    To avoid reaching token limit, don't respond more than fifty groups in one response. You should respond the output json string without any interaction and explanation.


================================================
File: rdagent/scenarios/qlib/proposal/factor_proposal.py
================================================
import json
from pathlib import Path
from typing import List, Tuple

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.factor_coder.factor import FactorExperiment, FactorTask
from rdagent.components.proposal import FactorHypothesis2Experiment, FactorHypothesisGen
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import Hypothesis, Scenario, Trace
from rdagent.scenarios.qlib.experiment.factor_experiment import QlibFactorExperiment

prompt_dict = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")

QlibFactorHypothesis = Hypothesis


class QlibFactorHypothesisGen(FactorHypothesisGen):
    def __init__(self, scen: Scenario) -> Tuple[dict, bool]:
        super().__init__(scen)

    def prepare_context(self, trace: Trace) -> Tuple[dict, bool]:
        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )
        context_dict = {
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "RAG": None,
            "hypothesis_output_format": prompt_dict["hypothesis_output_format"],
            "hypothesis_specification": prompt_dict["factor_hypothesis_specification"],
        }
        return context_dict, True

    def convert_response(self, response: str) -> Hypothesis:
        response_dict = json.loads(response)
        hypothesis = QlibFactorHypothesis(
            hypothesis=response_dict["hypothesis"],
            reason=response_dict["reason"],
            concise_reason=response_dict["concise_reason"],
            concise_observation=response_dict["concise_observation"],
            concise_justification=response_dict["concise_justification"],
            concise_knowledge=response_dict["concise_knowledge"],
        )
        return hypothesis


class QlibFactorHypothesis2Experiment(FactorHypothesis2Experiment):
    def prepare_context(self, hypothesis: Hypothesis, trace: Trace) -> Tuple[dict | bool]:
        scenario = trace.scen.get_scenario_all_desc()
        experiment_output_format = prompt_dict["factor_experiment_output_format"]

        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )

        experiment_list: List[FactorExperiment] = [t[0] for t in trace.hist]

        factor_list = []
        for experiment in experiment_list:
            factor_list.extend(experiment.sub_tasks)

        return {
            "target_hypothesis": str(hypothesis),
            "scenario": scenario,
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "experiment_output_format": experiment_output_format,
            "target_list": factor_list,
            "RAG": None,
        }, True

    def convert_response(self, response: str, hypothesis: Hypothesis, trace: Trace) -> FactorExperiment:
        response_dict = json.loads(response)
        tasks = []

        for factor_name in response_dict:
            description = response_dict[factor_name]["description"]
            formulation = response_dict[factor_name]["formulation"]
            variables = response_dict[factor_name]["variables"]
            tasks.append(
                FactorTask(
                    factor_name=factor_name,
                    factor_description=description,
                    factor_formulation=formulation,
                    variables=variables,
                )
            )

        exp = QlibFactorExperiment(tasks, hypothesis=hypothesis)
        exp.based_experiments = [QlibFactorExperiment(sub_tasks=[])] + [t[0] for t in trace.hist if t[1]]

        unique_tasks = []

        for task in tasks:
            duplicate = False
            for based_exp in exp.based_experiments:
                for sub_task in based_exp.sub_tasks:
                    if task.factor_name == sub_task.factor_name:
                        duplicate = True
                        break
                if duplicate:
                    break
            if not duplicate:
                unique_tasks.append(task)

        exp.tasks = unique_tasks
        return exp



================================================
File: rdagent/scenarios/qlib/proposal/model_proposal.py
================================================
import json
from pathlib import Path
from typing import List, Tuple

from jinja2 import Environment, StrictUndefined

from rdagent.components.coder.model_coder.model import ModelExperiment, ModelTask
from rdagent.components.proposal import ModelHypothesis2Experiment, ModelHypothesisGen
from rdagent.core.prompts import Prompts
from rdagent.core.proposal import Hypothesis, Scenario, Trace
from rdagent.scenarios.qlib.experiment.model_experiment import QlibModelExperiment

prompt_dict = Prompts(file_path=Path(__file__).parent.parent / "prompts.yaml")

QlibModelHypothesis = Hypothesis


class QlibModelHypothesisGen(ModelHypothesisGen):
    def __init__(self, scen: Scenario) -> Tuple[dict, bool]:
        super().__init__(scen)

    def prepare_context(self, trace: Trace) -> Tuple[dict, bool]:
        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )
        context_dict = {
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "RAG": "In Quantitative Finance, market data could be time-series, and GRU model/LSTM model are suitable for them. Do not generate GNN model as for now.",
            "hypothesis_output_format": prompt_dict["hypothesis_output_format"],
            "hypothesis_specification": prompt_dict["model_hypothesis_specification"],
        }
        return context_dict, True

    def convert_response(self, response: str) -> Hypothesis:
        response_dict = json.loads(response)
        hypothesis = QlibModelHypothesis(
            hypothesis=response_dict["hypothesis"],
            reason=response_dict["reason"],
            concise_reason=response_dict["concise_reason"],
            concise_observation=response_dict["concise_observation"],
            concise_justification=response_dict["concise_justification"],
            concise_knowledge=response_dict["concise_knowledge"],
        )
        return hypothesis


class QlibModelHypothesis2Experiment(ModelHypothesis2Experiment):
    def prepare_context(self, hypothesis: Hypothesis, trace: Trace) -> Tuple[dict, bool]:
        scenario = trace.scen.get_scenario_all_desc()
        experiment_output_format = prompt_dict["model_experiment_output_format"]

        hypothesis_and_feedback = (
            (
                Environment(undefined=StrictUndefined)
                .from_string(prompt_dict["hypothesis_and_feedback"])
                .render(trace=trace)
            )
            if len(trace.hist) > 0
            else "No previous hypothesis and feedback available since it's the first round."
        )

        experiment_list: List[ModelExperiment] = [t[0] for t in trace.hist]

        model_list = []
        for experiment in experiment_list:
            model_list.extend(experiment.sub_tasks)

        return {
            "target_hypothesis": str(hypothesis),
            "scenario": scenario,
            "hypothesis_and_feedback": hypothesis_and_feedback,
            "experiment_output_format": experiment_output_format,
            "target_list": model_list,
            "RAG": None,
        }, True

    def convert_response(self, response: str, hypothesis: Hypothesis, trace: Trace) -> ModelExperiment:
        response_dict = json.loads(response)
        tasks = []
        for model_name in response_dict:
            description = response_dict[model_name]["description"]
            formulation = response_dict[model_name]["formulation"]
            architecture = response_dict[model_name]["architecture"]
            variables = response_dict[model_name]["variables"]
            hyperparameters = response_dict[model_name]["hyperparameters"]
            model_type = response_dict[model_name]["model_type"]
            tasks.append(
                ModelTask(
                    name=model_name,
                    description=description,
                    formulation=formulation,
                    architecture=architecture,
                    variables=variables,
                    hyperparameters=hyperparameters,
                    model_type=model_type,
                )
            )
        exp = QlibModelExperiment(tasks, hypothesis=hypothesis)
        exp.based_experiments = [t[0] for t in trace.hist if t[1]]
        return exp



================================================
File: rdagent/utils/__init__.py
================================================
"""
This is some common utils functions.
it is not binding to the scenarios or framework (So it is not placed in rdagent.core.utils)
"""

# TODO: merge the common utils in `rdagent.core.utils` into this folder
# TODO: split the utils in this module into different modules in the future.

import hashlib
import importlib
import json
import re
import sys
from pathlib import Path
from types import ModuleType
from typing import Union

from rdagent.oai.llm_conf import LLM_SETTINGS
from rdagent.utils.agent.tpl import T


def get_module_by_module_path(module_path: Union[str, ModuleType]) -> ModuleType:
    """Load module from path like a/b/c/d.py or a.b.c.d

    :param module_path:
    :return:
    :raises: ModuleNotFoundError
    """
    if module_path is None:
        raise ModuleNotFoundError("None is passed in as parameters as module_path")

    if isinstance(module_path, ModuleType):
        module = module_path
    else:
        if module_path.endswith(".py"):
            module_name = re.sub("^[^a-zA-Z_]+", "", re.sub("[^0-9a-zA-Z_]", "", module_path[:-3].replace("/", "_")))
            module_spec = importlib.util.spec_from_file_location(module_name, module_path)
            if module_spec is None:
                raise ModuleNotFoundError(f"Cannot find module at {module_path}")
            module = importlib.util.module_from_spec(module_spec)
            sys.modules[module_name] = module
            if module_spec.loader is not None:
                module_spec.loader.exec_module(module)
            else:
                raise ModuleNotFoundError(f"Cannot load module at {module_path}")
        else:
            module = importlib.import_module(module_path)
    return module


def convert2bool(value: Union[str, bool]) -> bool:
    """
    Motivation: the return value of LLM is not stable. Try to convert the value into bool
    """
    # TODO: if we have more similar functions, we can build a library to converting unstable LLM response to stable results.
    if isinstance(value, str):
        v = value.lower().strip()
        if v in ["true", "yes", "ok"]:
            return True
        if v in ["false", "no"]:
            return False
        raise ValueError(f"Can not convert {value} to bool")
    elif isinstance(value, bool):
        return value
    else:
        raise ValueError(f"Unknown value type {value} to bool")


def remove_ansi_codes(s: str) -> str:
    """
    It is for removing ansi ctrl characters in the string(e.g. colored text)
    """
    ansi_escape = re.compile(r"\x1B\[[0-?]*[ -/]*[@-~]")
    return ansi_escape.sub("", s)


def filter_progress_bar(stdout: str) -> str:
    """
    Filter out progress bars from stdout using regex.
    """
    from rdagent.oai.llm_utils import APIBackend  # avoid circular import

    # Initial progress bar regex pattern
    progress_bar_re = (
        r"(\d+/\d+\s+[━]+\s+\d+s?\s+\d+ms/step.*?\u0008+|"
        r"\d+/\d+\s+[━]+\s+\d+s?\s+\d+ms/step|"
        r"\d+/\d+\s+[━]+\s+\d+s?\s+\d+ms/step.*|"
        r"\d+/\d+\s+[━]+.*?\u0008+|"
        r"\d+/\d+\s+[━]+.*|[ ]*\u0008+|"
        r"\d+%\|[█▏▎▍▌▋▊▉]+\s+\|\s+\d+/\d+\s+\[\d{2}:\d{2}<\d{2}:\d{2},\s+\d+\.\d+it/s\]|"
        r"\d+%\|[█]+\|\s+\d+/\d+\s+\[\d{2}:\d{2}<\d{2}:\d{2},\s*\d+\.\d+it/s\])"
    )

    filtered_stdout = remove_ansi_codes(stdout)
    filtered_stdout = re.sub(progress_bar_re, "", filtered_stdout)
    filtered_stdout = re.sub(r"\s*\n\s*", "\n", filtered_stdout)

    needs_sub = True
    # Attempt further filtering up to 5 times
    for _ in range(5):
        filtered_stdout_shortened = filtered_stdout
        system_prompt = T(".prompts:filter_progress_bar.system").r()

        for __ in range(10):
            user_prompt = T(".prompts:filter_progress_bar.user").r(
                stdout=filtered_stdout_shortened,
            )
            stdout_token_size = APIBackend().build_messages_and_calculate_token(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
            )
            if stdout_token_size < LLM_SETTINGS.chat_token_limit * 0.1:
                return filtered_stdout_shortened
            elif stdout_token_size > LLM_SETTINGS.chat_token_limit * 0.6:
                filtered_stdout_shortened = filtered_stdout_shortened[
                    len(filtered_stdout_shortened) // 4 : len(filtered_stdout_shortened) * 3 // 4
                ]
            else:
                break

        response = json.loads(
            APIBackend().build_messages_and_create_chat_completion(
                user_prompt=user_prompt,
                system_prompt=system_prompt,
                json_mode=True,
                json_target_type=dict,
            )
        )
        needs_sub = response.get("needs_sub", True)
        regex_patterns = response.get("regex_patterns", [])
        try:
            if isinstance(regex_patterns, list):
                for pattern in regex_patterns:
                    filtered_stdout = re.sub(pattern, "", filtered_stdout)
            else:
                filtered_stdout = re.sub(regex_patterns, "", filtered_stdout)

            if not needs_sub:
                break
            filtered_stdout = re.sub(r"\s*\n\s*", "\n", filtered_stdout)
        except re.error as e:  # sometime the generated regex pattern is invalid  and yield exception.
            from rdagent.log import rdagent_logger as logger

            logger.error(f"Error in filtering progress bar: due to {e}")
    return filtered_stdout


def remove_path_info_from_str(base_path: Path, target_string: str) -> str:
    """
    Remove the absolute path from the target string
    """
    target_string = re.sub(str(base_path), "...", target_string)
    target_string = re.sub(str(base_path.absolute()), "...", target_string)
    return target_string


def md5_hash(input_string: str) -> str:
    hash_md5 = hashlib.md5(usedforsecurity=False)
    input_bytes = input_string.encode("utf-8")
    hash_md5.update(input_bytes)
    return hash_md5.hexdigest()



================================================
File: rdagent/utils/env.py
================================================
"""
The motiviation of the utils is for environment management

Tries to create uniform environment for the agent to run;
- All the code and data is expected included in one folder
"""

# TODO: move the scenario specific docker env into other folders.

import json
import os
import pickle
import re
import shutil
import subprocess
import time
import uuid
import zipfile
from abc import abstractmethod
from pathlib import Path
from types import MappingProxyType
from typing import Any, Generic, Mapping, Optional, TypeVar

import docker  # type: ignore[import-untyped]
import docker.models  # type: ignore[import-untyped]
import docker.models.containers  # type: ignore[import-untyped]
import docker.types  # type: ignore[import-untyped]
from pydantic import BaseModel, model_validator
from pydantic_settings import SettingsConfigDict
from rich import print
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.rule import Rule
from rich.table import Table

from rdagent.core.conf import ExtendedBaseSettings
from rdagent.core.experiment import RD_AGENT_SETTINGS
from rdagent.log import rdagent_logger as logger
from rdagent.oai.llm_utils import md5_hash
from rdagent.utils.workflow import wait_retry


class EnvConf(ExtendedBaseSettings):
    default_entry: str
    extra_volumes: dict = {}
    running_timeout_period: int = 600  # 10 minutes
    # helper settings to support transparent;
    enable_cache: bool = True
    retry_count: int = 5  # retry count for the docker run
    retry_wait_seconds: int = 10  # retry wait seconds for the docker run


ASpecificEnvConf = TypeVar("ASpecificEnvConf", bound=EnvConf)


class Env(Generic[ASpecificEnvConf]):
    """
    We use BaseModel as the setting due to the features it provides
    - It provides base typing and checking features.
    - loading and dumping the information will be easier: for example, we can use package like `pydantic-yaml`
    """

    conf: ASpecificEnvConf  # different env have different conf.

    def __init__(self, conf: ASpecificEnvConf):
        self.conf = conf

    def zip_a_folder_into_a_file(self, folder_path: str, zip_file_path: str) -> None:
        """
        Zip a folder into a file, use zipfile instead of subprocess
        """
        with zipfile.ZipFile(zip_file_path, "w") as z:
            for root, _, files in os.walk(folder_path):
                for file in files:
                    z.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_path))

    def unzip_a_file_into_a_folder(self, zip_file_path: str, folder_path: str) -> None:
        """
        Unzip a file into a folder, use zipfile instead of subprocess
        """
        # Clear folder_path before extracting
        if os.path.exists(folder_path):
            shutil.rmtree(folder_path)
        os.makedirs(folder_path)

        with zipfile.ZipFile(zip_file_path, "r") as z:
            z.extractall(folder_path)

    @abstractmethod
    def prepare(self, *args, **kwargs) -> None:  # type: ignore[no-untyped-def]
        """
        Prepare for the environment based on it's configure
        """

    def run(self, entry: str | None = None, local_path: str = ".", env: dict | None = None, **kwargs: dict) -> str:
        """
        Run the folder under the environment.

        Parameters
        ----------
        entry : str | None
            We may we the entry point when we run it.
            For example, we may have different entries when we run and summarize the project.
        local_path : str | None
            the local path (to project, mainly for code) will be mounted into the docker
            Here are some examples for a None local path
            - for example, run docker for updating the data in the extra_volumes.
            - simply run the image. The results are produced by output or network
        env : dict | None
            Run the code with your specific environment.

        Returns
        -------
            the stdout
        """
        stdout, _ = self.run_ret_code(entry=entry, local_path=local_path, env=env, **kwargs)
        return stdout

    def __run_ret_code_with_retry(
        self,
        entry: str | None = None,
        local_path: str = ".",
        env: dict | None = None,
        running_extra_volume: Mapping = MappingProxyType({}),
        remove_timestamp: bool = True,
    ) -> tuple[str, int]:
        # TODO: remove_timestamp can be implemented in a shallower way...
        for retry_index in range(self.conf.retry_count + 1):
            try:
                start = time.time()
                log_output, return_code = self._run_ret_code(
                    entry, local_path, env, running_extra_volume=running_extra_volume, remove_timestamp=remove_timestamp
                )
                end = time.time()
                if end - start >= self.conf.running_timeout_period:
                    print(
                        f"[red]The running time exceeds {self.conf.running_timeout_period} seconds, so the process is killed.[/red]"
                    )
                    log_output += f"\n\nThe running time exceeds {self.conf.running_timeout_period} seconds, so the process is killed."
                return log_output, return_code
            except Exception as e:
                if retry_index == self.conf.retry_count:
                    raise
                logger.warning(
                    f"Error while running the container: {e}, current try index: {retry_index + 1}, {self.conf.retry_count - retry_index - 1} retries left."
                )
                time.sleep(self.conf.retry_wait_seconds)
        raise RuntimeError  # for passing CI

    def run_ret_code(
        self,
        entry: str | None = None,
        local_path: str = ".",
        env: dict | None = None,
        **kwargs: dict,
    ) -> tuple[str, int]:
        """
        Run the folder under the environment and return both the stdout and the exit code.

        Parameters
        ----------
        entry : str | None
            We may we the entry point when we run it.
            For example, we may have different entries when we run and summarize the project.
        local_path : str | None
            the local path (to project, mainly for code) will be mounted into the docker
            Here are some examples for a None local path
            - for example, run docker for updating the data in the extra_volumes.
            - simply run the image. The results are produced by output or network
        env : dict | None
            Run the code with your specific environment.

        Returns
        -------
            A tuple containing the stdout and the exit code
        """
        running_extra_volume = kwargs.get("running_extra_volume", {})
        if entry is None:
            entry = self.conf.default_entry

        entry_add_timeout = (
            f"/bin/sh -c 'timeout {self.conf.running_timeout_period} {entry}; "
            + "entry_exit_code=$?; "
            + (f"chmod -R 777 {self.conf.mount_path}; " if hasattr(self.conf, "mount_path") else "")
            + "exit $entry_exit_code'"
        )

        if self.conf.enable_cache:
            stdout, return_code = self.cached_run(entry_add_timeout, local_path, env, running_extra_volume)
        else:
            stdout, return_code = self.__run_ret_code_with_retry(
                entry_add_timeout, local_path, env, running_extra_volume, remove_timestamp=False
            )

        return stdout, return_code

    def cached_run(
        self,
        entry: str | None = None,
        local_path: str = ".",
        env: dict | None = None,
        running_extra_volume: Mapping = MappingProxyType({}),
        remove_timestamp: bool = True,
    ) -> tuple[str, int]:
        """
        Run the folder under the environment.
        Will cache the output and the folder diff for next round of running.
        Use the python codes and the parameters(entry, running_extra_volume) as key to hash the input.
        """
        target_folder = Path(RD_AGENT_SETTINGS.pickle_cache_folder_path_str) / f"utils.env.run"
        target_folder.mkdir(parents=True, exist_ok=True)

        # we must add the information of data (beyound code) into the key.
        # Otherwise, all commands operating on data will become invalue (e.g. rm -r submission.csv)
        # So we recursively walk in the folder and add the sorted relative filename list as part of the key.
        data_key = []
        for path in Path(local_path).rglob("*"):
            p = str(path.relative_to(Path(local_path)))
            if p.startswith("__pycache__"):
                continue
            data_key.append(p)
        data_key = sorted(data_key)

        key = md5_hash(
            json.dumps(
                [
                    [str(path.relative_to(Path(local_path))), path.read_text()]
                    for path in sorted(Path(local_path).rglob("*.py"))
                ]
            )
            + json.dumps({"entry": entry, "running_extra_volume": dict(running_extra_volume)})
            + json.dumps({"extra_volumes": self.conf.extra_volumes})
            + json.dumps(data_key)
        )
        if Path(target_folder / f"{key}.pkl").exists() and Path(target_folder / f"{key}.zip").exists():
            with open(target_folder / f"{key}.pkl", "rb") as f:
                ret: tuple[str, int] = pickle.load(f)
            self.unzip_a_file_into_a_folder(str(target_folder / f"{key}.zip"), local_path)
        else:
            ret = self.__run_ret_code_with_retry(entry, local_path, env, running_extra_volume, remove_timestamp)
            with open(target_folder / f"{key}.pkl", "wb") as f:
                pickle.dump(ret, f)
            self.zip_a_folder_into_a_file(local_path, str(target_folder / f"{key}.zip"))
        return ret

    @abstractmethod
    def _run_ret_code(
        self,
        entry: str | None,
        local_path: str = ".",
        env: dict | None = None,
        running_extra_volume: Mapping = MappingProxyType({}),
        **kwargs: Any,
    ) -> tuple[str, int]:
        """
        Execute the specified entry point within the given environment and local path.

        Parameters
        ----------
        entry : str | None
            The entry point to execute. If None, defaults to the configured entry.
        local_path : str
            The local directory path where the execution should occur.
        env : dict | None
            Environment variables to set during execution.
        kwargs : dict
            Additional keyword arguments for execution customization.

        Returns
        -------
        tuple[str, int]
            A tuple containing the standard output and the exit code of the execution.
        """
        pass


# class EnvWithCache
#

## Local Environment -----


class LocalConf(EnvConf):
    bin_path: str = ""
    """path like <path1>:<path2>:<path3>, which will be prepend to bin path."""

    retry_count: int = 0  # retry count for; run `retry_count + 1` times


ASpecificLocalConf = TypeVar("ASpecificLocalConf", bound=LocalConf)


class LocalEnv(Env[ASpecificLocalConf]):
    """
    Sometimes local environment may be more convinient for testing
    """

    def prepare(self) -> None: ...

    def _run_ret_code(
        self,
        entry: str | None = None,
        local_path: str | None = None,
        env: dict | None = None,
        running_extra_volume: Mapping = MappingProxyType({}),
        **kwargs: dict,
    ) -> tuple[str, int]:

        # mocking the volumes
        volumes = {}
        if self.conf.extra_volumes is not None:
            for lp, rp in self.conf.extra_volumes.items():
                volumes[lp] = rp
        for lp, rp in running_extra_volume.items():
            volumes[lp] = rp

        for rp, lp in volumes.items():
            link_path = Path(lp)
            real_path = Path(rp)
            if not link_path.parent.exists():
                link_path.parent.mkdir(parents=True, exist_ok=True)
            if link_path.exists() or link_path.is_symlink():
                link_path.unlink()
            link_path.symlink_to(real_path)

        if env is None:
            env = {}

        path = [*self.conf.bin_path.split(":"), "/bin/", "/usr/bin/", *env.get("PATH", "").split(":")]
        env["PATH"] = ":".join(path)

        if entry is None:
            entry = self.conf.default_entry

        print(Rule("[bold green]LocalEnv Logs Begin[/bold green]", style="dark_orange"))
        table = Table(title="Run Info", show_header=False)
        table.add_column("Key", style="bold cyan")
        table.add_column("Value", style="bold magenta")
        table.add_row("Entry", entry)
        table.add_row("Local Path", local_path)
        table.add_row("Env", "\n".join(f"{k}:{v}" for k, v in env.items()))
        table.add_row("Volumes", "\n".join(f"{k}:{v}" for k, v in volumes.items()))
        print(table)

        cwd = None
        if local_path:
            cwd = Path(local_path).resolve()

        result = subprocess.run(entry, cwd=cwd, env={**os.environ, **env}, capture_output=True, text=True, shell=True)
        combined_output = result.stderr + result.stdout  # Combine stdout and stderr
        Console().print(combined_output, markup=False)
        print(Rule("[bold green]LocalEnv Logs End[/bold green]", style="dark_orange"))

        return combined_output, result.returncode


class CondaConf(LocalConf):
    conda_env_name: str
    default_entry: str = "python main.py"

    @model_validator(mode="after")
    def change_bin_path(self, **data: Any) -> "CondaConf":
        conda_path_result = subprocess.run(
            f"conda run -n {self.conda_env_name} --no-capture-output env | grep '^PATH='",
            capture_output=True,
            text=True,
            shell=True,
        )
        self.bin_path = conda_path_result.stdout.strip().split("=")[1] if conda_path_result.returncode == 0 else ""
        return self


class MLECondaConf(CondaConf):
    enable_cache: bool = False  # aligning with the docker settings.


## Docker Environment -----
class DockerConf(EnvConf):
    build_from_dockerfile: bool = False
    dockerfile_folder_path: Optional[Path] = (
        None  # the path to the dockerfile optional path provided when build_from_dockerfile is False
    )
    image: str  # the image you want to build
    mount_path: str  # the path in the docker image to mount the folder
    default_entry: str  # the entry point of the image

    extra_volumes: dict = {}
    extra_volume_mode: str = "ro"  # by default. only the mount_path should be writable, others are changed to read-only
    # Sometime, we need maintain some extra data for the workspace.
    # And the extra data may be shared and the downloading can be time consuming.
    # So we just want to download it once.
    network: str | None = "bridge"  # the network mode for the docker
    shm_size: str | None = None
    enable_gpu: bool = True  # because we will automatically disable GPU if not available. So we enable it by default.
    mem_limit: str | None = "48g"  # Add memory limit attribute

    running_timeout_period: int = 3600  # 1 hour

    enable_cache: bool = True  # enable the cache mechanism

    retry_count: int = 5  # retry count for the docker run
    retry_wait_seconds: int = 10  # retry wait seconds for the docker run


class QlibDockerConf(DockerConf):
    model_config = SettingsConfigDict(env_prefix="QLIB_DOCKER_")

    build_from_dockerfile: bool = True
    dockerfile_folder_path: Path = Path(__file__).parent.parent / "scenarios" / "qlib" / "docker"
    image: str = "local_qlib:latest"
    mount_path: str = "/workspace/qlib_workspace/"
    default_entry: str = "qrun conf.yaml"
    extra_volumes: dict = {str(Path("~/.qlib/").expanduser().resolve().absolute()): "/root/.qlib/"}
    shm_size: str | None = "16g"
    enable_gpu: bool = True


class DMDockerConf(DockerConf):
    model_config = SettingsConfigDict(env_prefix="DM_DOCKER_")

    build_from_dockerfile: bool = True
    dockerfile_folder_path: Path = Path(__file__).parent.parent / "scenarios" / "data_mining" / "docker"
    image: str = "local_dm:latest"
    mount_path: str = "/workspace/dm_workspace/"
    default_entry: str = "python train.py"
    extra_volumes: dict = {
        str(
            Path("~/.rdagent/.data/physionet.org/files/mimic-eicu-fiddle-feature/1.0.0/FIDDLE_mimic3/")
            .expanduser()
            .resolve()
            .absolute()
        ): "/root/.data/"
    }
    shm_size: str | None = "16g"


class KGDockerConf(DockerConf):
    model_config = SettingsConfigDict(env_prefix="KG_DOCKER_")

    build_from_dockerfile: bool = True
    dockerfile_folder_path: Path = Path(__file__).parent.parent / "scenarios" / "kaggle" / "docker" / "kaggle_docker"
    image: str = "local_kg:latest"
    # image: str = "gcr.io/kaggle-gpu-images/python:latest"
    mount_path: str = "/workspace/kg_workspace/"
    default_entry: str = "python train.py"
    # extra_volumes: dict = {
    #     # TODO connect to the place where the data is stored
    #     Path("git_ignore_folder/data").resolve(): "/root/.data/"
    # }

    running_timeout_period: int = 600
    mem_limit: str | None = (
        "48g"  # Add memory limit attribute # new-york-city-taxi-fare-prediction may need more memory
    )


class DSDockerConf(DockerConf):
    model_config = SettingsConfigDict(env_prefix="DS_DOCKER_")

    build_from_dockerfile: bool = False
    image: str = "gcr.io/kaggle-gpu-images/python:latest"
    mount_path: str = "/kaggle/workspace"
    default_entry: str = "python main.py"

    running_timeout_period: int = 600
    mem_limit: str | None = (
        "48g"  # Add memory limit attribute # new-york-city-taxi-fare-prediction may need more memory
    )


class MLEBDockerConf(DockerConf):
    model_config = SettingsConfigDict(env_prefix="MLEB_DOCKER_")

    build_from_dockerfile: bool = True
    dockerfile_folder_path: Path = Path(__file__).parent.parent / "scenarios" / "kaggle" / "docker" / "mle_bench_docker"
    image: str = "local_mle:latest"
    # image: str = "gcr.io/kaggle-gpu-images/python:latest"
    mount_path: str = "/workspace/data_folder/"
    default_entry: str = "mlebench prepare --all"
    # extra_volumes: dict = {
    #     # TODO connect to the place where the data is stored
    #     Path("git_ignore_folder/data").resolve(): "/root/.data/"
    # }
    mem_limit: str | None = (
        "48g"  # Add memory limit attribute # new-york-city-taxi-fare-prediction may need more memory
    )
    enable_cache: bool = False


# physionet.org/files/mimic-eicu-fiddle-feature/1.0.0/FIDDLE_mimic3
class DockerEnv(Env[DockerConf]):
    # TODO: Save the output into a specific file

    def prepare(self, *args, **kwargs) -> None:  # type: ignore[no-untyped-def]
        """
        Download image if it doesn't exist
        """
        client = docker.from_env()
        if (
            self.conf.build_from_dockerfile
            and self.conf.dockerfile_folder_path is not None
            and self.conf.dockerfile_folder_path.exists()
        ):
            logger.info(f"Building the image from dockerfile: {self.conf.dockerfile_folder_path}")
            resp_stream = client.api.build(
                path=str(self.conf.dockerfile_folder_path), tag=self.conf.image, network_mode=self.conf.network
            )
            if isinstance(resp_stream, str):
                logger.info(resp_stream)
            with Progress(SpinnerColumn(), TextColumn("{task.description}")) as p:
                task = p.add_task("[cyan]Building image...")
                for part in resp_stream:
                    lines = part.decode("utf-8").split("\r\n")
                    for line in lines:
                        if line.strip():
                            status_dict = json.loads(line)
                            if "error" in status_dict:
                                p.update(task, description=f"[red]error: {status_dict['error']}")
                                raise docker.errors.BuildError(status_dict["error"], "")
                            if "stream" in status_dict:
                                p.update(task, description=status_dict["stream"])
            logger.info(f"Finished building the image from dockerfile: {self.conf.dockerfile_folder_path}")
        try:
            client.images.get(self.conf.image)
        except docker.errors.ImageNotFound:
            image_pull = client.api.pull(self.conf.image, stream=True, decode=True)
            current_status = ""
            layer_set = set()
            completed_layers = 0
            with Progress(TextColumn("{task.description}"), TextColumn("{task.fields[progress]}")) as sp:
                main_task = sp.add_task("[cyan]Pulling image...", progress="")
                status_task = sp.add_task("[bright_magenta]layer status", progress="")
                for line in image_pull:
                    if "error" in line:
                        sp.update(status_task, description=f"[red]error", progress=line["error"])
                        raise docker.errors.APIError(line["error"])

                    layer_id = line["id"]
                    status = line["status"]
                    p_text = line.get("progress", None)

                    if layer_id not in layer_set:
                        layer_set.add(layer_id)

                    if p_text:
                        current_status = p_text

                    if status == "Pull complete" or status == "Already exists":
                        completed_layers += 1

                    sp.update(main_task, progress=f"[green]{completed_layers}[white]/{len(layer_set)} layers completed")
                    sp.update(
                        status_task,
                        description=f"[bright_magenta]layer {layer_id} [yellow]{status}",
                        progress=current_status,
                    )
        except docker.errors.APIError as e:
            raise RuntimeError(f"Error while pulling the image: {e}")

    def _gpu_kwargs(self, client: docker.DockerClient) -> dict:  # type: ignore[no-any-unimported]
        """get gpu kwargs based on its availability"""
        if not self.conf.enable_gpu:
            return {}
        gpu_kwargs = {
            "device_requests": (
                [docker.types.DeviceRequest(count=-1, capabilities=[["gpu"]])] if self.conf.enable_gpu else None
            ),
        }

        @wait_retry(5, 10)
        def _f() -> dict:
            try:
                client.containers.run(self.conf.image, "nvidia-smi", **gpu_kwargs)
                logger.info("GPU Devices are available.")
            except docker.errors.APIError:
                return {}
            return gpu_kwargs

        return _f()

    def replace_time_info(self, input_string: str) -> str:
        """To remove any time related information from the logs since it will destroy the cache mechanism"""
        """We currently set this function as default, but it can be changed in the future"""
        datetime_pattern = r"\b\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(?:\.\d+)?\b"
        output_string = re.sub(datetime_pattern, "[DATETIME]", input_string)
        return output_string

    def _run_ret_code(
        self,
        entry: str | None = None,
        local_path: str = ".",
        env: dict | None = None,
        running_extra_volume: Mapping = MappingProxyType({}),
        remove_timestamp: bool = True,
        **kwargs: Any,
    ) -> tuple[str, int]:
        if env is None:
            env = {}
        env["PYTHONWARNINGS"] = "ignore"
        env["TF_CPP_MIN_LOG_LEVEL"] = "2"
        env["PYTHONUNBUFFERED"] = "1"
        client = docker.from_env()

        volumes = {}
        if local_path is not None:
            local_path = os.path.abspath(local_path)
            volumes[local_path] = {"bind": self.conf.mount_path, "mode": "rw"}
        if self.conf.extra_volumes is not None:
            for lp, rp in self.conf.extra_volumes.items():
                volumes[lp] = {"bind": rp, "mode": self.conf.extra_volume_mode}
        for lp, rp in running_extra_volume.items():
            volumes[lp] = {"bind": rp, "mode": self.conf.extra_volume_mode}

        log_output = ""

        try:
            container: docker.models.containers.Container = client.containers.run(  # type: ignore[no-any-unimported]
                image=self.conf.image,
                command=entry,
                volumes=volumes,
                environment=env,
                detach=True,
                working_dir=self.conf.mount_path,
                # auto_remove=True, # remove too fast might cause the logs not to be get
                network=self.conf.network,
                shm_size=self.conf.shm_size,
                mem_limit=self.conf.mem_limit,  # Set memory limit
                **self._gpu_kwargs(client),
            )
            logs = container.logs(stream=True)
            print(Rule("[bold green]Docker Logs Begin[/bold green]", style="dark_orange"))
            table = Table(title="Run Info", show_header=False)
            table.add_column("Key", style="bold cyan")
            table.add_column("Value", style="bold magenta")
            table.add_row("Image", self.conf.image)
            table.add_row("Container ID", container.id)
            table.add_row("Container Name", container.name)
            table.add_row("Entry", entry)
            table.add_row("Env", "\n".join(f"{k}:{v}" for k, v in env.items()))
            table.add_row("Volumes", "\n".join(f"{k}:{v}" for k, v in volumes.items()))
            print(table)
            for log in logs:
                decoded_log = log.strip().decode()
                decoded_log = self.replace_time_info(decoded_log) if remove_timestamp else decoded_log
                Console().print(decoded_log, markup=False)
                log_output += decoded_log + "\n"
            exit_status = container.wait()["StatusCode"]
            container.stop()
            container.remove()
            print(Rule("[bold green]Docker Logs End[/bold green]", style="dark_orange"))
            return log_output, exit_status
        except docker.errors.ContainerError as e:
            raise RuntimeError(f"Error while running the container: {e}")
        except docker.errors.ImageNotFound:
            raise RuntimeError("Docker image not found.")
        except docker.errors.APIError as e:
            raise RuntimeError(f"Error while running the container: {e}")

    def dump_python_code_run_and_get_results(
        self,
        code: str,
        dump_file_names: list[str],
        local_path: str,
        env: dict | None = None,
        running_extra_volume: Mapping = MappingProxyType({}),
        code_dump_file_py_name: Optional[str] = None,
    ) -> tuple[str, list]:
        """
        Dump the code into the local path and run the code.
        """
        random_file_name = f"{uuid.uuid4()}.py" if code_dump_file_py_name is None else f"{code_dump_file_py_name}.py"
        with open(os.path.join(local_path, random_file_name), "w") as f:
            f.write(code)
        entry = f"python {random_file_name}"
        log_output = self.run(entry, local_path, env, running_extra_volume=dict(running_extra_volume))
        results = []
        os.remove(os.path.join(local_path, random_file_name))
        for name in dump_file_names:
            if os.path.exists(os.path.join(local_path, f"{name}")):
                results.append(pickle.load(open(os.path.join(local_path, f"{name}"), "rb")))
                os.remove(os.path.join(local_path, f"{name}"))
            else:
                return log_output, []
        return log_output, results


class QTDockerEnv(DockerEnv):
    """Qlib Torch Docker"""

    def __init__(self, conf: DockerConf = QlibDockerConf()):
        super().__init__(conf)

    def prepare(self, *args, **kwargs) -> None:  # type: ignore[explicit-override, no-untyped-def]
        """
        Download image & data if it doesn't exist
        """
        super().prepare()
        qlib_data_path = next(iter(self.conf.extra_volumes.keys()))
        if not (Path(qlib_data_path) / "qlib_data" / "cn_data").exists():
            logger.info("We are downloading!")
            cmd = "python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --interval 1d --delete_old False"
            self.run(entry=cmd)
        else:
            logger.info("Data already exists. Download skipped.")


class DMDockerEnv(DockerEnv):
    """Qlib Torch Docker"""

    def __init__(self, conf: DockerConf = DMDockerConf()):
        super().__init__(conf)

    def prepare(self, username: str, password: str) -> None:
        """
        Download image & data if it doesn't exist
        """
        super().prepare()
        data_path = next(iter(self.conf.extra_volumes.keys()))
        if not (Path(data_path)).exists():
            logger.info("We are downloading!")
            cmd = "wget -r -N -c -np --user={} --password={} -P ~/.rdagent/.data/ https://physionet.org/files/mimic-eicu-fiddle-feature/1.0.0/".format(
                username, password
            )
            os.system(cmd)
        else:
            logger.info("Data already exists. Download skipped.")


class KGDockerEnv(DockerEnv):
    """Kaggle Competition Docker"""

    def __init__(self, competition: str | None = None, conf: DockerConf = KGDockerConf()):
        super().__init__(conf)


class MLEBDockerEnv(DockerEnv):
    """MLEBench Docker"""

    def __init__(self, conf: DockerConf = MLEBDockerConf()):
        super().__init__(conf)



================================================
File: rdagent/utils/fmt.py
================================================
"""
Tools that support generating better formats.
"""


def shrink_text(text: str, context_lines: int = 200) -> str:
    """
    When the context is too long, hide the part in the middle.

        text before
        ... (XXXXX lines are hidden) ...
        text after
    """
    lines = text.splitlines()
    total_lines = len(lines)

    if total_lines <= context_lines:
        return text

    # Calculate how many lines to show from start and end
    half_lines = context_lines // 2
    start = "\n".join(lines[:half_lines])
    end = "\n".join(lines[-half_lines:])

    # Count the number of lines we're hiding
    hidden_lines = total_lines - half_lines * 2

    return f"{start}\n... ({hidden_lines} lines are hidden) ...\n{end}"



================================================
File: rdagent/utils/prompts.yaml
================================================
filter_progress_bar:
  system: |
    You are an assistant helping to analyze and filter training log messages and a progress bar output from a given text. Evaluate the text to determine if training log messages and a progress bar output patterns are present and, if so, generate a list of regex patterns to remove them. 
    Additionally, indicate whether substitution is needed. If the input exceeds a token limit, the system will provide only a shortened portion of the text.
    Note: About the training log message, if the log message contains useful information like loss or accuracy and it is reported in each epoch, it should not be removed. If the log message is not useful, for example, reporting nan in each iteration or just reporting the iteration number, please remove them.

    Respond in the following JSON format and order:
    ```json
    {
        "needs_sub": <true/false>, 
        "regex_patterns": ["regex pattern 1", "regex pattern 2", ...]
    }
  user: |
    The following text contains stdout:

    {{ stdout }}

    Check if the text contains training log messages and progress bar patterns. If patterns are found, provide a list of regex patterns to filter them. Otherwise, indicate that substitution is not needed.



================================================
File: rdagent/utils/workflow.py
================================================
"""
This is a class that try to store/resume/traceback the workflow session


Postscripts:
- Originally, I want to implement it in a more general way with python generator.
  However, Python generator is not picklable (dill does not support pickle as well)

"""

import datetime
import pickle
import time
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Optional, TypeVar, Union, cast

from tqdm.auto import tqdm

from rdagent.log import rdagent_logger as logger


class LoopMeta(type):
    @staticmethod
    def _get_steps(bases: tuple[type, ...]) -> list[str]:
        """
        Recursively get all the `steps` from the base classes and combine them into a single list.

        Args:
            bases (tuple): A tuple of base classes.

        Returns:
            List[Callable]: A list of steps combined from all base classes.
        """
        steps = []
        for base in bases:
            for step in LoopMeta._get_steps(base.__bases__) + getattr(base, "steps", []):
                if step not in steps:
                    steps.append(step)
        return steps

    def __new__(mcs, clsname: str, bases: tuple[type, ...], attrs: dict[str, Any]) -> Any:
        """
        Create a new class with combined steps from base classes and current class.

        Args:
            clsname (str): Name of the new class.
            bases (tuple): Base classes.
            attrs (dict): Attributes of the new class.

        Returns:
            LoopMeta: A new instance of LoopMeta.
        """
        steps = LoopMeta._get_steps(bases)  # all the base classes of parents
        for name, attr in attrs.items():
            if not name.startswith("_") and callable(attr):
                if name not in steps:
                    # NOTE: if we override the step in the subclass
                    # Then it is not the new step. So we skip it.
                    steps.append(name)
        attrs["steps"] = steps
        return super().__new__(mcs, clsname, bases, attrs)


@dataclass
class LoopTrace:
    start: datetime.datetime  # the start time of the trace
    end: datetime.datetime  # the end time of the trace
    step_idx: int
    # TODO: more information about the trace


class LoopBase:
    """
    Assumption:
    - The last step is responsible for recording information!!!!
    """

    steps: list[str]  # a list of steps to work on
    loop_trace: dict[int, list[LoopTrace]]

    skip_loop_error: tuple[type[BaseException], ...] = ()  # you can define a list of error that will skip current loop

    EXCEPTION_KEY = "_EXCEPTION"

    def __init__(self) -> None:
        self.loop_idx = 0  # current loop index
        self.step_idx = 0  # the index of next step to be run
        self.loop_prev_out: dict[str, Any] = {}  # the step results of current loop
        self.loop_trace = defaultdict(list[LoopTrace])  # the key is the number of loop
        self.session_folder = logger.log_trace_path / "__session__"

    def run(self, step_n: int | None = None, loop_n: int | None = None) -> None:
        """

        Parameters
        ----------
        step_n : int | None
            How many steps to run;
            `None` indicates to run forever until error or KeyboardInterrupt
        loop_n: int | None
            How many steps to run; if current loop is incomplete, it will be counted as the first loop for completion
            `None` indicates to run forever until error or KeyboardInterrupt
        """
        with tqdm(total=len(self.steps), desc="Workflow Progress", unit="step") as pbar:
            while True:
                if step_n is not None:
                    if step_n <= 0:
                        break
                    step_n -= 1
                if loop_n is not None:
                    if loop_n <= 0:
                        break

                li, si = self.loop_idx, self.step_idx
                name = self.steps[si]
                logger.info(f"Start Loop {li}, Step {si}: {name}")
                with logger.tag(f"Loop_{li}.{name}"):
                    start = datetime.datetime.now(datetime.timezone.utc)
                    func: Callable[..., Any] = cast(Callable[..., Any], getattr(self, name))
                    try:
                        self.loop_prev_out[name] = func(self.loop_prev_out)
                        # TODO: Fix the error logger.exception(f"Skip loop {li} due to {e}")
                    except Exception as e:
                        if isinstance(e, self.skip_loop_error):
                            # FIXME: This does not support previous demo (due to their last step is not for recording)
                            logger.warning(f"Skip loop {li} due to {e}")
                            # NOTE: strong assumption!  The last step is responsible for recording information
                            self.step_idx = len(self.steps) - 1  # directly jump to the last step.
                            self.loop_prev_out[self.EXCEPTION_KEY] = e
                            continue
                        else:
                            raise
                    finally:
                        # make sure failure steps are displayed correclty
                        end = datetime.datetime.now(datetime.timezone.utc)
                        self.loop_trace[li].append(LoopTrace(start, end, step_idx=si))

                        # Update tqdm progress bar directly to step_idx
                        pbar.n = si + 1
                        pbar.set_postfix(
                            loop_index=li, step_index=si + 1, step_name=name
                        )  # step_name indicate  last finished step_name

                # index increase and save session
                self.step_idx = (self.step_idx + 1) % len(self.steps)
                if self.step_idx == 0:  # reset to step 0 in next round
                    self.loop_idx += 1
                    if loop_n is not None:
                        loop_n -= 1
                    self.loop_prev_out = {}
                    pbar.reset()  # reset the progress bar for the next loop

                self.dump(self.session_folder / f"{li}" / f"{si}_{name}")  # save a snapshot after the session

    def dump(self, path: str | Path) -> None:
        path = Path(path)
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("wb") as f:
            pickle.dump(self, f)

    @classmethod
    def load(
        cls, path: Union[str, Path], output_path: Optional[Union[str, Path]] = None, do_truncate: bool = False
    ) -> "LoopBase":
        path = Path(path)
        with path.open("rb") as f:
            session = cast(LoopBase, pickle.load(f))

        # set session folder
        if output_path:
            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)
            session.session_folder = output_path / "__session__"

        # set trace path
        logger.set_trace_path(session.session_folder.parent)

        # truncate future message
        if do_truncate:
            max_loop = max(session.loop_trace.keys())
            logger.storage.truncate(time=session.loop_trace[max_loop][-1].end)
        return session


ASpecificRet = TypeVar("ASpecificRet")


def wait_retry(
    retry_n: int = 3, sleep_time: int = 1, transform_args_fn: Callable[[tuple, dict], tuple[tuple, dict]] | None = None
) -> Callable[[Callable[..., ASpecificRet]], Callable[..., ASpecificRet]]:
    """Decorator to wait and retry the function for retry_n times.

    Example:
    >>> import time
    >>> @wait_retry(retry_n=2, sleep_time=1)
    ... def test_func():
    ...     global counter
    ...     counter += 1
    ...     if counter < 3:
    ...         raise ValueError("Counter is less than 3")
    ...     return counter
    >>> counter = 0
    >>> try:
    ...     test_func()
    ... except ValueError as e:
    ...     print(f"Caught an exception: {e}")
    Error: Counter is less than 3
    Error: Counter is less than 3
    Caught an exception: Counter is less than 3
    >>> counter
    2
    """
    assert retry_n > 0, "retry_n should be greater than 0"

    def decorator(f: Callable[..., ASpecificRet]) -> Callable[..., ASpecificRet]:
        def wrapper(*args: Any, **kwargs: Any) -> ASpecificRet:
            for i in range(retry_n + 1):
                try:
                    return f(*args, **kwargs)
                except Exception as e:
                    print(f"Error: {e}")
                    time.sleep(sleep_time)
                    if i == retry_n:
                        raise
                    # Update args and kwargs using the transform function if provided.
                    if transform_args_fn is not None:
                        args, kwargs = transform_args_fn(args, kwargs)
            else:
                # just for passing mypy CI.
                return f(*args, **kwargs)

        return wrapper

    return decorator



================================================
File: rdagent/utils/agent/__init__.py
================================================
from .workflow import build_cls_from_json_with_retry

__all__ = ["build_cls_from_json_with_retry"]



================================================
File: rdagent/utils/agent/ret.py
================================================
"""
The output of a agent is very important.

We think this part can be shared.
"""

import json
import re
from abc import abstractclassmethod
from typing import Any

from rdagent.utils.agent.tpl import T


class AgentOut:
    json_mode: bool = False  # To get the output, is json_mode required.

    @abstractclassmethod
    def get_spec(cls, **context: Any) -> str:
        raise NotImplementedError(f"Please implement the `get_spec` method")

    @classmethod
    def extract_output(cls, resp: str) -> Any:
        raise resp


class PythonAgentOut(AgentOut):
    @classmethod
    def get_spec(cls):
        return T(".tpl:PythonAgentOut").r()

    @classmethod
    def extract_output(cls, resp: str):
        match = re.search(r".*```[Pp]ython\n(.*)\n```.*", resp, re.DOTALL)
        if match:
            code = match.group(1)
            return code
        return resp


class BatchEditOut(AgentOut):
    json_mode: bool = True

    @classmethod
    def get_spec(cls, with_del=True):
        return T(".tpl:BatchEditOut").r(with_del=with_del)

    @classmethod
    def extract_output(cls, resp: str):
        return json.loads(resp)



================================================
File: rdagent/utils/agent/tpl.py
================================================
"""
Here are some infrastructure to build a agent

The motivation of template and AgentOutput Design
"""

import inspect
from pathlib import Path
from typing import Any

import yaml
from jinja2 import Environment, StrictUndefined

from rdagent.core.utils import SingletonBaseClass
from rdagent.log import rdagent_logger as logger

DIRNAME = Path(__file__).absolute().resolve().parent
PROJ_PATH = DIRNAME.parent.parent


# class T(SingletonBaseClass): TODO: singleton does not support args now.
class RDAT:
    """
    RD-Agent's Template
    Use the simplest way to (C)reate a Template and (r)ender it!!
    """

    def __init__(self, uri: str):
        """
        here are some uri usages
            case 1) "a.b.c:x.y.z"
                It will load DIRNAME/a/b/c.yaml as `yaml` and load yaml[x][y][z]

                Form example, if you want to load "rdagent/scenarios/kaggle/experiment/prompts.yaml"
                `a.b.c` should be "scenarios.kaggle.experiment.prompts" and "rdagent" should be exclude
            case 2) ".c:x.y.z"
                It will load c.yaml in caller's (who call `T(uri)`) directory as `yaml` and load yaml[x][y][z]

            the loaded content will be saved in `self.template`
        """
        self.uri = uri
        # Inspect the calling stack to get the caller's directory
        stack = inspect.stack()
        caller_frame = stack[1]
        caller_module = inspect.getmodule(caller_frame[0])
        if caller_module and caller_module.__file__:
            caller_dir = Path(caller_module.__file__).parent
        else:
            caller_dir = DIRNAME

        # Parse the URI
        path_part, yaml_path = uri.split(":")
        yaml_keys = yaml_path.split(".")

        if path_part.startswith("."):
            yaml_file_path = caller_dir / f"{path_part[1:].replace('.', '/')}.yaml"
            try:
                # modify the uri to a raltive path to the project for easier finding prompts.yaml
                self.uri = f"{str(caller_dir.resolve().relative_to(PROJ_PATH)).replace('/', '.')}{uri}"
            except ValueError:
                pass
        else:
            yaml_file_path = (PROJ_PATH / path_part.replace(".", "/")).with_suffix(".yaml")

        # Load the YAML file
        with open(yaml_file_path, "r") as file:
            yaml_content = yaml.safe_load(file)

        # Traverse the YAML content to get the desired template
        for key in yaml_keys:
            yaml_content = yaml_content[key]

        self.template = yaml_content

    def r(self, **context: Any) -> str:
        """
        Render the template with the given context.
        """
        rendered = Environment(undefined=StrictUndefined).from_string(self.template).render(**context).strip("\n")
        while "\n\n\n" in rendered:
            rendered = rendered.replace("\n\n\n", "\n\n")
        rendered = "\n".join(line for line in rendered.splitlines() if line.strip())
        logger.log_object(
            obj={
                "uri": self.uri,
                "template": self.template,
                "context": context,
                "rendered": rendered,
            },
            tag="debug_tpl",
        )
        return rendered


T = RDAT  # shortcuts



================================================
File: rdagent/utils/agent/tpl.yaml
================================================
PythonAgentOut: |-
  The return code should be like
  ```Python
  <You code>
  ```
  
BatchEditOut: |-
  You should return a edition that applies to multiple files in a workspace in JSON.
  Except for the model file, other files should not be renamed.
  Files that do not need to be modified do not need to be included in the returned dict.

  For example:
  Inject the code into the folder. Your file name should always contain the suffix. Your file name keys should be unique to avoid delete or replace conflicts.
  {
      <file name1>: "<code>",  // indicate writing <code> into <file name1> (create new file or replace existing file)
      {% if with_del %}
      <file name2>: "__DEL__"  // indicate removing file name2. When we want to replace a file to a new one, we usually use this
      {% else %}
      <file name2>(optional): "<code>"  // indicate writing <code> into <file name2> (create new file or replace existing file)
      {% endif %}
  }



================================================
File: rdagent/utils/agent/workflow.py
================================================
import json
from typing import Any, Callable, Type, TypeVar, Union, cast

from rdagent.core.exception import FormatError
from rdagent.log import rdagent_logger as logger

T = TypeVar("T")


def build_cls_from_json_with_retry(
    cls: Type[T],
    system_prompt: str,
    user_prompt: str,
    retry_n: int = 5,
    init_kwargs_update_func: Callable[[dict[str, Any]], dict[str, Any]] | None = None,
    **kwargs: dict,
) -> T:
    """
    Parameters
    ----------
    cls : Type[T]
        The class type to be instantiated with the response data.
    system_prompt : str
        The initial prompt provided to the system for context.
    user_prompt : str
        The prompt given by the user to guide the response generation.
    retry_n : int
        The number of attempts to retry in case of failure.
    init_kwargs_update_func : Union[Callable[[dict], dict], None]
        A function that takes the initial keyword arguments as input and returns the updated keyword arguments.
        This function can be used to modify the response data before it is used to instantiate the class.

    **kwargs
        Additional keyword arguments passed to the API call.

    Returns
    -------
    T
        An instance of the specified class type created from the response data.
    """
    from rdagent.oai.llm_utils import APIBackend  # avoid circular import

    for i in range(retry_n):
        # currently, it only handle exception caused by initial class
        resp = APIBackend().build_messages_and_create_chat_completion(
            user_prompt=user_prompt, system_prompt=system_prompt, json_mode=True, **kwargs  # type: ignore[arg-type]
        )
        try:
            resp_dict = json.loads(resp)
            if init_kwargs_update_func:
                resp_dict = init_kwargs_update_func(resp_dict)
            return cls(**resp_dict)
        except Exception as e:
            logger.warning(f"Attempt {i + 1}: The previous attempt didn't work due to: {e}")
            user_prompt = user_prompt + f"\n\nAttempt {i + 1}: The previous attempt didn't work due to: {e}"
    raise FormatError("Unable to produce a JSON response that meets the specified requirements.")



================================================
File: rdagent/utils/repo/README.md
================================================
# RepoAnalyzer

RepoAnalyzer is a Python utility for analyzing and summarizing the contents of a Python repository. It provides a high-level overview of the repository structure, including a tree-like representation of the directory structure and details about files, classes, and functions.

## Features

- Generate a tree-like structure of the repository
- Summarize an entire repository
- Adjust verbosity levels for summaries
- Extract content from specific files
- Analyze Python files for classes and functions


## Usage

### Basic Usage

```python
from repo_utils import RepoAnalyzer

# Initialize the RepoAnalyzer with the path to your repository
repo_analyzer = RepoAnalyzer("/path/to/your/repo")

# Generate a summary of the repository
summary = repo_analyzer.summarize_repo()
print(summary)

# Extract content from specific files
highlighted_content = repo_analyzer.highlight(["file1.py", "file2.py"])
print(highlighted_content)
```

### Adjusting Verbosity Levels

You can adjust the verbosity of the summary using the following parameters:

- `verbose_level`: Controls the overall detail level of the summary
  - 0: Minimal (file names only)
  - 1: Default (file info, class names, function names)
  - 2+: Detailed (includes method details within classes)
- `doc_str_level`: Controls the inclusion of docstrings (0-2)
- `sign_level`: Controls the inclusion of function signatures (0-2)

Example:

```python
detailed_summary = repo_analyzer.summarize_repo(verbose_level=2, doc_str_level=1, sign_level=1)
print(detailed_summary)
```

## Example Output

### Repository Summary

```
Workspace Summary for my_project
========================================

Repository Structure:
my_project/
├── main.py
├── utils/
│   ├── helper.py
│   └── config.py
├── models/
│   ├── model_a.py
│   └── model_b.py

This workspace contains 5 Python files.

File 1 of 5:
File: main.py
----------------------------------------
This file contains 1 class and 2 top-level functions.

Class: MainApp
  Description: Main application class for the project.
  This class has 3 methods.

Function: setup_logging
  Accepts parameters: log_level
  Purpose: Configure the logging for the application.

Function: main
  Purpose: Entry point of the application.

...
```

### File Highlight

```python
highlighted_content = repo_analyzer.highlight(["main.py"])
print(highlighted_content["main.py"])
```

This will print the entire content of the `main.py` file.

## Key Components

### RepoAnalyzer Class

The main class that provides the functionality for analyzing repositories.

#### Methods:

- `summarize_repo(verbose_level=1, doc_str_level=1, sign_level=1)`: Generates a comprehensive summary of the repository, including a tree-like structure.
- `highlight(file_names)`: Extracts and returns the content of specified files.

### Tree-like Structure

The summary now includes a visual representation of the repository's directory structure, making it easier to understand the overall organization of the project.


================================================
File: rdagent/utils/repo/diff.py
================================================
import difflib
import fnmatch
from pathlib import Path


def generate_diff(dir1: str, dir2: str, file_pattern: str = "*.py") -> list[str]:
    """
    Generate a diff between two directories (from dir1 to dir2) using files that match the specified file pattern.
    This function mimics the behavior of `diff -durN dir1 dir2` in Linux.

    Args:
        dir1 (str): Path to the first directory.
        dir2 (str): Path to the second directory.
        file_pattern (str, optional): Glob pattern to filter files. Defaults to "*.py".

    Returns:
        list[str]: A list of diffs for files that differ between the two directories.
    """

    dir1_files = {f.relative_to(dir1) for f in Path(dir1).rglob(file_pattern) if f.is_file()}
    dir2_files = {f.relative_to(dir2) for f in Path(dir2).rglob(file_pattern) if f.is_file()}

    all_files = dir1_files.union(dir2_files)
    file_dict1 = {}
    file_dict2 = {}
    for file in all_files:
        file1 = Path(dir1) / file
        file2 = Path(dir2) / file
        if file1.exists():
            with file1.open() as f1:
                file_dict1[str(file)] = f1.read()
        else:
            file_dict1[str(file)] = ""
        if file2.exists():
            with file2.open() as f2:
                file_dict2[str(file)] = f2.read()
        else:
            file_dict2[str(file)] = ""
    return generate_diff_from_dict(file_dict1, file_dict2, file_pattern="*")


def generate_diff_from_dict(file_dict1: dict, file_dict2: dict, file_pattern: str = "*.py") -> list[str]:
    """
    Generate a diff between two dictionaries of file contents.
    The dictionaries should be of the format {file_path: file_content}.

    Returns:
        List[str]: A list of diffs for files that are different between the two dictionaries.
    """
    diff_files = []
    all_files = set(file_dict1.keys()).union(file_dict2.keys())
    for file in sorted(all_files):
        if not fnmatch.fnmatch(file, file_pattern):
            continue
        content1 = file_dict1.get(file, "")
        content2 = file_dict2.get(file, "")
        diff = list(
            difflib.unified_diff(
                content1.splitlines(keepends=True),
                content2.splitlines(keepends=True),
                fromfile=file if file in file_dict1 else file + " (empty file)",
                tofile=file if file in file_dict2 else file + " (empty file)",
            )
        )
        if diff:
            diff_files.extend(diff)
    return diff_files



================================================
File: rdagent/utils/repo/repo_utils.py
================================================
import ast
import inspect
import os
from pathlib import Path
from typing import Dict, List, Union


class RepoAnalyzer:
    def __init__(self, repo_path: str):
        self.repo_path = Path(repo_path)
        self.summaries = {}

    def summarize_repo(self, verbose_level: int = 1, doc_str_level: int = 1, sign_level: int = 1) -> str:
        """
        Generate a natural language summary of the entire repository workspace.

        :param verbose_level: Level of verbosity for the summary (0-2)
        :param doc_str_level: Level of detail for docstrings (0-2)
        :param sign_level: Level of detail for function signatures (0-2)
        :return: A string containing the workspace summary
        """
        file_summaries = []
        tree_structure = self._generate_tree_structure()

        for root, _, files in os.walk(self.repo_path):
            for file in files:
                if file.endswith(".py"):
                    file_path = Path(root) / file
                    relative_path = file_path.relative_to(self.repo_path)
                    file_summaries.append(self._summarize_file(file_path, verbose_level, doc_str_level, sign_level))

        total_files = len(file_summaries)
        workspace_summary = f"Workspace Summary for {self.repo_path.name}\n"
        workspace_summary += f"{'=' * 40}\n\n"
        workspace_summary += "Workspace Structure:\n"
        workspace_summary += tree_structure
        workspace_summary += (
            f"\nThis workspace contains {total_files} Python file{'s' if total_files != 1 else ''}.\n\n"
        )

        for i, summary in enumerate(file_summaries, 1):
            workspace_summary += f"File {i} of {total_files}:\n{summary}\n"

        workspace_summary += f"\nEnd of Workspace Summary for {self.repo_path.name}"
        return workspace_summary

    def _generate_tree_structure(self) -> str:
        """
        Generate a tree-like structure of the repository.
        """
        tree = []
        for root, dirs, files in os.walk(self.repo_path):
            level = root.replace(str(self.repo_path), "").count(os.sep)
            indent = "│   " * (level - 1) + "├── " if level > 0 else ""
            rel_path = os.path.relpath(root, self.repo_path)
            tree.append(f"{indent}{os.path.basename(root)}/")

            subindent = "│   " * level + "├── "
            for file in files:
                if file.endswith(".py"):
                    tree.append(f"{subindent}{file}")

        return "\n".join(tree)

    def _summarize_file(self, file_path: Path, verbose_level: int, doc_str_level: int, sign_level: int) -> str:
        with open(file_path, "r") as f:
            content = f.read()

        tree = ast.parse(content)
        summary = f"File: {file_path.relative_to(self.repo_path)}\n"
        summary += f"{'-' * 40}\n"

        classes = [node for node in ast.iter_child_nodes(tree) if isinstance(node, ast.ClassDef)]
        functions = [node for node in ast.iter_child_nodes(tree) if isinstance(node, ast.FunctionDef)]

        if classes:
            summary += f"This file contains {len(classes)} class{'es' if len(classes) > 1 else ''}.\n"
        if functions:
            summary += f"This file contains {len(functions)} top-level function{'s' if len(functions) > 1 else ''}.\n"

        for node in classes + functions:
            if isinstance(node, ast.ClassDef):
                summary += self._summarize_class(node, verbose_level, doc_str_level, sign_level)
            elif isinstance(node, ast.FunctionDef):
                summary += self._summarize_function(node, verbose_level, doc_str_level, sign_level)

        return summary

    def _summarize_class(self, node: ast.ClassDef, verbose_level: int, doc_str_level: int, sign_level: int) -> str:
        summary = f"\nClass: {node.name}\n"
        if doc_str_level > 0 and ast.get_docstring(node):
            summary += f"  Description: {ast.get_docstring(node).split('.')[0]}.\n"

        methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]
        if methods:
            summary += f"  This class has {len(methods)} method{'s' if len(methods) > 1 else ''}.\n"

        if verbose_level > 1:
            for method in methods:
                summary += self._summarize_function(method, verbose_level, doc_str_level, sign_level, indent="  ")
        return summary

    def _summarize_function(
        self, node: ast.FunctionDef, verbose_level: int, doc_str_level: int, sign_level: int, indent: str = ""
    ) -> str:
        summary = f"{indent}Function: {node.name}\n"
        if sign_level > 0:
            # Generate the function signature
            args = []
            for arg in node.args.args:
                arg_str = arg.arg
                if arg.annotation:
                    arg_str += f": {ast.unparse(arg.annotation)}"
                args.append(arg_str)

            if node.args.vararg:
                args.append(f"*{node.args.vararg.arg}")
            if node.args.kwarg:
                args.append(f"**{node.args.kwarg.arg}")

            returns = f" -> {ast.unparse(node.returns)}" if node.returns else ""
            signature = f"{node.name}({', '.join(args)}){returns}"
            summary += f"{indent}  Signature: {signature}\n"

        if doc_str_level > 0 and ast.get_docstring(node):
            doc = ast.get_docstring(node)
            summary += f"{indent}  Purpose: {doc.split('.')[0]}.\n"
        return summary

    def highlight(self, file_names: Union[str, List[str]]) -> Dict[str, str]:
        """
        Extract content from specified file(s) within the repo.

        :param file_names: A single file name or a list of file names to highlight
        :return: Dictionary of file names and their content
        """
        if isinstance(file_names, str):
            file_names = [file_names]

        highlighted_content = {}
        for file_name in file_names:
            file_path = self.repo_path / file_name
            if file_path.exists() and file_path.is_file():
                with open(file_path, "r") as f:
                    highlighted_content[file_name] = f.read()
            else:
                highlighted_content[file_name] = f"File not found: {file_name}"

        return highlighted_content


if __name__ == "__main__":
    analyzer = RepoAnalyzer(repo_path="features")
    summary = analyzer.summarize_repo(verbose_level=2, doc_str_level=2, sign_level=2)
    print(summary)
    highlighted_files = analyzer.highlight(
        file_names=["utils/repo/repo_utils.py", "components/benchmark/eval_method.py"]
    )
    print("\nHighlighted Files:")
    for file_name, content in highlighted_files.items():
        print(f"\n{file_name}\n{'=' * len(file_name)}\n{content}")



================================================
File: requirements/docs.txt
================================================
# Requirements for docs.
autodoc-pydantic
coverage
furo
git-changelog
mypy[reports]
myst-parser
pytest
Sphinx
sphinx-autobuild
sphinx-click
sphinx-togglebutton
sphinx_rtd_theme



================================================
File: requirements/lint.txt
================================================
# Requirements for lint.
black
isort
mypy
ruff
toml-sort
types-PyYAML
types-psutil
types-tqdm



================================================
File: requirements/package.txt
================================================
# Requirements for package.
build
setuptools-scm
twine
wheel



================================================
File: requirements/test.txt
================================================
# Requirements for test.
coverage
pytest



================================================
File: test/oai/test_advanced.py
================================================
"""
We have implemented a basic version of litellm.
Not all features in the interface are included.
Therefore, the advanced tests will be placed in a separate file for easier testing of litellm.
"""

import json
import random
import unittest

from rdagent.oai.llm_utils import APIBackend


def _worker(system_prompt, user_prompt):
    api = APIBackend()
    return api.build_messages_and_create_chat_completion(
        system_prompt=system_prompt,
        user_prompt=user_prompt,
    )


class TestAdvanced(unittest.TestCase):

    def test_chat_cache_multiprocess(self) -> None:
        """
        Tests:
        - Multi process, ask same question, enable cache
            - 2 pass
            - cache is not missed & same question get different answer.
        """
        from rdagent.core.utils import LLM_CACHE_SEED_GEN, multiprocessing_wrapper
        from rdagent.oai.llm_conf import LLM_SETTINGS

        system_prompt = "You are a helpful assistant."
        user_prompt = f"Give me {2} random country names, list {2} cities in each country, and introduce them"

        origin_value = (
            LLM_SETTINGS.use_auto_chat_cache_seed_gen,
            LLM_SETTINGS.use_chat_cache,
            LLM_SETTINGS.dump_chat_cache,
        )

        LLM_SETTINGS.use_chat_cache = True
        LLM_SETTINGS.dump_chat_cache = True

        LLM_SETTINGS.use_auto_chat_cache_seed_gen = True

        func_calls = [(_worker, (system_prompt, user_prompt)) for _ in range(4)]

        LLM_CACHE_SEED_GEN.set_seed(10)
        responses1 = multiprocessing_wrapper(func_calls, n=4)
        LLM_CACHE_SEED_GEN.set_seed(20)
        responses2 = multiprocessing_wrapper(func_calls, n=4)
        LLM_CACHE_SEED_GEN.set_seed(10)
        responses3 = multiprocessing_wrapper(func_calls, n=4)

        # Reset, for other tests
        (
            LLM_SETTINGS.use_auto_chat_cache_seed_gen,
            LLM_SETTINGS.use_chat_cache,
            LLM_SETTINGS.dump_chat_cache,
        ) = origin_value
        for i in range(len(func_calls)):
            assert (
                responses1[i] != responses2[i] and responses1[i] == responses3[i]
            ), "Responses sequence should be determined by 'init_chat_cache_seed'"
            for j in range(i + 1, len(func_calls)):
                assert (
                    responses1[i] != responses1[j] and responses2[i] != responses2[j]
                ), "Same question should get different response when use_auto_chat_cache_seed_gen=True"

    def test_chat_multi_round(self) -> None:
        system_prompt = "You are a helpful assistant."
        fruit_name = random.SystemRandom().choice(["apple", "banana", "orange", "grape", "watermelon"])
        user_prompt_1 = (
            f"I will tell you a name of fruit, please remember them and tell me later. "
            f"The name is {fruit_name}. Once you remember it, please answer OK."
        )
        user_prompt_2 = "What is the name of the fruit I told you before?"

        session = APIBackend().build_chat_session(session_system_prompt=system_prompt)

        response_1 = session.build_chat_completion(user_prompt=user_prompt_1)
        assert response_1 is not None
        assert "ok" in response_1.lower()
        response2 = session.build_chat_completion(user_prompt=user_prompt_2)
        assert response2 is not None

    def test_chat_cache(self) -> None:
        """
        Tests:
        - Single process, ask same question, enable cache
            - 2 pass
            - cache is not missed & same question get different answer.
        """
        from rdagent.core.utils import LLM_CACHE_SEED_GEN
        from rdagent.oai.llm_conf import LLM_SETTINGS

        system_prompt = "You are a helpful assistant."
        user_prompt = f"Give me {2} random country names, list {2} cities in each country, and introduce them"

        origin_value = (
            LLM_SETTINGS.use_auto_chat_cache_seed_gen,
            LLM_SETTINGS.use_chat_cache,
            LLM_SETTINGS.dump_chat_cache,
        )

        LLM_SETTINGS.use_chat_cache = True
        LLM_SETTINGS.dump_chat_cache = True

        LLM_SETTINGS.use_auto_chat_cache_seed_gen = True

        LLM_CACHE_SEED_GEN.set_seed(10)
        response1 = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )
        response2 = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        LLM_CACHE_SEED_GEN.set_seed(20)
        response3 = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )
        response4 = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        LLM_CACHE_SEED_GEN.set_seed(10)
        response5 = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )
        response6 = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        # Reset, for other tests
        (
            LLM_SETTINGS.use_auto_chat_cache_seed_gen,
            LLM_SETTINGS.use_chat_cache,
            LLM_SETTINGS.dump_chat_cache,
        ) = origin_value

        assert (
            response1 != response3 and response2 != response4
        ), "Responses sequence should be determined by 'init_chat_cache_seed'"
        assert (
            response1 == response5 and response2 == response6
        ), "Responses sequence should be determined by 'init_chat_cache_seed'"
        assert (
            response1 != response2 and response3 != response4 and response5 != response6
        ), "Same question should get different response when use_auto_chat_cache_seed_gen=True"


if __name__ == "__main__":
    unittest.main()



================================================
File: test/oai/test_completion.py
================================================
import json
import unittest

from rdagent.oai.llm_utils import APIBackend


class TestChatCompletion(unittest.TestCase):
    def test_chat_completion(self) -> None:
        system_prompt = "You are a helpful assistant."
        user_prompt = "What is your name?"
        response = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )
        assert response is not None
        assert isinstance(response, str)

    def test_chat_completion_json_mode(self) -> None:
        system_prompt = "You are a helpful assistant. answer in Json format."
        user_prompt = "What is your name?"
        response = APIBackend().build_messages_and_create_chat_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            json_mode=True,
        )
        assert response is not None
        assert isinstance(response, str)
        json.loads(response)

    def test_build_messages_and_calculate_token(self) -> None:
        system_prompt = "You are a helpful assistant."
        user_prompt = "What is your name?"
        token = APIBackend().build_messages_and_calculate_token(user_prompt=user_prompt, system_prompt=system_prompt)
        assert token is not None
        assert isinstance(token, int)


if __name__ == "__main__":
    unittest.main()



================================================
File: test/oai/test_embedding_and_similarity.py
================================================
import unittest

from rdagent.oai.llm_utils import (
    APIBackend,
    calculate_embedding_distance_between_str_list,
)


class TestEmbedding(unittest.TestCase):
    def test_embedding(self) -> None:
        emb = APIBackend().create_embedding("hello")
        assert emb is not None
        assert isinstance(emb, list)
        assert len(emb) > 0

    def test_embedding_list(self) -> None:
        emb = APIBackend().create_embedding(["hello", "hi"])
        assert emb is not None
        assert isinstance(emb, list)
        assert len(emb) == 2

    def test_embedding_similarity(self) -> None:
        similarity = calculate_embedding_distance_between_str_list(["Hello"], ["Hi"])[0][0]
        assert similarity is not None
        assert isinstance(similarity, float)
        min_similarity_threshold = 0.8
        assert similarity >= min_similarity_threshold


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/README.md
================================================
# 🐳 Run Docker & Qlib
---

## 📄 Description
This guide explains how to run the Qlib Docker test file located at `test/utils/test_env.py` in the RD-Agent repository.

---

## 🚀 Running Instructions

### 1. Install the required Python libraries
- Ensure that the `docker` Python library is installed:
    ```sh
    pip install docker
    ```

### 2. Run the test script
- Execute the test script to verify the Docker environment setup:
    ```sh
    python test/utils/test_env.py
    ```

### Troubleshooting
- **PermissionError: [Errno 13] Permission denied.**
    > This error occurs when the current user does not have the necessary permissions to access the Docker socket. To resolve this issue, follow these steps:

1. **Add the current user to the `docker` group**
Docker requires root or `docker` group user permissions to access the Docker socket. Add the current user to the `docker` group:
    ```sh
    sudo usermod -aG docker $USER
    ```

2. **Refresh group changes**
To apply the group changes, log out and log back in, or use the following command:
    ```sh
    newgrp docker
    ```

3. **Verify Docker access**
Run the following command to ensure that Docker can be accessed:
    ```sh
    docker run hello-world
    ```

4. **Rerun the test script**
    After completing these steps, rerun the test script:
    ```sh
    python test/utils/test_env.py
    ```
---
## 🛠️ Detailed Qlib Docker Function Framework

Here, we provide an overview of the specific functions within the Qlib Docker framework, their purposes, and examples of how to call them.

### QTDockerEnv Class in `env.py`

The `QTDockerEnv` class is responsible for setting up and running Docker environments for Qlib experiments. 

#### Methods:

1. **prepare()**
   - **Purpose**: Prepares the Docker environment for running experiments. This includes building the Docker image if necessary.
   - **Example**:
     ```python
     qtde = QTDockerEnv()
     qtde.prepare()
     ```

2. **run(local_path: str, entry: str) -> str**
   - **Purpose**: Runs a specified entry point (e.g., a configuration file) in the prepared Docker environment.
   - **Parameters**:
     - `local_path`: Path to the local directory to mount into the Docker container.
     - `entry`: Command or entry point to run inside the Docker container.
   - **Returns**: The stdout output from the Docker container.
   - **Example**:
     ```python
     result = qtde.run(local_path="/path/to/env_tpl", entry="qrun conf.yaml")
     ```
---
### 📊 Expected Output

Upon successful execution, the test script will produce analysis results of benchmark returns and various risk metrics. The expected output should be similar to:

```
'The following are analysis results of benchmark return (1 day).'
risk
mean               0.000477
std                0.012295
annualized_return  0.113561
information_ratio  0.598699
max_drawdown      -0.370479

'The following are analysis results of the excess return without cost (1 day).'
risk
mean               0.000530
std                0.005718
annualized_return  0.126029
information_ratio  1.428574
max_drawdown      -0.072310

'The following are analysis results of the excess return with cost (1 day).'
risk
mean               0.000339
std                0.005717
annualized_return  0.080654
information_ratio  0.914486
max_drawdown      -0.086083

'The following are analysis results of indicators (1 day).'
value
ffr    1.0
pa     0.0
pos    0.0
```

By following these steps and using the provided functions, you should be able to run the Qlib Docker tests and obtain the expected analysis results.


================================================
File: test/utils/test_agent_infra.py
================================================
import unittest

from rdagent.oai.llm_utils import APIBackend
from rdagent.utils.agent.ret import PythonAgentOut
from rdagent.utils.agent.tpl import T


class TestAgentInfra(unittest.TestCase):
    def test_agent_infra(self):
        # NOTE: It is not serious. It is just for testing
        sys_prompt = T("components.proposal.prompts:hypothesis_gen.system_prompt").r(
            targets="targets",
            scenario=T("scenarios.qlib.experiment.prompts:qlib_model_background").r(),
            hypothesis_output_format=PythonAgentOut.get_spec(),
            hypothesis_specification=PythonAgentOut.get_spec(),
        )
        user_prompt = T("components.proposal.prompts:hypothesis_gen.user_prompt").r(
            hypothesis_and_feedback="No Feedback",
            RAG="No RAG",
            targets="targets",
        )
        resp = APIBackend().build_messages_and_create_chat_completion(user_prompt=user_prompt, system_prompt=sys_prompt)
        code = PythonAgentOut.extract_output(resp)

        print(code)


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/test_conf.py
================================================
import unittest


class ConfUtils(unittest.TestCase):

    def test_conf(self):
        import os

        from rdagent.utils.env import EnvConf, QlibDockerConf

        os.environ["MEM_LIMIT"] = "200g"
        assert QlibDockerConf().mem_limit == "200g"  # base class will affect subclasses
        os.environ["QLIB_DOCKER_MEM_LIMIT"] = "300g"
        assert QlibDockerConf().mem_limit == "300g"  # more accurate subclass will override the base class

        os.environ["default_entry"] = "which python"
        os.environ["ENABLE_CACHE"] = "False"

        assert EnvConf().enable_cache is False
        assert QlibDockerConf().enable_cache is False

        os.environ["ENABLE_CACHE"] = "True"
        assert EnvConf().enable_cache is True
        assert QlibDockerConf().enable_cache is True


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/test_env.py
================================================
import os
import sys
import unittest
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent.parent))
import shutil

from rdagent.utils.env import (
    CondaConf,
    LocalConf,
    LocalEnv,
    QlibDockerConf,
    QTDockerEnv,
)

DIRNAME = Path(__file__).absolute().resolve().parent


class QlibLocalEnv(LocalEnv):
    def prepare(self) -> None:
        if not (Path("~/.qlib/qlib_data/cn_data").expanduser().resolve().exists()):
            self.run(
                entry="python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn",
            )
        else:
            print("Data already exists. Download skipped.")


class EnvUtils(unittest.TestCase):
    def setUp(self):
        self.test_workspace = DIRNAME / "test_workspace"
        self.test_workspace.mkdir(exist_ok=True)

    def tearDown(self):
        if self.test_workspace.exists():
            shutil.rmtree(self.test_workspace)

    # NOTE: Since I don't know the exact environment in which it will be used, here's just an example.
    # NOTE: Because you need to download the data during the prepare process. So you need to have pyqlib in your environment.
    def test_local(self):
        local_conf = LocalConf(
            bin_path="/home/v-linlanglv/miniconda3/envs/RD-Agent-310/bin",
            default_entry="qrun conf.yaml",
        )
        qle = QlibLocalEnv(conf=local_conf)
        qle.prepare()
        conf_path = str(DIRNAME / "env_tpl" / "conf.yaml")
        qle.run(entry="qrun " + conf_path)
        mlrun_p = DIRNAME / "env_tpl" / "mlruns"
        self.assertTrue(mlrun_p.exists(), f"Expected output file {mlrun_p} not found")

    def test_local_simple(self):
        local_conf = LocalConf(bin_path="/home/xiaoyang/miniconda3/bin/", default_entry="which python")
        le = LocalEnv(conf=local_conf)
        print(local_conf)
        le.prepare()
        code_path = DIRNAME / "tmp_code"
        code_path.mkdir(exist_ok=True)
        res, code = le.run_ret_code(local_path=str(code_path))
        print(res, code)

    def test_conda_simple(self):
        conda_conf = CondaConf(default_entry="which python", conda_env_name="MLE")
        le = LocalEnv(conf=conda_conf)
        le.prepare()
        code_path = DIRNAME / "tmp_code"
        code_path.mkdir(exist_ok=True)
        res, code = le.run_ret_code(local_path=str(code_path))
        print(res, code)

    def test_docker(self):
        """We will mount `env_tpl` into the docker image.
        And run the docker image with `qrun conf.yaml`
        """
        qtde = QTDockerEnv()
        qtde.prepare()  # you can prepare for multiple times. It is expected to handle it correctly
        # qtde.run("nvidia-smi")  # NOTE: you can check your GPU with this command
        # the stdout are returned as result
        result = qtde.run(local_path=str(DIRNAME / "env_tpl"), entry="qrun conf.yaml")

        mlrun_p = DIRNAME / "env_tpl" / "mlruns"
        self.assertTrue(mlrun_p.exists(), f"Expected output file {mlrun_p} not found")

        # read experiment
        result = qtde.run(local_path=str(DIRNAME / "env_tpl"), entry="python read_exp_res.py")
        print(result)

    def test_run_ret_code(self):
        """Test the run_ret_code method of QTDockerEnv with both valid and invalid commands."""
        qtde = QTDockerEnv()
        qtde.prepare()

        # Test with a valid command
        result, return_code = qtde.run_ret_code(entry='echo "Hello, World!"', local_path=str(self.test_workspace))
        print(return_code)
        assert return_code == 0, f"Expected return code 0, but got {return_code}"
        assert "Hello, World!" in result, "Expected output not found in result"

        # Test with an invalid command
        _, return_code = qtde.run_ret_code(entry="invalid_command", local_path=str(self.test_workspace))
        print(return_code)
        assert return_code != 0, "Expected non-zero return code for invalid command"

        dc = QlibDockerConf()
        dc.running_timeout_period = 1
        qtde = QTDockerEnv(dc)
        result, return_code = qtde.run_ret_code(entry="sleep 2", local_path=str(self.test_workspace))
        print(result)
        assert return_code == 124, "Expected return code 124 for timeout"

    def test_docker_mem(self):
        cmd = 'python -c \'print("start"); import numpy as np;  size_mb = 500; size = size_mb * 1024 * 1024 // 8; array = np.random.randn(size).astype(np.float64); print("success")\''

        qtde = QTDockerEnv(QlibDockerConf(mem_limit="10m"))
        qtde.prepare()
        result = qtde.run(local_path=str(DIRNAME / "env_tpl"), entry=cmd)
        self.assertTrue(not result.strip().endswith("success"))

        qtde = QTDockerEnv(QlibDockerConf(mem_limit="1g"))
        qtde.prepare()
        result = qtde.run(local_path=str(DIRNAME / "env_tpl"), entry=cmd)
        self.assertTrue(result.strip().endswith("success"))

        # The above command equals to the follow commands with dockr cli.sh
        # docker run  --memory=10m  -it --rm local_qlib:latest python -c 'import numpy as np; print(123);  size_mb = 1; size = size_mb * 1024 * 1024 // 8; array = np.random.randn(size).astype(np.float64); array[0], array[-1] = 1.0, 1.0; print(321)'
        # docker run  --memory=10g  -it --rm local_qlib:latest python -c 'import numpy as np; print(123);  size_mb = 1; size = size_mb * 1024 * 1024 // 8; array = np.random.randn(size).astype(np.float64); array[0], array[-1] = 1.0, 1.0; print(321)'


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/test_import.py
================================================
import importlib
import os
import unittest
from pathlib import Path

import pytest


@pytest.mark.offline
class TestRDAgentImports(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.rdagent_directory = Path(__file__).resolve().parent.parent.parent
        cls.modules = list(cls.import_all_modules_from_directory(cls.rdagent_directory))

    @staticmethod
    def import_all_modules_from_directory(directory):
        for file in directory.joinpath("rdagent").rglob("*.py"):
            fstr = str(file)
            if "meta_tpl" in fstr:
                continue
            if "template" in fstr or "tpl" in fstr:
                continue
            if "model_coder" in fstr:
                continue
            if "llm_st" in fstr:
                continue
            if (
                fstr.endswith("rdagent/log/ui/app.py")
                or fstr.endswith("rdagent/app/cli.py")
                or fstr.endswith("rdagent/app/CI/run.py")
                or fstr.endswith("rdagent/app/utils/ape.py")
            ):
                # the entrance points
                continue

            yield fstr[fstr.index("rdagent") : -3].replace("/", ".")

    def test_import_modules(self):
        print(self.modules)
        for module_name in self.modules:
            with self.subTest(module=module_name):
                try:
                    print(module_name)
                    importlib.import_module(module_name)
                except Exception as e:
                    self.fail(f"Failed to import {module_name}: {e}")


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/test_kaggle.py
================================================
import unittest
from pathlib import Path

import nbformat
from rich import print

from rdagent.app.kaggle.conf import KAGGLE_IMPLEMENT_SETTING
from rdagent.oai.llm_utils import APIBackend
from rdagent.scenarios.kaggle.experiment.workspace import KGFBWorkspace
from rdagent.scenarios.kaggle.kaggle_crawler import download_data
from rdagent.utils.agent.ret import PythonAgentOut
from rdagent.utils.agent.tpl import T


class TestTpl(unittest.TestCase):
    def test_competition_template(self):
        """
        export KG_COMPETITION=<competition_name> before running this test
        """
        competition = KAGGLE_IMPLEMENT_SETTING.competition
        print(f"[bold orange]{competition}[/bold orange]")
        download_data(competition)
        ws = KGFBWorkspace(
            template_folder_path=Path(__file__).parent.parent.parent
            / KAGGLE_IMPLEMENT_SETTING.template_path
            / f"{competition}",
        )
        print(ws.workspace_path)
        ws.execute()
        success = (ws.workspace_path / "submission.csv").exists()
        self.assertTrue(success, "submission.csv is not generated")
        # ws.clear()


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/test_misc.py
================================================
import unittest

import pytest

from rdagent.core.utils import SingletonBaseClass


class A(SingletonBaseClass):
    def __init__(self, **kwargs):
        print(self, "__init__", kwargs)  # make sure the __init__ is called only once.
        self.kwargs = kwargs

    def __str__(self) -> str:
        return f"{self.__class__.__name__}.{getattr(self, 'kwargs', None)}"

    def __repr__(self) -> str:
        return self.__str__()


@pytest.mark.offline
class MiscTest(unittest.TestCase):
    def test_singleton(self):
        print("a1=================")
        a1 = A()
        print("a2=================")
        a2 = A()
        print("a3=================")
        a3 = A(x=3)
        print("a4=================")
        a4 = A(x=2)
        print("a5=================")
        a5 = A(b=3)
        print("a6=================")
        a6 = A(x=3)

        # Check that a1 and a2 are the same instance
        self.assertIs(a1, a2)

        # Check that a3 and a6 are the same instance
        self.assertIs(a3, a6)

        # Check that a1 and a3 are different instances
        self.assertIsNot(a1, a3)

        # Check that a3 and a4 are different instances
        self.assertIsNot(a3, a4)

        # Check that a4 and a5 are different instances
        self.assertIsNot(a4, a5)

        # Check that a5 and a6 are different instances
        self.assertIsNot(a5, a6)

        print(id(a1), id(a2), id(a3), id(a4), id(a5), id(a6))

        print("...................... Start testing pickle ......................")

        # Test pickle
        import pickle

        with self.assertRaises(pickle.PicklingError):
            with open("a3.pkl", "wb") as f:
                pickle.dump(a3, f)
        # NOTE: If the pickle feature is not disabled,
        # loading a3.pkl will return a1, and a1 will be updated with a3's attributes.
        # print(a1.kwargs)
        # with open("a3.pkl", "rb") as f:
        #     a3_pkl = pickle.load(f)
        # print(id(a3), id(a3_pkl))  # not the same object
        # print(a1.kwargs)  # a1 will be changed.


if __name__ == "__main__":
    unittest.main()



================================================
File: test/utils/coder/test_CoSTEER.py
================================================
import unittest


class CoSTEERTest(unittest.TestCase):

    def setUp(self):
        self.test_competition = "aerial-cactus-identification"

    def tearDown(self):
        pass

    def to_str(self, obj):
        return "".join(str(obj).split())

    def test_data_loader(self):
        from rdagent.components.coder.data_science.raw_data_loader.test import (
            develop_one_competition,
        )

        # if all tasks in exp are failed, will raise CoderError
        exp = develop_one_competition(self.test_competition)

    def test_feature(self):
        from rdagent.components.coder.data_science.feature.test import (
            develop_one_competition,
        )

        exp = develop_one_competition(self.test_competition)

    def test_model(self):
        from rdagent.components.coder.data_science.model.test import (
            develop_one_competition,
        )

        exp = develop_one_competition(self.test_competition)

    def test_ensemble(self):
        from rdagent.components.coder.data_science.ensemble.test import (
            develop_one_competition,
        )

        exp = develop_one_competition(self.test_competition)

    def test_workflow(self):
        from rdagent.components.coder.data_science.workflow.test import (
            develop_one_competition,
        )

        exp = develop_one_competition(self.test_competition)


if __name__ == "__main__":
    unittest.main()
    # pytest test/utils/coder/test_CoSTEER.py



================================================
File: test/utils/env_tpl/README.md
================================================

# Introduction

It is a template for testing.



================================================
File: test/utils/env_tpl/conf.yaml
================================================
qlib_init:
    provider_uri: "~/.qlib/qlib_data/cn_data"
    region: cn
market: &market csi300
benchmark: &benchmark SH000300
data_handler_config: &data_handler_config
    start_time: 2008-01-01
    end_time: 2020-08-01
    fit_start_time: 2008-01-01
    fit_end_time: 2014-12-31
    instruments: *market
port_analysis_config: &port_analysis_config
    strategy:
        class: TopkDropoutStrategy
        module_path: qlib.contrib.strategy
        kwargs:
            signal: <PRED>
            topk: 50
            n_drop: 5
    backtest:
        start_time: 2017-01-01
        end_time: 2020-08-01
        account: 100000000
        benchmark: *benchmark
        exchange_kwargs:
            limit_threshold: 0.095
            deal_price: close
            open_cost: 0.0005
            close_cost: 0.0015
            min_cost: 5
task:
    model:
        class: LGBModel
        module_path: qlib.contrib.model.gbdt
        kwargs:
            loss: mse
            colsample_bytree: 0.8879
            learning_rate: 0.2
            subsample: 0.8789
            lambda_l1: 205.6999
            lambda_l2: 580.9768
            max_depth: 8
            num_leaves: 210
            num_threads: 20
    dataset:
        class: DatasetH
        module_path: qlib.data.dataset
        kwargs:
            handler:
                class: Alpha158
                module_path: qlib.contrib.data.handler
                kwargs: *data_handler_config
            segments:
                train: [2008-01-01, 2014-12-31]
                valid: [2015-01-01, 2016-12-31]
                test: [2017-01-01, 2020-08-01]
    record: 
        - class: SignalRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            model: <MODEL>
            dataset: <DATASET>
        - class: SigAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            ana_long_short: False
            ann_scaler: 252
        - class: PortAnaRecord
          module_path: qlib.workflow.record_temp
          kwargs: 
            config: *port_analysis_config



================================================
File: test/utils/env_tpl/read_exp.py
================================================
import qlib
from mlflow.entities import ViewType
from mlflow.tracking import MlflowClient

qlib.init()

from qlib.workflow import R

# here is the documents of the https://qlib.readthedocs.io/en/latest/component/recorder.html

# TODO: list all the recorder and metrics

# Assuming you have already listed the experiments
experiments = R.list_experiments()

# Iterate through each experiment to list its recorders and metrics
experiment_name = None
for experiment in experiments:
    print(f"Experiment: {experiment}")
    recorders = R.list_recorders(experiment_name=experiment)
    # print(recorders)
    for recorder_id in recorders:
        if recorder_id is not None:
            experiment_name = experiment
        print(f"Recorder ID: {recorder_id}")
        recorder = R.get_recorder(recorder_id=recorder_id, experiment_name=experiment)
        metrics = recorder.list_metrics()
        print(f"Metrics: {metrics}")

# TODO: get the latest recorder

recorder_list = R.list_recorders(experiment_name="workflow")
end_times = {key: value.info["end_time"] for key, value in recorder_list.items()}
sorted_end_times = dict(sorted(end_times.items(), key=lambda item: item[1], reverse=True))

latest_recorder_id = next(iter(sorted_end_times))
print(f"Latest recorder ID: {latest_recorder_id}")

latest_recorder = R.get_recorder(experiment_name=experiment_name, recorder_id=latest_recorder_id)
print(f"Latest recorder: {latest_recorder}")

pred_df = latest_recorder.load_object("pred.pkl")
print("pred_df", pred_df)

ic_df = latest_recorder.load_object("sig_analysis/ic.pkl")
print("ic_df: ", ic_df)

ric_df = latest_recorder.load_object("sig_analysis/ric.pkl")
print("ric_df: ", ric_df)

print("list_metrics: ", latest_recorder.list_metrics())
print("IC: ", latest_recorder.list_metrics()["IC"])
print("ICIR: ", latest_recorder.list_metrics()["ICIR"])
print("Rank IC: ", latest_recorder.list_metrics()["Rank IC"])
print("Rank ICIR: ", latest_recorder.list_metrics()["Rank ICIR"])



================================================
File: .github/FUNDING.yml
================================================
github:
  - MIIC-finance



================================================
File: .github/PULL_REQUEST_TEMPLATE.md
================================================
<!--- Thank you for submitting a Pull Request! In order to make our work smoother. -->
<!--- please make sure your Pull Request meets the following requirements: -->
<!---   1. Provide a general summary of your changes in the Title above; -->
<!---   2. Add appropriate prefixes to titles, such as `build:`, `chore:`, `ci:`, `docs:`, `feat:`, `fix:`, `perf:`, `refactor:`, `revert:`, `style:`, `test:`(Ref: https://www.conventionalcommits.org/). -->
<!--- Category: -->
<!--- Patch Updates: `fix:` -->
<!---   Example: fix(auth): correct login validation issue -->
<!--- minor update (introduces new functionality): `feat` -->
<!---   Example: feature(parser): add ability to parse arrays -->
<!--- major update(destructive update): Include BREAKING CHANGE in the commit message footer, or add `! ` in the commit footer to indicate that there is a destructive update. -->
<!---   Example: feat(auth)! : remove support for old authentication method -->
<!--- Other updates: `build:`, `chore:`, `ci:`, `docs:`, `perf:`, `refactor:`, `revert:`, `style:`, `test:`. -->

## Description
<!--- Describe your changes in detail -->

## Motivation and Context
<!--- Are there any related issues? If so, please put the link here. -->
<!--- Why is this change required? What problem does it solve? -->

## How Has This Been Tested?
<!---  Put an `x` in all the boxes that apply: --->
- [ ] If you are adding a new feature, test on your own test scripts.

<!--- **ATTENTION**: If you are adding a new feature, please make sure your codes are **correctly tested**. If our test scripts do not cover your cases, please provide your own test scripts under the `tests` folder and test them. More information about test scripts can be found [here](https://docs.python.org/3/library/unittest.html#basic-example), or you could refer to those we provide under the `tests` folder. -->

## Screenshots of Test Results (if appropriate):
1. Your own tests:

## Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [ ] Fix bugs
- [ ] Add new feature
- [ ] Update documentation



================================================
File: .github/dependabot.yml
================================================
updates:
  - commit-message:
      prefix: build(actions)
    directory: /
    package-ecosystem: github-actions
    schedule:
      interval: weekly
  - commit-message:
      prefix: build(requirements)
    directory: /
    groups:
      dev:
        dependency-type: development
      prod:
        dependency-type: production
    package-ecosystem: pip
    schedule:
      interval: weekly
version: 2



================================================
File: .github/ISSUE_TEMPLATE/bug-report.md
================================================
---
name: "\U0001F41B Bug Report"
about: Submit a bug report to help us improve RD-Agent
labels: bug

---

## 🐛 Bug Description

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:

1.
2.
3.


## Expected Behavior

<!-- A clear and concise description of what you expected to happen. -->

## Screenshot

<!-- A screenshot of the error message or anything shouldn't appear-->

## Environment

**Note**: Users can run `rdagent collect_info` to get system information and paste it directly here.

 - Name of current operating system:
 - Processor architecture:
 - System, version, and hardware information:
 - Version number of the system:
 - Python version:
 - Container ID:
 - Container Name:
 - Container Status:
 - Image ID used by the container:
 - Image tag used by the container:
 - Container port mapping:
 - Container Label:
 - Startup Commands:
 - RD-Agent version:
 - Package version:

## Additional Notes

<!-- Add any other information about the problem here. -->



================================================
File: .github/ISSUE_TEMPLATE/documentation.md
================================================
---
name: "\U0001F4D6 Documentation"
about: Report an issue related to documentation

---

## 📖 Documentation

<!-- Please specify whether it's tutorial part or API reference part, and describe it.-->



================================================
File: .github/ISSUE_TEMPLATE/feature-request.md
================================================
---
name: "\U0001F31FFeature Request"
about: Request for a new RD-Agent feature
labels: enhancement

---

## 🌟 Feature Description
<!-- A clear and concise description of the feature proposal -->

## Motivation

1. Application scenario
2. Related works (Papers, Github repos etc.):
3. Any other relevant and important information:

<!-- Please describe why the feature is important. -->

## Alternatives

<!-- A short description of any alternative solutions or features you've considered. -->

## Additional Notes

<!-- Add any other context or screenshots about the feature request here. -->



================================================
File: .github/ISSUE_TEMPLATE/question.md
================================================
---
name: "❓Questions & Help"
about: Have some questions? We can offer help.
labels: question

---

## ❓ Questions and Help

We sincerely suggest you to carefully read the [documentation](http://rdagent.readthedocs.io/). After that, if you still feel puzzled, please describe the question clearly under this issue.



================================================
File: .github/workflows/ci.yml
================================================
concurrency:
  cancel-in-progress: true
  group: ${{ github.workflow }}-${{ github.ref }}
jobs:
  ci:
    if: ${{ !cancelled() && ! failure() }}
    needs: dependabot
    runs-on: ubuntu-latest
    steps:
      - name: checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: recursive
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          cache: pip
          python-version: ${{ matrix.python-version }}
      - run: env | sort
      - run: make dev
      - name: lint test docs and build
        run: make lint docs-gen test-offline # test docs build
    strategy:
      matrix:
        python-version:
          - '3.10'
          - '3.11'
  dependabot:
    if: ${{ github.actor == 'dependabot[bot]' && startsWith(github.head_ref, 'dependabot/pip/') }}
    permissions:
      contents: write
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.head_ref }}
      - name: Set up Git
        run: |
          git config --global user.name github-actions
          git config --global user.email github-actions@github.com
      - name: Set up Python with multiple versions.
        uses: actions/setup-python@v5
        with:
          cache: pip
          python-version: |
            3.10
            3.11
      - name: Install pipenv using pipx
        run: pipx install pipenv
      - name: Generate constraints for all supported Python versions
        run: |
          CI= PYTHON_VERSION=3.10 make constraints
          CI= PYTHON_VERSION=3.11 make constraints
      - name: Push changes if applicable
        run: |
          if [[ -n `git status --porcelain` ]]; then
            git commit -a -m "build: Update constraints for dependabot."
            git push
          fi
name: CI
on:
  pull_request:
    types:
      - opened
      - synchronize
  push:
    branches:
      - main



================================================
File: .github/workflows/pr.yml
================================================
name: Lint pull request title

on:
  pull_request:
    types:
      - opened
      - synchronize
      - reopened
      - edited

concurrency:
  cancel-in-progress: true
  group: ${{ github.workflow }}-${{ github.ref }}

jobs:
  lint-title:
    runs-on: ubuntu-latest
    steps:
      # This step is necessary because the lint title uses the .commitlintrc.js file in the project root directory.
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '16'

      - name: Install commitlint
        run: npm install --save-dev @commitlint/{config-conventional,cli}

      - name: Validate PR Title with commitlint
        env:
          BODY: ${{ github.event.pull_request.title }}
        run: |
          echo "$BODY" | npx commitlint --config .commitlintrc.js



================================================
File: .github/workflows/readthedocs-preview.yml
================================================
concurrency:
  cancel-in-progress: true
  group: ${{ github.workflow }}-${{ github.ref }}
jobs:
  documentation-links:
    runs-on: ubuntu-latest
    steps:
      - uses: readthedocs/actions/preview@v1
        with:
          project-slug: RDAgent
name: Read the Docs Pull Request Preview
on:
  pull_request_target:
    types:
      - opened
permissions:
  pull-requests: write



================================================
File: .github/workflows/release.yml
================================================
name: Release
on:
  push:
    branches:
      - main
permissions:
  contents: read
jobs:
  release_and_publish:
    permissions:
      contents: write
      pull-requests: read
    runs-on: ubuntu-latest
    steps:
      - name: Release please
        id: release_please
        uses: googleapis/release-please-action@v4
        with:
          # The current PAT (personal access token) was created on 2024-08-05,
          # since the maximum validity of PAT is 1 year, you need to change the PAT before 2025-08-05.
          token: ${{ secrets.PAT }}
          release-type: simple
      - uses: actions/checkout@v4
        if: ${{ steps.release_please.outputs.release_created }}
        with:
          fetch-depth: 0
      - name: Set up Python
        if: ${{ steps.release_please.outputs.release_created }}
        uses: actions/setup-python@v5
        with:
          cache: pip
          python-version: '3.10'
      - name: Install dependencies
        if: ${{ steps.release_please.outputs.release_created }}
        run: |
          python -m pip install --upgrade pip
          pip install setuptools wheel twine  #  better-exceptions(optional for debug)
      - run: env | sort
        if: ${{ steps.release_please.outputs.release_created }}
      - run: make dev
        if: ${{ steps.release_please.outputs.release_created }}
      - run: make build
        if: ${{ steps.release_please.outputs.release_created }}
      - name: upload
        if: ${{ steps.release_please.outputs.release_created }}
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_TOKEN }}
        run: |
          make upload



================================================
File: .streamlit/config.toml
================================================
[client]
showSidebarNavigation = false

